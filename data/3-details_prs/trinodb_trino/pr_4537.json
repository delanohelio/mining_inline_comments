{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU1MTU5MDIy", "number": 4537, "title": "Iceberg connector documentation", "bodyText": "", "createdAt": "2020-07-22T14:34:51Z", "url": "https://github.com/trinodb/trino/pull/4537", "merged": true, "mergeCommit": {"oid": "8b6055f45305101197a89776c1b449d06579b8e8"}, "closed": true, "closedAt": "2020-09-08T23:43:14Z", "author": {"login": "djsstarburst"}, "timelineItems": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc4draDABqjM1ODY1MDY2MTE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdHAZfigFqTQ4NDU2NDUzNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgyMjc4NTE1", "url": "https://github.com/trinodb/trino/pull/4537#pullrequestreview-482278515", "createdAt": "2020-09-04T00:09:09Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 30, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQwMDowOTowOVrOHM7Syg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQwMDo0NDoyM1rOHM71hQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMxNjQyNg==", "bodyText": "I think we should remove this list, as we don't have it for other connectors. Many of these are basics that are expected, and can be or are documented implicitly with examples.", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483316426", "createdAt": "2020-09-04T00:09:09Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMxNjUzOA==", "bodyText": "This isn't accurate as there are many Presto types not supported by Iceberg (for example, Geospatial data types)", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483316538", "createdAt": "2020-09-04T00:09:32Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMxNjk3Ng==", "bodyText": "Replace first two sentences:\n\nIceberg is designed to improve on the known scalability limitations of Hive, which stores table metadata in a metastore that is backed by a relational database such as MySQL.", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483316976", "createdAt": "2020-09-04T00:11:12Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMxNjk4Ng==", "bodyText": "Change \"Hive connector\" to a link\n:doc:`/connector/hive`", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483316986", "createdAt": "2020-09-04T00:11:14Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMxNzI1Mw==", "bodyText": "Let's remove this now, as it should be usable, as shown by our tests", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483317253", "createdAt": "2020-09-04T00:12:21Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMxNzQ2Ng==", "bodyText": "Let's remove this section, as this is expected, given it follows the Iceberg specification. We don't need to link end users to specific tests.", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483317466", "createdAt": "2020-09-04T00:13:15Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMDM4NA==", "bodyText": "Let's change these to \"January 1, 1970\" to avoid having a period in the middle.\nFor all of these, it would be useful to have a friendly explanation first:\n\nA partition is created for each year.", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483320384", "createdAt": "2020-09-04T00:24:31Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMDc2Nw==", "bodyText": "No need to uppercase timestamp here, since it's used as word, not a SQL keyword.\nMaybe instead of \"rounded down\" we could say\n\nThe partition value is a timestamp with the minutes and seconds set to zero", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483320767", "createdAt": "2020-09-04T00:25:59Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMDg0MQ==", "bodyText": "Make the separators as long as the longest text", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483320841", "createdAt": "2020-09-04T00:26:18Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMTM2NQ==", "bodyText": "The data is hashed into the specified number of buckets. The partition value ...", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483321365", "createdAt": "2020-09-04T00:28:28Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMTQ5Nw==", "bodyText": "Remove VARCHAR here (implied by \"characters\")", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483321497", "createdAt": "2020-09-04T00:29:04Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMTYzMw==", "bodyText": "In this example, the table is partitioned by month and further divided into 10 buckets based on a hash of the account number", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483321633", "createdAt": "2020-09-04T00:29:34Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMTg0NA==", "bodyText": "Add code quotes around WHERE", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483321844", "createdAt": "2020-09-04T00:30:31Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMTkxOQ==", "bodyText": "Code quotes around \"c\" and \"1\"", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483321919", "createdAt": "2020-09-04T00:30:47Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMjEwNg==", "bodyText": "It's better to use a more realistic example, similar to the one above.", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483322106", "createdAt": "2020-09-04T00:31:30Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMjI1Nw==", "bodyText": "Currently, the Iceberg connector only supports deletion by partition.", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483322257", "createdAt": "2020-09-04T00:32:04Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet\n+released.  When v2 is released, it will support row-level deletion and the\n+Iceberg connector will be updated to provide row-level deletion.\n+\n+The Iceberg connector currently supports only deletion by partition.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMjQ1Mg==", "bodyText": "Let's remove this paragraph, as we avoid making forward-looking statements like this in documentation. It's also a bit confusing, as the user might think the connector will automatically support it when v2 is available.", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483322452", "createdAt": "2020-09-04T00:32:55Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMjYwMA==", "bodyText": "Missing a word or something here: \"because the doesn't span\"", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483322600", "createdAt": "2020-09-04T00:33:29Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet\n+released.  When v2 is released, it will support row-level deletion and the\n+Iceberg connector will be updated to provide row-level deletion.\n+\n+The Iceberg connector currently supports only deletion by partition.\n+This SQL below will fail because the doesn't span entire partitions::", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMjczNw==", "bodyText": "Let's change this to text\n\nwhere table snapshots are identified by snapshot IDs\n\nI don't think saying \"long\" or \"numeric\" is needed here", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483322737", "createdAt": "2020-09-04T00:34:07Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet\n+released.  When v2 is released, it will support row-level deletion and the\n+Iceberg connector will be updated to provide row-level deletion.\n+\n+The Iceberg connector currently supports only deletion by partition.\n+This SQL below will fail because the doesn't span entire partitions::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1 AND d > 10\n+\n+Rolling Back to a Previous Snapshot\n+-----------------------------------\n+\n+Iceberg supports a \"snapshot\" model of data, where table snapshots are\n+identified by an ``long`` snapshot_ids.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMjgzMw==", "bodyText": "Capitalize ID", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483322833", "createdAt": "2020-09-04T00:34:38Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet\n+released.  When v2 is released, it will support row-level deletion and the\n+Iceberg connector will be updated to provide row-level deletion.\n+\n+The Iceberg connector currently supports only deletion by partition.\n+This SQL below will fail because the doesn't span entire partitions::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1 AND d > 10\n+\n+Rolling Back to a Previous Snapshot\n+-----------------------------------\n+\n+Iceberg supports a \"snapshot\" model of data, where table snapshots are\n+identified by an ``long`` snapshot_ids.\n+\n+The connector provides a system snapshots table for each Iceberg table.  Snapshots are\n+identified by BIGINT snapshot ids.  You can find the latest snapshot id for table", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMjk4MA==", "bodyText": "Code quotes around \"foo\". But please use a more realistic name.", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483322980", "createdAt": "2020-09-04T00:35:08Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet\n+released.  When v2 is released, it will support row-level deletion and the\n+Iceberg connector will be updated to provide row-level deletion.\n+\n+The Iceberg connector currently supports only deletion by partition.\n+This SQL below will fail because the doesn't span entire partitions::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1 AND d > 10\n+\n+Rolling Back to a Previous Snapshot\n+-----------------------------------\n+\n+Iceberg supports a \"snapshot\" model of data, where table snapshots are\n+identified by an ``long`` snapshot_ids.\n+\n+The connector provides a system snapshots table for each Iceberg table.  Snapshots are\n+identified by BIGINT snapshot ids.  You can find the latest snapshot id for table\n+foo by running the following command::", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyMzA3NA==", "bodyText": "Identifiers use double quotes\nFROM \"foo$snapshots\"", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483323074", "createdAt": "2020-09-04T00:35:26Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet\n+released.  When v2 is released, it will support row-level deletion and the\n+Iceberg connector will be updated to provide row-level deletion.\n+\n+The Iceberg connector currently supports only deletion by partition.\n+This SQL below will fail because the doesn't span entire partitions::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1 AND d > 10\n+\n+Rolling Back to a Previous Snapshot\n+-----------------------------------\n+\n+Iceberg supports a \"snapshot\" model of data, where table snapshots are\n+identified by an ``long`` snapshot_ids.\n+\n+The connector provides a system snapshots table for each Iceberg table.  Snapshots are\n+identified by BIGINT snapshot ids.  You can find the latest snapshot id for table\n+foo by running the following command::\n+\n+    SELECT snapshot_id FROM 'foo$snapshots' ORDER BY committed_at DESC LIMIT 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyNDI3MQ==", "bodyText": "It seems strange to mention Hive syntax here for the Iceberg connector. Instead, we should simply describe this feature on its own terms.", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483324271", "createdAt": "2020-09-04T00:40:12Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet\n+released.  When v2 is released, it will support row-level deletion and the\n+Iceberg connector will be updated to provide row-level deletion.\n+\n+The Iceberg connector currently supports only deletion by partition.\n+This SQL below will fail because the doesn't span entire partitions::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1 AND d > 10\n+\n+Rolling Back to a Previous Snapshot\n+-----------------------------------\n+\n+Iceberg supports a \"snapshot\" model of data, where table snapshots are\n+identified by an ``long`` snapshot_ids.\n+\n+The connector provides a system snapshots table for each Iceberg table.  Snapshots are\n+identified by BIGINT snapshot ids.  You can find the latest snapshot id for table\n+foo by running the following command::\n+\n+    SELECT snapshot_id FROM 'foo$snapshots' ORDER BY committed_at DESC LIMIT 1\n+\n+A SQL procedure ``system.rollback_to_snapshot`` allows the caller to roll back\n+the state of the table to a previous snapshot id::\n+\n+    CALL system.rollback_to_snapshot(schema_name, table_name, snapshot_id)\n+\n+Schema Evolution\n+----------------\n+\n+Iceberg and the Iceberg connector support schema evolution, with safe\n+column add, drop, reorder and rename operations, including in nested structures.\n+Table partitioning can also be changed and the connector can still\n+query data created before the partitioning change.\n+\n+Migrating Existing Tables\n+-------------------------\n+\n+The connector can read from or write to Hive tables that have been migrated to Iceberg.\n+Currently, there is no Presto support to migrate Hive tables to Presto, so you will\n+need to use either the Iceberg API or Spark.\n+\n+System Tables and Columns\n+-------------------------\n+\n+The connector supports ``table$partitions`` as a substitute for Hive's ``SHOW PARTITIONS``.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyNDMxNA==", "bodyText": "Does this work? We don't have any code or tests for this.", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483324314", "createdAt": "2020-09-04T00:40:21Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet\n+released.  When v2 is released, it will support row-level deletion and the\n+Iceberg connector will be updated to provide row-level deletion.\n+\n+The Iceberg connector currently supports only deletion by partition.\n+This SQL below will fail because the doesn't span entire partitions::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1 AND d > 10\n+\n+Rolling Back to a Previous Snapshot\n+-----------------------------------\n+\n+Iceberg supports a \"snapshot\" model of data, where table snapshots are\n+identified by an ``long`` snapshot_ids.\n+\n+The connector provides a system snapshots table for each Iceberg table.  Snapshots are\n+identified by BIGINT snapshot ids.  You can find the latest snapshot id for table\n+foo by running the following command::\n+\n+    SELECT snapshot_id FROM 'foo$snapshots' ORDER BY committed_at DESC LIMIT 1\n+\n+A SQL procedure ``system.rollback_to_snapshot`` allows the caller to roll back\n+the state of the table to a previous snapshot id::\n+\n+    CALL system.rollback_to_snapshot(schema_name, table_name, snapshot_id)\n+\n+Schema Evolution\n+----------------\n+\n+Iceberg and the Iceberg connector support schema evolution, with safe\n+column add, drop, reorder and rename operations, including in nested structures.\n+Table partitioning can also be changed and the connector can still\n+query data created before the partitioning change.\n+\n+Migrating Existing Tables\n+-------------------------\n+\n+The connector can read from or write to Hive tables that have been migrated to Iceberg.\n+Currently, there is no Presto support to migrate Hive tables to Presto, so you will\n+need to use either the Iceberg API or Spark.\n+\n+System Tables and Columns\n+-------------------------\n+\n+The connector supports ``table$partitions`` as a substitute for Hive's ``SHOW PARTITIONS``.\n+The differences are that it returns some partition metrics for each partition value\n+and you can also use it on unpartitioned tables.\n+\n+Iceberg supports ``$snapshot_id`` and ``$snapshot_timestamp_ms`` as hidden columns.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyNDgwMQ==", "bodyText": "The connector should support table names like abc@123 or abc$partitions@456 but I can't find any tests for this. It might just work -- we should add tests.", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483324801", "createdAt": "2020-09-04T00:42:24Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet\n+released.  When v2 is released, it will support row-level deletion and the\n+Iceberg connector will be updated to provide row-level deletion.\n+\n+The Iceberg connector currently supports only deletion by partition.\n+This SQL below will fail because the doesn't span entire partitions::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1 AND d > 10\n+\n+Rolling Back to a Previous Snapshot\n+-----------------------------------\n+\n+Iceberg supports a \"snapshot\" model of data, where table snapshots are\n+identified by an ``long`` snapshot_ids.\n+\n+The connector provides a system snapshots table for each Iceberg table.  Snapshots are\n+identified by BIGINT snapshot ids.  You can find the latest snapshot id for table\n+foo by running the following command::\n+\n+    SELECT snapshot_id FROM 'foo$snapshots' ORDER BY committed_at DESC LIMIT 1\n+\n+A SQL procedure ``system.rollback_to_snapshot`` allows the caller to roll back\n+the state of the table to a previous snapshot id::\n+\n+    CALL system.rollback_to_snapshot(schema_name, table_name, snapshot_id)\n+\n+Schema Evolution\n+----------------\n+\n+Iceberg and the Iceberg connector support schema evolution, with safe\n+column add, drop, reorder and rename operations, including in nested structures.\n+Table partitioning can also be changed and the connector can still\n+query data created before the partitioning change.\n+\n+Migrating Existing Tables\n+-------------------------\n+\n+The connector can read from or write to Hive tables that have been migrated to Iceberg.\n+Currently, there is no Presto support to migrate Hive tables to Presto, so you will\n+need to use either the Iceberg API or Spark.\n+\n+System Tables and Columns\n+-------------------------\n+\n+The connector supports ``table$partitions`` as a substitute for Hive's ``SHOW PARTITIONS``.\n+The differences are that it returns some partition metrics for each partition value\n+and you can also use it on unpartitioned tables.\n+\n+Iceberg supports ``$snapshot_id`` and ``$snapshot_timestamp_ms`` as hidden columns.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyNDMxNA=="}, "originalCommit": null, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyNDg1MA==", "bodyText": "Add code quotes around the format names.", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483324850", "createdAt": "2020-09-04T00:42:36Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet\n+released.  When v2 is released, it will support row-level deletion and the\n+Iceberg connector will be updated to provide row-level deletion.\n+\n+The Iceberg connector currently supports only deletion by partition.\n+This SQL below will fail because the doesn't span entire partitions::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1 AND d > 10\n+\n+Rolling Back to a Previous Snapshot\n+-----------------------------------\n+\n+Iceberg supports a \"snapshot\" model of data, where table snapshots are\n+identified by an ``long`` snapshot_ids.\n+\n+The connector provides a system snapshots table for each Iceberg table.  Snapshots are\n+identified by BIGINT snapshot ids.  You can find the latest snapshot id for table\n+foo by running the following command::\n+\n+    SELECT snapshot_id FROM 'foo$snapshots' ORDER BY committed_at DESC LIMIT 1\n+\n+A SQL procedure ``system.rollback_to_snapshot`` allows the caller to roll back\n+the state of the table to a previous snapshot id::\n+\n+    CALL system.rollback_to_snapshot(schema_name, table_name, snapshot_id)\n+\n+Schema Evolution\n+----------------\n+\n+Iceberg and the Iceberg connector support schema evolution, with safe\n+column add, drop, reorder and rename operations, including in nested structures.\n+Table partitioning can also be changed and the connector can still\n+query data created before the partitioning change.\n+\n+Migrating Existing Tables\n+-------------------------\n+\n+The connector can read from or write to Hive tables that have been migrated to Iceberg.\n+Currently, there is no Presto support to migrate Hive tables to Presto, so you will\n+need to use either the Iceberg API or Spark.\n+\n+System Tables and Columns\n+-------------------------\n+\n+The connector supports ``table$partitions`` as a substitute for Hive's ``SHOW PARTITIONS``.\n+The differences are that it returns some partition metrics for each partition value\n+and you can also use it on unpartitioned tables.\n+\n+Iceberg supports ``$snapshot_id`` and ``$snapshot_timestamp_ms`` as hidden columns.\n+These columns allow users to query an old version of the table. Think of this\n+as a time-travel feature which lets you query your table's snapshot at a given time.\n+\n+Iceberg Table Properties\n+------------------------\n+\n+================================================== ============================================================ ============\n+Property Name                                      Description                                                  Default\n+================================================== ============================================================ ============\n+``format``                                         Optionally specifies the format of table data files;         ORC\n+                                                   either PARQUET or ORC.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyNTAzOA==", "bodyText": "Let's get rid of the default column and make it part of the description as necessary\n\nOptionally specifies the format of table data files. Must be PARQUET or ORC. Defaults to ORC.", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483325038", "createdAt": "2020-09-04T00:43:21Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet\n+released.  When v2 is released, it will support row-level deletion and the\n+Iceberg connector will be updated to provide row-level deletion.\n+\n+The Iceberg connector currently supports only deletion by partition.\n+This SQL below will fail because the doesn't span entire partitions::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1 AND d > 10\n+\n+Rolling Back to a Previous Snapshot\n+-----------------------------------\n+\n+Iceberg supports a \"snapshot\" model of data, where table snapshots are\n+identified by an ``long`` snapshot_ids.\n+\n+The connector provides a system snapshots table for each Iceberg table.  Snapshots are\n+identified by BIGINT snapshot ids.  You can find the latest snapshot id for table\n+foo by running the following command::\n+\n+    SELECT snapshot_id FROM 'foo$snapshots' ORDER BY committed_at DESC LIMIT 1\n+\n+A SQL procedure ``system.rollback_to_snapshot`` allows the caller to roll back\n+the state of the table to a previous snapshot id::\n+\n+    CALL system.rollback_to_snapshot(schema_name, table_name, snapshot_id)\n+\n+Schema Evolution\n+----------------\n+\n+Iceberg and the Iceberg connector support schema evolution, with safe\n+column add, drop, reorder and rename operations, including in nested structures.\n+Table partitioning can also be changed and the connector can still\n+query data created before the partitioning change.\n+\n+Migrating Existing Tables\n+-------------------------\n+\n+The connector can read from or write to Hive tables that have been migrated to Iceberg.\n+Currently, there is no Presto support to migrate Hive tables to Presto, so you will\n+need to use either the Iceberg API or Spark.\n+\n+System Tables and Columns\n+-------------------------\n+\n+The connector supports ``table$partitions`` as a substitute for Hive's ``SHOW PARTITIONS``.\n+The differences are that it returns some partition metrics for each partition value\n+and you can also use it on unpartitioned tables.\n+\n+Iceberg supports ``$snapshot_id`` and ``$snapshot_timestamp_ms`` as hidden columns.\n+These columns allow users to query an old version of the table. Think of this\n+as a time-travel feature which lets you query your table's snapshot at a given time.\n+\n+Iceberg Table Properties\n+------------------------\n+\n+================================================== ============================================================ ============\n+Property Name                                      Description                                                  Default\n+================================================== ============================================================ ============\n+``format``                                         Optionally specifies the format of table data files;         ORC", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 175}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyNTExNg==", "bodyText": "Code quotes on column names", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483325116", "createdAt": "2020-09-04T00:43:39Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet\n+released.  When v2 is released, it will support row-level deletion and the\n+Iceberg connector will be updated to provide row-level deletion.\n+\n+The Iceberg connector currently supports only deletion by partition.\n+This SQL below will fail because the doesn't span entire partitions::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1 AND d > 10\n+\n+Rolling Back to a Previous Snapshot\n+-----------------------------------\n+\n+Iceberg supports a \"snapshot\" model of data, where table snapshots are\n+identified by an ``long`` snapshot_ids.\n+\n+The connector provides a system snapshots table for each Iceberg table.  Snapshots are\n+identified by BIGINT snapshot ids.  You can find the latest snapshot id for table\n+foo by running the following command::\n+\n+    SELECT snapshot_id FROM 'foo$snapshots' ORDER BY committed_at DESC LIMIT 1\n+\n+A SQL procedure ``system.rollback_to_snapshot`` allows the caller to roll back\n+the state of the table to a previous snapshot id::\n+\n+    CALL system.rollback_to_snapshot(schema_name, table_name, snapshot_id)\n+\n+Schema Evolution\n+----------------\n+\n+Iceberg and the Iceberg connector support schema evolution, with safe\n+column add, drop, reorder and rename operations, including in nested structures.\n+Table partitioning can also be changed and the connector can still\n+query data created before the partitioning change.\n+\n+Migrating Existing Tables\n+-------------------------\n+\n+The connector can read from or write to Hive tables that have been migrated to Iceberg.\n+Currently, there is no Presto support to migrate Hive tables to Presto, so you will\n+need to use either the Iceberg API or Spark.\n+\n+System Tables and Columns\n+-------------------------\n+\n+The connector supports ``table$partitions`` as a substitute for Hive's ``SHOW PARTITIONS``.\n+The differences are that it returns some partition metrics for each partition value\n+and you can also use it on unpartitioned tables.\n+\n+Iceberg supports ``$snapshot_id`` and ``$snapshot_timestamp_ms`` as hidden columns.\n+These columns allow users to query an old version of the table. Think of this\n+as a time-travel feature which lets you query your table's snapshot at a given time.\n+\n+Iceberg Table Properties\n+------------------------\n+\n+================================================== ============================================================ ============\n+Property Name                                      Description                                                  Default\n+================================================== ============================================================ ============\n+``format``                                         Optionally specifies the format of table data files;         ORC\n+                                                   either PARQUET or ORC.\n+\n+``partitioning``                                   Optionally specifies table partitioning.\n+                                                   If a table is partitioned by columns c1 and c2, the", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyNTE1Ng==", "bodyText": "End in period", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483325156", "createdAt": "2020-09-04T00:43:52Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet\n+released.  When v2 is released, it will support row-level deletion and the\n+Iceberg connector will be updated to provide row-level deletion.\n+\n+The Iceberg connector currently supports only deletion by partition.\n+This SQL below will fail because the doesn't span entire partitions::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1 AND d > 10\n+\n+Rolling Back to a Previous Snapshot\n+-----------------------------------\n+\n+Iceberg supports a \"snapshot\" model of data, where table snapshots are\n+identified by an ``long`` snapshot_ids.\n+\n+The connector provides a system snapshots table for each Iceberg table.  Snapshots are\n+identified by BIGINT snapshot ids.  You can find the latest snapshot id for table\n+foo by running the following command::\n+\n+    SELECT snapshot_id FROM 'foo$snapshots' ORDER BY committed_at DESC LIMIT 1\n+\n+A SQL procedure ``system.rollback_to_snapshot`` allows the caller to roll back\n+the state of the table to a previous snapshot id::\n+\n+    CALL system.rollback_to_snapshot(schema_name, table_name, snapshot_id)\n+\n+Schema Evolution\n+----------------\n+\n+Iceberg and the Iceberg connector support schema evolution, with safe\n+column add, drop, reorder and rename operations, including in nested structures.\n+Table partitioning can also be changed and the connector can still\n+query data created before the partitioning change.\n+\n+Migrating Existing Tables\n+-------------------------\n+\n+The connector can read from or write to Hive tables that have been migrated to Iceberg.\n+Currently, there is no Presto support to migrate Hive tables to Presto, so you will\n+need to use either the Iceberg API or Spark.\n+\n+System Tables and Columns\n+-------------------------\n+\n+The connector supports ``table$partitions`` as a substitute for Hive's ``SHOW PARTITIONS``.\n+The differences are that it returns some partition metrics for each partition value\n+and you can also use it on unpartitioned tables.\n+\n+Iceberg supports ``$snapshot_id`` and ``$snapshot_timestamp_ms`` as hidden columns.\n+These columns allow users to query an old version of the table. Think of this\n+as a time-travel feature which lets you query your table's snapshot at a given time.\n+\n+Iceberg Table Properties\n+------------------------\n+\n+================================================== ============================================================ ============\n+Property Name                                      Description                                                  Default\n+================================================== ============================================================ ============\n+``format``                                         Optionally specifies the format of table data files;         ORC\n+                                                   either PARQUET or ORC.\n+\n+``partitioning``                                   Optionally specifies table partitioning.\n+                                                   If a table is partitioned by columns c1 and c2, the\n+                                                   partitioning property would be\n+                                                   ``partitioning = ARRAY['c1', 'c2']``\n+\n+``location``                                       Optionally specifies the file system location URI for\n+                                                   the table", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyNTMxNw==", "bodyText": "Use code quotes for PARQUET or write it as the name Parquet. Code quotes around column names and location.", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483325317", "createdAt": "2020-09-04T00:44:23Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.\n+===================================== ================================================================================================\n+\n+In this example, the table is partitioned by month and by a hash of the account number::\n+\n+    CREATE TABLE iceberg.testdb.sample_partitioned (\n+        order_date DATE,\n+        account_number BIGINT,\n+        customer VARCHAR)\n+    WITH (partitioning = ARRAY['month(order_date)', 'bucket(account_number, 10)'])\n+\n+Deletion by Partition\n+---------------------\n+\n+For partitioned tables, the Iceberg connector supports the deletion of entire\n+partitions if the WHERE clause specifies an identity transform of a partition\n+column.  Given this table definition::\n+\n+    CREATE TABLE test_partition_deletion (\n+        c integer,\n+        d double)\n+        WITH (partitioning = ARRAY['c'])\n+\n+This SQL will delete all partitions for which c is 1::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1\n+\n+The current Iceberg connector is based on Iceberg v1.  Iceberg v2 is not yet\n+released.  When v2 is released, it will support row-level deletion and the\n+Iceberg connector will be updated to provide row-level deletion.\n+\n+The Iceberg connector currently supports only deletion by partition.\n+This SQL below will fail because the doesn't span entire partitions::\n+\n+    DELETE FROM test_partition_deletion\n+    WHERE c = 1 AND d > 10\n+\n+Rolling Back to a Previous Snapshot\n+-----------------------------------\n+\n+Iceberg supports a \"snapshot\" model of data, where table snapshots are\n+identified by an ``long`` snapshot_ids.\n+\n+The connector provides a system snapshots table for each Iceberg table.  Snapshots are\n+identified by BIGINT snapshot ids.  You can find the latest snapshot id for table\n+foo by running the following command::\n+\n+    SELECT snapshot_id FROM 'foo$snapshots' ORDER BY committed_at DESC LIMIT 1\n+\n+A SQL procedure ``system.rollback_to_snapshot`` allows the caller to roll back\n+the state of the table to a previous snapshot id::\n+\n+    CALL system.rollback_to_snapshot(schema_name, table_name, snapshot_id)\n+\n+Schema Evolution\n+----------------\n+\n+Iceberg and the Iceberg connector support schema evolution, with safe\n+column add, drop, reorder and rename operations, including in nested structures.\n+Table partitioning can also be changed and the connector can still\n+query data created before the partitioning change.\n+\n+Migrating Existing Tables\n+-------------------------\n+\n+The connector can read from or write to Hive tables that have been migrated to Iceberg.\n+Currently, there is no Presto support to migrate Hive tables to Presto, so you will\n+need to use either the Iceberg API or Spark.\n+\n+System Tables and Columns\n+-------------------------\n+\n+The connector supports ``table$partitions`` as a substitute for Hive's ``SHOW PARTITIONS``.\n+The differences are that it returns some partition metrics for each partition value\n+and you can also use it on unpartitioned tables.\n+\n+Iceberg supports ``$snapshot_id`` and ``$snapshot_timestamp_ms`` as hidden columns.\n+These columns allow users to query an old version of the table. Think of this\n+as a time-travel feature which lets you query your table's snapshot at a given time.\n+\n+Iceberg Table Properties\n+------------------------\n+\n+================================================== ============================================================ ============\n+Property Name                                      Description                                                  Default\n+================================================== ============================================================ ============\n+``format``                                         Optionally specifies the format of table data files;         ORC\n+                                                   either PARQUET or ORC.\n+\n+``partitioning``                                   Optionally specifies table partitioning.\n+                                                   If a table is partitioned by columns c1 and c2, the\n+                                                   partitioning property would be\n+                                                   ``partitioning = ARRAY['c1', 'c2']``\n+\n+``location``                                       Optionally specifies the file system location URI for\n+                                                   the table\n+================================================== ============================================================ ============\n+\n+The table definition below specifies format PARQUET, partitioning by columns c1 and c2,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 187}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgyMjkxMjQy", "url": "https://github.com/trinodb/trino/pull/4537#pullrequestreview-482291242", "createdAt": "2020-09-04T00:56:15Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQwMDo1NjoxNlrOHM8AoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQwMDo1NjoxNlrOHM8AoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMyODE2MQ==", "bodyText": "This actually supports more than just characters: https://iceberg.apache.org/spec/#truncate-transform-details", "url": "https://github.com/trinodb/trino/pull/4537#discussion_r483328161", "createdAt": "2020-09-04T00:56:16Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/iceberg.rst", "diffHunk": "@@ -0,0 +1,197 @@\n+=================\n+Iceberg Connector\n+=================\n+\n+Overview\n+--------\n+\n+Apache Iceberg is an open table format for huge analytic datasets.\n+The Iceberg connector allows querying data stored in\n+files written in Iceberg format, as defined in the\n+`Iceberg Table Spec <https://iceberg.apache.org/spec/>`_.\n+\n+The Iceberg connector supports:\n+\n+* All Presto types, including TIME, TIMESTAMP, DECIMAL\n+* CREATE TABLE and CREATE TABLE AS\n+* Partitioning tables by any Presto type\n+* Dropping tables\n+* Renaming tables\n+* Deletion by partition\n+* Table and column statistics\n+* Table and column comments\n+* Rolling back a table to a previous state\n+* Schema evolution\n+\n+The Iceberg table state is maintained in metadata files. All changes to table state\n+create a new metadata file and replace the old metadata with an atomic swap.\n+The table metadata file tracks the table schema, partitioning config,\n+custom properties, and snapshots of the table contents.\n+\n+Iceberg data files can be stored in either Parquet or ORC format, as\n+determined by the ``format`` property in the table definition.  The\n+table ``format`` defaults to ``ORC``.\n+\n+Iceberg is designed to improve on the known scalabilty limitations of Hive.  Hive\n+stores table metadata in an object store backed by a database like MySQL.  It tracks\n+partition locations in the metastore, but not individual data files.  Presto queries\n+using the Hive connector must first call the metastore to get partition locations,\n+then call the underlying filesystem to list all data files inside each partition,\n+and then read metadata from each data file.\n+\n+Since Iceberg stores the paths to data files in the metadata files, it\n+only consults the underlying file system for files that must be read.\n+\n+Status\n+------\n+\n+The Iceberg connector is not yet supported for production use.\n+\n+Compatibility With Spark Iceberg\n+--------------------------------\n+\n+Tests have been written to demonstrate that data written by Presto can be read\n+using Spark SQL with the Iceberg format, and vice versa.  See\n+`TestSparkCompatibility <https://github.com/prestosql/presto/blob/master/presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestSparkCompatibility.java>`_.\n+\n+Configuration\n+-------------\n+\n+Iceberg supports the same metastore configuration properties as the Hive connector.\n+At a minimum, ``hive.metastore.uri`` must be configured:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    hive.metastore.uri=thrift://localhost:9083\n+\n+Partitioned Tables\n+------------------\n+\n+Iceberg supports partitioning by specifying transforms over the table columns.\n+A partition is created for each unique tuple value produced by the transforms.\n+Identity transforms are simply the column name. Other transforms are:\n+\n+===================================== ================================================================================================\n+Transform                             Description\n+===================================== ================================================================================================\n+``year(ts)``                          The partition value is the integer difference in years between ``ts`` and January 1st. 1970.\n+\n+``month(ts)``                         The partition value is the integer difference in months between ``ts`` and January 1st. 1970.\n+\n+``day(ts)``                           The partition value is the integer difference in days between ``ts`` and January 1st. 1970.\n+\n+``hour(ts)``                          The partition value is a TIMESTAMP rounded down to the nearest hour.\n+\n+``bucket(x, nbuckets)``               The partition value is an integer hash of ``x``, with a value between 0 and ``nbuckets - 1`` inclusive.\n+\n+``truncate(s, nchars)``               The partition value is the first ``nchars`` characters of VARCHAR ``s``.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 88}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "831af36a4d865a648b636082bca08d8e28b3aa4d", "author": {"user": {"login": "djsstarburst", "name": "David Stryker"}}, "url": "https://github.com/trinodb/trino/commit/831af36a4d865a648b636082bca08d8e28b3aa4d", "committedDate": "2020-09-08T20:34:59Z", "message": "Iceberg connector documentation\n\nThis documentation has been reviewed by the doc group."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "831af36a4d865a648b636082bca08d8e28b3aa4d", "author": {"user": {"login": "djsstarburst", "name": "David Stryker"}}, "url": "https://github.com/trinodb/trino/commit/831af36a4d865a648b636082bca08d8e28b3aa4d", "committedDate": "2020-09-08T20:34:59Z", "message": "Iceberg connector documentation\n\nThis documentation has been reviewed by the doc group."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg0NTY0NTM3", "url": "https://github.com/trinodb/trino/pull/4537#pullrequestreview-484564537", "createdAt": "2020-09-08T23:42:33Z", "commit": {"oid": "831af36a4d865a648b636082bca08d8e28b3aa4d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4834, "cost": 1, "resetAt": "2021-10-28T20:13:43Z"}}}