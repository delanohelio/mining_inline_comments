{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg0MjkzNDc0", "number": 3009, "title": "Allow to set custom metastore in HiveQueryRunner", "bodyText": "Allow to set custom metastore in HiveQueryRunner", "createdAt": "2020-03-05T14:02:52Z", "url": "https://github.com/trinodb/trino/pull/3009", "merged": true, "mergeCommit": {"oid": "65092482fc78293f7474146a9d759c8584ba145b"}, "closed": true, "closedAt": "2020-03-05T21:27:56Z", "author": {"login": "kokosing"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcKsDDcgFqTM2OTU5Mjg1NQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcKwsMDABqjMxMDI1OTA3MDA=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5NTkyODU1", "url": "https://github.com/trinodb/trino/pull/3009#pullrequestreview-369592855", "createdAt": "2020-03-05T14:04:29Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5NzE5NzI5", "url": "https://github.com/trinodb/trino/pull/3009#pullrequestreview-369719729", "createdAt": "2020-03-05T16:27:25Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQxNjoyNzoyNVrOFyalZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQxNjoyOToyOFrOFyarNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQwODY3Nw==", "bodyText": "setInitialTables", "url": "https://github.com/trinodb/trino/pull/3009#discussion_r388408677", "createdAt": "2020-03-05T16:27:25Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/HiveQueryRunner.java", "diffHunk": "@@ -68,82 +70,94 @@ private HiveQueryRunner()\n     private static final String TPCH_BUCKETED_SCHEMA = \"tpch_bucketed\";\n     private static final DateTimeZone TIME_ZONE = DateTimeZone.forID(\"America/Bahia_Banderas\");\n \n-    public static DistributedQueryRunner createQueryRunner(TpchTable<?>... tables)\n+    public static DistributedQueryRunner create()\n             throws Exception\n     {\n-        return createQueryRunner(ImmutableList.copyOf(tables));\n+        return builder().build();\n     }\n \n-    public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> tables)\n-            throws Exception\n+    public static Builder builder()\n     {\n-        return createQueryRunner(tables, ImmutableMap.of(), Optional.empty());\n+        return new Builder();\n     }\n \n-    public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> tables, Map<String, String> extraProperties, Optional<Path> baseDataDir)\n-            throws Exception\n+    public static class Builder\n+            extends DistributedQueryRunner.Builder\n     {\n-        return createQueryRunner(tables, extraProperties, ImmutableMap.of(), baseDataDir);\n-    }\n+        private Map<String, String> extraHiveProperties = ImmutableMap.of();\n+        private List<TpchTable<?>> tables = ImmutableList.of();\n \n-    public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> tables, Map<String, String> extraProperties, Map<String, String> extraHiveProperties, Optional<Path> baseDataDir)\n-            throws Exception\n-    {\n-        assertEquals(DateTimeZone.getDefault(), TIME_ZONE, \"Timezone not configured correctly. Add -Duser.timezone=America/Bahia_Banderas to your JVM arguments\");\n-        setupLogging();\n+        protected Builder()\n+        {\n+            super(createSession(Optional.of(new SelectedRole(ROLE, Optional.of(\"admin\")))));\n+        }\n \n-        DistributedQueryRunner queryRunner = DistributedQueryRunner\n-                .builder(createSession(Optional.of(new SelectedRole(ROLE, Optional.of(\"admin\")))))\n-                .setNodeCount(4)\n-                .setExtraProperties(extraProperties)\n-                .setBaseDataDir(baseDataDir)\n-                .build();\n+        public Builder setExtraHiveProperties(Map<String, String> extraHiveProperties)\n+        {\n+            this.extraHiveProperties = ImmutableMap.copyOf(requireNonNull(extraHiveProperties, \"extraHiveProperties is null\"));\n+            return this;\n+        }\n \n-        try {\n-            queryRunner.installPlugin(new TpchPlugin());\n-            queryRunner.createCatalog(\"tpch\", \"tpch\");\n-\n-            File baseDir = queryRunner.getCoordinator().getBaseDataDir().resolve(\"hive_data\").toFile();\n-\n-            FileHiveMetastore metastore = new FileHiveMetastore(HDFS_ENVIRONMENT, baseDir.toURI().toString(), \"test\");\n-            queryRunner.installPlugin(new TestingHivePlugin(metastore));\n-\n-            Map<String, String> hiveProperties = ImmutableMap.<String, String>builder()\n-                    .put(\"hive.time-zone\", TIME_ZONE.getID())\n-                    .put(\"hive.max-partitions-per-scan\", \"1000\")\n-                    .put(\"hive.assume-canonical-partition-keys\", \"true\")\n-                    .build();\n-\n-            hiveProperties = new HashMap<>(hiveProperties);\n-            hiveProperties.putAll(extraHiveProperties);\n-            hiveProperties.putIfAbsent(\"hive.security\", \"sql-standard\");\n-\n-            Map<String, String> hiveBucketedProperties = ImmutableMap.<String, String>builder()\n-                    .putAll(hiveProperties)\n-                    .put(\"hive.max-initial-split-size\", \"10kB\") // so that each bucket has multiple splits\n-                    .put(\"hive.max-split-size\", \"10kB\") // so that each bucket has multiple splits\n-                    .put(\"hive.storage-format\", \"TEXTFILE\") // so that there's no minimum split size for the file\n-                    .put(\"hive.compression-codec\", \"NONE\") // so that the file is splittable\n-                    .build();\n-            queryRunner.createCatalog(HIVE_CATALOG, HIVE_CATALOG, hiveProperties);\n-            queryRunner.createCatalog(HIVE_BUCKETED_CATALOG, HIVE_CATALOG, hiveBucketedProperties);\n-\n-            HiveIdentity identity = new HiveIdentity(SESSION);\n-            if (!metastore.getDatabase(TPCH_SCHEMA).isPresent()) {\n-                metastore.createDatabase(identity, createDatabaseMetastoreObject(TPCH_SCHEMA));\n-                copyTpchTables(queryRunner, \"tpch\", TINY_SCHEMA_NAME, createSession(Optional.empty()), tables);\n-            }\n+        public Builder setTables(Iterable<TpchTable<?>> tables)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQwODg3Mg==", "bodyText": "setHiveProperties? not sure why we had \"extra\" here", "url": "https://github.com/trinodb/trino/pull/3009#discussion_r388408872", "createdAt": "2020-03-05T16:27:44Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/HiveQueryRunner.java", "diffHunk": "@@ -68,82 +70,94 @@ private HiveQueryRunner()\n     private static final String TPCH_BUCKETED_SCHEMA = \"tpch_bucketed\";\n     private static final DateTimeZone TIME_ZONE = DateTimeZone.forID(\"America/Bahia_Banderas\");\n \n-    public static DistributedQueryRunner createQueryRunner(TpchTable<?>... tables)\n+    public static DistributedQueryRunner create()\n             throws Exception\n     {\n-        return createQueryRunner(ImmutableList.copyOf(tables));\n+        return builder().build();\n     }\n \n-    public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> tables)\n-            throws Exception\n+    public static Builder builder()\n     {\n-        return createQueryRunner(tables, ImmutableMap.of(), Optional.empty());\n+        return new Builder();\n     }\n \n-    public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> tables, Map<String, String> extraProperties, Optional<Path> baseDataDir)\n-            throws Exception\n+    public static class Builder\n+            extends DistributedQueryRunner.Builder\n     {\n-        return createQueryRunner(tables, extraProperties, ImmutableMap.of(), baseDataDir);\n-    }\n+        private Map<String, String> extraHiveProperties = ImmutableMap.of();\n+        private List<TpchTable<?>> tables = ImmutableList.of();\n \n-    public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> tables, Map<String, String> extraProperties, Map<String, String> extraHiveProperties, Optional<Path> baseDataDir)\n-            throws Exception\n-    {\n-        assertEquals(DateTimeZone.getDefault(), TIME_ZONE, \"Timezone not configured correctly. Add -Duser.timezone=America/Bahia_Banderas to your JVM arguments\");\n-        setupLogging();\n+        protected Builder()\n+        {\n+            super(createSession(Optional.of(new SelectedRole(ROLE, Optional.of(\"admin\")))));\n+        }\n \n-        DistributedQueryRunner queryRunner = DistributedQueryRunner\n-                .builder(createSession(Optional.of(new SelectedRole(ROLE, Optional.of(\"admin\")))))\n-                .setNodeCount(4)\n-                .setExtraProperties(extraProperties)\n-                .setBaseDataDir(baseDataDir)\n-                .build();\n+        public Builder setExtraHiveProperties(Map<String, String> extraHiveProperties)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQwOTkxNQ==", "bodyText": "since you inherit parent builder, you should pass your own Builder type there and the parent builder should return T from build methods... otherwise code using builder needs to use methods in certain specific order, not the way builder should work\nfollow-up maybe", "url": "https://github.com/trinodb/trino/pull/3009#discussion_r388409915", "createdAt": "2020-03-05T16:29:08Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/HiveQueryRunner.java", "diffHunk": "@@ -68,82 +70,94 @@ private HiveQueryRunner()\n     private static final String TPCH_BUCKETED_SCHEMA = \"tpch_bucketed\";\n     private static final DateTimeZone TIME_ZONE = DateTimeZone.forID(\"America/Bahia_Banderas\");\n \n-    public static DistributedQueryRunner createQueryRunner(TpchTable<?>... tables)\n+    public static DistributedQueryRunner create()\n             throws Exception\n     {\n-        return createQueryRunner(ImmutableList.copyOf(tables));\n+        return builder().build();\n     }\n \n-    public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> tables)\n-            throws Exception\n+    public static Builder builder()\n     {\n-        return createQueryRunner(tables, ImmutableMap.of(), Optional.empty());\n+        return new Builder();\n     }\n \n-    public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> tables, Map<String, String> extraProperties, Optional<Path> baseDataDir)\n-            throws Exception\n+    public static class Builder\n+            extends DistributedQueryRunner.Builder", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQxMDE2Nw==", "bodyText": "Why is this default?? TODO maybe", "url": "https://github.com/trinodb/trino/pull/3009#discussion_r388410167", "createdAt": "2020-03-05T16:29:28Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/HiveQueryRunner.java", "diffHunk": "@@ -68,82 +70,94 @@ private HiveQueryRunner()\n     private static final String TPCH_BUCKETED_SCHEMA = \"tpch_bucketed\";\n     private static final DateTimeZone TIME_ZONE = DateTimeZone.forID(\"America/Bahia_Banderas\");\n \n-    public static DistributedQueryRunner createQueryRunner(TpchTable<?>... tables)\n+    public static DistributedQueryRunner create()\n             throws Exception\n     {\n-        return createQueryRunner(ImmutableList.copyOf(tables));\n+        return builder().build();\n     }\n \n-    public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> tables)\n-            throws Exception\n+    public static Builder builder()\n     {\n-        return createQueryRunner(tables, ImmutableMap.of(), Optional.empty());\n+        return new Builder();\n     }\n \n-    public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> tables, Map<String, String> extraProperties, Optional<Path> baseDataDir)\n-            throws Exception\n+    public static class Builder\n+            extends DistributedQueryRunner.Builder\n     {\n-        return createQueryRunner(tables, extraProperties, ImmutableMap.of(), baseDataDir);\n-    }\n+        private Map<String, String> extraHiveProperties = ImmutableMap.of();\n+        private List<TpchTable<?>> tables = ImmutableList.of();\n \n-    public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> tables, Map<String, String> extraProperties, Map<String, String> extraHiveProperties, Optional<Path> baseDataDir)\n-            throws Exception\n-    {\n-        assertEquals(DateTimeZone.getDefault(), TIME_ZONE, \"Timezone not configured correctly. Add -Duser.timezone=America/Bahia_Banderas to your JVM arguments\");\n-        setupLogging();\n+        protected Builder()\n+        {\n+            super(createSession(Optional.of(new SelectedRole(ROLE, Optional.of(\"admin\")))));\n+        }\n \n-        DistributedQueryRunner queryRunner = DistributedQueryRunner\n-                .builder(createSession(Optional.of(new SelectedRole(ROLE, Optional.of(\"admin\")))))\n-                .setNodeCount(4)\n-                .setExtraProperties(extraProperties)\n-                .setBaseDataDir(baseDataDir)\n-                .build();\n+        public Builder setExtraHiveProperties(Map<String, String> extraHiveProperties)\n+        {\n+            this.extraHiveProperties = ImmutableMap.copyOf(requireNonNull(extraHiveProperties, \"extraHiveProperties is null\"));\n+            return this;\n+        }\n \n-        try {\n-            queryRunner.installPlugin(new TpchPlugin());\n-            queryRunner.createCatalog(\"tpch\", \"tpch\");\n-\n-            File baseDir = queryRunner.getCoordinator().getBaseDataDir().resolve(\"hive_data\").toFile();\n-\n-            FileHiveMetastore metastore = new FileHiveMetastore(HDFS_ENVIRONMENT, baseDir.toURI().toString(), \"test\");\n-            queryRunner.installPlugin(new TestingHivePlugin(metastore));\n-\n-            Map<String, String> hiveProperties = ImmutableMap.<String, String>builder()\n-                    .put(\"hive.time-zone\", TIME_ZONE.getID())\n-                    .put(\"hive.max-partitions-per-scan\", \"1000\")\n-                    .put(\"hive.assume-canonical-partition-keys\", \"true\")\n-                    .build();\n-\n-            hiveProperties = new HashMap<>(hiveProperties);\n-            hiveProperties.putAll(extraHiveProperties);\n-            hiveProperties.putIfAbsent(\"hive.security\", \"sql-standard\");\n-\n-            Map<String, String> hiveBucketedProperties = ImmutableMap.<String, String>builder()\n-                    .putAll(hiveProperties)\n-                    .put(\"hive.max-initial-split-size\", \"10kB\") // so that each bucket has multiple splits\n-                    .put(\"hive.max-split-size\", \"10kB\") // so that each bucket has multiple splits\n-                    .put(\"hive.storage-format\", \"TEXTFILE\") // so that there's no minimum split size for the file\n-                    .put(\"hive.compression-codec\", \"NONE\") // so that the file is splittable\n-                    .build();\n-            queryRunner.createCatalog(HIVE_CATALOG, HIVE_CATALOG, hiveProperties);\n-            queryRunner.createCatalog(HIVE_BUCKETED_CATALOG, HIVE_CATALOG, hiveBucketedProperties);\n-\n-            HiveIdentity identity = new HiveIdentity(SESSION);\n-            if (!metastore.getDatabase(TPCH_SCHEMA).isPresent()) {\n-                metastore.createDatabase(identity, createDatabaseMetastoreObject(TPCH_SCHEMA));\n-                copyTpchTables(queryRunner, \"tpch\", TINY_SCHEMA_NAME, createSession(Optional.empty()), tables);\n-            }\n+        public Builder setTables(Iterable<TpchTable<?>> tables)\n+        {\n+            this.tables = ImmutableList.copyOf(requireNonNull(tables, \"tables is null\"));\n+            return this;\n+        }\n \n-            if (!metastore.getDatabase(TPCH_BUCKETED_SCHEMA).isPresent()) {\n-                metastore.createDatabase(identity, createDatabaseMetastoreObject(TPCH_BUCKETED_SCHEMA));\n-                copyTpchTablesBucketed(queryRunner, \"tpch\", TINY_SCHEMA_NAME, createBucketedSession(Optional.empty()), tables);\n-            }\n+        public DistributedQueryRunner build()\n+                throws Exception\n+        {\n+            assertEquals(DateTimeZone.getDefault(), TIME_ZONE, \"Timezone not configured correctly. Add -Duser.timezone=America/Bahia_Banderas to your JVM arguments\");\n+            setupLogging();\n \n-            return queryRunner;\n-        }\n-        catch (Exception e) {\n-            queryRunner.close();\n-            throw e;\n+            DistributedQueryRunner queryRunner = super.build();\n+\n+            try {\n+                queryRunner.installPlugin(new TpchPlugin());\n+                queryRunner.createCatalog(\"tpch\", \"tpch\");\n+\n+                File baseDir = queryRunner.getCoordinator().getBaseDataDir().resolve(\"hive_data\").toFile();\n+\n+                FileHiveMetastore metastore = new FileHiveMetastore(HDFS_ENVIRONMENT, baseDir.toURI().toString(), \"test\");\n+                queryRunner.installPlugin(new TestingHivePlugin(metastore));\n+\n+                Map<String, String> hiveProperties = ImmutableMap.<String, String>builder()\n+                        .put(\"hive.time-zone\", TIME_ZONE.getID())\n+                        .put(\"hive.max-partitions-per-scan\", \"1000\")\n+                        .put(\"hive.assume-canonical-partition-keys\", \"true\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 137}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5ba0f10bf00285b9ce166bfa7504f7616acae7df", "author": {"user": {"login": "kokosing", "name": "Grzegorz Kokosi\u0144ski"}}, "url": "https://github.com/trinodb/trino/commit/5ba0f10bf00285b9ce166bfa7504f7616acae7df", "committedDate": "2020-03-05T19:28:47Z", "message": "Introduce Builder for HiveQueryRunner"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3296c1713bc48387f9e53e9ed3f70cfa9744ddf1", "author": {"user": {"login": "kokosing", "name": "Grzegorz Kokosi\u0144ski"}}, "url": "https://github.com/trinodb/trino/commit/3296c1713bc48387f9e53e9ed3f70cfa9744ddf1", "committedDate": "2020-03-05T19:28:47Z", "message": "Cleanup in HiveQueryRunner"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "05cf6ba4bc6481651c67e6a40e31b31be85623b5", "author": {"user": {"login": "kokosing", "name": "Grzegorz Kokosi\u0144ski"}}, "url": "https://github.com/trinodb/trino/commit/05cf6ba4bc6481651c67e6a40e31b31be85623b5", "committedDate": "2020-03-05T19:28:48Z", "message": "Allow to set custom metastore in HiveQueryRunner"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "05cf6ba4bc6481651c67e6a40e31b31be85623b5", "author": {"user": {"login": "kokosing", "name": "Grzegorz Kokosi\u0144ski"}}, "url": "https://github.com/trinodb/trino/commit/05cf6ba4bc6481651c67e6a40e31b31be85623b5", "committedDate": "2020-03-05T19:28:48Z", "message": "Allow to set custom metastore in HiveQueryRunner"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1630, "cost": 1, "resetAt": "2021-10-28T20:13:43Z"}}}