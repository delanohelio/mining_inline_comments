{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAyMTgwNzU2", "number": 5538, "title": "Add support for MICROS parquet timestamp encoding", "bodyText": "Fixes #5483", "createdAt": "2020-10-13T10:33:39Z", "url": "https://github.com/trinodb/trino/pull/5538", "merged": true, "mergeCommit": {"oid": "e87933e31c94cb77c2fefda4c1af0363f2c04665"}, "closed": true, "closedAt": "2020-10-15T20:14:10Z", "author": {"login": "findepi"}, "timelineItems": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdSGGlvABqjM4NzA1NTcyMjc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdSZU94ABqjM4NzU0ODEzNTc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3MzI2ODIz", "url": "https://github.com/trinodb/trino/pull/5538#pullrequestreview-507326823", "createdAt": "2020-10-13T11:18:39Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3MzkyMjA0", "url": "https://github.com/trinodb/trino/pull/5538#pullrequestreview-507392204", "createdAt": "2020-10-13T12:44:47Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3NDg3MzU5", "url": "https://github.com/trinodb/trino/pull/5538#pullrequestreview-507487359", "createdAt": "2020-10-13T14:19:01Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNDoxOTowMVrOHgpMGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNDoxOTowMVrOHgpMGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk5MTMyMw==", "bodyText": "Should we do the same for TimestampWithTimeZone?", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r503991323", "createdAt": "2020-10-13T14:19:01Z", "author": {"login": "alexjo2144"}, "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/TimestampMicrosColumnReader.java", "diffHunk": "@@ -40,9 +44,15 @@ protected void readValue(BlockBuilder blockBuilder, Type type)\n         if (definitionLevel == columnDescriptor.getMaxDefinitionLevel()) {\n             long epochMicros = valuesReader.readLong();\n             // TODO: specialize the class at creation time\n-            if (type.equals(TIMESTAMP_MICROS)) {\n+            if (type.equals(TIMESTAMP_MILLIS)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3NTg4NjQ3", "url": "https://github.com/trinodb/trino/pull/5538#pullrequestreview-507588647", "createdAt": "2020-10-13T15:53:02Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNTo1MzowM1rOHgtxrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNTo1MzowM1rOHgtxrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA2NjQ3OQ==", "bodyText": "It would be nice to have a concise comment on steps needed to create a file (if we wanted to add some rows to it later on).", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r504066479", "createdAt": "2020-10-13T15:53:03Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,209 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.common.io.Resources;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HivePageSourceFactory;\n+import io.prestosql.plugin.hive.HivePageSourceFactory.ReaderPageSourceWithProjections;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.HiveTimestampPrecision;\n+import io.prestosql.plugin.hive.HiveType;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.time.LocalDateTime;\n+import java.time.ZoneId;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static io.prestosql.plugin.hive.HiveColumnHandle.ColumnType.REGULAR;\n+import static io.prestosql.plugin.hive.HiveColumnHandle.createBaseColumn;\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_TIMESTAMP;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static io.prestosql.spi.type.TimestampType.createTimestampType;\n+import static io.prestosql.spi.type.TimestampWithTimeZoneType.createTimestampWithTimeZoneType;\n+import static io.prestosql.testing.MaterializedResult.materializeSourceDataStream;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_LIB;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+                for (int i = 0; i < firstPage.getPositionCount(); i++) {\n+                    expectedValues.next();\n+                }\n+\n+                while (!pageSource.isFinished()) {\n+                    Page page = pageSource.getNextPage();\n+                    if (page == null) {\n+                        continue;\n+                    }\n+                    Block block = page.getBlock(0);\n+\n+                    for (int i = 0; i < block.getPositionCount(); i++) {\n+                        assertEquals(TIMESTAMP_MILLIS.getObjectValue(session, block, i), expectedValues.next());\n+                    }\n+                }\n+            }\n+\n+            assertFalse(expectedValues.hasNext());\n+        }\n+    }\n+\n+    @Test(dataProvider = \"testTimestampMicrosDataProvider\")\n+    public void testTimestampMicros(HiveTimestampPrecision timestampPrecision, LocalDateTime expected)\n+            throws Exception\n+    {\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setTimestampPrecision(timestampPrecision));\n+\n+        File parquetFile = new File(Resources.getResource(\"issue-5483.parquet\").toURI());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 144}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3NTkzMjYw", "url": "https://github.com/trinodb/trino/pull/5538#pullrequestreview-507593260", "createdAt": "2020-10-13T15:57:40Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNTo1Nzo0MFrOHgt_6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNTo1OTo1OFrOHguGXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA3MDEyMg==", "bodyText": "Since this doesn't really represent nanosOfSecond (per comment below), I'd call it differently. Something like nanoAdjustment (in line with the name of the argument to ofEpochSecond)", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r504070122", "createdAt": "2020-10-13T15:57:40Z", "author": {"login": "martint"}, "path": "presto-spi/src/main/java/io/prestosql/spi/type/SqlTimestampWithTimeZone.java", "diffHunk": "@@ -129,4 +136,18 @@ public int hashCode()\n     {\n         return Objects.hash(precision, epochMillis, picosOfMilli, timeZoneKey);\n     }\n+\n+    /**\n+     * @return timestamp with time zone rounded to nanosecond precision\n+     */\n+    public ZonedDateTime toZonedDateTime()\n+    {\n+        long epochSecond = floorDiv(epochMillis, MILLISECONDS_PER_SECOND);\n+        int nanosOfSecond = floorMod(epochMillis, MILLISECONDS_PER_SECOND) * NANOSECONDS_PER_MILLISECOND +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA3MTc3NQ==", "bodyText": "Add tests for negative epoch where it has to round up and down. That's typically where any bugs with handling negative epochs can be observed.", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r504071775", "createdAt": "2020-10-13T15:59:58Z", "author": {"login": "martint"}, "path": "presto-spi/src/test/java/io/prestosql/spi/type/TestSqlTimestampWithTimeZone.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.spi.type;\n+\n+import org.testng.annotations.Test;\n+\n+import java.time.ZonedDateTime;\n+\n+import static io.prestosql.spi.type.TimeZoneKey.UTC_KEY;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestSqlTimestampWithTimeZone\n+{\n+    @Test\n+    public void testToZonedDateTime()\n+    {\n+        assertEquals(\n+                new SqlTimestampWithTimeZone(9, 1234567890123L, 123_000_000, UTC_KEY).toZonedDateTime(),\n+                ZonedDateTime.parse(\"2009-02-13T23:31:30.123123Z[UTC]\"));\n+\n+        // negative epoch\n+        assertEquals(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA4MTMxMzU0", "url": "https://github.com/trinodb/trino/pull/5538#pullrequestreview-508131354", "createdAt": "2020-10-14T08:52:25Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1MjoyNVrOHhI7jA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1MjoyNVrOHhI7jA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxMTM3Mg==", "bodyText": "@martint 's #5528 change the class name, so i will pick a different name, and un-depend the PRs", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r504511372", "createdAt": "2020-10-14T08:52:25Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,209 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.common.io.Resources;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HivePageSourceFactory;\n+import io.prestosql.plugin.hive.HivePageSourceFactory.ReaderPageSourceWithProjections;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.HiveTimestampPrecision;\n+import io.prestosql.plugin.hive.HiveType;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.time.LocalDateTime;\n+import java.time.ZoneId;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static io.prestosql.plugin.hive.HiveColumnHandle.ColumnType.REGULAR;\n+import static io.prestosql.plugin.hive.HiveColumnHandle.createBaseColumn;\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_TIMESTAMP;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static io.prestosql.spi.type.TimestampType.createTimestampType;\n+import static io.prestosql.spi.type.TimestampWithTimeZoneType.createTimestampWithTimeZoneType;\n+import static io.prestosql.testing.MaterializedResult.materializeSourceDataStream;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_LIB;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 76}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "989b8d952d02c395bbb9a1597ed779805ac8446a", "author": {"user": {"login": "findepi", "name": "Piotr Findeisen"}}, "url": "https://github.com/trinodb/trino/commit/989b8d952d02c395bbb9a1597ed779805ac8446a", "committedDate": "2020-10-14T08:53:44Z", "message": "Support testing timestamp(>3) with time zone"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA4MTM1MDYy", "url": "https://github.com/trinodb/trino/pull/5538#pullrequestreview-508135062", "createdAt": "2020-10-14T08:56:53Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1Njo1M1rOHhJHcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1Njo1M1rOHhJHcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxNDQxOA==", "bodyText": "I picked the test class name in line with my suggestion over here: https://github.com/prestosql/presto/pull/5528/files#r504512999", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r504514418", "createdAt": "2020-10-14T08:56:53Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestTimestampMicros.java", "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.io.Resources;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HivePageSourceFactory;\n+import io.prestosql.plugin.hive.HivePageSourceFactory.ReaderPageSourceWithProjections;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.HiveTimestampPrecision;\n+import io.prestosql.plugin.hive.HiveType;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.time.LocalDateTime;\n+import java.time.ZoneId;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static io.prestosql.plugin.hive.HiveColumnHandle.ColumnType.REGULAR;\n+import static io.prestosql.plugin.hive.HiveColumnHandle.createBaseColumn;\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_TIMESTAMP;\n+import static io.prestosql.spi.type.TimestampType.createTimestampType;\n+import static io.prestosql.spi.type.TimestampWithTimeZoneType.createTimestampWithTimeZoneType;\n+import static io.prestosql.testing.MaterializedResult.materializeSourceDataStream;\n+import static org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_LIB;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestTimestampMicros", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f1546a5c36a7b07578ba40e9aad1c75a073578ad", "author": {"user": {"login": "findepi", "name": "Piotr Findeisen"}}, "url": "https://github.com/trinodb/trino/commit/f1546a5c36a7b07578ba40e9aad1c75a073578ad", "committedDate": "2020-10-14T08:58:15Z", "message": "Add support for MICROS parquet timestamp encoding"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "f1546a5c36a7b07578ba40e9aad1c75a073578ad", "author": {"user": {"login": "findepi", "name": "Piotr Findeisen"}}, "url": "https://github.com/trinodb/trino/commit/f1546a5c36a7b07578ba40e9aad1c75a073578ad", "committedDate": "2020-10-14T08:58:15Z", "message": "Add support for MICROS parquet timestamp encoding"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3436, "cost": 1, "resetAt": "2021-10-28T20:13:43Z"}}}