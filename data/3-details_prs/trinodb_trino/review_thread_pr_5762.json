{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTEzMzY0ODA4", "number": 5762, "reviewThreads": {"totalCount": 28, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzowNzowNlrOE0BW6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNToxNzozMVrOE02o3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTgzNjU3OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzowNzowNlrOHrljEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzowNzowNlrOHrljEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NjAwMA==", "bodyText": "Change commit message to\nOptimize Kafka timestamp pushdown test case", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515466000", "createdAt": "2020-10-31T07:07:06Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -15,39 +15,46 @@\n \n import com.google.common.collect.ImmutableMap;\n import com.google.common.util.concurrent.Futures;\n+import io.airlift.log.Logger;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTgzOTE1OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoxMToxNFrOHrlkNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoxMToxNFrOHrlkNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NjI5Mg==", "bodyText": "compute dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult) on a variable and use for both assertions", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515466292", "createdAt": "2020-10-31T07:11:14Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTg0MTQwOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoxNDozNVrOHrllNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoxNDozNVrOHrllNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NjU1MA==", "bodyText": "static import format", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515466550", "createdAt": "2020-10-31T07:14:35Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getDumpMessages().size(); i++) {\n+            String dumpMessage = recordMessage.getDumpMessages().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+        sb.append(String.format(\"dump test sql:%s\\n\", sql));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 128}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTg0MzA0OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoxNzoxNlrOHrll9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoxNzoxNlrOHrll9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2Njc0Mg==", "bodyText": "Drop dump prefix from dump section headers.\nIt seems 5 is also not needed here.\nAlso I think it would be more readable if capture the result int List<String> and use Joiner.on(\"/n\").join(....) at the end.\nSome vertical whitespace would make this method a bit readable too.", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515466742", "createdAt": "2020-10-31T07:17:16Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTg0MzQxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoxODoyNVrOHrlmJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoxODoyNVrOHrlmJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2Njc5MQ==", "bodyText": "maybe buildDebugDumpString", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515466791", "createdAt": "2020-10-31T07:18:25Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTg0MzYxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoxODo1NFrOHrlmPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoxODo1NFrOHrlmPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NjgxNA==", "bodyText": "I think you can inline this one.", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515466814", "createdAt": "2020-10-31T07:18:54Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getDumpMessages().size(); i++) {\n+            String dumpMessage = recordMessage.getDumpMessages().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+        sb.append(String.format(\"dump test sql:%s\\n\", sql));\n+        sb.append(\"dump test sql result:\").append(\"\\n\").append(dumpResult(queryResult.getResult()));\n+        sql = String.format(\"SELECT _partition_id,_partition_offset,_timestamp FROM default.%s WHERE _partition_offset between %s and %s order by _partition_id, _timestamp\",\n+                topic, recordMessage.getStartOffset(), recordMessage.getStartOffset() + MESSAGE_NUM);\n+        MaterializedResult rows = queryRunner.execute(getSession(), sql);\n+        sb.append(\"dump data check result:\").append(\"\\n\").append(dumpResult(rows)).append(\"\\n\");\n+        return sb.toString();\n+    }\n+\n+    private String dumpResult(MaterializedResult rows)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTg0NDk1OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyMDo0MVrOHrlm0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyMDo0MVrOHrlm0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2Njk2MA==", "bodyText": "I don't understand it. What does it mean?", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515466960", "createdAt": "2020-10-31T07:20:41Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getDumpMessages().size(); i++) {\n+            String dumpMessage = recordMessage.getDumpMessages().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+        sb.append(String.format(\"dump test sql:%s\\n\", sql));\n+        sb.append(\"dump test sql result:\").append(\"\\n\").append(dumpResult(queryResult.getResult()));\n+        sql = String.format(\"SELECT _partition_id,_partition_offset,_timestamp FROM default.%s WHERE _partition_offset between %s and %s order by _partition_id, _timestamp\",\n+                topic, recordMessage.getStartOffset(), recordMessage.getStartOffset() + MESSAGE_NUM);\n+        MaterializedResult rows = queryRunner.execute(getSession(), sql);\n+        sb.append(\"dump data check result:\").append(\"\\n\").append(dumpResult(rows)).append(\"\\n\");\n+        return sb.toString();\n+    }\n+\n+    private String dumpResult(MaterializedResult rows)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        for (MaterializedRow row : rows) {\n+            sb.append(String.format(\"  dump values:%s\\n\", row.toString()));\n+        }\n+        return sb.toString();\n     }\n \n     private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n     {\n         return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n     }\n \n-    private Pair<String, String> createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n+    private RecordMessage createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n     {\n         String startTime = null;\n         String endTime = null;\n+        long startOffset = -1;\n+        List<String> dumpMsgs = new ArrayList<>();\n         Future<RecordMetadata> lastSendFuture = Futures.immediateFuture(null);\n         long lastTimeStamp = -1;\n+        // keep silence for start of multi-revoke", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 160}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTg0NjI2OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyMzowMVrOHrlnYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyNjo1NVrOHrlofA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzEwNw==", "bodyText": "name variable timestampTestMessageSignatures", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467107", "createdAt": "2020-10-31T07:23:01Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getDumpMessages().size(); i++) {\n+            String dumpMessage = recordMessage.getDumpMessages().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+        sb.append(String.format(\"dump test sql:%s\\n\", sql));\n+        sb.append(\"dump test sql result:\").append(\"\\n\").append(dumpResult(queryResult.getResult()));\n+        sql = String.format(\"SELECT _partition_id,_partition_offset,_timestamp FROM default.%s WHERE _partition_offset between %s and %s order by _partition_id, _timestamp\",\n+                topic, recordMessage.getStartOffset(), recordMessage.getStartOffset() + MESSAGE_NUM);\n+        MaterializedResult rows = queryRunner.execute(getSession(), sql);\n+        sb.append(\"dump data check result:\").append(\"\\n\").append(dumpResult(rows)).append(\"\\n\");\n+        return sb.toString();\n+    }\n+\n+    private String dumpResult(MaterializedResult rows)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        for (MaterializedRow row : rows) {\n+            sb.append(String.format(\"  dump values:%s\\n\", row.toString()));\n+        }\n+        return sb.toString();\n     }\n \n     private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n     {\n         return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n     }\n \n-    private Pair<String, String> createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n+    private RecordMessage createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n     {\n         String startTime = null;\n         String endTime = null;\n+        long startOffset = -1;\n+        List<String> dumpMsgs = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzM4OA==", "bodyText": "Also use ImmutableList.Builder for collection", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467388", "createdAt": "2020-10-31T07:26:55Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getDumpMessages().size(); i++) {\n+            String dumpMessage = recordMessage.getDumpMessages().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+        sb.append(String.format(\"dump test sql:%s\\n\", sql));\n+        sb.append(\"dump test sql result:\").append(\"\\n\").append(dumpResult(queryResult.getResult()));\n+        sql = String.format(\"SELECT _partition_id,_partition_offset,_timestamp FROM default.%s WHERE _partition_offset between %s and %s order by _partition_id, _timestamp\",\n+                topic, recordMessage.getStartOffset(), recordMessage.getStartOffset() + MESSAGE_NUM);\n+        MaterializedResult rows = queryRunner.execute(getSession(), sql);\n+        sb.append(\"dump data check result:\").append(\"\\n\").append(dumpResult(rows)).append(\"\\n\");\n+        return sb.toString();\n+    }\n+\n+    private String dumpResult(MaterializedResult rows)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        for (MaterializedRow row : rows) {\n+            sb.append(String.format(\"  dump values:%s\\n\", row.toString()));\n+        }\n+        return sb.toString();\n     }\n \n     private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n     {\n         return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n     }\n \n-    private Pair<String, String> createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n+    private RecordMessage createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n     {\n         String startTime = null;\n         String endTime = null;\n+        long startOffset = -1;\n+        List<String> dumpMsgs = new ArrayList<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzEwNw=="}, "originalCommit": null, "originalPosition": 157}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTg0NjQzOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyMzoyN1rOHrlndQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyMzoyN1rOHrlndQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzEyNQ==", "bodyText": "testMessageSignatures", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467125", "createdAt": "2020-10-31T07:23:27Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -219,4 +278,64 @@ private void createMessages(String topicName)\n         }\n         lastSendFuture.get();\n     }\n+\n+    private static class RecordMessage\n+    {\n+        private final String startTime;\n+        private final String endTime;\n+        /**\n+         * Record first Offset for dump\n+         */\n+        private final Long startOffset;\n+        /**\n+         * Record 5 expected messages for dump\n+         */\n+        private final List<String> dumpMessages;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 215}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTg0NzI3OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyNDo0MFrOHrln0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyNDo0MFrOHrln0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzIxOQ==", "bodyText": "long?", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467219", "createdAt": "2020-10-31T07:24:40Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -219,4 +278,64 @@ private void createMessages(String topicName)\n         }\n         lastSendFuture.get();\n     }\n+\n+    private static class RecordMessage\n+    {\n+        private final String startTime;\n+        private final String endTime;\n+        /**\n+         * Record first Offset for dump\n+         */\n+        private final Long startOffset;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 211}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTg0NzQxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyNDo1M1rOHrln4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyNDo1M1rOHrln4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzIzNA==", "bodyText": "I think this comments does not help. Remove maybe.", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467234", "createdAt": "2020-10-31T07:24:53Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -219,4 +278,64 @@ private void createMessages(String topicName)\n         }\n         lastSendFuture.get();\n     }\n+\n+    private static class RecordMessage\n+    {\n+        private final String startTime;\n+        private final String endTime;\n+        /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 208}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTg0ODE0OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyNTo1N1rOHrloMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0NTo1N1rOHr66Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzMxMg==", "bodyText": "Replace with single remainingRetries", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467312", "createdAt": "2020-10-31T07:25:57Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -219,4 +278,64 @@ private void createMessages(String topicName)\n         }\n         lastSendFuture.get();\n     }\n+\n+    private static class RecordMessage\n+    {\n+        private final String startTime;\n+        private final String endTime;\n+        /**\n+         * Record first Offset for dump\n+         */\n+        private final Long startOffset;\n+        /**\n+         * Record 5 expected messages for dump\n+         */\n+        private final List<String> dumpMessages;\n+\n+        public RecordMessage(String startTime, String endTime, Long startOffset, List<String> dumpMessages)\n+        {\n+            this.startTime = startTime;\n+            this.endTime = endTime;\n+            this.startOffset = startOffset;\n+            this.dumpMessages = dumpMessages;\n+        }\n+\n+        public String getStartTime()\n+        {\n+            return startTime;\n+        }\n+\n+        public String getEndTime()\n+        {\n+            return endTime;\n+        }\n+\n+        public Long getStartOffset()\n+        {\n+            return startOffset;\n+        }\n+\n+        public List<String> getDumpMessages()\n+        {\n+            return dumpMessages;\n+        }\n+    }\n+\n+    private static class RetryAnalyzerForTimestampTest\n+            implements IRetryAnalyzer\n+    {\n+        int count = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 249}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxNTkzOA==", "bodyText": "I do not see this change addressed", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515815938", "createdAt": "2020-11-02T08:45:57Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -219,4 +278,64 @@ private void createMessages(String topicName)\n         }\n         lastSendFuture.get();\n     }\n+\n+    private static class RecordMessage\n+    {\n+        private final String startTime;\n+        private final String endTime;\n+        /**\n+         * Record first Offset for dump\n+         */\n+        private final Long startOffset;\n+        /**\n+         * Record 5 expected messages for dump\n+         */\n+        private final List<String> dumpMessages;\n+\n+        public RecordMessage(String startTime, String endTime, Long startOffset, List<String> dumpMessages)\n+        {\n+            this.startTime = startTime;\n+            this.endTime = endTime;\n+            this.startOffset = startOffset;\n+            this.dumpMessages = dumpMessages;\n+        }\n+\n+        public String getStartTime()\n+        {\n+            return startTime;\n+        }\n+\n+        public String getEndTime()\n+        {\n+            return endTime;\n+        }\n+\n+        public Long getStartOffset()\n+        {\n+            return startOffset;\n+        }\n+\n+        public List<String> getDumpMessages()\n+        {\n+            return dumpMessages;\n+        }\n+    }\n+\n+    private static class RetryAnalyzerForTimestampTest\n+            implements IRetryAnalyzer\n+    {\n+        int count = 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzMxMg=="}, "originalCommit": null, "originalPosition": 249}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTg0ODQxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyNjoyMVrOHrloTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyNjoyMVrOHrloTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzM0Mg==", "bodyText": "Rename to FixedCountRetryAnalyzer", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467342", "createdAt": "2020-10-31T07:26:21Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -219,4 +278,64 @@ private void createMessages(String topicName)\n         }\n         lastSendFuture.get();\n     }\n+\n+    private static class RecordMessage\n+    {\n+        private final String startTime;\n+        private final String endTime;\n+        /**\n+         * Record first Offset for dump\n+         */\n+        private final Long startOffset;\n+        /**\n+         * Record 5 expected messages for dump\n+         */\n+        private final List<String> dumpMessages;\n+\n+        public RecordMessage(String startTime, String endTime, Long startOffset, List<String> dumpMessages)\n+        {\n+            this.startTime = startTime;\n+            this.endTime = endTime;\n+            this.startOffset = startOffset;\n+            this.dumpMessages = dumpMessages;\n+        }\n+\n+        public String getStartTime()\n+        {\n+            return startTime;\n+        }\n+\n+        public String getEndTime()\n+        {\n+            return endTime;\n+        }\n+\n+        public Long getStartOffset()\n+        {\n+            return startOffset;\n+        }\n+\n+        public List<String> getDumpMessages()\n+        {\n+            return dumpMessages;\n+        }\n+    }\n+\n+    private static class RetryAnalyzerForTimestampTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 246}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTg0OTY0OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyOToxMFrOHrlo6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyOToxMFrOHrlo6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzQ5Ng==", "bodyText": "If you are breaking parameter list then put each parameter in separate line please.", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467496", "createdAt": "2020-10-31T07:29:10Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getDumpMessages().size(); i++) {\n+            String dumpMessage = recordMessage.getDumpMessages().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+        sb.append(String.format(\"dump test sql:%s\\n\", sql));\n+        sb.append(\"dump test sql result:\").append(\"\\n\").append(dumpResult(queryResult.getResult()));\n+        sql = String.format(\"SELECT _partition_id,_partition_offset,_timestamp FROM default.%s WHERE _partition_offset between %s and %s order by _partition_id, _timestamp\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTg0OTk0OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyOToxOVrOHrlpBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwNzoyOToxOVrOHrlpBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2NzUyNg==", "bodyText": "inline", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515467526", "createdAt": "2020-10-31T07:29:19Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -132,51 +139,95 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = RetryAnalyzerForTimestampTest.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n         String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpResultAndData(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult))\n+                .isEqualTo(2);\n+    }\n+\n+    private String dumpResultAndData(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"dump 5 expected messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getDumpMessages().size(); i++) {\n+            String dumpMessage = recordMessage.getDumpMessages().get(i);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjUzMDg0OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODozODo0M1rOHr6qZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODozODo0M1rOHr6qZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxMTk0MQ==", "bodyText": "drop dot from commit message\nWe roughly try to follow https://chris.beams.io/posts/git-commit/", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515811941", "createdAt": "2020-11-02T08:38:43Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -13,41 +13,49 @@\n  */\n package io.prestosql.plugin.kafka;\n \n+import com.google.common.collect.ImmutableList;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjU0MzMwOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0MTo1M1rOHr6xbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0MTo1M1rOHr6xbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxMzc0Mg==", "bodyText": "s/dumpMsg/debugDumptString", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515813742", "createdAt": "2020-11-02T08:41:53Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -113,70 +121,124 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n     {\n         createMessages(topicNameOffset);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n                 topicNameOffset);\n \n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        String dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjU0NDMwOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0MjowOVrOHr6yBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0MjowOVrOHr6yBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxMzg5Mg==", "bodyText": "s/dempMsg/debugDumpString", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515813892", "createdAt": "2020-11-02T08:42:09Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -113,70 +121,124 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n     {\n         createMessages(topicNameOffset);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n                 topicNameOffset);\n \n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        String dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        String dumpMsg = buildDebugDumpString(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjU0NzI2OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0MzowMlrOHr6zuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0MzowMlrOHr6zuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxNDMyOA==", "bodyText": "nit: this signature seems long enough to justify putting each argument in separate line", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515814328", "createdAt": "2020-11-02T08:43:02Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -113,70 +121,124 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n     {\n         createMessages(topicNameOffset);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n                 topicNameOffset);\n \n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        String dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        String dumpMsg = buildDebugDumpString(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n+    }\n+\n+    private String buildDebugDumpString(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjU1MzIxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0NDozOFrOHr63RA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0NDozOFrOHr63RA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxNTIzNg==", "bodyText": "drop \"dump \" just: \"SQL results:%s\"", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515815236", "createdAt": "2020-11-02T08:44:38Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -113,70 +121,124 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n     {\n         createMessages(topicNameOffset);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n                 topicNameOffset);\n \n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        String dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        String dumpMsg = buildDebugDumpString(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n+    }\n+\n+    private String buildDebugDumpString(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        // dump main messages\n+        sb.append(\"Main messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getTestMessageSignatures().size(); i++) {\n+            String dumpMessage = recordMessage.getTestMessageSignatures().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+\n+        // dump sql and result\n+        sb.append(format(\"dump test sql:%s\\n\", sql));\n+        sb.append(\"dump test sql result:\").append(\"\\n\").append(dumpSqlResult(queryResult.getResult()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 171}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjU1NDYzOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0NTowM1rOHr64HQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0NTowM1rOHr64HQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxNTQ1Mw==", "bodyText": "drop \"dump \" just \"Test SQL:%s\"", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515815453", "createdAt": "2020-11-02T08:45:03Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -113,70 +121,124 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n     {\n         createMessages(topicNameOffset);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n                 topicNameOffset);\n \n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        String dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        String dumpMsg = buildDebugDumpString(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n+    }\n+\n+    private String buildDebugDumpString(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        // dump main messages\n+        sb.append(\"Main messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getTestMessageSignatures().size(); i++) {\n+            String dumpMessage = recordMessage.getTestMessageSignatures().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+\n+        // dump sql and result\n+        sb.append(format(\"dump test sql:%s\\n\", sql));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 170}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjU2NzEwOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0ODo0NFrOHr6_iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0ODo0NFrOHr6_iA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxNzM1Mg==", "bodyText": "Please update this comment to be more descriptive here too", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515817352", "createdAt": "2020-11-02T08:48:44Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -196,14 +262,18 @@ else if (messageNum == TIMESTAMP_TEST_END_INDEX) {\n                                 .format(LocalDateTime.ofInstant(Instant.ofEpochMilli(r.timestamp()), ZoneId.of(\"UTC\")));\n                     }\n                     // Sleep for a while to ensure different timestamps for different messages..\n-                    Thread.sleep(20);\n+                    Thread.sleep(100);\n+                }\n+                else if (messageNum == TIMESTAMP_TEST_COUNT) {\n+                    // Keep silence to record other messages.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 232}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjU3MDY4OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0OTo0N1rOHr7BpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo0OTo0N1rOHr7BpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxNzg5Mg==", "bodyText": "maybe just testMessageSignatures", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515817892", "createdAt": "2020-11-02T08:49:47Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -187,7 +249,11 @@ private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQue\n                     RecordMetadata r = lastSendFuture.get();\n                     assertTrue(lastTimeStamp != r.timestamp());\n                     lastTimeStamp = r.timestamp();\n-                    if (messageNum == TIMESTAMP_TEST_START_INDEX) {\n+                    timestampTestMessageSignatures.add(format(\"timestamp:%s: partitionId:%s, offset:%s\", r.timestamp(), r.partition(), r.offset()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 216}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMjU5MDMyOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo1NToyM1rOHr7NWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwODo1NToyM1rOHr7NWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgyMDg5MQ==", "bodyText": "I suggested to inline this one. If you insist on keeping it separate please rename to  buildResultsDebugDumpString", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r515820891", "createdAt": "2020-11-02T08:55:23Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -113,70 +121,124 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n     {\n         createMessages(topicNameOffset);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n                 topicNameOffset);\n \n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        String dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        dumpMsg = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        String dumpMsg = buildDebugDumpString(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", dumpMsg)\n+                .isEqualTo(2);\n+    }\n+\n+    private String buildDebugDumpString(String topic, RecordMessage recordMessage, DistributedQueryRunner queryRunner, String sql, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        // dump main messages\n+        sb.append(\"Main messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getTestMessageSignatures().size(); i++) {\n+            String dumpMessage = recordMessage.getTestMessageSignatures().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+\n+        // dump sql and result\n+        sb.append(format(\"dump test sql:%s\\n\", sql));\n+        sb.append(\"dump test sql result:\").append(\"\\n\").append(dumpSqlResult(queryResult.getResult()));\n+\n+        // dump data in kafka\n+        sql = format(\"SELECT _partition_id,_partition_offset,_timestamp FROM default.%s WHERE _partition_offset between %s and %s order by _partition_id, _timestamp\",\n+                topic,\n+                recordMessage.getStartOffset(),\n+                recordMessage.getStartOffset() + MESSAGE_NUM);\n+        MaterializedResult rows = queryRunner.execute(getSession(), sql);\n+        sb.append(\"dump data check result:\").append(\"\\n\").append(dumpSqlResult(rows)).append(\"\\n\");\n+        return sb.toString();\n+    }\n+\n+    private String dumpSqlResult(MaterializedResult rows)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 183}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzODU1Nzk1OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNToxNTo1MFrOHszf9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNToxNTo1MFrOHszf9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc0MzE1Nw==", "bodyText": "There is still dot at the end of commit message :)", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r516743157", "createdAt": "2020-11-03T15:15:50Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -13,41 +13,49 @@\n  */", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzODU2MTMzOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNToxNjozMlrOHsziHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNToxNjozMlrOHsziHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc0MzcwOA==", "bodyText": "I suggested to rename this one to testMethodSignatures for brevity.", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r516743708", "createdAt": "2020-11-03T15:16:32Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -113,70 +121,128 @@ public void testOffsetPushDown() throws ExecutionException, InterruptedException\n     {\n         createMessages(topicNameOffset);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n                 topicNameOffset);\n \n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n \n-        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+        sql = format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n                 topicNameOffset);\n \n         queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n         assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameCreateTime);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameCreateTime, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, recordMessage.getStartTime(), recordMessage.getEndTime());\n \n         // timestamp_upper_bound_force_push_down_enabled default as false.\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+        String debugDumpString = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", debugDumpString)\n+                .isEqualTo(998);\n \n         // timestamp_upper_bound_force_push_down_enabled set as true.\n         Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n                 .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n                 .build();\n \n         queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+        debugDumpString = buildDebugDumpString(topicNameCreateTime, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", debugDumpString)\n+                .isEqualTo(2);\n     }\n \n-    @Test\n+    @Test(retryAnalyzer = FixedCountRetryAnalyzer.class)\n     public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n     {\n-        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        RecordMessage recordMessage = createTimestampTestMessages(topicNameLogAppend);\n         DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n         // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n-        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n-                topicNameLogAppend, timePair.first(), timePair.second());\n+        String sql = format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameLogAppend, recordMessage.getStartTime(), recordMessage.getEndTime());\n         ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n-        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+\n+        String debugDumpString = buildDebugDumpString(topicNameLogAppend, recordMessage, queryRunner, sql, queryResult);\n+        assertThat(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions())\n+                .describedAs(\"with dump:\\n%s\", debugDumpString)\n+                .isEqualTo(2);\n+    }\n+\n+    private String buildDebugDumpString(String topic,\n+                                        RecordMessage recordMessage,\n+                                        DistributedQueryRunner queryRunner,\n+                                        String sql,\n+                                        ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        // dump main messages\n+        sb.append(\"Main messages:\").append(\"\\n\");\n+        for (int i = 0; i < recordMessage.getTestMessageSignatures().size(); i++) {\n+            String dumpMessage = recordMessage.getTestMessageSignatures().get(i);\n+            sb.append(\"  \").append(dumpMessage);\n+            if (i == TIMESTAMP_TEST_START_INDEX) {\n+                sb.append(\", startTime:\").append(recordMessage.getStartTime());\n+            }\n+            else if (i == TIMESTAMP_TEST_END_INDEX) {\n+                sb.append(\", endTime:\").append(recordMessage.getEndTime());\n+            }\n+            sb.append(\"\\n\");\n+        }\n+\n+        // dump sql and result\n+        sb.append(format(\"test sql:%s\\n\", sql));\n+        sb.append(\"test sql result:\").append(\"\\n\").append(buildResultsDebugDumpString(queryResult.getResult()));\n+\n+        // dump data in kafka\n+        sql = format(\"SELECT _partition_id,_partition_offset,_timestamp FROM default.%s WHERE _partition_offset between %s and %s order by _partition_id, _timestamp\",\n+                topic,\n+                recordMessage.getStartOffset(),\n+                recordMessage.getStartOffset() + MESSAGE_NUM);\n+        MaterializedResult rows = queryRunner.execute(getSession(), sql);\n+        sb.append(\"data check result:\").append(\"\\n\").append(buildResultsDebugDumpString(rows)).append(\"\\n\");\n+        return sb.toString();\n+    }\n+\n+    private String buildResultsDebugDumpString(MaterializedResult rows)\n+    {\n+        StringBuilder sb = new StringBuilder();\n+        for (MaterializedRow row : rows) {\n+            sb.append(format(\"  dump values:%s\\n\", row.toString()));\n+        }\n+        return sb.toString();\n     }\n \n     private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n     {\n         return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n     }\n \n-    private Pair<String, String> createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n+    private RecordMessage createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n     {\n         String startTime = null;\n         String endTime = null;\n+        long startOffset = -1;\n+        ImmutableList.Builder<String> timestampTestMessageSignatures = ImmutableList.builder();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 207}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzODU2MzUyOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNToxNjo1OFrOHszjbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNToxNjo1OFrOHszjbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc0NDA0NQ==", "bodyText": "I suggested to keep just one field remainingRetries here.", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r516744045", "createdAt": "2020-11-03T15:16:58Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -219,4 +293,58 @@ private void createMessages(String topicName)\n         }\n         lastSendFuture.get();\n     }\n+\n+    private static class RecordMessage\n+    {\n+        private final String startTime;\n+        private final String endTime;\n+        private final long startOffset;\n+        private final List<String> testMessageSignatures;\n+\n+        public RecordMessage(String startTime, String endTime, Long startOffset, List<String> testMessageSignatures)\n+        {\n+            this.startTime = startTime;\n+            this.endTime = endTime;\n+            this.startOffset = startOffset;\n+            this.testMessageSignatures = testMessageSignatures;\n+        }\n+\n+        public String getStartTime()\n+        {\n+            return startTime;\n+        }\n+\n+        public String getEndTime()\n+        {\n+            return endTime;\n+        }\n+\n+        public Long getStartOffset()\n+        {\n+            return startOffset;\n+        }\n+\n+        public List<String> getTestMessageSignatures()\n+        {\n+            return testMessageSignatures;\n+        }\n+    }\n+\n+    private static class FixedCountRetryAnalyzer\n+            implements IRetryAnalyzer\n+    {\n+        int count = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 293}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzODU2NjA3OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNToxNzozMVrOHszk-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNToxNzozMVrOHszk-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc0NDQ0MQ==", "bodyText": "frout? What did you mean here?", "url": "https://github.com/trinodb/trino/pull/5762#discussion_r516744441", "createdAt": "2020-11-03T15:17:31Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -196,14 +266,18 @@ else if (messageNum == TIMESTAMP_TEST_END_INDEX) {\n                                 .format(LocalDateTime.ofInstant(Instant.ofEpochMilli(r.timestamp()), ZoneId.of(\"UTC\")));\n                     }\n                     // Sleep for a while to ensure different timestamps for different messages..\n-                    Thread.sleep(20);\n+                    Thread.sleep(100);\n+                }\n+                else if (messageNum == TIMESTAMP_TEST_COUNT) {\n+                    // Avoid back messages in test has impact frout messages.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 236}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4874, "cost": 1, "resetAt": "2021-11-12T12:57:47Z"}}}