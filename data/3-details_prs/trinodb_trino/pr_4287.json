{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQyMzI2NzY5", "number": 4287, "title": "Implement base functionality for kafka connector inserts", "bodyText": "This is the first of 5 PRs based on #4230\nThis PR focuses on adding code to the kafka connector to support inserts. Something to note is that inserting won't actually work because none of the encoders are included. Each of the four encoders will have their own follow-up PRs after this one gets merged. This PR also lays out the base interfaces and classes for the encoder.\nI have changed how the encoder works since #4230 . Each RowEncoder has a collection field that will store row data until requested as a byte[] by the KafkaPageSink. The page sink will call overloaded put(EncoderColumnHandle, value) methods to add values to the row. Then once the page sink gets the byte[] from the RowEncoder it calls a clear() method that clears the backing collection.\nThis PR is part of the implementation for #3980\nedit: I added the CSV encoder to a separate commit in this pr just to have an encoder that works\nedit: Other encoders are blocked by this pr for now. They can be found at these links\n\nJson Encoder PR - #4477\nRaw Encoder PR - #4417\nAvro Encoder PR - #4418", "createdAt": "2020-06-30T22:20:12Z", "url": "https://github.com/trinodb/trino/pull/4287", "merged": true, "mergeCommit": {"oid": "fa99b33f1b5de154a20027db77f8782e0fea6432"}, "closed": true, "closedAt": "2020-07-10T10:45:10Z", "author": {"login": "charlesjmorgan"}, "timelineItems": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcwehlegBqjM1MDAwNjcwMjY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABczhUCBAFqTQ0NjMwNDg2OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwNjEzNzY0", "url": "https://github.com/trinodb/trino/pull/4287#pullrequestreview-440613764", "createdAt": "2020-07-01T07:29:54Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 26, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwNzoyOTo1NFrOGraJNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwNzo0Njo0NlrOGrarqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MDI5NA==", "bodyText": "Do we need to pass serializers here? could this be internal implementation detail of KafkaProducerFactory? You could change KafkaProducerFactory to always create KafkaProducer<byte[], byte[]> (no need for generics).", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448170294", "createdAt": "2020-07-01T07:29:54Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final ConnectorSession session;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            ConnectorSession session,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = (requireNonNull(ImmutableList.copyOf(columns), \"columns is null\"));\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        this.session = requireNonNull(session, \"session is null\");\n+        this.producer = requireNonNull(producerFactory.create(new ByteArraySerializer(), new ByteArraySerializer()), \"producerFactory is null\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MTEwMQ==", "bodyText": "it is not used here.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448171101", "createdAt": "2020-07-01T07:31:21Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MTY4NQ==", "bodyText": "Why is this user error?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448171685", "createdAt": "2020-07-01T07:32:29Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSinkProvider.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.DispatchingRowEncoderFactory;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorInsertTableHandle;\n+import io.prestosql.spi.connector.ConnectorOutputTableHandle;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorPageSinkProvider;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTransactionHandle;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Optional;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaPageSinkProvider\n+        implements ConnectorPageSinkProvider\n+{\n+    private final DispatchingRowEncoderFactory encoderFactory;\n+    private final PlainTextKafkaProducerFactory producerFactory;\n+\n+    @Inject\n+    public KafkaPageSinkProvider(DispatchingRowEncoderFactory encoderFactory, PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.encoderFactory = requireNonNull(encoderFactory, \"encoderFactory is null\");\n+        this.producerFactory = requireNonNull(producerFactory, \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorOutputTableHandle tableHandle)\n+    {\n+        throw new UnsupportedOperationException(\"Table creation is not supported by the kafka connector\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorInsertTableHandle tableHandle)\n+    {\n+        requireNonNull(tableHandle, \"tableHandle is null\");\n+        checkArgument(tableHandle instanceof KafkaTableHandle, \"tableHandle is not an instance of KafkaTableHandle\");\n+        KafkaTableHandle handle = (KafkaTableHandle) tableHandle;\n+\n+        ImmutableSet.Builder<EncoderColumnHandle> keyColumns = ImmutableSet.builder();\n+        ImmutableSet.Builder<EncoderColumnHandle> messageColumns = ImmutableSet.builder();\n+        handle.getColumns().forEach(col -> {\n+            if (col.isInternal()) {\n+                throw new PrestoException(GENERIC_USER_ERROR, \"unexpected internal column\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MTk4Ng==", "bodyText": "I think this check is not needed.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448171986", "createdAt": "2020-07-01T07:33:06Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSinkProvider.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.DispatchingRowEncoderFactory;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorInsertTableHandle;\n+import io.prestosql.spi.connector.ConnectorOutputTableHandle;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorPageSinkProvider;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTransactionHandle;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Optional;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaPageSinkProvider\n+        implements ConnectorPageSinkProvider\n+{\n+    private final DispatchingRowEncoderFactory encoderFactory;\n+    private final PlainTextKafkaProducerFactory producerFactory;\n+\n+    @Inject\n+    public KafkaPageSinkProvider(DispatchingRowEncoderFactory encoderFactory, PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.encoderFactory = requireNonNull(encoderFactory, \"encoderFactory is null\");\n+        this.producerFactory = requireNonNull(producerFactory, \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorOutputTableHandle tableHandle)\n+    {\n+        throw new UnsupportedOperationException(\"Table creation is not supported by the kafka connector\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorInsertTableHandle tableHandle)\n+    {\n+        requireNonNull(tableHandle, \"tableHandle is null\");\n+        checkArgument(tableHandle instanceof KafkaTableHandle, \"tableHandle is not an instance of KafkaTableHandle\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MjcyOQ==", "bodyText": "PlainTextKafkaProducerFactory is not extendable, there is no need to install this module here. Install it in KafkaConnectorFactory in a normal way.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448172729", "createdAt": "2020-07-01T07:34:28Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPlugin.java", "diffHunk": "@@ -25,6 +25,7 @@\n {\n     public static final Module DEFAULT_EXTENSION = binder -> {\n         binder.install(new KafkaConsumerModule());\n+        binder.install(new KafkaProducerModule());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3Mjk1Mw==", "bodyText": "Why do you need this try-catch?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448172953", "createdAt": "2020-07-01T07:34:54Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaProducerModule.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.inject.Binder;\n+import com.google.inject.Module;\n+import com.google.inject.Scopes;\n+import io.prestosql.spi.PrestoException;\n+\n+import java.util.Properties;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+\n+public class KafkaProducerModule\n+        implements Module\n+{\n+    @Override\n+    public void configure(Binder binder)\n+    {\n+        try {\n+            binder.bind(Properties.class).toInstance(new Properties());\n+            binder.bind(PlainTextKafkaProducerFactory.class).toConstructor(PlainTextKafkaProducerFactory.class.getConstructor(KafkaConfig.class, Properties.class)).in(Scopes.SINGLETON);\n+        }\n+        catch (NoSuchMethodException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3MzEzMw==", "bodyText": "Why is it an user error?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448173133", "createdAt": "2020-07-01T07:35:12Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaProducerModule.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.inject.Binder;\n+import com.google.inject.Module;\n+import com.google.inject.Scopes;\n+import io.prestosql.spi.PrestoException;\n+\n+import java.util.Properties;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+\n+public class KafkaProducerModule\n+        implements Module\n+{\n+    @Override\n+    public void configure(Binder binder)\n+    {\n+        try {\n+            binder.bind(Properties.class).toInstance(new Properties());\n+            binder.bind(PlainTextKafkaProducerFactory.class).toConstructor(PlainTextKafkaProducerFactory.class.getConstructor(KafkaConfig.class, Properties.class)).in(Scopes.SINGLETON);\n+        }\n+        catch (NoSuchMethodException e) {\n+            throw new PrestoException(GENERIC_USER_ERROR, e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3Mzk5Mg==", "bodyText": "make sure you are using unique code:\nkokosing@m16:~/presto$ git grep '0x0101_0000'\npresto-record-decoder/src/main/java/io/prestosql/decoder/DecoderErrorCode.java:        errorCode = new ErrorCode(code + 0x0101_0000, name(), type);\n\nWhy do you use io.prestosql.plugin.kafka.KafkaErrorCode?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448173992", "createdAt": "2020-07-01T07:37:01Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/EncoderErrorCode.java", "diffHunk": "@@ -0,0 +1,40 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import io.prestosql.spi.ErrorCode;\n+import io.prestosql.spi.ErrorCodeSupplier;\n+import io.prestosql.spi.ErrorType;\n+\n+import static io.prestosql.spi.ErrorType.EXTERNAL;\n+\n+public enum EncoderErrorCode\n+        implements ErrorCodeSupplier\n+{\n+    /** A requested data conversion is not supported */\n+    ENCODER_CONVERSION_NOT_SUPPORTED(0, EXTERNAL);\n+\n+    private final ErrorCode errorCode;\n+\n+    EncoderErrorCode(int code, ErrorType type)\n+    {\n+        errorCode = new ErrorCode(code + 0x0101_0000, name(), type);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NDI1MQ==", "bodyText": "separate commit", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448174251", "createdAt": "2020-07-01T07:37:32Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -148,7 +148,7 @@ private static DistributedQueryRunner createKafkaQueryRunner(\n             Map<String, String> kafkaProperties = new HashMap<>(ImmutableMap.copyOf(extraKafkaProperties));\n             kafkaProperties.putIfAbsent(\"kafka.nodes\", testingKafka.getConnectString());\n             kafkaProperties.putIfAbsent(\"kafka.connect-timeout\", \"120s\");\n-            kafkaProperties.putIfAbsent(\"kafka.default-schema\", \"default\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NDMwMg==", "bodyText": "separate commit", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448174302", "createdAt": "2020-07-01T07:37:37Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -203,6 +203,7 @@ public static void main(String[] args)\n         Logging.initialize();\n         DistributedQueryRunner queryRunner = builder(new TestingKafka())\n                 .setTables(TpchTable.getTables())\n+                .setExtraProperties(ImmutableMap.of(\"http-server.http.port\", \"8080\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NDY2Ng==", "bodyText": "what is a raw topic name? how does it differ to topic name?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448174666", "createdAt": "2020-07-01T07:38:20Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -44,18 +50,17 @@\n         extends AbstractTestIntegrationSmokeTest\n {\n     private TestingKafka testingKafka;\n-    private String topicName;\n+    private String rawTopicName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NTAwNA==", "bodyText": "You might want to use io.prestosql.testing.TestngUtils#toDataProvider", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448175004", "createdAt": "2020-07-01T07:39:00Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NjEwNw==", "bodyText": "I would inline these", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448176107", "createdAt": "2020-07-01T07:41:03Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NjIzNw==", "bodyText": "roundTripTestSetup -> testCase?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448176237", "createdAt": "2020-07-01T07:41:17Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NjM1Ng==", "bodyText": "RoundTripTestSetup -> RoundTripTestCase?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448176356", "createdAt": "2020-07-01T07:41:34Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NjgxMg==", "bodyText": "please remove this try-catch. IllegalArgumentException is already a runtime exception, also it is test code so there is no need to use PrestoException", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448176812", "createdAt": "2020-07-01T07:42:25Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3Njk0Ng==", "bodyText": "Don't you need to create a topic here?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448176946", "createdAt": "2020-07-01T07:42:41Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3NzUxMg==", "bodyText": "The best would be have simple use case already implemented (possibly in separate commit) to see that all of this actually work.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448177512", "createdAt": "2020-07-01T07:43:48Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3ODA2Mg==", "bodyText": "fieldNames.indexOf(..)?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448178062", "createdAt": "2020-07-01T07:44:50Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3ODE2NQ==", "bodyText": "remove try-catch", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448178165", "createdAt": "2020-07-01T07:45:03Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {\n+            int index = -1;\n+            for (int i = 0; i < fieldNames.size(); i++) {\n+                if (fieldNames.get(i).equals(fieldName)) {\n+                    index = i;\n+                }\n+            }\n+            return index;\n+        }\n+\n+        public String getFieldName(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldNames.get(index);\n+            }\n+            catch (IllegalArgumentException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3ODM1MQ==", "bodyText": "use checkArgument instead", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448178351", "createdAt": "2020-07-01T07:45:26Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {\n+            int index = -1;\n+            for (int i = 0; i < fieldNames.size(); i++) {\n+                if (fieldNames.get(i).equals(fieldName)) {\n+                    index = i;\n+                }\n+            }\n+            return index;\n+        }\n+\n+        public String getFieldName(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldNames.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getFieldName(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldNames.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3ODU5Mg==", "bodyText": "String.join()?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448178592", "createdAt": "2020-07-01T07:45:53Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {\n+            int index = -1;\n+            for (int i = 0; i < fieldNames.size(); i++) {\n+                if (fieldNames.get(i).equals(fieldName)) {\n+                    index = i;\n+                }\n+            }\n+            return index;\n+        }\n+\n+        public String getFieldName(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldNames.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getFieldName(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldNames.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));\n+            }\n+        }\n+\n+        public String getFieldNames()\n+        {\n+            StringBuilder sb = new StringBuilder();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3ODcwOQ==", "bodyText": "remove try-catch", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448178709", "createdAt": "2020-07-01T07:46:08Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {\n+            int index = -1;\n+            for (int i = 0; i < fieldNames.size(); i++) {\n+                if (fieldNames.get(i).equals(fieldName)) {\n+                    index = i;\n+                }\n+            }\n+            return index;\n+        }\n+\n+        public String getFieldName(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldNames.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getFieldName(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldNames.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));\n+            }\n+        }\n+\n+        public String getFieldNames()\n+        {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(getFieldName(0));\n+            for (int i = 1; i < length; i++) {\n+                sb.append(\", \");\n+                sb.append(fieldNames.get(i));\n+            }\n+            return sb.toString();\n+        }\n+\n+        public Object getFieldValue(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldValues.get(index);\n+            }\n+            catch (IllegalArgumentException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3ODgxNA==", "bodyText": "use checkArgument instead", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448178814", "createdAt": "2020-07-01T07:46:20Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {\n+            int index = -1;\n+            for (int i = 0; i < fieldNames.size(); i++) {\n+                if (fieldNames.get(i).equals(fieldName)) {\n+                    index = i;\n+                }\n+            }\n+            return index;\n+        }\n+\n+        public String getFieldName(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldNames.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getFieldName(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldNames.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));\n+            }\n+        }\n+\n+        public String getFieldNames()\n+        {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(getFieldName(0));\n+            for (int i = 1; i < length; i++) {\n+                sb.append(\", \");\n+                sb.append(fieldNames.get(i));\n+            }\n+            return sb.toString();\n+        }\n+\n+        public Object getFieldValue(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldValues.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public Object getFieldValue(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldValues.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3ODkxOQ==", "bodyText": "String.join()?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448178919", "createdAt": "2020-07-01T07:46:30Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {\n+            int index = -1;\n+            for (int i = 0; i < fieldNames.size(); i++) {\n+                if (fieldNames.get(i).equals(fieldName)) {\n+                    index = i;\n+                }\n+            }\n+            return index;\n+        }\n+\n+        public String getFieldName(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldNames.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getFieldName(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldNames.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));\n+            }\n+        }\n+\n+        public String getFieldNames()\n+        {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(getFieldName(0));\n+            for (int i = 1; i < length; i++) {\n+                sb.append(\", \");\n+                sb.append(fieldNames.get(i));\n+            }\n+            return sb.toString();\n+        }\n+\n+        public Object getFieldValue(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldValues.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public Object getFieldValue(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldValues.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));\n+            }\n+        }\n+\n+        public String getFieldValues()\n+        {\n+            StringBuilder sb = new StringBuilder();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 207}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3OTExMg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        return tableName;  // for test case label in IDE\n          \n          \n            \n                        return tableName; // for test case label in IDE", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448179112", "createdAt": "2020-07-01T07:46:46Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -116,6 +121,148 @@ private KafkaTopicFieldDescription createOneFieldDescription(String name, Type t\n         return new KafkaTopicFieldDescription(name, type, mapping, null, dataFormat, null, false);\n     }\n \n+    @Test(dataProvider = \"testRoundTripAllFormatsDataProvider\")\n+    public void testRoundTripAllFormats(RoundTripTestSetup roundTripTestSetup)\n+    {\n+        String tableName = roundTripTestSetup.getTableName();\n+        String fieldNames = roundTripTestSetup.getFieldNames();\n+        String fieldValues = roundTripTestSetup.getFieldValues();\n+        String comparableName = roundTripTestSetup.getFieldName(\"f_bigint\");\n+        Object comparableValue = roundTripTestSetup.getFieldValue(\"f_bigint\");\n+\n+        assertUpdate(\"INSERT into write_test.\" + tableName + \" (\" + fieldNames + \") VALUES (\" + fieldValues + \")\", 1);\n+        assertQuery(\"SELECT \" + fieldNames + \" FROM write_test.\" + tableName + \" WHERE \" + comparableName + \" = \" + comparableValue, \"VALUES (\" + fieldValues + \")\");\n+    }\n+\n+    @DataProvider(name = \"testRoundTripAllFormatsDataProvider\")\n+    public final Object[][] testRoundTripAllFormatsDataProvider()\n+    {\n+        return testRoundTripAllFormatsData().stream()\n+                .map(roundTripTestSetup -> new Object[] {roundTripTestSetup})\n+                .toArray(Object[][]::new);\n+    }\n+\n+    private List<RoundTripTestSetup> testRoundTripAllFormatsData()\n+    {\n+        return ImmutableList.<TestKafkaIntegrationSmokeTest.RoundTripTestSetup>builder()\n+                .build();\n+    }\n+\n+    protected static final class RoundTripTestSetup\n+    {\n+        private final String tableName;\n+        private final List<String> fieldNames;\n+        private final List<Object> fieldValues;\n+        private final int length;\n+\n+        public RoundTripTestSetup(String tableName, String fieldName, Object fieldValue)\n+        {\n+            this(tableName, ImmutableList.of(fieldName), ImmutableList.of(fieldValue));\n+        }\n+\n+        public RoundTripTestSetup(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        {\n+            try {\n+                checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+                this.tableName = requireNonNull(tableName, \"tableName is null\");\n+                this.fieldNames = ImmutableList.copyOf(fieldNames);\n+                this.fieldValues = ImmutableList.copyOf(fieldValues);\n+                this.length = fieldNames.size();\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getTableName()\n+        {\n+            return tableName;\n+        }\n+\n+        private int getIndex(String fieldName)\n+        {\n+            int index = -1;\n+            for (int i = 0; i < fieldNames.size(); i++) {\n+                if (fieldNames.get(i).equals(fieldName)) {\n+                    index = i;\n+                }\n+            }\n+            return index;\n+        }\n+\n+        public String getFieldName(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldNames.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public String getFieldName(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldNames.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));\n+            }\n+        }\n+\n+        public String getFieldNames()\n+        {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(getFieldName(0));\n+            for (int i = 1; i < length; i++) {\n+                sb.append(\", \");\n+                sb.append(fieldNames.get(i));\n+            }\n+            return sb.toString();\n+        }\n+\n+        public Object getFieldValue(int index)\n+        {\n+            try {\n+                checkArgument(index >= 0 && index < length, \"index out of bounds\");\n+                return fieldValues.get(index);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+\n+        public Object getFieldValue(String fieldName)\n+        {\n+            int index = getIndex(fieldName);\n+            if (index > -1) {\n+                return fieldValues.get(index);\n+            }\n+            else {\n+                throw new PrestoException(GENERIC_USER_ERROR, format(\"field with name %s does not exist\", fieldName));\n+            }\n+        }\n+\n+        public String getFieldValues()\n+        {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(getFieldValue(0));\n+            for (int i = 1; i < length; i++) {\n+                sb.append(\", \");\n+                sb.append(fieldValues.get(i));\n+            }\n+            return sb.toString();\n+        }\n+\n+        @Override\n+        public String toString()\n+        {\n+            return tableName;  // for test case label in IDE", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 219}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwOTk1NDE2", "url": "https://github.com/trinodb/trino/pull/4287#pullrequestreview-440995416", "createdAt": "2020-07-01T15:50:38Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNTo1MDozOFrOGrrtFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNzowMTo1MVrOGruN0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ1ODAwNA==", "bodyText": "Please file an issue to support transactional inserts in Kafka and reference it here as //TODO", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448458004", "createdAt": "2020-07-01T15:50:38Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaMetadata.java", "diffHunk": "@@ -235,4 +246,28 @@ private KafkaTopicDescription getRequiredTopicDescription(SchemaTableName schema\n                 .map(Optional::get)\n                 .findFirst();\n     }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginInsert(ConnectorSession session, ConnectorTableHandle tableHandle, List<ColumnHandle> columns)\n+    {\n+        KafkaTableHandle table = (KafkaTableHandle) tableHandle;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ3NTAxOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    this.columns = (requireNonNull(ImmutableList.copyOf(columns), \"columns is null\"));\n          \n          \n            \n                    this.columns = requireNonNull(ImmutableList.copyOf(columns), \"columns is null\");", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448475018", "createdAt": "2020-07-01T16:19:07Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final ConnectorSession session;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            ConnectorSession session,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = (requireNonNull(ImmutableList.copyOf(columns), \"columns is null\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ3Njg2NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    else if (type == VARBINARY) {\n          \n          \n            \n                    else if (VarbinaryType.isVarbinaryType(type)) {", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448476865", "createdAt": "2020-07-01T16:22:03Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final ConnectorSession session;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            ConnectorSession session,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = (requireNonNull(ImmutableList.copyOf(columns), \"columns is null\"));\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        this.session = requireNonNull(session, \"session is null\");\n+        this.producer = requireNonNull(producerFactory.create(new ByteArraySerializer(), new ByteArraySerializer()), \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public CompletableFuture<?> appendPage(Page page)\n+    {\n+        for (int position = 0; position < page.getPositionCount(); position++) {\n+            for (int channel = 0; channel < page.getChannelCount(); channel++) {\n+                appendColumn(messageEncoder, page, channel, position);\n+            }\n+            producer.send(new ProducerRecord<>(topicName, messageEncoder.toByteArray()));\n+            keyEncoder.clear();\n+            messageEncoder.clear();\n+        }\n+        producer.flush();\n+        return NOT_BLOCKED;\n+    }\n+\n+    private void appendColumn(RowEncoder rowEncoder, Page page, int channel, int position)\n+    {\n+        Block block = page.getBlock(channel);\n+        EncoderColumnHandle columnHandle = columns.get(channel);\n+        Type type = columns.get(channel).getType();\n+        if (block.isNull(position)) {\n+            rowEncoder.putNullValue(columnHandle);\n+        }\n+        else if (type == BOOLEAN) {\n+            rowEncoder.put(columnHandle, type.getBoolean(block, position));\n+        }\n+        else if (type == BIGINT) {\n+            rowEncoder.put(columnHandle, type.getLong(block, position));\n+        }\n+        else if (type == INTEGER) {\n+            rowEncoder.put(columnHandle, toIntExact(type.getLong(block, position)));\n+        }\n+        else if (type == SMALLINT) {\n+            rowEncoder.put(columnHandle, Shorts.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == TINYINT) {\n+            rowEncoder.put(columnHandle, SignedBytes.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == DOUBLE) {\n+            rowEncoder.put(columnHandle, type.getDouble(block, position));\n+        }\n+        else if (type == REAL) {\n+            rowEncoder.put(columnHandle, intBitsToFloat(toIntExact(type.getLong(block, position))));\n+        }\n+        else if (type instanceof VarcharType) {\n+            rowEncoder.put(columnHandle, type.getSlice(block, position).toStringUtf8());\n+        }\n+        else if (type == VARBINARY) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ3OTU1OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    else if (type instanceof VarcharType) {\n          \n          \n            \n                    isVarcharType(type)\n          \n      \n    \n    \n  \n\nio.prestosql.spi.type.Varchars#isVarcharType", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448479558", "createdAt": "2020-07-01T16:26:44Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final ConnectorSession session;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            ConnectorSession session,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = (requireNonNull(ImmutableList.copyOf(columns), \"columns is null\"));\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        this.session = requireNonNull(session, \"session is null\");\n+        this.producer = requireNonNull(producerFactory.create(new ByteArraySerializer(), new ByteArraySerializer()), \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public CompletableFuture<?> appendPage(Page page)\n+    {\n+        for (int position = 0; position < page.getPositionCount(); position++) {\n+            for (int channel = 0; channel < page.getChannelCount(); channel++) {\n+                appendColumn(messageEncoder, page, channel, position);\n+            }\n+            producer.send(new ProducerRecord<>(topicName, messageEncoder.toByteArray()));\n+            keyEncoder.clear();\n+            messageEncoder.clear();\n+        }\n+        producer.flush();\n+        return NOT_BLOCKED;\n+    }\n+\n+    private void appendColumn(RowEncoder rowEncoder, Page page, int channel, int position)\n+    {\n+        Block block = page.getBlock(channel);\n+        EncoderColumnHandle columnHandle = columns.get(channel);\n+        Type type = columns.get(channel).getType();\n+        if (block.isNull(position)) {\n+            rowEncoder.putNullValue(columnHandle);\n+        }\n+        else if (type == BOOLEAN) {\n+            rowEncoder.put(columnHandle, type.getBoolean(block, position));\n+        }\n+        else if (type == BIGINT) {\n+            rowEncoder.put(columnHandle, type.getLong(block, position));\n+        }\n+        else if (type == INTEGER) {\n+            rowEncoder.put(columnHandle, toIntExact(type.getLong(block, position)));\n+        }\n+        else if (type == SMALLINT) {\n+            rowEncoder.put(columnHandle, Shorts.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == TINYINT) {\n+            rowEncoder.put(columnHandle, SignedBytes.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == DOUBLE) {\n+            rowEncoder.put(columnHandle, type.getDouble(block, position));\n+        }\n+        else if (type == REAL) {\n+            rowEncoder.put(columnHandle, intBitsToFloat(toIntExact(type.getLong(block, position))));\n+        }\n+        else if (type instanceof VarcharType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ4MjM4Ng==", "bodyText": "Do we need to be synchronous here? Maybe we should just wake up sender here and return the future we got from last producer.send call?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448482386", "createdAt": "2020-07-01T16:31:29Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final ConnectorSession session;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            ConnectorSession session,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = (requireNonNull(ImmutableList.copyOf(columns), \"columns is null\"));\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        this.session = requireNonNull(session, \"session is null\");\n+        this.producer = requireNonNull(producerFactory.create(new ByteArraySerializer(), new ByteArraySerializer()), \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public CompletableFuture<?> appendPage(Page page)\n+    {\n+        for (int position = 0; position < page.getPositionCount(); position++) {\n+            for (int channel = 0; channel < page.getChannelCount(); channel++) {\n+                appendColumn(messageEncoder, page, channel, position);\n+            }\n+            producer.send(new ProducerRecord<>(topicName, messageEncoder.toByteArray()));\n+            keyEncoder.clear();\n+            messageEncoder.clear();\n+        }\n+        producer.flush();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5NDE3Mg==", "bodyText": "Defensive copy properties. Or maybe just drop the parameter because you are binding it to empty properties anyway.\nIf we actually need the parameter add the marker annotation @ForKafkaProducer as Properties commonly used type and you can easily need to bind multiple instances at different places in the codebase. As an example look for @ForBaseJdbc", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448494172", "createdAt": "2020-07-01T16:53:04Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/PlainTextKafkaProducerFactory.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.spi.HostAddress;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.common.serialization.Serializer;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Properties;\n+import java.util.Set;\n+\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.joining;\n+import static org.apache.kafka.clients.producer.ProducerConfig.ACKS_CONFIG;\n+import static org.apache.kafka.clients.producer.ProducerConfig.BOOTSTRAP_SERVERS_CONFIG;\n+import static org.apache.kafka.clients.producer.ProducerConfig.LINGER_MS_CONFIG;\n+\n+public class PlainTextKafkaProducerFactory\n+{\n+    private final Set<HostAddress> nodes;\n+    private final Properties properties;\n+\n+    @Inject\n+    public PlainTextKafkaProducerFactory(KafkaConfig kafkaConfig, Properties properties)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5NDUxOQ==", "bodyText": "prepare properties in the constructor.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448494519", "createdAt": "2020-07-01T16:53:41Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/PlainTextKafkaProducerFactory.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.spi.HostAddress;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.common.serialization.Serializer;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Properties;\n+import java.util.Set;\n+\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.joining;\n+import static org.apache.kafka.clients.producer.ProducerConfig.ACKS_CONFIG;\n+import static org.apache.kafka.clients.producer.ProducerConfig.BOOTSTRAP_SERVERS_CONFIG;\n+import static org.apache.kafka.clients.producer.ProducerConfig.LINGER_MS_CONFIG;\n+\n+public class PlainTextKafkaProducerFactory\n+{\n+    private final Set<HostAddress> nodes;\n+    private final Properties properties;\n+\n+    @Inject\n+    public PlainTextKafkaProducerFactory(KafkaConfig kafkaConfig, Properties properties)\n+    {\n+        requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+        this.nodes = ImmutableSet.copyOf(kafkaConfig.getNodes());\n+        this.properties = requireNonNull(properties, \"properties is null\");\n+    }\n+\n+    public <K, V> KafkaProducer<K, V> create(Serializer<K> keySerializer, Serializer<V> messageSerializer)\n+    {\n+        properties.put(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5NzQxMw==", "bodyText": "It is not unique right now. Same value is used in DecoderErrorCode. I think @findepi  pointed that out in previous review.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448497413", "createdAt": "2020-07-01T16:58:48Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/EncoderErrorCode.java", "diffHunk": "@@ -0,0 +1,40 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import io.prestosql.spi.ErrorCode;\n+import io.prestosql.spi.ErrorCodeSupplier;\n+import io.prestosql.spi.ErrorType;\n+\n+import static io.prestosql.spi.ErrorType.EXTERNAL;\n+\n+public enum EncoderErrorCode\n+        implements ErrorCodeSupplier\n+{\n+    /** A requested data conversion is not supported */\n+    ENCODER_CONVERSION_NOT_SUPPORTED(0, EXTERNAL);\n+\n+    private final ErrorCode errorCode;\n+\n+    EncoderErrorCode(int code, ErrorType type)\n+    {\n+        errorCode = new ErrorCode(code + 0x0101_0000, name(), type);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE3Mzk5Mg=="}, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5OTE1NQ==", "bodyText": "please rename the put methods to putObject, putLong, putInt ...\nHaving that many overrides is very error prone. Especially when there are built-in type conversions in the language between the types for which you provide overrides.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448499155", "createdAt": "2020-07-01T17:01:51Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+\n+import java.nio.ByteBuffer;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+import static java.lang.String.format;\n+\n+public interface RowEncoder\n+{\n+    byte[] toByteArray();\n+\n+    void clear();\n+\n+    RowEncoder putNullValue(EncoderColumnHandle columnHandle);\n+\n+    default RowEncoder put(EncoderColumnHandle columnHandle, Object value)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwOTA4OTE0", "url": "https://github.com/trinodb/trino/pull/4287#pullrequestreview-440908914", "createdAt": "2020-07-01T14:13:42Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNDoxMzo0MlrOGrnuSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxOTozODoxN1rOGry8Kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODM5Mjc3OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                            .map(columnHandle -> (KafkaColumnHandle) columnHandle)\n          \n          \n            \n                                            .map(KafkaColumnHandle.class::cast)", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448392779", "createdAt": "2020-07-01T14:13:42Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaMetadata.java", "diffHunk": "@@ -82,7 +87,10 @@ public KafkaTableHandle getTableHandle(ConnectorSession session, SchemaTableName\n                         getDataFormat(kafkaTopicDescription.getKey()),\n                         getDataFormat(kafkaTopicDescription.getMessage()),\n                         kafkaTopicDescription.getKey().flatMap(KafkaTopicFieldGroup::getDataSchema),\n-                        kafkaTopicDescription.getMessage().flatMap(KafkaTopicFieldGroup::getDataSchema)))\n+                        kafkaTopicDescription.getMessage().flatMap(KafkaTopicFieldGroup::getDataSchema),\n+                        getColumnHandles(schemaTableName).values().stream()\n+                                .map(columnHandle -> (KafkaColumnHandle) columnHandle)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NjE0Nw==", "bodyText": "Maybe we should have a constant for this?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448576147", "createdAt": "2020-07-01T19:37:23Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSinkProvider.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.DispatchingRowEncoderFactory;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorInsertTableHandle;\n+import io.prestosql.spi.connector.ConnectorOutputTableHandle;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorPageSinkProvider;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTransactionHandle;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_AVRO_SCHEMA_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaPageSinkProvider\n+        implements ConnectorPageSinkProvider\n+{\n+    private final DispatchingRowEncoderFactory encoderFactory;\n+    private final PlainTextKafkaProducerFactory producerFactory;\n+\n+    @Inject\n+    public KafkaPageSinkProvider(DispatchingRowEncoderFactory encoderFactory, PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.encoderFactory = requireNonNull(encoderFactory, \"encoderFactory is null\");\n+        this.producerFactory = requireNonNull(producerFactory, \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorOutputTableHandle tableHandle)\n+    {\n+        throw new UnsupportedOperationException(\"Table creation is not supported by the kafka connector\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorInsertTableHandle tableHandle)\n+    {\n+        requireNonNull(tableHandle, \"tableHandle is null\");\n+        KafkaTableHandle handle = (KafkaTableHandle) tableHandle;\n+\n+        ImmutableSet.Builder<EncoderColumnHandle> keyColumns = ImmutableSet.builder();\n+        ImmutableSet.Builder<EncoderColumnHandle> messageColumns = ImmutableSet.builder();\n+        handle.getColumns().forEach(col -> {\n+            if (col.isInternal()) {\n+                throw new IllegalArgumentException(format(\"unexpected internal column handle '%s'\", col.getName()));\n+            }\n+            if (col.isKeyDecoder()) {\n+                keyColumns.add(col);\n+            }\n+            else {\n+                messageColumns.add(col);\n+            }\n+        });\n+\n+        RowEncoder keyEncoder = encoderFactory.create(\n+                handle.getKeyDataFormat(),\n+                ImmutableMap.of(\"dataSchema\", getDataSchema(handle.getKeyDataSchemaLocation())),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NjU1NA==", "bodyText": "It would seem more appropriate for this method to return an Optional<String>.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r448576554", "createdAt": "2020-07-01T19:38:17Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSinkProvider.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.DispatchingRowEncoderFactory;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorInsertTableHandle;\n+import io.prestosql.spi.connector.ConnectorOutputTableHandle;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorPageSinkProvider;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTransactionHandle;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_AVRO_SCHEMA_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaPageSinkProvider\n+        implements ConnectorPageSinkProvider\n+{\n+    private final DispatchingRowEncoderFactory encoderFactory;\n+    private final PlainTextKafkaProducerFactory producerFactory;\n+\n+    @Inject\n+    public KafkaPageSinkProvider(DispatchingRowEncoderFactory encoderFactory, PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.encoderFactory = requireNonNull(encoderFactory, \"encoderFactory is null\");\n+        this.producerFactory = requireNonNull(producerFactory, \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorOutputTableHandle tableHandle)\n+    {\n+        throw new UnsupportedOperationException(\"Table creation is not supported by the kafka connector\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorInsertTableHandle tableHandle)\n+    {\n+        requireNonNull(tableHandle, \"tableHandle is null\");\n+        KafkaTableHandle handle = (KafkaTableHandle) tableHandle;\n+\n+        ImmutableSet.Builder<EncoderColumnHandle> keyColumns = ImmutableSet.builder();\n+        ImmutableSet.Builder<EncoderColumnHandle> messageColumns = ImmutableSet.builder();\n+        handle.getColumns().forEach(col -> {\n+            if (col.isInternal()) {\n+                throw new IllegalArgumentException(format(\"unexpected internal column handle '%s'\", col.getName()));\n+            }\n+            if (col.isKeyDecoder()) {\n+                keyColumns.add(col);\n+            }\n+            else {\n+                messageColumns.add(col);\n+            }\n+        });\n+\n+        RowEncoder keyEncoder = encoderFactory.create(\n+                handle.getKeyDataFormat(),\n+                ImmutableMap.of(\"dataSchema\", getDataSchema(handle.getKeyDataSchemaLocation())),\n+                keyColumns.build());\n+\n+        RowEncoder messageEncoder = encoderFactory.create(\n+                handle.getMessageDataFormat(),\n+                ImmutableMap.of(\"dataSchema\", getDataSchema(handle.getMessageDataSchemaLocation())),\n+                messageColumns.build());\n+\n+        return new KafkaPageSink(\n+                handle.getTopicName(),\n+                handle.getColumns(),\n+                keyEncoder,\n+                messageEncoder,\n+                session,\n+                producerFactory);\n+    }\n+\n+    private String getDataSchema(Optional<String> dataSchemaLocation)\n+    {\n+        String dataSchema = \"\";\n+        try {\n+            if (dataSchemaLocation.isPresent()) {\n+                dataSchema = Files.readString(Paths.get(dataSchemaLocation.get()));\n+            }\n+        }\n+        catch (IOException e) {\n+            throw new PrestoException(KAFKA_AVRO_SCHEMA_ERROR, format(\"Unable to read data schema at '%s'\", dataSchemaLocation.get()), e);\n+        }\n+        return dataSchema;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 109}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxNzQ2NTg0", "url": "https://github.com/trinodb/trino/pull/4287#pullrequestreview-441746584", "createdAt": "2020-07-02T14:42:36Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNDo0MjozN1rOGsQA6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNDo0MjozN1rOGsQA6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA1MjkwNw==", "bodyText": "return dataSchemaLocation.map(location -> {\n            try {\n                return Files.readString(Paths.get(location));\n            }\n            catch (IOException e) {\n                throw new PrestoException(KAFKA_AVRO_SCHEMA_ERROR, format(\"Unable to read data schema at '%s'\", location), e);\n            }\n        });", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449052907", "createdAt": "2020-07-02T14:42:37Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSinkProvider.java", "diffHunk": "@@ -95,17 +94,18 @@ public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHa\n                 producerFactory);\n     }\n \n-    private String getDataSchema(Optional<String> dataSchemaLocation)\n+    private Optional<String> getDataSchema(Optional<String> dataSchemaLocation)\n     {\n-        String dataSchema = \"\";\n-        try {\n-            if (dataSchemaLocation.isPresent()) {\n-                dataSchema = Files.readString(Paths.get(dataSchemaLocation.get()));\n+        if (dataSchemaLocation.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxNzU1MDk0", "url": "https://github.com/trinodb/trino/pull/4287#pullrequestreview-441755094", "createdAt": "2020-07-02T14:51:47Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNDo1MTo0N1rOGsQZEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNDo1MTo0N1rOGsQZEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA1OTA4OA==", "bodyText": "Maybe putValueFromBlock? I do not know why but when I see putValueAt method which takes block I think it will be putting something into the block (though blocks are not mutable).", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449059088", "createdAt": "2020-07-02T14:51:47Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -29,7 +31,12 @@\n \n     void clear();\n \n-    RowEncoder putNullValue(EncoderColumnHandle columnHandle);\n+    RowEncoder putValueAt(EncoderColumnHandle columnHandle, ConnectorSession session, Block block, int position);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxNzk3NjU2", "url": "https://github.com/trinodb/trino/pull/4287#pullrequestreview-441797656", "createdAt": "2020-07-02T15:24:58Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNToyNDo1OFrOGsRwUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNToyNDo1OFrOGsRwUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA4MTQyNA==", "bodyText": "We should probably close the producer here, as well.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449081424", "createdAt": "2020-07-02T15:24:58Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final ConnectorSession session;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            ConnectorSession session,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = requireNonNull(ImmutableList.copyOf(columns), \"columns is null\");\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        this.session = requireNonNull(session, \"session is null\");\n+        this.producer = requireNonNull(producerFactory.create(), \"producerFactory is null\");\n+    }\n+\n+    @Override\n+    public CompletableFuture<?> appendPage(Page page)\n+    {\n+        int partition;\n+        for (int position = 0; position < page.getPositionCount(); position++) {\n+            partition = 1;\n+            for (int channel = 0; channel < page.getChannelCount(); channel++) {\n+                if (!columns.get(channel).isInternal()) {\n+                    if (columns.get(channel).isKeyDecoder()) {\n+                        keyEncoder.putValueAt(columns.get(channel), session, page.getBlock(channel), position);\n+                    }\n+                    else {\n+                        messageEncoder.putValueAt(columns.get(channel), session, page.getBlock(channel), position);\n+                    }\n+                }\n+                else {\n+                    partition = toIntExact(columns.get(channel).getType().getLong(page.getBlock(channel), position));\n+                }\n+            }\n+            producer.send(new ProducerRecord<>(topicName, partition, keyEncoder.toByteArray(), messageEncoder.toByteArray()));\n+            keyEncoder.clear();\n+            messageEncoder.clear();\n+        }\n+        return NOT_BLOCKED;\n+    }\n+\n+    @Override\n+    public CompletableFuture<Collection<Slice>> finish()\n+    {\n+        producer.close();\n+        return completedFuture(ImmutableList.of());\n+    }\n+\n+    @Override\n+    public void abort() {}", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 93}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxOTA2MTEz", "url": "https://github.com/trinodb/trino/pull/4287#pullrequestreview-441906113", "createdAt": "2020-07-02T17:47:45Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNzo0Nzo0NlrOGsXovw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxODo1NDo0MFrOGsZmqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE3Nzc5MQ==", "bodyText": "You don't need this.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449177791", "createdAt": "2020-07-02T17:47:46Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+\n+public interface RowEncoder\n+{\n+    byte[] toByteArray();\n+\n+    void clear();\n+\n+    default RowEncoder putValueFromBlock(EncoderColumnHandle columnHandle, ConnectorSession session, Block block, int position)\n+    {\n+        Type type = columnHandle.getType();\n+        if (block.isNull(position)) {\n+            return this.putNullValue(columnHandle);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE3ODEwMA==", "bodyText": "Do we need this method?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449178100", "createdAt": "2020-07-02T17:48:23Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+\n+public interface RowEncoder\n+{\n+    byte[] toByteArray();\n+\n+    void clear();\n+\n+    default RowEncoder putValueFromBlock(EncoderColumnHandle columnHandle, ConnectorSession session, Block block, int position)\n+    {\n+        Type type = columnHandle.getType();\n+        if (block.isNull(position)) {\n+            return this.putNullValue(columnHandle);\n+        }\n+        else if (type == BOOLEAN) {\n+            return this.putBoolean(columnHandle, type.getBoolean(block, position));\n+        }\n+        else if (type == BIGINT) {\n+            return this.putLong(columnHandle, type.getLong(block, position));\n+        }\n+        else if (type == INTEGER) {\n+            return this.putInt(columnHandle, toIntExact(type.getLong(block, position)));\n+        }\n+        else if (type == SMALLINT) {\n+            return this.putShort(columnHandle, Shorts.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == TINYINT) {\n+            return this.putByte(columnHandle, SignedBytes.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == DOUBLE) {\n+            return this.putDouble(columnHandle, type.getDouble(block, position));\n+        }\n+        else if (type == REAL) {\n+            return this.putFloat(columnHandle, intBitsToFloat(toIntExact(type.getLong(block, position))));\n+        }\n+        else if (isVarcharType(type)) {\n+            return this.putString(columnHandle, type.getSlice(block, position).toStringUtf8());\n+        }\n+        else if (isVarbinaryType(type)) {\n+            return this.putByteBuffer(columnHandle, type.getSlice(block, position).toByteBuffer());\n+        }\n+        else if (type == DATE) {\n+            return this.putSqlDate(columnHandle, (SqlDate) type.getObjectValue(session, block, position));\n+        }\n+        else if (type == TIME) {\n+            return this.putSqlTime(columnHandle, (SqlTime) type.getObjectValue(session, block, position));\n+        }\n+        else if (type == TIME_WITH_TIME_ZONE) {\n+            return this.putSqlTimeWithTimeZone(columnHandle, (SqlTimeWithTimeZone) type.getObjectValue(session, block, position));\n+        }\n+        else if (type instanceof TimestampType) {\n+            return this.putSqlTimestamp(columnHandle, (SqlTimestamp) type.getObjectValue(session, block, position));\n+        }\n+        else if (type instanceof TimestampWithTimeZoneType) {\n+            return this.putSqlTimestampWithTimeZone(columnHandle, (SqlTimestampWithTimeZone) type.getObjectValue(session, block, position));\n+        }\n+        else {\n+            throw new UnsupportedOperationException(format(\"Column '%s' does not support 'null' value\", columnHandle.getName()));\n+        }\n+    }\n+\n+    default RowEncoder putNullValue(EncoderColumnHandle columnHandle)\n+    {\n+        throw new UnsupportedOperationException(format(\"Column '%s' does not support 'null' value\", columnHandle.getName()));\n+    }\n+\n+    default RowEncoder putObject(EncoderColumnHandle columnHandle, Object value)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE4MDUyOQ==", "bodyText": "This is not absolutely required, as close() calls flush().", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449180529", "createdAt": "2020-07-02T17:53:14Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/csv/CsvRowEncoder.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.csv;\n+\n+import au.com.bytecode.opencsv.CSVWriter;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.type.Type;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.util.Objects.requireNonNull;\n+\n+public class CsvRowEncoder\n+        implements RowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"csv\";\n+\n+    private final Map<EncoderColumnHandle, Integer> positions;\n+    private String[] row;\n+    private final int size;\n+\n+    public CsvRowEncoder(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        this.parseColumns(columnHandles);\n+\n+        ImmutableMap.Builder<EncoderColumnHandle, Integer> positionsBuilder = ImmutableMap.builder();\n+        int i = 0;\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            positionsBuilder.put(columnHandle, i);\n+            i++;\n+        }\n+        this.positions = positionsBuilder.build();\n+\n+        this.size = columnHandles.size();\n+        this.row = new String[this.size];\n+    }\n+\n+    // performs checks on column handles\n+    private void parseColumns(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            try {\n+                requireNonNull(columnHandle, \"columnHandle is null\");\n+                String columnName = columnHandle.getName();\n+                Type columnType = columnHandle.getType();\n+\n+                checkArgument(!columnHandle.isInternal(), \"unexpected internal column'%s'\", columnName);\n+                checkArgument(columnHandle.getFormatHint() == null, \"unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnName);\n+                checkArgument(columnHandle.getDataFormat() == null, \"unexpected data format '%s' defined for column '%s'\", columnHandle.getDataFormat(), columnName);\n+\n+                checkArgument(isSupportedType(columnType), \"unsupported column type '%s' for column '%s'\", columnType, columnName);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+    }\n+\n+    private boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        try (ByteArrayOutputStream byteArrayOuts = new ByteArrayOutputStream();\n+                OutputStreamWriter outsWriter = new OutputStreamWriter(byteArrayOuts, StandardCharsets.UTF_8);\n+                CSVWriter writer = new CSVWriter(outsWriter, ',', '\"', \"\")) {\n+            writer.writeNext(row);\n+            writer.flush();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE4MTM3Mg==", "bodyText": "The check for whether the column is valid happens at a higher level.  I don't think this should be the encoder's concern.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449181372", "createdAt": "2020-07-02T17:54:56Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/csv/CsvRowEncoder.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.csv;\n+\n+import au.com.bytecode.opencsv.CSVWriter;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.type.Type;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.util.Objects.requireNonNull;\n+\n+public class CsvRowEncoder\n+        implements RowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"csv\";\n+\n+    private final Map<EncoderColumnHandle, Integer> positions;\n+    private String[] row;\n+    private final int size;\n+\n+    public CsvRowEncoder(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        this.parseColumns(columnHandles);\n+\n+        ImmutableMap.Builder<EncoderColumnHandle, Integer> positionsBuilder = ImmutableMap.builder();\n+        int i = 0;\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            positionsBuilder.put(columnHandle, i);\n+            i++;\n+        }\n+        this.positions = positionsBuilder.build();\n+\n+        this.size = columnHandles.size();\n+        this.row = new String[this.size];\n+    }\n+\n+    // performs checks on column handles\n+    private void parseColumns(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            try {\n+                requireNonNull(columnHandle, \"columnHandle is null\");\n+                String columnName = columnHandle.getName();\n+                Type columnType = columnHandle.getType();\n+\n+                checkArgument(!columnHandle.isInternal(), \"unexpected internal column'%s'\", columnName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE4MTg4OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private void parseColumns(Set<EncoderColumnHandle> columnHandles)\n          \n          \n            \n                private void validateColumns(Set<EncoderColumnHandle> columnHandles)", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449181888", "createdAt": "2020-07-02T17:56:00Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/csv/CsvRowEncoder.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.csv;\n+\n+import au.com.bytecode.opencsv.CSVWriter;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.type.Type;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.util.Objects.requireNonNull;\n+\n+public class CsvRowEncoder\n+        implements RowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"csv\";\n+\n+    private final Map<EncoderColumnHandle, Integer> positions;\n+    private String[] row;\n+    private final int size;\n+\n+    public CsvRowEncoder(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        this.parseColumns(columnHandles);\n+\n+        ImmutableMap.Builder<EncoderColumnHandle, Integer> positionsBuilder = ImmutableMap.builder();\n+        int i = 0;\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            positionsBuilder.put(columnHandle, i);\n+            i++;\n+        }\n+        this.positions = positionsBuilder.build();\n+\n+        this.size = columnHandles.size();\n+        this.row = new String[this.size];\n+    }\n+\n+    // performs checks on column handles\n+    private void parseColumns(Set<EncoderColumnHandle> columnHandles)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE4Mzc2OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        throw new RuntimeException(e);\n          \n          \n            \n                        throw new UncheckedIOException(e);", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449183769", "createdAt": "2020-07-02T17:59:11Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/csv/CsvRowEncoder.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.csv;\n+\n+import au.com.bytecode.opencsv.CSVWriter;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.type.Type;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_USER_ERROR;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.util.Objects.requireNonNull;\n+\n+public class CsvRowEncoder\n+        implements RowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"csv\";\n+\n+    private final Map<EncoderColumnHandle, Integer> positions;\n+    private String[] row;\n+    private final int size;\n+\n+    public CsvRowEncoder(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        this.parseColumns(columnHandles);\n+\n+        ImmutableMap.Builder<EncoderColumnHandle, Integer> positionsBuilder = ImmutableMap.builder();\n+        int i = 0;\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            positionsBuilder.put(columnHandle, i);\n+            i++;\n+        }\n+        this.positions = positionsBuilder.build();\n+\n+        this.size = columnHandles.size();\n+        this.row = new String[this.size];\n+    }\n+\n+    // performs checks on column handles\n+    private void parseColumns(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            try {\n+                requireNonNull(columnHandle, \"columnHandle is null\");\n+                String columnName = columnHandle.getName();\n+                Type columnType = columnHandle.getType();\n+\n+                checkArgument(!columnHandle.isInternal(), \"unexpected internal column'%s'\", columnName);\n+                checkArgument(columnHandle.getFormatHint() == null, \"unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnName);\n+                checkArgument(columnHandle.getDataFormat() == null, \"unexpected data format '%s' defined for column '%s'\", columnHandle.getDataFormat(), columnName);\n+\n+                checkArgument(isSupportedType(columnType), \"unsupported column type '%s' for column '%s'\", columnType, columnName);\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new PrestoException(GENERIC_USER_ERROR, e);\n+            }\n+        }\n+    }\n+\n+    private boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        try (ByteArrayOutputStream byteArrayOuts = new ByteArrayOutputStream();\n+                OutputStreamWriter outsWriter = new OutputStreamWriter(byteArrayOuts, StandardCharsets.UTF_8);\n+                CSVWriter writer = new CSVWriter(outsWriter, ',', '\"', \"\")) {\n+            writer.writeNext(row);\n+            writer.flush();\n+            return byteArrayOuts.toByteArray();\n+        }\n+        catch (IOException e) {\n+            throw new RuntimeException(e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE5MzY2OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            Optional<KafkaTopicFieldGroup> key = tableTemplate.getKey();\n          \n          \n            \n                            if (tableTemplate.getKey().isPresent()) {\n          \n          \n            \n                                if (tableTemplate.getKey().get().getDataSchema().isPresent()) {\n          \n          \n            \n                                    KafkaTopicFieldGroup keyTemplate = tableTemplate.getKey().get();\n          \n          \n            \n                                    key = Optional.of(new KafkaTopicFieldGroup(keyTemplate.getDataFormat(), Optional.of(KafkaQueryRunner.class.getResource(keyTemplate.getDataSchema().get()).getPath()), keyTemplate.getFields()));\n          \n          \n            \n                                }\n          \n          \n            \n                            }\n          \n          \n            \n                            Optional<KafkaTopicFieldGroup> key = tableTemplate.getKey()\n          \n          \n            \n                                    .map(keyTemplate -> new KafkaTopicFieldGroup(\n          \n          \n            \n                                            keyTemplate.getDataFormat(),\n          \n          \n            \n                                            keyTemplate.getDataSchema().map(schema -> KafkaQueryRunner.class.getResource(schema).getPath()),\n          \n          \n            \n                                            keyTemplate.getFields()));", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449193668", "createdAt": "2020-07-02T18:19:43Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -134,9 +138,42 @@ private static DistributedQueryRunner createKafkaQueryRunner(\n \n             Map<SchemaTableName, KafkaTopicDescription> tpchTopicDescriptions = createTpchTopicDescriptions(queryRunner.getCoordinator().getMetadata(), tables);\n \n+            List<String> tableNames = new ArrayList<>(4);\n+            tableNames.add(\"all_datatypes_csv\");\n+\n+            JsonCodec<KafkaTopicDescription> topicDescriptionJsonCodec = new CodecSupplier<>(KafkaTopicDescription.class, queryRunner.getMetadata()).get();\n+\n+            ImmutableMap.Builder<SchemaTableName, KafkaTopicDescription> testTopicDescriptions = ImmutableMap.builder();\n+            for (String tableName : tableNames) {\n+                testingKafka.createTopics(\"write_test.\" + tableName);\n+                SchemaTableName table = new SchemaTableName(\"write_test\", tableName);\n+                KafkaTopicDescription tableTemplate = topicDescriptionJsonCodec.fromJson(toByteArray(TestMinimalFunctionality.class.getResourceAsStream(format(\"/write_test/%s.json\", tableName))));\n+                Optional<KafkaTopicFieldGroup> key = tableTemplate.getKey();\n+                if (tableTemplate.getKey().isPresent()) {\n+                    if (tableTemplate.getKey().get().getDataSchema().isPresent()) {\n+                        KafkaTopicFieldGroup keyTemplate = tableTemplate.getKey().get();\n+                        key = Optional.of(new KafkaTopicFieldGroup(keyTemplate.getDataFormat(), Optional.of(KafkaQueryRunner.class.getResource(keyTemplate.getDataSchema().get()).getPath()), keyTemplate.getFields()));\n+                    }\n+                }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIxMDAyNg==", "bodyText": "You could use functional style here, too.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449210026", "createdAt": "2020-07-02T18:54:40Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -134,9 +138,42 @@ private static DistributedQueryRunner createKafkaQueryRunner(\n \n             Map<SchemaTableName, KafkaTopicDescription> tpchTopicDescriptions = createTpchTopicDescriptions(queryRunner.getCoordinator().getMetadata(), tables);\n \n+            List<String> tableNames = new ArrayList<>(4);\n+            tableNames.add(\"all_datatypes_csv\");\n+\n+            JsonCodec<KafkaTopicDescription> topicDescriptionJsonCodec = new CodecSupplier<>(KafkaTopicDescription.class, queryRunner.getMetadata()).get();\n+\n+            ImmutableMap.Builder<SchemaTableName, KafkaTopicDescription> testTopicDescriptions = ImmutableMap.builder();\n+            for (String tableName : tableNames) {\n+                testingKafka.createTopics(\"write_test.\" + tableName);\n+                SchemaTableName table = new SchemaTableName(\"write_test\", tableName);\n+                KafkaTopicDescription tableTemplate = topicDescriptionJsonCodec.fromJson(toByteArray(TestMinimalFunctionality.class.getResourceAsStream(format(\"/write_test/%s.json\", tableName))));\n+                Optional<KafkaTopicFieldGroup> key = tableTemplate.getKey();\n+                if (tableTemplate.getKey().isPresent()) {\n+                    if (tableTemplate.getKey().get().getDataSchema().isPresent()) {\n+                        KafkaTopicFieldGroup keyTemplate = tableTemplate.getKey().get();\n+                        key = Optional.of(new KafkaTopicFieldGroup(keyTemplate.getDataFormat(), Optional.of(KafkaQueryRunner.class.getResource(keyTemplate.getDataSchema().get()).getPath()), keyTemplate.getFields()));\n+                    }\n+                }\n+                Optional<KafkaTopicFieldGroup> message = tableTemplate.getMessage();\n+                if (tableTemplate.getMessage().isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 44}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyMzE0NzAy", "url": "https://github.com/trinodb/trino/pull/4287#pullrequestreview-442314702", "createdAt": "2020-07-03T10:24:49Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxMDoyNTo0NlrOGsr1GA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxMTowMjowMFrOGssw-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUwODYzMg==", "bodyText": "Would you mind moving to the previous place, before getName, to match ctor params?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449508632", "createdAt": "2020-07-03T10:25:46Z", "author": {"login": "findepi"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaColumnHandle.java", "diffHunk": "@@ -152,6 +147,13 @@ public boolean isInternal()\n         return internal;\n     }\n \n+    @Override\n+    @JsonProperty\n+    public int getOrdinalPosition()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUwOTI2OA==", "bodyText": "Unused, remove.\nFYI #4330", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449509268", "createdAt": "2020-07-03T10:27:19Z", "author": {"login": "findepi"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/EncoderColumnHandle.java", "diffHunk": "@@ -0,0 +1,35 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.type.Type;\n+\n+public interface EncoderColumnHandle\n+        extends ColumnHandle\n+{\n+    boolean isInternal();\n+\n+    String getFormatHint();\n+\n+    Type getType();\n+\n+    String getName();\n+\n+    String getMapping();\n+\n+    String getDataFormat();\n+\n+    int getOrdinalPosition();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxMTE3Mg==", "bodyText": "Since order of columnHandles is of utmost importance here,\nthis should be a list.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449511172", "createdAt": "2020-07-03T10:31:34Z", "author": {"login": "findepi"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/csv/CsvRowEncoder.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.csv;\n+\n+import au.com.bytecode.opencsv.CSVWriter;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.type.Type;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.io.UncheckedIOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_ENCODER_ERROR;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.util.Objects.requireNonNull;\n+\n+public class CsvRowEncoder\n+        implements RowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"csv\";\n+\n+    private final Map<EncoderColumnHandle, Integer> positions;\n+    private String[] row;\n+    private final int size;\n+\n+    public CsvRowEncoder(Set<EncoderColumnHandle> columnHandles)\n+    {\n+        this.validateColumns(columnHandles);\n+\n+        ImmutableMap.Builder<EncoderColumnHandle, Integer> positionsBuilder = ImmutableMap.builder();\n+        int i = 0;\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            positionsBuilder.put(columnHandle, i);\n+            i++;\n+        }\n+        this.positions = positionsBuilder.build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxMTkyMw==", "bodyText": "reorder methods in the order in which they are used (logically):\n\nputValueFromBlock (or appendValue)\ntoByteArray\nclear", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449511923", "createdAt": "2020-07-03T10:33:15Z", "author": {"login": "findepi"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+\n+public interface RowEncoder\n+{\n+    byte[] toByteArray();\n+\n+    void clear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTIwMw==", "bodyText": "The encoders will see every column and some encoders (like CSV or RAW) care getting data\nin order. Since encoders are initialized with the list of columns, they know the columns.\nBy passing column index rather than column handle, we can save lookup like here\nio.prestosql.plugin.kafka.encoder.csv.CsvRowEncoder#getPosition\nFurthermore, we can guarantee that columns are being provided in order.\nThis would unlock certain optimizations in encoders like CSV or RAW, which\nwould be writing to a byte buffer directly.\nTo express this in the method signature, I would remove the column parameter\naltogether.\nThird, we could weave in the ConnectorSession session during encoder initialization,\nthis would allow encoder to precalculate (\"compile\") encoding strategies for each\nof the columns, if it wishes to do so.\nTherefore I propose to have this as the signature:\n/**\n * Adds value for the next column to the row being encoded.\n */\nvoid appendValue(Block block, int position)", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449515203", "createdAt": "2020-07-03T10:41:12Z", "author": {"login": "findepi"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+\n+public interface RowEncoder\n+{\n+    byte[] toByteArray();\n+\n+    void clear();\n+\n+    default RowEncoder putValueFromBlock(EncoderColumnHandle columnHandle, ConnectorSession session, Block block, int position)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTQwNA==", "bodyText": "The interface has 3 actual methods:\n\nputValueFromBlock (or appendValue)\ntoByteArray\nclear\n\nAll the other, default methods are not used by users of the interface.\nThey are convenience methods added for the sake of the implementations.\nMove them to AbstractRowEncoder (as protected).", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449515404", "createdAt": "2020-07-03T10:41:42Z", "author": {"login": "findepi"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+\n+public interface RowEncoder\n+{\n+    byte[] toByteArray();\n+\n+    void clear();\n+\n+    default RowEncoder putValueFromBlock(EncoderColumnHandle columnHandle, ConnectorSession session, Block block, int position)\n+    {\n+        Type type = columnHandle.getType();\n+        if (block.isNull(position)) {\n+            return putNullValue(columnHandle);\n+        }\n+        else if (type == BOOLEAN) {\n+            return putBoolean(columnHandle, type.getBoolean(block, position));\n+        }\n+        else if (type == BIGINT) {\n+            return putLong(columnHandle, type.getLong(block, position));\n+        }\n+        else if (type == INTEGER) {\n+            return putInt(columnHandle, toIntExact(type.getLong(block, position)));\n+        }\n+        else if (type == SMALLINT) {\n+            return putShort(columnHandle, Shorts.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == TINYINT) {\n+            return putByte(columnHandle, SignedBytes.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == DOUBLE) {\n+            return putDouble(columnHandle, type.getDouble(block, position));\n+        }\n+        else if (type == REAL) {\n+            return putFloat(columnHandle, intBitsToFloat(toIntExact(type.getLong(block, position))));\n+        }\n+        else if (isVarcharType(type)) {\n+            return putString(columnHandle, type.getSlice(block, position).toStringUtf8());\n+        }\n+        else if (isVarbinaryType(type)) {\n+            return putByteBuffer(columnHandle, type.getSlice(block, position).toByteBuffer());\n+        }\n+        else if (type == DATE) {\n+            return putSqlDate(columnHandle, (SqlDate) type.getObjectValue(session, block, position));\n+        }\n+        else if (type == TIME) {\n+            return putSqlTime(columnHandle, (SqlTime) type.getObjectValue(session, block, position));\n+        }\n+        else if (type == TIME_WITH_TIME_ZONE) {\n+            return putSqlTimeWithTimeZone(columnHandle, (SqlTimeWithTimeZone) type.getObjectValue(session, block, position));\n+        }\n+        else if (type instanceof TimestampType) {\n+            return putSqlTimestamp(columnHandle, (SqlTimestamp) type.getObjectValue(session, block, position));\n+        }\n+        else if (type instanceof TimestampWithTimeZoneType) {\n+            return putSqlTimestampWithTimeZone(columnHandle, (SqlTimestampWithTimeZone) type.getObjectValue(session, block, position));\n+        }\n+        else {\n+            throw new UnsupportedOperationException(format(\"Column '%s' does not support 'null' value\", columnHandle.getName()));\n+        }\n+    }\n+\n+    default RowEncoder putNullValue(EncoderColumnHandle columnHandle)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNjE5Mw==", "bodyText": "As noted elsewhere:\n\nSet columnHandles -> List columnHandles\nadd ConnectorSession session (as first arg perhaps)", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449516193", "createdAt": "2020-07-03T10:43:30Z", "author": {"login": "findepi"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoderFactory.java", "diffHunk": "@@ -0,0 +1,22 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import java.util.Optional;\n+import java.util.Set;\n+\n+public interface RowEncoderFactory\n+{\n+    RowEncoder create(Optional<String> dataSchema, Set<EncoderColumnHandle> columnHandles);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNzYyMw==", "bodyText": "KAFKA_SCHEMA_ERROR", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449517623", "createdAt": "2020-07-03T10:46:39Z", "author": {"login": "findepi"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaErrorCode.java", "diffHunk": "@@ -18,11 +18,16 @@\n import io.prestosql.spi.ErrorType;\n \n import static io.prestosql.spi.ErrorType.EXTERNAL;\n+import static io.prestosql.spi.ErrorType.INTERNAL_ERROR;\n \n public enum KafkaErrorCode\n         implements ErrorCodeSupplier\n {\n-    KAFKA_SPLIT_ERROR(0, EXTERNAL);\n+    KAFKA_SPLIT_ERROR(0, EXTERNAL),\n+    KAFKA_AVRO_SCHEMA_ERROR(1, EXTERNAL),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxODYxNg==", "bodyText": "KAFKA_ENCODER_ERROR is probably not EXTERNAL\nanyway, i am not convinced there is a value in discerning these", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449518616", "createdAt": "2020-07-03T10:48:58Z", "author": {"login": "findepi"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaErrorCode.java", "diffHunk": "@@ -18,11 +18,16 @@\n import io.prestosql.spi.ErrorType;\n \n import static io.prestosql.spi.ErrorType.EXTERNAL;\n+import static io.prestosql.spi.ErrorType.INTERNAL_ERROR;\n \n public enum KafkaErrorCode\n         implements ErrorCodeSupplier\n {\n-    KAFKA_SPLIT_ERROR(0, EXTERNAL);\n+    KAFKA_SPLIT_ERROR(0, EXTERNAL),\n+    KAFKA_AVRO_SCHEMA_ERROR(1, EXTERNAL),\n+    KAFKA_PRODUCER_ERROR(2, INTERNAL_ERROR),\n+    KAFKA_ENCODER_ERROR(3, EXTERNAL)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxOTY5Mw==", "bodyText": "why?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449519693", "createdAt": "2020-07-03T10:51:13Z", "author": {"login": "findepi"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaMetadata.java", "diffHunk": "@@ -107,13 +115,16 @@ public ConnectorTableMetadata getTableMetadata(ConnectorSession session, Connect\n                 .collect(toImmutableList());\n     }\n \n-    @SuppressWarnings(\"ValueOfIncrementOrDecrementUsed\")\n     @Override\n     public Map<String, ColumnHandle> getColumnHandles(ConnectorSession session, ConnectorTableHandle tableHandle)\n     {\n         KafkaTableHandle kafkaTableHandle = convertTableHandle(tableHandle);\n+        return getColumnHandles(kafkaTableHandle.toSchemaTableName());\n+    }\n \n-        SchemaTableName schemaTableName = kafkaTableHandle.toSchemaTableName();\n+    @SuppressWarnings(\"ValueOfIncrementOrDecrementUsed\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUyMDUzOA==", "bodyText": "add\n// TODO: support transactional inserts https://github.com/prestosql/presto/issues/4303\n\nhere as well", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449520538", "createdAt": "2020-07-03T10:53:31Z", "author": {"login": "findepi"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaMetadata.java", "diffHunk": "@@ -235,4 +246,29 @@ private KafkaTopicDescription getRequiredTopicDescription(SchemaTableName schema\n                 .map(Optional::get)\n                 .findFirst();\n     }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginInsert(ConnectorSession session, ConnectorTableHandle tableHandle, List<ColumnHandle> columns)\n+    {\n+        // TODO: support transactional inserts https://github.com/prestosql/presto/issues/4303\n+        KafkaTableHandle table = (KafkaTableHandle) tableHandle;\n+\n+        return new KafkaTableHandle(\n+                table.getSchemaName(),\n+                table.getTableName(),\n+                table.getTopicName(),\n+                table.getKeyDataFormat(),\n+                table.getMessageDataFormat(),\n+                table.getKeyDataSchemaLocation(),\n+                table.getMessageDataSchemaLocation(),\n+                columns.stream()\n+                        .map(KafkaColumnHandle.class::cast)\n+                        .collect(toImmutableList()));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishInsert(ConnectorSession session, ConnectorInsertTableHandle insertHandle, Collection<Slice> fragments, Collection<ComputedStatistics> computedStatistics)\n+    {\n+        return Optional.empty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUyMTA4Nw==", "bodyText": "This should be (and must be) equivalent to table.getColumns() except for internal columns.\nAdd something like\n// Encoders must be give all columns\nList<KafkaColumnHandle> actualColumns = ((KafkaTableHandle) tableHandle).getColumns().stream()\n        .filter(column -> !column.isInternal())\n        .collect(toImmutableList());\ncheckArgument(columns.equals(actualColumns), \"Unexpected columns, expected: %s, got: %s\", actualColumns, columns);\n\nthen you can use actualColumns here.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449521087", "createdAt": "2020-07-03T10:54:57Z", "author": {"login": "findepi"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaMetadata.java", "diffHunk": "@@ -235,4 +246,29 @@ private KafkaTopicDescription getRequiredTopicDescription(SchemaTableName schema\n                 .map(Optional::get)\n                 .findFirst();\n     }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginInsert(ConnectorSession session, ConnectorTableHandle tableHandle, List<ColumnHandle> columns)\n+    {\n+        // TODO: support transactional inserts https://github.com/prestosql/presto/issues/4303\n+        KafkaTableHandle table = (KafkaTableHandle) tableHandle;\n+\n+        return new KafkaTableHandle(\n+                table.getSchemaName(),\n+                table.getTableName(),\n+                table.getTopicName(),\n+                table.getKeyDataFormat(),\n+                table.getMessageDataFormat(),\n+                table.getKeyDataSchemaLocation(),\n+                table.getMessageDataSchemaLocation(),\n+                columns.stream()\n+                        .map(KafkaColumnHandle.class::cast)\n+                        .collect(toImmutableList()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUyMzk2Mg==", "bodyText": "properties is mutable, so defensive copy would be needed here\n\nchange Properties properties field to Map<...>  properties and initialize it using ImmutableMap\nby using Map<String, Object> you can call the overload of KafkaProducer that does not require you to convert to Properties", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r449523962", "createdAt": "2020-07-03T11:02:00Z", "author": {"login": "findepi"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/PlainTextKafkaProducerFactory.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.spi.HostAddress;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Properties;\n+import java.util.Set;\n+\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.joining;\n+import static org.apache.kafka.clients.producer.ProducerConfig.ACKS_CONFIG;\n+import static org.apache.kafka.clients.producer.ProducerConfig.BOOTSTRAP_SERVERS_CONFIG;\n+import static org.apache.kafka.clients.producer.ProducerConfig.LINGER_MS_CONFIG;\n+\n+public class PlainTextKafkaProducerFactory\n+{\n+    private final Properties properties;\n+\n+    @Inject\n+    public PlainTextKafkaProducerFactory(KafkaConfig kafkaConfig)\n+    {\n+        requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+        Set<HostAddress> nodes = ImmutableSet.copyOf(kafkaConfig.getNodes());\n+        this.properties = new Properties();\n+        this.properties.put(\n+                BOOTSTRAP_SERVERS_CONFIG,\n+                nodes.stream()\n+                        .map(HostAddress::toString)\n+                        .collect(joining(\",\")));\n+        this.properties.put(ACKS_CONFIG, \"all\");\n+        this.properties.put(LINGER_MS_CONFIG, 5);  // reduces number of requests sent, adds 5ms of latency in the absence of load\n+    }\n+\n+    public KafkaProducer<byte[], byte[]> create()\n+    {\n+        return new KafkaProducer<>(properties, new ByteArraySerializer(), new ByteArraySerializer());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQzMDAzNDQw", "url": "https://github.com/trinodb/trino/pull/4287#pullrequestreview-443003440", "createdAt": "2020-07-06T11:46:50Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQxMTo0Njo1MFrOGtT9kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQxMTo1MTowNVrOGtUFOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2NjE2Mw==", "bodyText": "The position argument here represents the index of the column from the table schema, which is not necessarily the same as the position in the result set, so we will still need a mapping (though it would be just an array lookup).\n\nFurthermore, we can guarantee that columns are being provided in order.\nThis would unlock certain optimizations in encoders like CSV or RAW, which\nwould be writing to a byte buffer directly.\n\nI don't know if that's necessarily the case.  It would be pretty constraining for the API to require values be provided in the order columns appear in the table schema.\nPerhaps we would be better off passing the entire page, instead of individual blocks, and let the encoder decide how to iterate over columns.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450166163", "createdAt": "2020-07-06T11:46:50Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+\n+public interface RowEncoder\n+{\n+    byte[] toByteArray();\n+\n+    void clear();\n+\n+    default RowEncoder putValueFromBlock(EncoderColumnHandle columnHandle, ConnectorSession session, Block block, int position)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxNTIwMw=="}, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2ODEyMA==", "bodyText": "I think we should do some additional validation on the columns here.  For example, if the table schema specifies a key, we would need to ensure that all key columns are present.  We could also do the check for internal columns here - make sure that only supported ones are present.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450168120", "createdAt": "2020-07-06T11:51:05Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_PRODUCER_ERROR;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final ConnectorSession session;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            ConnectorSession session,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = requireNonNull(ImmutableList.copyOf(columns), \"columns is null\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 55}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ0MDIzMzQz", "url": "https://github.com/trinodb/trino/pull/4287#pullrequestreview-444023343", "createdAt": "2020-07-07T15:48:16Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxNTo0ODoxNlrOGuEy_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxNTo0ODoxNlrOGuEy_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk2NjI2OQ==", "bodyText": "This should go to previous commit.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450966269", "createdAt": "2020-07-07T15:48:16Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaErrorCode.java", "diffHunk": "@@ -24,7 +24,7 @@\n         implements ErrorCodeSupplier\n {\n     KAFKA_SPLIT_ERROR(0, EXTERNAL),\n-    KAFKA_AVRO_SCHEMA_ERROR(1, EXTERNAL),\n+    KAFKA_SCHEMA_ERROR(1, EXTERNAL),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ0MDQ4MTY2", "url": "https://github.com/trinodb/trino/pull/4287#pullrequestreview-444048166", "createdAt": "2020-07-07T16:16:36Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxNjoxNjozNlrOGuF8yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxNjo0NzozNVrOGuHHjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk4NTE2MA==", "bodyText": "Naming is fine. As for the annotation,  drop it (and one below) in separate commit.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450985160", "createdAt": "2020-07-07T16:16:36Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaMetadata.java", "diffHunk": "@@ -107,13 +115,16 @@ public ConnectorTableMetadata getTableMetadata(ConnectorSession session, Connect\n                 .collect(toImmutableList());\n     }\n \n-    @SuppressWarnings(\"ValueOfIncrementOrDecrementUsed\")\n     @Override\n     public Map<String, ColumnHandle> getColumnHandles(ConnectorSession session, ConnectorTableHandle tableHandle)\n     {\n         KafkaTableHandle kafkaTableHandle = convertTableHandle(tableHandle);\n+        return getColumnHandles(kafkaTableHandle.toSchemaTableName());\n+    }\n \n-        SchemaTableName schemaTableName = kafkaTableHandle.toSchemaTableName();\n+    @SuppressWarnings(\"ValueOfIncrementOrDecrementUsed\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTUxOTY5Mw=="}, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk5MjM2MQ==", "bodyText": "The callback can be run in very different thread than the one calling producer.send. Do we know if exception thrown by the callback will be always propagated back to Presto thread. I.e if we do not get exception directly here, will we still get it on producer.flush() called from finish()?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450992361", "createdAt": "2020-07-07T16:28:15Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_PRODUCER_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = requireNonNull(ImmutableList.copyOf(columns), \"columns is null\");\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        requireNonNull(producerFactory, \"producerFactory is null\");\n+        this.producer = producerFactory.create();\n+    }\n+\n+    @Override\n+    public CompletableFuture<?> appendPage(Page page)\n+    {\n+        for (int position = 0; position < page.getPositionCount(); position++) {\n+            for (int channel = 0; channel < page.getChannelCount(); channel++) {\n+                if (columns.get(channel).isKeyCodec()) {\n+                    keyEncoder.appendColumnValue(page.getBlock(channel), position);\n+                }\n+                else {\n+                    messageEncoder.appendColumnValue(page.getBlock(channel), position);\n+                }\n+            }\n+            producer.send(new ProducerRecord<>(topicName, keyEncoder.toByteArray(), messageEncoder.toByteArray()), (recordMetadata, e) -> {\n+                if (e != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDk5ODg1NQ==", "bodyText": "It cannot return different list than one we pass to it as an argument, right? Otherwise the columHandles stored here would not be compatible with loop which calls to appendColumnValue. Change to void (unless I miss something).", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r450998855", "createdAt": "2020-07-07T16:38:49Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/AbstractRowEncoder.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public abstract class AbstractRowEncoder\n+        implements RowEncoder\n+{\n+    protected final ConnectorSession session;\n+    protected final List<EncoderColumnHandle> columnHandles;\n+\n+    /**\n+     * The current column index for appending values to the row encoder.\n+     * Gets incremented by appendColumnValue and set back to zero when the encoder is reset.\n+     */\n+    protected int currentColumnIndex;\n+\n+    protected AbstractRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        this.session = requireNonNull(session, \"session is null\");\n+        requireNonNull(columnHandles, \"columnHandles is null\");\n+        this.columnHandles = validateColumns(columnHandles);\n+        this.currentColumnIndex = 0;\n+    }\n+\n+    /**\n+     * Performs any checks on the column handle field values\n+     */\n+    protected abstract List<EncoderColumnHandle> validateColumns(List<EncoderColumnHandle> columnHandles);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwMDYzNw==", "bodyText": "checkState that currentColumnIndex < columnHandles.size()", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451000637", "createdAt": "2020-07-07T16:41:45Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/AbstractRowEncoder.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public abstract class AbstractRowEncoder\n+        implements RowEncoder\n+{\n+    protected final ConnectorSession session;\n+    protected final List<EncoderColumnHandle> columnHandles;\n+\n+    /**\n+     * The current column index for appending values to the row encoder.\n+     * Gets incremented by appendColumnValue and set back to zero when the encoder is reset.\n+     */\n+    protected int currentColumnIndex;\n+\n+    protected AbstractRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        this.session = requireNonNull(session, \"session is null\");\n+        requireNonNull(columnHandles, \"columnHandles is null\");\n+        this.columnHandles = validateColumns(columnHandles);\n+        this.currentColumnIndex = 0;\n+    }\n+\n+    /**\n+     * Performs any checks on the column handle field values\n+     */\n+    protected abstract List<EncoderColumnHandle> validateColumns(List<EncoderColumnHandle> columnHandles);\n+\n+    @Override\n+    public RowEncoder appendColumnValue(Block block, int position)\n+    {\n+        Type type = columnHandles.get(currentColumnIndex).getType();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwMTE0Nw==", "bodyText": "should we checkState that currentColumnIndex == columnHandles.size()?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451001147", "createdAt": "2020-07-07T16:42:32Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/AbstractRowEncoder.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public abstract class AbstractRowEncoder\n+        implements RowEncoder\n+{\n+    protected final ConnectorSession session;\n+    protected final List<EncoderColumnHandle> columnHandles;\n+\n+    /**\n+     * The current column index for appending values to the row encoder.\n+     * Gets incremented by appendColumnValue and set back to zero when the encoder is reset.\n+     */\n+    protected int currentColumnIndex;\n+\n+    protected AbstractRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        this.session = requireNonNull(session, \"session is null\");\n+        requireNonNull(columnHandles, \"columnHandles is null\");\n+        this.columnHandles = validateColumns(columnHandles);\n+        this.currentColumnIndex = 0;\n+    }\n+\n+    /**\n+     * Performs any checks on the column handle field values\n+     */\n+    protected abstract List<EncoderColumnHandle> validateColumns(List<EncoderColumnHandle> columnHandles);\n+\n+    @Override\n+    public RowEncoder appendColumnValue(Block block, int position)\n+    {\n+        Type type = columnHandles.get(currentColumnIndex).getType();\n+        if (block.isNull(position)) {\n+            appendNullValue();\n+        }\n+        else if (type == BOOLEAN) {\n+            appendBoolean(type.getBoolean(block, position));\n+        }\n+        else if (type == BIGINT) {\n+            appendLong(type.getLong(block, position));\n+        }\n+        else if (type == INTEGER) {\n+            appendInt(toIntExact(type.getLong(block, position)));\n+        }\n+        else if (type == SMALLINT) {\n+            appendShort(Shorts.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == TINYINT) {\n+            appendByte(SignedBytes.checkedCast(type.getLong(block, position)));\n+        }\n+        else if (type == DOUBLE) {\n+            appendDouble(type.getDouble(block, position));\n+        }\n+        else if (type == REAL) {\n+            appendFloat(intBitsToFloat(toIntExact(type.getLong(block, position))));\n+        }\n+        else if (isVarcharType(type)) {\n+            appendString(type.getSlice(block, position).toStringUtf8());\n+        }\n+        else if (isVarbinaryType(type)) {\n+            appendByteBuffer(type.getSlice(block, position).toByteBuffer());\n+        }\n+        else if (type == DATE) {\n+            appendSqlDate((SqlDate) type.getObjectValue(session, block, position));\n+        }\n+        else if (type == TIME) {\n+            appendSqlTime((SqlTime) type.getObjectValue(session, block, position));\n+        }\n+        else if (type == TIME_WITH_TIME_ZONE) {\n+            appendSqlTimeWithTimeZone((SqlTimeWithTimeZone) type.getObjectValue(session, block, position));\n+        }\n+        else if (type instanceof TimestampType) {\n+            appendSqlTimestamp((SqlTimestamp) type.getObjectValue(session, block, position));\n+        }\n+        else if (type instanceof TimestampWithTimeZoneType) {\n+            appendSqlTimestampWithTimeZone((SqlTimestampWithTimeZone) type.getObjectValue(session, block, position));\n+        }\n+        else {\n+            throw new UnsupportedOperationException(format(\"Column '%s' does not support 'null' value\", columnHandles.get(currentColumnIndex).getName()));\n+        }\n+        currentColumnIndex++;\n+        return this;\n+    }\n+\n+    // these append value methods should be overridden for each row encoder\n+    // only the methods with types supported by the data format should be overridden\n+    protected void appendNullValue()\n+    {\n+        throw new UnsupportedOperationException(format(\"Column '%s' does not support 'null' value\", columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendLong(long value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", long.class.getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendInt(int value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", int.class.getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendShort(short value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", short.class.getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendByte(byte value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", byte.class.getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendDouble(double value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", double.class.getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendFloat(float value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", float.class.getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendBoolean(boolean value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", boolean.class.getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendString(String value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", value.getClass().getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendByteBuffer(ByteBuffer value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", value.getClass().getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendSqlDate(SqlDate value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", value.getClass().getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendSqlTime(SqlTime value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", value.getClass().getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendSqlTimeWithTimeZone(SqlTimeWithTimeZone value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", value.getClass().getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendSqlTimestamp(SqlTimestamp value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", value.getClass().getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    protected void appendSqlTimestampWithTimeZone(SqlTimestampWithTimeZone value)\n+    {\n+        throw new UnsupportedOperationException(format(\"Unsupported type '%s' for column '%s'\", value.getClass().getName(), columnHandles.get(currentColumnIndex).getName()));\n+    }\n+\n+    @Override\n+    public void reset()\n+    {\n+        currentColumnIndex = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwMzQ4Ng==", "bodyText": "not really needed right?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451003486", "createdAt": "2020-07-07T16:46:10Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/csv/CsvRowEncoder.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.csv;\n+\n+import au.com.bytecode.opencsv.CSVWriter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.io.UncheckedIOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.List;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+\n+public class CsvRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"csv\";\n+\n+    private final int size;\n+    private String[] row;\n+\n+    public CsvRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+        this.size = columnHandles.size();\n+        this.row = new String[this.size];\n+    }\n+\n+    @Override\n+    protected List<EncoderColumnHandle> validateColumns(List<EncoderColumnHandle> columnHandles)\n+    {\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(columnHandle.getDataFormat() == null, \"unexpected data format '%s' defined for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName());\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"unsupported column type '%s' for column '%s'\", columnHandle.getType(), columnHandle.getName());\n+        }\n+        return ImmutableList.copyOf(columnHandles);\n+    }\n+\n+    private boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        row[currentColumnIndex] = null;\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        row[currentColumnIndex] = Long.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        row[currentColumnIndex] = Integer.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        row[currentColumnIndex] = Short.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        row[currentColumnIndex] = Byte.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        row[currentColumnIndex] = Double.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        row[currentColumnIndex] = Float.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        row[currentColumnIndex] = Boolean.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        row[currentColumnIndex] = value;\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        try (ByteArrayOutputStream byteArrayOuts = new ByteArrayOutputStream();\n+                OutputStreamWriter outsWriter = new OutputStreamWriter(byteArrayOuts, StandardCharsets.UTF_8);\n+                CSVWriter writer = new CSVWriter(outsWriter, ',', '\"', \"\")) {\n+            writer.writeNext(row);\n+            writer.flush();\n+            return byteArrayOuts.toByteArray();\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    @Override\n+    protected void clear()\n+    {\n+        row = new String[size];", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAwNDMwMA==", "bodyText": "do we support overriding special characters on read? If so we should also support them on write.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r451004300", "createdAt": "2020-07-07T16:47:35Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/csv/CsvRowEncoder.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.csv;\n+\n+import au.com.bytecode.opencsv.CSVWriter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.io.UncheckedIOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.List;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+\n+public class CsvRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"csv\";\n+\n+    private final int size;\n+    private String[] row;\n+\n+    public CsvRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+        this.size = columnHandles.size();\n+        this.row = new String[this.size];\n+    }\n+\n+    @Override\n+    protected List<EncoderColumnHandle> validateColumns(List<EncoderColumnHandle> columnHandles)\n+    {\n+        for (EncoderColumnHandle columnHandle : columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(columnHandle.getDataFormat() == null, \"unexpected data format '%s' defined for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName());\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"unsupported column type '%s' for column '%s'\", columnHandle.getType(), columnHandle.getName());\n+        }\n+        return ImmutableList.copyOf(columnHandles);\n+    }\n+\n+    private boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        row[currentColumnIndex] = null;\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        row[currentColumnIndex] = Long.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        row[currentColumnIndex] = Integer.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        row[currentColumnIndex] = Short.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        row[currentColumnIndex] = Byte.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        row[currentColumnIndex] = Double.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        row[currentColumnIndex] = Float.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        row[currentColumnIndex] = Boolean.toString(value);\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        row[currentColumnIndex] = value;\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        try (ByteArrayOutputStream byteArrayOuts = new ByteArrayOutputStream();\n+                OutputStreamWriter outsWriter = new OutputStreamWriter(byteArrayOuts, StandardCharsets.UTF_8);\n+                CSVWriter writer = new CSVWriter(outsWriter, ',', '\"', \"\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 136}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ1NTgyNzA2", "url": "https://github.com/trinodb/trino/pull/4287#pullrequestreview-445582706", "createdAt": "2020-07-09T12:49:44Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxMjo0OTo0NFrOGvPoZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxMzowMzo1OFrOGvQJ8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE5MjM1Ng==", "bodyText": "make it return long.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r452192356", "createdAt": "2020-07-09T12:49:44Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_PRODUCER_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+    private final ErrorCountingCallback errorCounter;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = requireNonNull(ImmutableList.copyOf(columns), \"columns is null\");\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        requireNonNull(producerFactory, \"producerFactory is null\");\n+        this.producer = producerFactory.create();\n+        this.errorCounter = new ErrorCountingCallback();\n+    }\n+\n+    private static class ErrorCountingCallback\n+            implements Callback\n+    {\n+        private final AtomicLong errorCounter;\n+\n+        public ErrorCountingCallback()\n+        {\n+            this.errorCounter = new AtomicLong(0);\n+        }\n+\n+        @Override\n+        public void onCompletion(RecordMetadata recordMetadata, Exception e)\n+        {\n+            if (e != null) {\n+                errorCounter.incrementAndGet();\n+            }\n+        }\n+\n+        public AtomicLong getErrorCount()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE5Mjg5MQ==", "bodyText": "nit s/'%d'/%d/", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r452192891", "createdAt": "2020-07-09T12:50:38Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_PRODUCER_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+    private final ErrorCountingCallback errorCounter;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = requireNonNull(ImmutableList.copyOf(columns), \"columns is null\");\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        requireNonNull(producerFactory, \"producerFactory is null\");\n+        this.producer = producerFactory.create();\n+        this.errorCounter = new ErrorCountingCallback();\n+    }\n+\n+    private static class ErrorCountingCallback\n+            implements Callback\n+    {\n+        private final AtomicLong errorCounter;\n+\n+        public ErrorCountingCallback()\n+        {\n+            this.errorCounter = new AtomicLong(0);\n+        }\n+\n+        @Override\n+        public void onCompletion(RecordMetadata recordMetadata, Exception e)\n+        {\n+            if (e != null) {\n+                errorCounter.incrementAndGet();\n+            }\n+        }\n+\n+        public AtomicLong getErrorCount()\n+        {\n+            return errorCounter;\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<?> appendPage(Page page)\n+    {\n+        for (int position = 0; position < page.getPositionCount(); position++) {\n+            for (int channel = 0; channel < page.getChannelCount(); channel++) {\n+                if (columns.get(channel).isKeyCodec()) {\n+                    keyEncoder.appendColumnValue(page.getBlock(channel), position);\n+                }\n+                else {\n+                    messageEncoder.appendColumnValue(page.getBlock(channel), position);\n+                }\n+            }\n+            producer.send(new ProducerRecord<>(topicName, keyEncoder.toByteArray(), messageEncoder.toByteArray()), errorCounter);\n+\n+            keyEncoder.reset();\n+            messageEncoder.reset();\n+        }\n+        return NOT_BLOCKED;\n+    }\n+\n+    @Override\n+    public CompletableFuture<Collection<Slice>> finish()\n+    {\n+        producer.flush();\n+        producer.close();\n+        if (errorCounter.getErrorCount().get() > 0) {\n+            throw new PrestoException(KAFKA_PRODUCER_ERROR, format(\"'%d' producer record('s) failed to send\", errorCounter.getErrorCount().get()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE5NDg2Mw==", "bodyText": "Capitalize Kafka in commit messages and (if there are occurences) comments.", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r452194863", "createdAt": "2020-07-09T12:54:02Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaColumnHandle.java", "diffHunk": "@@ -25,7 +26,7 @@\n import static java.util.Objects.requireNonNull;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE5Nzc1Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    validateColumns(columnHandles);\n          \n          \n            \n                    this.columnHandles = ImmutableList.copyOf(columnHandles);\n          \n          \n            \n                    this.columnHandles = ImmutableList.copyOf(columnHandles);\n          \n          \n            \n                    validateColumns(this.columnHandles);", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r452197753", "createdAt": "2020-07-09T12:58:44Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/AbstractRowEncoder.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.primitives.Shorts;\n+import com.google.common.primitives.SignedBytes;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlDate;\n+import io.prestosql.spi.type.SqlTime;\n+import io.prestosql.spi.type.SqlTimeWithTimeZone;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.SqlTimestampWithTimeZone;\n+import io.prestosql.spi.type.TimestampType;\n+import io.prestosql.spi.type.TimestampWithTimeZoneType;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DateType.DATE;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TimeType.TIME;\n+import static io.prestosql.spi.type.TimeWithTimeZoneType.TIME_WITH_TIME_ZONE;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.VarbinaryType.isVarbinaryType;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public abstract class AbstractRowEncoder\n+        implements RowEncoder\n+{\n+    protected final ConnectorSession session;\n+    protected final List<EncoderColumnHandle> columnHandles;\n+\n+    /**\n+     * The current column index for appending values to the row encoder.\n+     * Gets incremented by appendColumnValue and set back to zero when the encoder is reset.\n+     */\n+    protected int currentColumnIndex;\n+\n+    protected AbstractRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        this.session = requireNonNull(session, \"session is null\");\n+        requireNonNull(columnHandles, \"columnHandles is null\");\n+        validateColumns(columnHandles);\n+        this.columnHandles = ImmutableList.copyOf(columnHandles);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjIwMDk0NQ==", "bodyText": "Any idea if we can write a test which triggers errorcounter?", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r452200945", "createdAt": "2020-07-09T13:03:58Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaPageSink.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.slice.Slice;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorPageSink;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_PRODUCER_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class KafkaPageSink\n+        implements ConnectorPageSink\n+{\n+    private final String topicName;\n+    private final List<KafkaColumnHandle> columns;\n+    private final RowEncoder keyEncoder;\n+    private final RowEncoder messageEncoder;\n+    private final KafkaProducer<byte[], byte[]> producer;\n+    private final ErrorCountingCallback errorCounter;\n+\n+    public KafkaPageSink(\n+            String topicName,\n+            List<KafkaColumnHandle> columns,\n+            RowEncoder keyEncoder,\n+            RowEncoder messageEncoder,\n+            PlainTextKafkaProducerFactory producerFactory)\n+    {\n+        this.topicName = requireNonNull(topicName, \"topicName is null\");\n+        this.columns = requireNonNull(ImmutableList.copyOf(columns), \"columns is null\");\n+        this.keyEncoder = requireNonNull(keyEncoder, \"keyEncoder is null\");\n+        this.messageEncoder = requireNonNull(messageEncoder, \"messageEncoder is null\");\n+        requireNonNull(producerFactory, \"producerFactory is null\");\n+        this.producer = producerFactory.create();\n+        this.errorCounter = new ErrorCountingCallback();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ1ODExODEy", "url": "https://github.com/trinodb/trino/pull/4287#pullrequestreview-445811812", "createdAt": "2020-07-09T17:10:11Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNzoxMDoxMlrOGvaTlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxNzoxMDoxMlrOGvaTlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM2NzI1Mg==", "bodyText": "I wonder if we need this method at all.  Perhaps the contract could be: add data, then generate the record, rinse, repeat.  Generating the byte[] could reset things (that would be an implementation detail).", "url": "https://github.com/trinodb/trino/pull/4287#discussion_r452367252", "createdAt": "2020-07-09T17:10:12Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder;\n+\n+import io.prestosql.spi.block.Block;\n+\n+public interface RowEncoder\n+{\n+    /**\n+     * Adds the value from the given block/position to the row being encoded\n+     */\n+    RowEncoder appendColumnValue(Block block, int position);\n+\n+    /**\n+     * Returns the encoded values as a byte array\n+     */\n+    byte[] toByteArray();\n+\n+    /**\n+     * Resets the encoder so that it can be used for the next row\n+     */\n+    void reset();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b67102f9096300bcf67354cd0c151e5b6a91faa2", "author": {"user": {"login": "charlesjmorgan", "name": "Charles Morgan"}}, "url": "https://github.com/trinodb/trino/commit/b67102f9096300bcf67354cd0c151e5b6a91faa2", "committedDate": "2020-07-09T18:14:02Z", "message": "Implement inserts for Kafka connector"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ebec263807acd62541aa83289e083298617c9d6", "author": {"user": {"login": "charlesjmorgan", "name": "Charles Morgan"}}, "url": "https://github.com/trinodb/trino/commit/1ebec263807acd62541aa83289e083298617c9d6", "committedDate": "2020-07-09T18:14:06Z", "message": "Add CSV encoder and CSV roundtrip test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "54be340f460d656aefb2f5959b0aa915fba59e89", "author": {"user": {"login": "charlesjmorgan", "name": "Charles Morgan"}}, "url": "https://github.com/trinodb/trino/commit/54be340f460d656aefb2f5959b0aa915fba59e89", "committedDate": "2020-07-09T18:14:09Z", "message": "Remove suppress warnings labels"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "54be340f460d656aefb2f5959b0aa915fba59e89", "author": {"user": {"login": "charlesjmorgan", "name": "Charles Morgan"}}, "url": "https://github.com/trinodb/trino/commit/54be340f460d656aefb2f5959b0aa915fba59e89", "committedDate": "2020-07-09T18:14:09Z", "message": "Remove suppress warnings labels"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2MzA0ODY5", "url": "https://github.com/trinodb/trino/pull/4287#pullrequestreview-446304869", "createdAt": "2020-07-10T10:44:58Z", "commit": {"oid": "54be340f460d656aefb2f5959b0aa915fba59e89"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 294, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}