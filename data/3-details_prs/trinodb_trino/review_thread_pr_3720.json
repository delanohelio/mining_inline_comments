{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE3NDQ3ODAy", "number": 3720, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNjo0MTo1OVrOD8JiqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwNDo0MTowMVrOD8UeNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0Mzk3NDgxOnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/rubix/TestRubixCaching.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNjo0MTo1OVrOGU6Wqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNjo0MTo1OVrOGU6Wqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU4MDc3OA==", "bodyText": "Please use some temp directory for that.\nEdit - oh - it actually uses tempDirectory. Plese name method storagePath(). And pass / to it instead .", "url": "https://github.com/trinodb/trino/pull/3720#discussion_r424580778", "createdAt": "2020-05-13T16:41:59Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/rubix/TestRubixCaching.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.rubix;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.io.CharStreams;\n+import com.google.common.io.Closer;\n+import com.qubole.rubix.bookkeeper.LocalDataTransferServer;\n+import com.qubole.rubix.core.CachingFileSystem;\n+import com.qubole.rubix.core.CachingFileSystemStats;\n+import com.qubole.rubix.core.utils.DummyClusterManager;\n+import io.prestosql.metadata.InternalNode;\n+import io.prestosql.plugin.base.CatalogName;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HdfsEnvironment.HdfsContext;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.spi.security.ConnectorIdentity;\n+import io.prestosql.testing.TestingNodeManager;\n+import net.jodah.failsafe.Failsafe;\n+import net.jodah.failsafe.RetryPolicy;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FilterFileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.lang.reflect.Field;\n+import java.net.URI;\n+import java.time.Duration;\n+import java.util.List;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.io.MoreFiles.deleteRecursively;\n+import static com.google.common.io.RecursiveDeleteOption.ALLOW_INSECURE;\n+import static com.qubole.rubix.spi.CacheConfig.setPrestoClusterManager;\n+import static io.airlift.testing.Assertions.assertGreaterThan;\n+import static io.airlift.testing.Assertions.assertInstanceOf;\n+import static io.prestosql.client.NodeVersion.UNKNOWN;\n+import static io.prestosql.plugin.hive.DynamicConfigurationProvider.setCacheKey;\n+import static java.lang.String.format;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.nio.file.Files.createDirectories;\n+import static java.nio.file.Files.createTempDirectory;\n+import static org.apache.parquet.Strings.join;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestRubixCaching\n+{\n+    private java.nio.file.Path tempDirectory;\n+    private FileSystem nonCachingFileSystem;\n+    private FileSystem cachingFileSystem;\n+\n+    @BeforeClass\n+    public void setup()\n+            throws IOException\n+    {\n+        tempDirectory = createTempDirectory(getClass().getSimpleName());\n+\n+        HdfsConfig config = new HdfsConfig();\n+        HdfsConfigurationInitializer configurationInitializer = new HdfsConfigurationInitializer(config);\n+        HiveHdfsConfiguration configuration = new HiveHdfsConfiguration(configurationInitializer, ImmutableSet.of());\n+        HdfsEnvironment environment = new HdfsEnvironment(configuration, config, new NoHdfsAuthentication());\n+        ConnectorIdentity identity = ConnectorIdentity.ofUser(\"user\");\n+        HdfsContext context = new HdfsContext(identity);\n+        Path path = getPath(\".\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0Mzk3OTc0OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/rubix/TestRubixCaching.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNjo0MzoxN1rOGU6ZzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwOToxNzo0N1rOGVTbxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU4MTU4MA==", "bodyText": "name variable nonCachingEnvironment. And below create new variable and name it cachingEnvironment. Generally try not to reuse variables in different contexts.", "url": "https://github.com/trinodb/trino/pull/3720#discussion_r424581580", "createdAt": "2020-05-13T16:43:17Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/rubix/TestRubixCaching.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.rubix;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.io.CharStreams;\n+import com.google.common.io.Closer;\n+import com.qubole.rubix.bookkeeper.LocalDataTransferServer;\n+import com.qubole.rubix.core.CachingFileSystem;\n+import com.qubole.rubix.core.CachingFileSystemStats;\n+import com.qubole.rubix.core.utils.DummyClusterManager;\n+import io.prestosql.metadata.InternalNode;\n+import io.prestosql.plugin.base.CatalogName;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HdfsEnvironment.HdfsContext;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.spi.security.ConnectorIdentity;\n+import io.prestosql.testing.TestingNodeManager;\n+import net.jodah.failsafe.Failsafe;\n+import net.jodah.failsafe.RetryPolicy;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FilterFileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.lang.reflect.Field;\n+import java.net.URI;\n+import java.time.Duration;\n+import java.util.List;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.io.MoreFiles.deleteRecursively;\n+import static com.google.common.io.RecursiveDeleteOption.ALLOW_INSECURE;\n+import static com.qubole.rubix.spi.CacheConfig.setPrestoClusterManager;\n+import static io.airlift.testing.Assertions.assertGreaterThan;\n+import static io.airlift.testing.Assertions.assertInstanceOf;\n+import static io.prestosql.client.NodeVersion.UNKNOWN;\n+import static io.prestosql.plugin.hive.DynamicConfigurationProvider.setCacheKey;\n+import static java.lang.String.format;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.nio.file.Files.createDirectories;\n+import static java.nio.file.Files.createTempDirectory;\n+import static org.apache.parquet.Strings.join;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestRubixCaching\n+{\n+    private java.nio.file.Path tempDirectory;\n+    private FileSystem nonCachingFileSystem;\n+    private FileSystem cachingFileSystem;\n+\n+    @BeforeClass\n+    public void setup()\n+            throws IOException\n+    {\n+        tempDirectory = createTempDirectory(getClass().getSimpleName());\n+\n+        HdfsConfig config = new HdfsConfig();\n+        HdfsConfigurationInitializer configurationInitializer = new HdfsConfigurationInitializer(config);\n+        HiveHdfsConfiguration configuration = new HiveHdfsConfiguration(configurationInitializer, ImmutableSet.of());\n+        HdfsEnvironment environment = new HdfsEnvironment(configuration, config, new NoHdfsAuthentication());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk5MTY4Ng==", "bodyText": "Extracted into separate methods", "url": "https://github.com/trinodb/trino/pull/3720#discussion_r424991686", "createdAt": "2020-05-14T09:17:47Z", "author": {"login": "sopel39"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/rubix/TestRubixCaching.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.rubix;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.io.CharStreams;\n+import com.google.common.io.Closer;\n+import com.qubole.rubix.bookkeeper.LocalDataTransferServer;\n+import com.qubole.rubix.core.CachingFileSystem;\n+import com.qubole.rubix.core.CachingFileSystemStats;\n+import com.qubole.rubix.core.utils.DummyClusterManager;\n+import io.prestosql.metadata.InternalNode;\n+import io.prestosql.plugin.base.CatalogName;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HdfsEnvironment.HdfsContext;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.spi.security.ConnectorIdentity;\n+import io.prestosql.testing.TestingNodeManager;\n+import net.jodah.failsafe.Failsafe;\n+import net.jodah.failsafe.RetryPolicy;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FilterFileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.lang.reflect.Field;\n+import java.net.URI;\n+import java.time.Duration;\n+import java.util.List;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.io.MoreFiles.deleteRecursively;\n+import static com.google.common.io.RecursiveDeleteOption.ALLOW_INSECURE;\n+import static com.qubole.rubix.spi.CacheConfig.setPrestoClusterManager;\n+import static io.airlift.testing.Assertions.assertGreaterThan;\n+import static io.airlift.testing.Assertions.assertInstanceOf;\n+import static io.prestosql.client.NodeVersion.UNKNOWN;\n+import static io.prestosql.plugin.hive.DynamicConfigurationProvider.setCacheKey;\n+import static java.lang.String.format;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.nio.file.Files.createDirectories;\n+import static java.nio.file.Files.createTempDirectory;\n+import static org.apache.parquet.Strings.join;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestRubixCaching\n+{\n+    private java.nio.file.Path tempDirectory;\n+    private FileSystem nonCachingFileSystem;\n+    private FileSystem cachingFileSystem;\n+\n+    @BeforeClass\n+    public void setup()\n+            throws IOException\n+    {\n+        tempDirectory = createTempDirectory(getClass().getSimpleName());\n+\n+        HdfsConfig config = new HdfsConfig();\n+        HdfsConfigurationInitializer configurationInitializer = new HdfsConfigurationInitializer(config);\n+        HiveHdfsConfiguration configuration = new HiveHdfsConfiguration(configurationInitializer, ImmutableSet.of());\n+        HdfsEnvironment environment = new HdfsEnvironment(configuration, config, new NoHdfsAuthentication());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU4MTU4MA=="}, "originalCommit": null, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0Mzk4OTgzOnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/rubix/TestRubixCaching.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNjo0NTo1OVrOGU6gQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwOToxODozNlrOGVTd3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU4MzIzNQ==", "bodyText": "feels like very much implementation detail.\nDo we care what is he class name? I would drop it. You are testing if cache works in testCacheRead - and that is what we care about.", "url": "https://github.com/trinodb/trino/pull/3720#discussion_r424583235", "createdAt": "2020-05-13T16:45:59Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/rubix/TestRubixCaching.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.rubix;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.io.CharStreams;\n+import com.google.common.io.Closer;\n+import com.qubole.rubix.bookkeeper.LocalDataTransferServer;\n+import com.qubole.rubix.core.CachingFileSystem;\n+import com.qubole.rubix.core.CachingFileSystemStats;\n+import com.qubole.rubix.core.utils.DummyClusterManager;\n+import io.prestosql.metadata.InternalNode;\n+import io.prestosql.plugin.base.CatalogName;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HdfsEnvironment.HdfsContext;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.spi.security.ConnectorIdentity;\n+import io.prestosql.testing.TestingNodeManager;\n+import net.jodah.failsafe.Failsafe;\n+import net.jodah.failsafe.RetryPolicy;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FilterFileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.lang.reflect.Field;\n+import java.net.URI;\n+import java.time.Duration;\n+import java.util.List;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.io.MoreFiles.deleteRecursively;\n+import static com.google.common.io.RecursiveDeleteOption.ALLOW_INSECURE;\n+import static com.qubole.rubix.spi.CacheConfig.setPrestoClusterManager;\n+import static io.airlift.testing.Assertions.assertGreaterThan;\n+import static io.airlift.testing.Assertions.assertInstanceOf;\n+import static io.prestosql.client.NodeVersion.UNKNOWN;\n+import static io.prestosql.plugin.hive.DynamicConfigurationProvider.setCacheKey;\n+import static java.lang.String.format;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.nio.file.Files.createDirectories;\n+import static java.nio.file.Files.createTempDirectory;\n+import static org.apache.parquet.Strings.join;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestRubixCaching\n+{\n+    private java.nio.file.Path tempDirectory;\n+    private FileSystem nonCachingFileSystem;\n+    private FileSystem cachingFileSystem;\n+\n+    @BeforeClass\n+    public void setup()\n+            throws IOException\n+    {\n+        tempDirectory = createTempDirectory(getClass().getSimpleName());\n+\n+        HdfsConfig config = new HdfsConfig();\n+        HdfsConfigurationInitializer configurationInitializer = new HdfsConfigurationInitializer(config);\n+        HiveHdfsConfiguration configuration = new HiveHdfsConfiguration(configurationInitializer, ImmutableSet.of());\n+        HdfsEnvironment environment = new HdfsEnvironment(configuration, config, new NoHdfsAuthentication());\n+        ConnectorIdentity identity = ConnectorIdentity.ofUser(\"user\");\n+        HdfsContext context = new HdfsContext(identity);\n+        Path path = getPath(\".\");\n+\n+        nonCachingFileSystem = environment.getFileSystem(context, path);\n+\n+        // create cache directories\n+        List<java.nio.file.Path> cacheDirectories = ImmutableList.of(\n+                tempDirectory.resolve(\"cache1\"),\n+                tempDirectory.resolve(\"cache2\"));\n+        for (java.nio.file.Path directory : cacheDirectories) {\n+            createDirectories(directory);\n+        }\n+\n+        // initialize rubix in master-only mode\n+        RubixConfig rubixConfig = new RubixConfig();\n+        rubixConfig.setCacheLocation(join(\n+                cacheDirectories.stream()\n+                        .map(java.nio.file.Path::toString)\n+                        .collect(toImmutableList()),\n+                \",\"));\n+        RubixConfigurationInitializer rubixConfigInitializer = new RubixConfigurationInitializer(rubixConfig);\n+        configurationInitializer = new HdfsConfigurationInitializer(config, ImmutableSet.of(\n+                // make sure that dummy cluster manager is used\n+                initConfig -> setPrestoClusterManager(initConfig, DummyClusterManager.class.getName())));\n+        RubixInitializer rubixInitializer = new RubixInitializer(\n+                new CatalogName(\"catalog\"),\n+                rubixConfigInitializer,\n+                configurationInitializer);\n+        InternalNode coordinatorNode = new InternalNode(\n+                \"master\",\n+                URI.create(\"http://127.0.0.1:8080\"),\n+                UNKNOWN,\n+                true);\n+        TestingNodeManager nodeManager = new TestingNodeManager(\n+                coordinatorNode,\n+                ImmutableList.of());\n+        rubixInitializer.initializeRubix(nodeManager);\n+\n+        // wait for rubix to start\n+        Failsafe.with(\n+                new RetryPolicy<>()\n+                        .withDelay(Duration.ofSeconds(1))\n+                        // unlimited attempts\n+                        .withMaxAttempts(-1)\n+                        .withMaxDuration(Duration.ofMinutes(1)))\n+                .run(() -> checkState(rubixConfigInitializer.isCacheReady()));\n+\n+        // get rubix caching filesystem\n+        configurationInitializer = new HdfsConfigurationInitializer(config, ImmutableSet.of(rubixConfigInitializer));\n+        configuration = new HiveHdfsConfiguration(configurationInitializer, ImmutableSet.of(\n+                (dynamicConfig, ignoredContext, ignoredUri) -> {\n+                    // make sure a new FS is created with Rubix caching enabled\n+                    setCacheKey(dynamicConfig, \"rubix\");\n+                    dynamicConfig.set(\"fs.file.impl\", CachingLocalFileSystem.class.getName());\n+                }));\n+        environment = new HdfsEnvironment(configuration, config, new NoHdfsAuthentication());\n+\n+        cachingFileSystem = environment.getFileSystem(context, path);\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void tearDown()\n+            throws IOException\n+    {\n+        try (Closer closer = Closer.create()) {\n+            closer.register(() -> deleteRecursively(tempDirectory, ALLOW_INSECURE));\n+            closer.register(() -> nonCachingFileSystem.close());\n+            closer.register(() -> cachingFileSystem.close());\n+            closer.register(LocalDataTransferServer::stopServer);\n+        }\n+    }\n+\n+    @Test\n+    public void testCacheRead()\n+            throws Exception\n+    {\n+        Path file = getPath(\"some_file\");\n+\n+        writeFile(nonCachingFileSystem.create(file), \"Hello world\");\n+\n+        double beforeCachedReads = getStats().getCachedReads();\n+\n+        assertEquals(readFile(cachingFileSystem.open(file)), \"Hello world\");\n+        assertEquals(readFile(cachingFileSystem.open(file)), \"Hello world\");\n+\n+        double afterCachedReads = getStats().getCachedReads();\n+        assertGreaterThan(afterCachedReads, beforeCachedReads);\n+    }\n+\n+    @Test\n+    public void testFileSystems()\n+    {\n+        assertInstanceOf(nonCachingFileSystem, FilterFileSystem.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk5MjIyMg==", "bodyText": "removed", "url": "https://github.com/trinodb/trino/pull/3720#discussion_r424992222", "createdAt": "2020-05-14T09:18:36Z", "author": {"login": "sopel39"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/rubix/TestRubixCaching.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.rubix;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.io.CharStreams;\n+import com.google.common.io.Closer;\n+import com.qubole.rubix.bookkeeper.LocalDataTransferServer;\n+import com.qubole.rubix.core.CachingFileSystem;\n+import com.qubole.rubix.core.CachingFileSystemStats;\n+import com.qubole.rubix.core.utils.DummyClusterManager;\n+import io.prestosql.metadata.InternalNode;\n+import io.prestosql.plugin.base.CatalogName;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HdfsEnvironment.HdfsContext;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.spi.security.ConnectorIdentity;\n+import io.prestosql.testing.TestingNodeManager;\n+import net.jodah.failsafe.Failsafe;\n+import net.jodah.failsafe.RetryPolicy;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FilterFileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.lang.reflect.Field;\n+import java.net.URI;\n+import java.time.Duration;\n+import java.util.List;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.io.MoreFiles.deleteRecursively;\n+import static com.google.common.io.RecursiveDeleteOption.ALLOW_INSECURE;\n+import static com.qubole.rubix.spi.CacheConfig.setPrestoClusterManager;\n+import static io.airlift.testing.Assertions.assertGreaterThan;\n+import static io.airlift.testing.Assertions.assertInstanceOf;\n+import static io.prestosql.client.NodeVersion.UNKNOWN;\n+import static io.prestosql.plugin.hive.DynamicConfigurationProvider.setCacheKey;\n+import static java.lang.String.format;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.nio.file.Files.createDirectories;\n+import static java.nio.file.Files.createTempDirectory;\n+import static org.apache.parquet.Strings.join;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestRubixCaching\n+{\n+    private java.nio.file.Path tempDirectory;\n+    private FileSystem nonCachingFileSystem;\n+    private FileSystem cachingFileSystem;\n+\n+    @BeforeClass\n+    public void setup()\n+            throws IOException\n+    {\n+        tempDirectory = createTempDirectory(getClass().getSimpleName());\n+\n+        HdfsConfig config = new HdfsConfig();\n+        HdfsConfigurationInitializer configurationInitializer = new HdfsConfigurationInitializer(config);\n+        HiveHdfsConfiguration configuration = new HiveHdfsConfiguration(configurationInitializer, ImmutableSet.of());\n+        HdfsEnvironment environment = new HdfsEnvironment(configuration, config, new NoHdfsAuthentication());\n+        ConnectorIdentity identity = ConnectorIdentity.ofUser(\"user\");\n+        HdfsContext context = new HdfsContext(identity);\n+        Path path = getPath(\".\");\n+\n+        nonCachingFileSystem = environment.getFileSystem(context, path);\n+\n+        // create cache directories\n+        List<java.nio.file.Path> cacheDirectories = ImmutableList.of(\n+                tempDirectory.resolve(\"cache1\"),\n+                tempDirectory.resolve(\"cache2\"));\n+        for (java.nio.file.Path directory : cacheDirectories) {\n+            createDirectories(directory);\n+        }\n+\n+        // initialize rubix in master-only mode\n+        RubixConfig rubixConfig = new RubixConfig();\n+        rubixConfig.setCacheLocation(join(\n+                cacheDirectories.stream()\n+                        .map(java.nio.file.Path::toString)\n+                        .collect(toImmutableList()),\n+                \",\"));\n+        RubixConfigurationInitializer rubixConfigInitializer = new RubixConfigurationInitializer(rubixConfig);\n+        configurationInitializer = new HdfsConfigurationInitializer(config, ImmutableSet.of(\n+                // make sure that dummy cluster manager is used\n+                initConfig -> setPrestoClusterManager(initConfig, DummyClusterManager.class.getName())));\n+        RubixInitializer rubixInitializer = new RubixInitializer(\n+                new CatalogName(\"catalog\"),\n+                rubixConfigInitializer,\n+                configurationInitializer);\n+        InternalNode coordinatorNode = new InternalNode(\n+                \"master\",\n+                URI.create(\"http://127.0.0.1:8080\"),\n+                UNKNOWN,\n+                true);\n+        TestingNodeManager nodeManager = new TestingNodeManager(\n+                coordinatorNode,\n+                ImmutableList.of());\n+        rubixInitializer.initializeRubix(nodeManager);\n+\n+        // wait for rubix to start\n+        Failsafe.with(\n+                new RetryPolicy<>()\n+                        .withDelay(Duration.ofSeconds(1))\n+                        // unlimited attempts\n+                        .withMaxAttempts(-1)\n+                        .withMaxDuration(Duration.ofMinutes(1)))\n+                .run(() -> checkState(rubixConfigInitializer.isCacheReady()));\n+\n+        // get rubix caching filesystem\n+        configurationInitializer = new HdfsConfigurationInitializer(config, ImmutableSet.of(rubixConfigInitializer));\n+        configuration = new HiveHdfsConfiguration(configurationInitializer, ImmutableSet.of(\n+                (dynamicConfig, ignoredContext, ignoredUri) -> {\n+                    // make sure a new FS is created with Rubix caching enabled\n+                    setCacheKey(dynamicConfig, \"rubix\");\n+                    dynamicConfig.set(\"fs.file.impl\", CachingLocalFileSystem.class.getName());\n+                }));\n+        environment = new HdfsEnvironment(configuration, config, new NoHdfsAuthentication());\n+\n+        cachingFileSystem = environment.getFileSystem(context, path);\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void tearDown()\n+            throws IOException\n+    {\n+        try (Closer closer = Closer.create()) {\n+            closer.register(() -> deleteRecursively(tempDirectory, ALLOW_INSECURE));\n+            closer.register(() -> nonCachingFileSystem.close());\n+            closer.register(() -> cachingFileSystem.close());\n+            closer.register(LocalDataTransferServer::stopServer);\n+        }\n+    }\n+\n+    @Test\n+    public void testCacheRead()\n+            throws Exception\n+    {\n+        Path file = getPath(\"some_file\");\n+\n+        writeFile(nonCachingFileSystem.create(file), \"Hello world\");\n+\n+        double beforeCachedReads = getStats().getCachedReads();\n+\n+        assertEquals(readFile(cachingFileSystem.open(file)), \"Hello world\");\n+        assertEquals(readFile(cachingFileSystem.open(file)), \"Hello world\");\n+\n+        double afterCachedReads = getStats().getCachedReads();\n+        assertGreaterThan(afterCachedReads, beforeCachedReads);\n+    }\n+\n+    @Test\n+    public void testFileSystems()\n+    {\n+        assertInstanceOf(nonCachingFileSystem, FilterFileSystem.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU4MzIzNQ=="}, "originalCommit": null, "originalPosition": 179}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0NDAwNTA1OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/rubix/TestRubixCaching.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNjo0OTo1NFrOGU6qFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNjo0OTo1NFrOGU6qFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU4NTc0OQ==", "bodyText": "getCachingStats?", "url": "https://github.com/trinodb/trino/pull/3720#discussion_r424585749", "createdAt": "2020-05-13T16:49:54Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/rubix/TestRubixCaching.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.rubix;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.io.CharStreams;\n+import com.google.common.io.Closer;\n+import com.qubole.rubix.bookkeeper.LocalDataTransferServer;\n+import com.qubole.rubix.core.CachingFileSystem;\n+import com.qubole.rubix.core.CachingFileSystemStats;\n+import com.qubole.rubix.core.utils.DummyClusterManager;\n+import io.prestosql.metadata.InternalNode;\n+import io.prestosql.plugin.base.CatalogName;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HdfsEnvironment.HdfsContext;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.spi.security.ConnectorIdentity;\n+import io.prestosql.testing.TestingNodeManager;\n+import net.jodah.failsafe.Failsafe;\n+import net.jodah.failsafe.RetryPolicy;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FilterFileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.lang.reflect.Field;\n+import java.net.URI;\n+import java.time.Duration;\n+import java.util.List;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.io.MoreFiles.deleteRecursively;\n+import static com.google.common.io.RecursiveDeleteOption.ALLOW_INSECURE;\n+import static com.qubole.rubix.spi.CacheConfig.setPrestoClusterManager;\n+import static io.airlift.testing.Assertions.assertGreaterThan;\n+import static io.airlift.testing.Assertions.assertInstanceOf;\n+import static io.prestosql.client.NodeVersion.UNKNOWN;\n+import static io.prestosql.plugin.hive.DynamicConfigurationProvider.setCacheKey;\n+import static java.lang.String.format;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.nio.file.Files.createDirectories;\n+import static java.nio.file.Files.createTempDirectory;\n+import static org.apache.parquet.Strings.join;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestRubixCaching\n+{\n+    private java.nio.file.Path tempDirectory;\n+    private FileSystem nonCachingFileSystem;\n+    private FileSystem cachingFileSystem;\n+\n+    @BeforeClass\n+    public void setup()\n+            throws IOException\n+    {\n+        tempDirectory = createTempDirectory(getClass().getSimpleName());\n+\n+        HdfsConfig config = new HdfsConfig();\n+        HdfsConfigurationInitializer configurationInitializer = new HdfsConfigurationInitializer(config);\n+        HiveHdfsConfiguration configuration = new HiveHdfsConfiguration(configurationInitializer, ImmutableSet.of());\n+        HdfsEnvironment environment = new HdfsEnvironment(configuration, config, new NoHdfsAuthentication());\n+        ConnectorIdentity identity = ConnectorIdentity.ofUser(\"user\");\n+        HdfsContext context = new HdfsContext(identity);\n+        Path path = getPath(\".\");\n+\n+        nonCachingFileSystem = environment.getFileSystem(context, path);\n+\n+        // create cache directories\n+        List<java.nio.file.Path> cacheDirectories = ImmutableList.of(\n+                tempDirectory.resolve(\"cache1\"),\n+                tempDirectory.resolve(\"cache2\"));\n+        for (java.nio.file.Path directory : cacheDirectories) {\n+            createDirectories(directory);\n+        }\n+\n+        // initialize rubix in master-only mode\n+        RubixConfig rubixConfig = new RubixConfig();\n+        rubixConfig.setCacheLocation(join(\n+                cacheDirectories.stream()\n+                        .map(java.nio.file.Path::toString)\n+                        .collect(toImmutableList()),\n+                \",\"));\n+        RubixConfigurationInitializer rubixConfigInitializer = new RubixConfigurationInitializer(rubixConfig);\n+        configurationInitializer = new HdfsConfigurationInitializer(config, ImmutableSet.of(\n+                // make sure that dummy cluster manager is used\n+                initConfig -> setPrestoClusterManager(initConfig, DummyClusterManager.class.getName())));\n+        RubixInitializer rubixInitializer = new RubixInitializer(\n+                new CatalogName(\"catalog\"),\n+                rubixConfigInitializer,\n+                configurationInitializer);\n+        InternalNode coordinatorNode = new InternalNode(\n+                \"master\",\n+                URI.create(\"http://127.0.0.1:8080\"),\n+                UNKNOWN,\n+                true);\n+        TestingNodeManager nodeManager = new TestingNodeManager(\n+                coordinatorNode,\n+                ImmutableList.of());\n+        rubixInitializer.initializeRubix(nodeManager);\n+\n+        // wait for rubix to start\n+        Failsafe.with(\n+                new RetryPolicy<>()\n+                        .withDelay(Duration.ofSeconds(1))\n+                        // unlimited attempts\n+                        .withMaxAttempts(-1)\n+                        .withMaxDuration(Duration.ofMinutes(1)))\n+                .run(() -> checkState(rubixConfigInitializer.isCacheReady()));\n+\n+        // get rubix caching filesystem\n+        configurationInitializer = new HdfsConfigurationInitializer(config, ImmutableSet.of(rubixConfigInitializer));\n+        configuration = new HiveHdfsConfiguration(configurationInitializer, ImmutableSet.of(\n+                (dynamicConfig, ignoredContext, ignoredUri) -> {\n+                    // make sure a new FS is created with Rubix caching enabled\n+                    setCacheKey(dynamicConfig, \"rubix\");\n+                    dynamicConfig.set(\"fs.file.impl\", CachingLocalFileSystem.class.getName());\n+                }));\n+        environment = new HdfsEnvironment(configuration, config, new NoHdfsAuthentication());\n+\n+        cachingFileSystem = environment.getFileSystem(context, path);\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void tearDown()\n+            throws IOException\n+    {\n+        try (Closer closer = Closer.create()) {\n+            closer.register(() -> deleteRecursively(tempDirectory, ALLOW_INSECURE));\n+            closer.register(() -> nonCachingFileSystem.close());\n+            closer.register(() -> cachingFileSystem.close());\n+            closer.register(LocalDataTransferServer::stopServer);\n+        }\n+    }\n+\n+    @Test\n+    public void testCacheRead()\n+            throws Exception\n+    {\n+        Path file = getPath(\"some_file\");\n+\n+        writeFile(nonCachingFileSystem.create(file), \"Hello world\");\n+\n+        double beforeCachedReads = getStats().getCachedReads();\n+\n+        assertEquals(readFile(cachingFileSystem.open(file)), \"Hello world\");\n+        assertEquals(readFile(cachingFileSystem.open(file)), \"Hello world\");\n+\n+        double afterCachedReads = getStats().getCachedReads();\n+        assertGreaterThan(afterCachedReads, beforeCachedReads);\n+    }\n+\n+    @Test\n+    public void testFileSystems()\n+    {\n+        assertInstanceOf(nonCachingFileSystem, FilterFileSystem.class);\n+        FileSystem rawNonCachingFileSystem = ((FilterFileSystem) nonCachingFileSystem).getRawFileSystem();\n+        assertInstanceOf(rawNonCachingFileSystem, LocalFileSystem.class);\n+\n+        assertInstanceOf(cachingFileSystem, FilterFileSystem.class);\n+        FileSystem rawCachingFileSystem = ((FilterFileSystem) cachingFileSystem).getRawFileSystem();\n+        assertInstanceOf(rawCachingFileSystem, CachingLocalFileSystem.class);\n+    }\n+\n+    private String readFile(FSDataInputStream inputStream)\n+            throws IOException\n+    {\n+        try {\n+            return CharStreams.toString(new InputStreamReader(inputStream, UTF_8));\n+        }\n+        finally {\n+            inputStream.close();\n+        }\n+    }\n+\n+    private void writeFile(FSDataOutputStream outputStream, String content)\n+            throws IOException\n+    {\n+        try {\n+            outputStream.writeBytes(content);\n+        }\n+        finally {\n+            outputStream.close();\n+        }\n+    }\n+\n+    private Path getPath(String path)\n+    {\n+        return new Path(format(\"file:///%s/storage/%s\", tempDirectory, path));\n+    }\n+\n+    private CachingFileSystemStats getStats()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 215}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0NTc2NTY1OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/rubix/TestRubixCaching.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwNDo0MTowMVrOGVL03A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwOTo1Mzo0MlrOGVUyoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg2NzAzNg==", "bodyText": "We should expose BookKeeperServer from RubixInitializer and call stopServer() on it in closer. Otherwise it will block bookKeeper port for rest of the tests in the run.", "url": "https://github.com/trinodb/trino/pull/3720#discussion_r424867036", "createdAt": "2020-05-14T04:41:01Z", "author": {"login": "shubhamtagra"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/rubix/TestRubixCaching.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.rubix;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.io.CharStreams;\n+import com.google.common.io.Closer;\n+import com.qubole.rubix.bookkeeper.LocalDataTransferServer;\n+import com.qubole.rubix.core.CachingFileSystem;\n+import com.qubole.rubix.core.CachingFileSystemStats;\n+import com.qubole.rubix.core.utils.DummyClusterManager;\n+import io.prestosql.metadata.InternalNode;\n+import io.prestosql.plugin.base.CatalogName;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HdfsEnvironment.HdfsContext;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.spi.security.ConnectorIdentity;\n+import io.prestosql.testing.TestingNodeManager;\n+import net.jodah.failsafe.Failsafe;\n+import net.jodah.failsafe.RetryPolicy;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FilterFileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.lang.reflect.Field;\n+import java.net.URI;\n+import java.time.Duration;\n+import java.util.List;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.io.MoreFiles.deleteRecursively;\n+import static com.google.common.io.RecursiveDeleteOption.ALLOW_INSECURE;\n+import static com.qubole.rubix.spi.CacheConfig.setPrestoClusterManager;\n+import static io.airlift.testing.Assertions.assertGreaterThan;\n+import static io.airlift.testing.Assertions.assertInstanceOf;\n+import static io.prestosql.client.NodeVersion.UNKNOWN;\n+import static io.prestosql.plugin.hive.DynamicConfigurationProvider.setCacheKey;\n+import static java.lang.String.format;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.nio.file.Files.createDirectories;\n+import static java.nio.file.Files.createTempDirectory;\n+import static org.apache.parquet.Strings.join;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestRubixCaching\n+{\n+    private java.nio.file.Path tempDirectory;\n+    private FileSystem nonCachingFileSystem;\n+    private FileSystem cachingFileSystem;\n+\n+    @BeforeClass\n+    public void setup()\n+            throws IOException\n+    {\n+        tempDirectory = createTempDirectory(getClass().getSimpleName());\n+\n+        HdfsConfig config = new HdfsConfig();\n+        HdfsConfigurationInitializer configurationInitializer = new HdfsConfigurationInitializer(config);\n+        HiveHdfsConfiguration configuration = new HiveHdfsConfiguration(configurationInitializer, ImmutableSet.of());\n+        HdfsEnvironment environment = new HdfsEnvironment(configuration, config, new NoHdfsAuthentication());\n+        ConnectorIdentity identity = ConnectorIdentity.ofUser(\"user\");\n+        HdfsContext context = new HdfsContext(identity);\n+        Path path = getPath(\".\");\n+\n+        nonCachingFileSystem = environment.getFileSystem(context, path);\n+\n+        // create cache directories\n+        List<java.nio.file.Path> cacheDirectories = ImmutableList.of(\n+                tempDirectory.resolve(\"cache1\"),\n+                tempDirectory.resolve(\"cache2\"));\n+        for (java.nio.file.Path directory : cacheDirectories) {\n+            createDirectories(directory);\n+        }\n+\n+        // initialize rubix in master-only mode\n+        RubixConfig rubixConfig = new RubixConfig();\n+        rubixConfig.setCacheLocation(join(\n+                cacheDirectories.stream()\n+                        .map(java.nio.file.Path::toString)\n+                        .collect(toImmutableList()),\n+                \",\"));\n+        RubixConfigurationInitializer rubixConfigInitializer = new RubixConfigurationInitializer(rubixConfig);\n+        configurationInitializer = new HdfsConfigurationInitializer(config, ImmutableSet.of(\n+                // make sure that dummy cluster manager is used\n+                initConfig -> setPrestoClusterManager(initConfig, DummyClusterManager.class.getName())));\n+        RubixInitializer rubixInitializer = new RubixInitializer(\n+                new CatalogName(\"catalog\"),\n+                rubixConfigInitializer,\n+                configurationInitializer);\n+        InternalNode coordinatorNode = new InternalNode(\n+                \"master\",\n+                URI.create(\"http://127.0.0.1:8080\"),\n+                UNKNOWN,\n+                true);\n+        TestingNodeManager nodeManager = new TestingNodeManager(\n+                coordinatorNode,\n+                ImmutableList.of());\n+        rubixInitializer.initializeRubix(nodeManager);\n+\n+        // wait for rubix to start\n+        Failsafe.with(\n+                new RetryPolicy<>()\n+                        .withDelay(Duration.ofSeconds(1))\n+                        // unlimited attempts\n+                        .withMaxAttempts(-1)\n+                        .withMaxDuration(Duration.ofMinutes(1)))\n+                .run(() -> checkState(rubixConfigInitializer.isCacheReady()));\n+\n+        // get rubix caching filesystem\n+        configurationInitializer = new HdfsConfigurationInitializer(config, ImmutableSet.of(rubixConfigInitializer));\n+        configuration = new HiveHdfsConfiguration(configurationInitializer, ImmutableSet.of(\n+                (dynamicConfig, ignoredContext, ignoredUri) -> {\n+                    // make sure a new FS is created with Rubix caching enabled\n+                    setCacheKey(dynamicConfig, \"rubix\");\n+                    dynamicConfig.set(\"fs.file.impl\", CachingLocalFileSystem.class.getName());\n+                }));\n+        environment = new HdfsEnvironment(configuration, config, new NoHdfsAuthentication());\n+\n+        cachingFileSystem = environment.getFileSystem(context, path);\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void tearDown()\n+            throws IOException\n+    {\n+        try (Closer closer = Closer.create()) {\n+            closer.register(() -> deleteRecursively(tempDirectory, ALLOW_INSECURE));\n+            closer.register(() -> nonCachingFileSystem.close());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTAxMzkyMQ==", "bodyText": "I will address that in next PR", "url": "https://github.com/trinodb/trino/pull/3720#discussion_r425013921", "createdAt": "2020-05-14T09:53:42Z", "author": {"login": "sopel39"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/rubix/TestRubixCaching.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.rubix;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.io.CharStreams;\n+import com.google.common.io.Closer;\n+import com.qubole.rubix.bookkeeper.LocalDataTransferServer;\n+import com.qubole.rubix.core.CachingFileSystem;\n+import com.qubole.rubix.core.CachingFileSystemStats;\n+import com.qubole.rubix.core.utils.DummyClusterManager;\n+import io.prestosql.metadata.InternalNode;\n+import io.prestosql.plugin.base.CatalogName;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HdfsEnvironment.HdfsContext;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.spi.security.ConnectorIdentity;\n+import io.prestosql.testing.TestingNodeManager;\n+import net.jodah.failsafe.Failsafe;\n+import net.jodah.failsafe.RetryPolicy;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FilterFileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.lang.reflect.Field;\n+import java.net.URI;\n+import java.time.Duration;\n+import java.util.List;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.io.MoreFiles.deleteRecursively;\n+import static com.google.common.io.RecursiveDeleteOption.ALLOW_INSECURE;\n+import static com.qubole.rubix.spi.CacheConfig.setPrestoClusterManager;\n+import static io.airlift.testing.Assertions.assertGreaterThan;\n+import static io.airlift.testing.Assertions.assertInstanceOf;\n+import static io.prestosql.client.NodeVersion.UNKNOWN;\n+import static io.prestosql.plugin.hive.DynamicConfigurationProvider.setCacheKey;\n+import static java.lang.String.format;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.nio.file.Files.createDirectories;\n+import static java.nio.file.Files.createTempDirectory;\n+import static org.apache.parquet.Strings.join;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestRubixCaching\n+{\n+    private java.nio.file.Path tempDirectory;\n+    private FileSystem nonCachingFileSystem;\n+    private FileSystem cachingFileSystem;\n+\n+    @BeforeClass\n+    public void setup()\n+            throws IOException\n+    {\n+        tempDirectory = createTempDirectory(getClass().getSimpleName());\n+\n+        HdfsConfig config = new HdfsConfig();\n+        HdfsConfigurationInitializer configurationInitializer = new HdfsConfigurationInitializer(config);\n+        HiveHdfsConfiguration configuration = new HiveHdfsConfiguration(configurationInitializer, ImmutableSet.of());\n+        HdfsEnvironment environment = new HdfsEnvironment(configuration, config, new NoHdfsAuthentication());\n+        ConnectorIdentity identity = ConnectorIdentity.ofUser(\"user\");\n+        HdfsContext context = new HdfsContext(identity);\n+        Path path = getPath(\".\");\n+\n+        nonCachingFileSystem = environment.getFileSystem(context, path);\n+\n+        // create cache directories\n+        List<java.nio.file.Path> cacheDirectories = ImmutableList.of(\n+                tempDirectory.resolve(\"cache1\"),\n+                tempDirectory.resolve(\"cache2\"));\n+        for (java.nio.file.Path directory : cacheDirectories) {\n+            createDirectories(directory);\n+        }\n+\n+        // initialize rubix in master-only mode\n+        RubixConfig rubixConfig = new RubixConfig();\n+        rubixConfig.setCacheLocation(join(\n+                cacheDirectories.stream()\n+                        .map(java.nio.file.Path::toString)\n+                        .collect(toImmutableList()),\n+                \",\"));\n+        RubixConfigurationInitializer rubixConfigInitializer = new RubixConfigurationInitializer(rubixConfig);\n+        configurationInitializer = new HdfsConfigurationInitializer(config, ImmutableSet.of(\n+                // make sure that dummy cluster manager is used\n+                initConfig -> setPrestoClusterManager(initConfig, DummyClusterManager.class.getName())));\n+        RubixInitializer rubixInitializer = new RubixInitializer(\n+                new CatalogName(\"catalog\"),\n+                rubixConfigInitializer,\n+                configurationInitializer);\n+        InternalNode coordinatorNode = new InternalNode(\n+                \"master\",\n+                URI.create(\"http://127.0.0.1:8080\"),\n+                UNKNOWN,\n+                true);\n+        TestingNodeManager nodeManager = new TestingNodeManager(\n+                coordinatorNode,\n+                ImmutableList.of());\n+        rubixInitializer.initializeRubix(nodeManager);\n+\n+        // wait for rubix to start\n+        Failsafe.with(\n+                new RetryPolicy<>()\n+                        .withDelay(Duration.ofSeconds(1))\n+                        // unlimited attempts\n+                        .withMaxAttempts(-1)\n+                        .withMaxDuration(Duration.ofMinutes(1)))\n+                .run(() -> checkState(rubixConfigInitializer.isCacheReady()));\n+\n+        // get rubix caching filesystem\n+        configurationInitializer = new HdfsConfigurationInitializer(config, ImmutableSet.of(rubixConfigInitializer));\n+        configuration = new HiveHdfsConfiguration(configurationInitializer, ImmutableSet.of(\n+                (dynamicConfig, ignoredContext, ignoredUri) -> {\n+                    // make sure a new FS is created with Rubix caching enabled\n+                    setCacheKey(dynamicConfig, \"rubix\");\n+                    dynamicConfig.set(\"fs.file.impl\", CachingLocalFileSystem.class.getName());\n+                }));\n+        environment = new HdfsEnvironment(configuration, config, new NoHdfsAuthentication());\n+\n+        cachingFileSystem = environment.getFileSystem(context, path);\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void tearDown()\n+            throws IOException\n+    {\n+        try (Closer closer = Closer.create()) {\n+            closer.register(() -> deleteRecursively(tempDirectory, ALLOW_INSECURE));\n+            closer.register(() -> nonCachingFileSystem.close());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg2NzAzNg=="}, "originalCommit": null, "originalPosition": 153}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4806, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}