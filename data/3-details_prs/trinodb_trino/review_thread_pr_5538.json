{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAyMTgwNzU2", "number": 5538, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNDoxOTowMVrOEtEFDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1Njo1M1rOEtYqNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1Njg4MjA2OnYy", "diffSide": "RIGHT", "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/TimestampMicrosColumnReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNDoxOTowMVrOHgpMGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNDo1Nzo0NlrOHgrFxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk5MTMyMw==", "bodyText": "Should we do the same for TimestampWithTimeZone?", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r503991323", "createdAt": "2020-10-13T14:19:01Z", "author": {"login": "alexjo2144"}, "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/TimestampMicrosColumnReader.java", "diffHunk": "@@ -40,9 +44,15 @@ protected void readValue(BlockBuilder blockBuilder, Type type)\n         if (definitionLevel == columnDescriptor.getMaxDefinitionLevel()) {\n             long epochMicros = valuesReader.readLong();\n             // TODO: specialize the class at creation time\n-            if (type.equals(TIMESTAMP_MICROS)) {\n+            if (type.equals(TIMESTAMP_MILLIS)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAyMjQ2OA==", "bodyText": "added", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r504022468", "createdAt": "2020-10-13T14:57:46Z", "author": {"login": "findepi"}, "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/TimestampMicrosColumnReader.java", "diffHunk": "@@ -40,9 +44,15 @@ protected void readValue(BlockBuilder blockBuilder, Type type)\n         if (definitionLevel == columnDescriptor.getMaxDefinitionLevel()) {\n             long epochMicros = valuesReader.readLong();\n             // TODO: specialize the class at creation time\n-            if (type.equals(TIMESTAMP_MICROS)) {\n+            if (type.equals(TIMESTAMP_MILLIS)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk5MTMyMw=="}, "originalCommit": null, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NzM1MzA0OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNTo1MzowM1rOHgtxrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1NjoyMVrOHhJGDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA2NjQ3OQ==", "bodyText": "It would be nice to have a concise comment on steps needed to create a file (if we wanted to add some rows to it later on).", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r504066479", "createdAt": "2020-10-13T15:53:03Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,209 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.common.io.Resources;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HivePageSourceFactory;\n+import io.prestosql.plugin.hive.HivePageSourceFactory.ReaderPageSourceWithProjections;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.HiveTimestampPrecision;\n+import io.prestosql.plugin.hive.HiveType;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.time.LocalDateTime;\n+import java.time.ZoneId;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static io.prestosql.plugin.hive.HiveColumnHandle.ColumnType.REGULAR;\n+import static io.prestosql.plugin.hive.HiveColumnHandle.createBaseColumn;\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_TIMESTAMP;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static io.prestosql.spi.type.TimestampType.createTimestampType;\n+import static io.prestosql.spi.type.TimestampWithTimeZoneType.createTimestampWithTimeZoneType;\n+import static io.prestosql.testing.MaterializedResult.materializeSourceDataStream;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_LIB;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+                for (int i = 0; i < firstPage.getPositionCount(); i++) {\n+                    expectedValues.next();\n+                }\n+\n+                while (!pageSource.isFinished()) {\n+                    Page page = pageSource.getNextPage();\n+                    if (page == null) {\n+                        continue;\n+                    }\n+                    Block block = page.getBlock(0);\n+\n+                    for (int i = 0; i < block.getPositionCount(); i++) {\n+                        assertEquals(TIMESTAMP_MILLIS.getObjectValue(session, block, i), expectedValues.next());\n+                    }\n+                }\n+            }\n+\n+            assertFalse(expectedValues.hasNext());\n+        }\n+    }\n+\n+    @Test(dataProvider = \"testTimestampMicrosDataProvider\")\n+    public void testTimestampMicros(HiveTimestampPrecision timestampPrecision, LocalDateTime expected)\n+            throws Exception\n+    {\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setTimestampPrecision(timestampPrecision));\n+\n+        File parquetFile = new File(Resources.getResource(\"issue-5483.parquet\").toURI());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxNDA2MQ==", "bodyText": "i named the file after #5483 issue and the issue description begins with how to steps. is there a better way to not only write those steps down, but also make them findable by someone else?", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r504514061", "createdAt": "2020-10-14T08:56:21Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,209 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.common.io.Resources;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HivePageSourceFactory;\n+import io.prestosql.plugin.hive.HivePageSourceFactory.ReaderPageSourceWithProjections;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.HiveTimestampPrecision;\n+import io.prestosql.plugin.hive.HiveType;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.time.LocalDateTime;\n+import java.time.ZoneId;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static io.prestosql.plugin.hive.HiveColumnHandle.ColumnType.REGULAR;\n+import static io.prestosql.plugin.hive.HiveColumnHandle.createBaseColumn;\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_TIMESTAMP;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static io.prestosql.spi.type.TimestampType.createTimestampType;\n+import static io.prestosql.spi.type.TimestampWithTimeZoneType.createTimestampWithTimeZoneType;\n+import static io.prestosql.testing.MaterializedResult.materializeSourceDataStream;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_LIB;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+                for (int i = 0; i < firstPage.getPositionCount(); i++) {\n+                    expectedValues.next();\n+                }\n+\n+                while (!pageSource.isFinished()) {\n+                    Page page = pageSource.getNextPage();\n+                    if (page == null) {\n+                        continue;\n+                    }\n+                    Block block = page.getBlock(0);\n+\n+                    for (int i = 0; i < block.getPositionCount(); i++) {\n+                        assertEquals(TIMESTAMP_MILLIS.getObjectValue(session, block, i), expectedValues.next());\n+                    }\n+                }\n+            }\n+\n+            assertFalse(expectedValues.hasNext());\n+        }\n+    }\n+\n+    @Test(dataProvider = \"testTimestampMicrosDataProvider\")\n+    public void testTimestampMicros(HiveTimestampPrecision timestampPrecision, LocalDateTime expected)\n+            throws Exception\n+    {\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setTimestampPrecision(timestampPrecision));\n+\n+        File parquetFile = new File(Resources.getResource(\"issue-5483.parquet\").toURI());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA2NjQ3OQ=="}, "originalCommit": null, "originalPosition": 144}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NzM3NjMyOnYy", "diffSide": "RIGHT", "path": "presto-spi/src/main/java/io/prestosql/spi/type/SqlTimestampWithTimeZone.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNTo1Nzo0MFrOHgt_6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo0MjowNFrOHhIgbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA3MDEyMg==", "bodyText": "Since this doesn't really represent nanosOfSecond (per comment below), I'd call it differently. Something like nanoAdjustment (in line with the name of the argument to ofEpochSecond)", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r504070122", "createdAt": "2020-10-13T15:57:40Z", "author": {"login": "martint"}, "path": "presto-spi/src/main/java/io/prestosql/spi/type/SqlTimestampWithTimeZone.java", "diffHunk": "@@ -129,4 +136,18 @@ public int hashCode()\n     {\n         return Objects.hash(precision, epochMillis, picosOfMilli, timeZoneKey);\n     }\n+\n+    /**\n+     * @return timestamp with time zone rounded to nanosecond precision\n+     */\n+    public ZonedDateTime toZonedDateTime()\n+    {\n+        long epochSecond = floorDiv(epochMillis, MILLISECONDS_PER_SECOND);\n+        int nanosOfSecond = floorMod(epochMillis, MILLISECONDS_PER_SECOND) * NANOSECONDS_PER_MILLISECOND +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUwNDQyOQ==", "bodyText": "i thought about that, but i decided to keep the current naming.\nfwiw, we had this naming question before in\nhttps://github.com/prestosql/presto/blob/92f01e33d6b18928063fbdf0fb3fe183648d14c2/presto-base-jdbc/src/main/java/io/prestosql/plugin/jdbc/StandardColumnMappings.java#L373-L376\nhttps://github.com/prestosql/presto/blob/16d0745857443743d893ca3c333b0ec7f551468d/presto-main/src/main/java/io/prestosql/operator/scalar/timestamptz/TimestampWithTimeZoneToTimestampWithTimeZoneCast.java#L85-L88\nhttps://github.com/prestosql/presto/blob/067b24e8e9dee9315ef3462ca8368992848525ef/presto-main/src/main/java/io/prestosql/operator/scalar/timestamp/TimestampToTimestampCast.java#L82-L86", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r504504429", "createdAt": "2020-10-14T08:42:04Z", "author": {"login": "findepi"}, "path": "presto-spi/src/main/java/io/prestosql/spi/type/SqlTimestampWithTimeZone.java", "diffHunk": "@@ -129,4 +136,18 @@ public int hashCode()\n     {\n         return Objects.hash(precision, epochMillis, picosOfMilli, timeZoneKey);\n     }\n+\n+    /**\n+     * @return timestamp with time zone rounded to nanosecond precision\n+     */\n+    public ZonedDateTime toZonedDateTime()\n+    {\n+        long epochSecond = floorDiv(epochMillis, MILLISECONDS_PER_SECOND);\n+        int nanosOfSecond = floorMod(epochMillis, MILLISECONDS_PER_SECOND) * NANOSECONDS_PER_MILLISECOND +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA3MDEyMg=="}, "originalCommit": null, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NzM4NjUyOnYy", "diffSide": "RIGHT", "path": "presto-spi/src/test/java/io/prestosql/spi/type/TestSqlTimestampWithTimeZone.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNTo1OTo1OFrOHguGXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1Nzo0MVrOHhJJjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA3MTc3NQ==", "bodyText": "Add tests for negative epoch where it has to round up and down. That's typically where any bugs with handling negative epochs can be observed.", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r504071775", "createdAt": "2020-10-13T15:59:58Z", "author": {"login": "martint"}, "path": "presto-spi/src/test/java/io/prestosql/spi/type/TestSqlTimestampWithTimeZone.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.spi.type;\n+\n+import org.testng.annotations.Test;\n+\n+import java.time.ZonedDateTime;\n+\n+import static io.prestosql.spi.type.TimeZoneKey.UTC_KEY;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestSqlTimestampWithTimeZone\n+{\n+    @Test\n+    public void testToZonedDateTime()\n+    {\n+        assertEquals(\n+                new SqlTimestampWithTimeZone(9, 1234567890123L, 123_000_000, UTC_KEY).toZonedDateTime(),\n+                ZonedDateTime.parse(\"2009-02-13T23:31:30.123123Z[UTC]\"));\n+\n+        // negative epoch\n+        assertEquals(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxNDk1Nw==", "bodyText": "tests added", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r504514957", "createdAt": "2020-10-14T08:57:41Z", "author": {"login": "findepi"}, "path": "presto-spi/src/test/java/io/prestosql/spi/type/TestSqlTimestampWithTimeZone.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.spi.type;\n+\n+import org.testng.annotations.Test;\n+\n+import java.time.ZonedDateTime;\n+\n+import static io.prestosql.spi.type.TimeZoneKey.UTC_KEY;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestSqlTimestampWithTimeZone\n+{\n+    @Test\n+    public void testToZonedDateTime()\n+    {\n+        assertEquals(\n+                new SqlTimestampWithTimeZone(9, 1234567890123L, 123_000_000, UTC_KEY).toZonedDateTime(),\n+                ZonedDateTime.parse(\"2009-02-13T23:31:30.123123Z[UTC]\"));\n+\n+        // negative epoch\n+        assertEquals(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA3MTc3NQ=="}, "originalCommit": null, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDIzNDkxOnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1MjoyNVrOHhI7jA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1MjoyNVrOHhI7jA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxMTM3Mg==", "bodyText": "@martint 's #5528 change the class name, so i will pick a different name, and un-depend the PRs", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r504511372", "createdAt": "2020-10-14T08:52:25Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,209 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.common.io.Resources;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HivePageSourceFactory;\n+import io.prestosql.plugin.hive.HivePageSourceFactory.ReaderPageSourceWithProjections;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.HiveTimestampPrecision;\n+import io.prestosql.plugin.hive.HiveType;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.time.LocalDateTime;\n+import java.time.ZoneId;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static io.prestosql.plugin.hive.HiveColumnHandle.ColumnType.REGULAR;\n+import static io.prestosql.plugin.hive.HiveColumnHandle.createBaseColumn;\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_TIMESTAMP;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static io.prestosql.spi.type.TimestampType.createTimestampType;\n+import static io.prestosql.spi.type.TimestampWithTimeZoneType.createTimestampWithTimeZoneType;\n+import static io.prestosql.testing.MaterializedResult.materializeSourceDataStream;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_LIB;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDI1Mzk4OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestTimestampMicros.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1Njo1M1rOHhJHcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1Njo1M1rOHhJHcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxNDQxOA==", "bodyText": "I picked the test class name in line with my suggestion over here: https://github.com/prestosql/presto/pull/5528/files#r504512999", "url": "https://github.com/trinodb/trino/pull/5538#discussion_r504514418", "createdAt": "2020-10-14T08:56:53Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestTimestampMicros.java", "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.io.Resources;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HivePageSourceFactory;\n+import io.prestosql.plugin.hive.HivePageSourceFactory.ReaderPageSourceWithProjections;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.HiveTimestampPrecision;\n+import io.prestosql.plugin.hive.HiveType;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.time.LocalDateTime;\n+import java.time.ZoneId;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static io.prestosql.plugin.hive.HiveColumnHandle.ColumnType.REGULAR;\n+import static io.prestosql.plugin.hive.HiveColumnHandle.createBaseColumn;\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_TIMESTAMP;\n+import static io.prestosql.spi.type.TimestampType.createTimestampType;\n+import static io.prestosql.spi.type.TimestampWithTimeZoneType.createTimestampWithTimeZoneType;\n+import static io.prestosql.testing.MaterializedResult.materializeSourceDataStream;\n+import static org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_LIB;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestTimestampMicros", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4694, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}