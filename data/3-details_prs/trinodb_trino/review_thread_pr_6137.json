{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI5MDg3Njkz", "number": 6137, "reviewThreads": {"totalCount": 74, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwNzowMDowOVrOE-hMDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwNTozMjoyN1rOFHBm2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzOTkwOTI2OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSessionProperties.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwNzowMDowOVrOH7t_hQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxMzoyNDoxOVrOH765-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4MTU3Mw==", "bodyText": "Can we have something like a SessionPropertyProvider ? So we can inject a Set<SessionPropertyProvider> and finally flatten when we create the Connector ? Set<List<PropertyMetadata>> looks a bit verbose ?\n@kokosing your insights ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532381573", "createdAt": "2020-11-30T07:00:09Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSessionProperties.java", "diffHunk": "@@ -20,19 +20,24 @@\n import javax.inject.Inject;\n \n import java.util.List;\n+import java.util.Set;\n \n public final class KafkaSessionProperties\n {\n     private static final String TIMESTAMP_UPPER_BOUND_FORCE_PUSH_DOWN_ENABLED = \"timestamp_upper_bound_force_push_down_enabled\";\n     private final List<PropertyMetadata<?>> sessionProperties;\n \n     @Inject\n-    public KafkaSessionProperties(KafkaConfig kafkaConfig)\n+    public KafkaSessionProperties(KafkaConfig kafkaConfig, @ForKafka Set<List<PropertyMetadata<?>>> extraProperties)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjU5MzE0NQ==", "bodyText": "See JdbcConnector to see how it can be implemented.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532593145", "createdAt": "2020-11-30T13:24:19Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSessionProperties.java", "diffHunk": "@@ -20,19 +20,24 @@\n import javax.inject.Inject;\n \n import java.util.List;\n+import java.util.Set;\n \n public final class KafkaSessionProperties\n {\n     private static final String TIMESTAMP_UPPER_BOUND_FORCE_PUSH_DOWN_ENABLED = \"timestamp_upper_bound_force_push_down_enabled\";\n     private final List<PropertyMetadata<?>> sessionProperties;\n \n     @Inject\n-    public KafkaSessionProperties(KafkaConfig kafkaConfig)\n+    public KafkaSessionProperties(KafkaConfig kafkaConfig, @ForKafka Set<List<PropertyMetadata<?>>> extraProperties)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4MTU3Mw=="}, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzOTkxMzE2OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaTableHandle.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwNzowMTo1N1rOH7uBqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1MzoyN1rOH8rQcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4MjEyMw==", "bodyText": "Can we have it as a Properties is subject is used only in SchemaRegistry or ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532382123", "createdAt": "2020-11-30T07:01:57Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaTableHandle.java", "diffHunk": "@@ -64,6 +66,8 @@ public KafkaTableHandle(\n             @JsonProperty(\"messageDataFormat\") String messageDataFormat,\n             @JsonProperty(\"keyDataSchemaLocation\") Optional<String> keyDataSchemaLocation,\n             @JsonProperty(\"messageDataSchemaLocation\") Optional<String> messageDataSchemaLocation,\n+            @JsonProperty(\"keySubject\") Optional<String> keySubject,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NTMyOQ==", "bodyText": "I am not sure if I understand, but if you mean Map<String, Object> by term Properties, then:\n\nI don't like using Map<String, Object>, it is easy to lose a control what it stored there. Let's try to avoid pythonization of java ;)", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533385329", "createdAt": "2020-12-01T12:53:27Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaTableHandle.java", "diffHunk": "@@ -64,6 +66,8 @@ public KafkaTableHandle(\n             @JsonProperty(\"messageDataFormat\") String messageDataFormat,\n             @JsonProperty(\"keyDataSchemaLocation\") Optional<String> keyDataSchemaLocation,\n             @JsonProperty(\"messageDataSchemaLocation\") Optional<String> messageDataSchemaLocation,\n+            @JsonProperty(\"keySubject\") Optional<String> keySubject,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4MjEyMw=="}, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzOTkxNTkyOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaTopicFieldGroup.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwNzowMzozMFrOH7uDZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1MjoxNFrOH8rN_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4MjU2Ng==", "bodyText": "Can we could them as a Map<String, Object> ? So in future other related properties can be passed without modifying these classes", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532382566", "createdAt": "2020-11-30T07:03:30Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaTopicFieldGroup.java", "diffHunk": "@@ -30,16 +30,19 @@\n {\n     private final String dataFormat;\n     private final Optional<String> dataSchema;\n+    private final Optional<String> subject;\n     private final List<KafkaTopicFieldDescription> fields;\n \n     @JsonCreator\n     public KafkaTopicFieldGroup(\n             @JsonProperty(\"dataFormat\") String dataFormat,\n             @JsonProperty(\"dataSchema\") Optional<String> dataSchema,\n+            @JsonProperty(\"decoderParams\") Optional<String> subject,\n             @JsonProperty(\"fields\") List<KafkaTopicFieldDescription> fields)\n     {\n         this.dataFormat = requireNonNull(dataFormat, \"dataFormat is null\");\n         this.dataSchema = requireNonNull(dataSchema, \"dataSchema is null\");\n+        this.subject = requireNonNull(subject, \"subject is null\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NDcwMQ==", "bodyText": "I don't like using Map<String, Object>, it is easy to lose a control what it stored there. Let's try to avoid pythonization of java ;)\nTo me it is ok how it is now.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533384701", "createdAt": "2020-12-01T12:52:14Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaTopicFieldGroup.java", "diffHunk": "@@ -30,16 +30,19 @@\n {\n     private final String dataFormat;\n     private final Optional<String> dataSchema;\n+    private final Optional<String> subject;\n     private final List<KafkaTopicFieldDescription> fields;\n \n     @JsonCreator\n     public KafkaTopicFieldGroup(\n             @JsonProperty(\"dataFormat\") String dataFormat,\n             @JsonProperty(\"dataSchema\") Optional<String> dataSchema,\n+            @JsonProperty(\"decoderParams\") Optional<String> subject,\n             @JsonProperty(\"fields\") List<KafkaTopicFieldDescription> fields)\n     {\n         this.dataFormat = requireNonNull(dataFormat, \"dataFormat is null\");\n         this.dataSchema = requireNonNull(dataSchema, \"dataSchema is null\");\n+        this.subject = requireNonNull(subject, \"subject is null\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4MjU2Ng=="}, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzOTkxNzg3OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/ContentSchemaReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwNzowNDoyMlrOH7uEew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwNjowMDo0MFrOH8ZTQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4Mjg0Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public interface ContentSchemaReader\n          \n          \n            \n            public interface SchemaReader", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532382843", "createdAt": "2020-11-30T07:04:22Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/ContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,27 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema;\n+\n+import io.prestosql.plugin.kafka.KafkaTableHandle;\n+\n+import java.util.Optional;\n+\n+public interface ContentSchemaReader", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzA5MTEzNw==", "bodyText": "@kokosing had a comment suggesting to be more specific, i.e. to differentiate between schema for a table, a record, a database schema, etc.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533091137", "createdAt": "2020-12-01T06:00:40Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/ContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,27 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema;\n+\n+import io.prestosql.plugin.kafka.KafkaTableHandle;\n+\n+import java.util.Optional;\n+\n+public interface ContentSchemaReader", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4Mjg0Mw=="}, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzOTkyNzMwOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwNzowODowOVrOH7uJfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwNzowODowOVrOH7uJfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4NDEyNQ==", "bodyText": "Any min/max restriction for it ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532384125", "createdAt": "2020-11-30T07:08:09Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryConfig.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import io.airlift.configuration.Config;\n+import io.airlift.configuration.ConfigDescription;\n+import io.airlift.units.Duration;\n+import io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy;\n+\n+import javax.validation.constraints.NotNull;\n+\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.IGNORE;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+public class ConfluentSchemaRegistryConfig\n+{\n+    private String confluentSchemaRegistryUrl;\n+    private int confluentSchemaRegistryClientCacheSize = 1000;\n+    private EmptyFieldStrategy emptyFieldStrategy = IGNORE;\n+    private Duration confluentSubjectsCacheRefreshInterval = new Duration(1, SECONDS);\n+\n+    @NotNull\n+    public String getConfluentSchemaRegistryUrl()\n+    {\n+        return confluentSchemaRegistryUrl;\n+    }\n+\n+    @Config(\"kafka.confluent-schema-registry-url\")\n+    @ConfigDescription(\"The url of the Confluent Schema Registry\")\n+    public ConfluentSchemaRegistryConfig setConfluentSchemaRegistryUrl(String confluentSchemaRegistryUrl)\n+    {\n+        this.confluentSchemaRegistryUrl = confluentSchemaRegistryUrl;\n+        return this;\n+    }\n+\n+    public int getConfluentSchemaRegistryClientCacheSize()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzOTkzNDAwOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/AbstractContentSchemaReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwNzoxMDo1NFrOH7uNOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwNjowMzoxN1rOH8ZWaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4NTA4MQ==", "bodyText": "What if we could pass the KafkaTableHandle to the underlying implementation ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532385081", "createdAt": "2020-11-30T07:10:54Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/AbstractContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema;\n+\n+import io.prestosql.plugin.kafka.KafkaTableHandle;\n+\n+import java.util.Optional;\n+\n+public abstract class AbstractContentSchemaReader\n+        implements ContentSchemaReader\n+{\n+    @Override\n+    public Optional<String> readKeyContentSchema(KafkaTableHandle tableHandle)\n+    {\n+        return readSchema(tableHandle.getKeyDataSchemaLocation(), tableHandle.getKeySubject());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzA5MTk0NQ==", "bodyText": "It would need to differentiate between key and message, so this class abstracts that so subclasses just pass the table handle. Does that sound good to you?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533091945", "createdAt": "2020-12-01T06:03:17Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/AbstractContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema;\n+\n+import io.prestosql.plugin.kafka.KafkaTableHandle;\n+\n+import java.util.Optional;\n+\n+public abstract class AbstractContentSchemaReader\n+        implements ContentSchemaReader\n+{\n+    @Override\n+    public Optional<String> readKeyContentSchema(KafkaTableHandle tableHandle)\n+    {\n+        return readSchema(tableHandle.getKeyDataSchemaLocation(), tableHandle.getKeySubject());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4NTA4MQ=="}, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzOTkzNzg5OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentModule.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwNzoxMjoxNVrOH7uPWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwNzoxMjoxNVrOH7uPWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4NTYyNQ==", "bodyText": "Do we really need this ForKafka ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532385625", "createdAt": "2020-11-30T07:12:15Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentModule.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.inject.Binder;\n+import com.google.inject.Provides;\n+import com.google.inject.Scopes;\n+import com.google.inject.TypeLiteral;\n+import io.airlift.configuration.AbstractConfigurationAwareModule;\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.RowDecoderFactory;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroDeserializer;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.ForKafka;\n+import io.prestosql.plugin.kafka.schema.ContentSchemaReader;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.session.PropertyMetadata;\n+\n+import javax.inject.Singleton;\n+\n+import java.util.List;\n+\n+import static com.google.inject.multibindings.MapBinder.newMapBinder;\n+import static com.google.inject.multibindings.Multibinder.newSetBinder;\n+import static io.airlift.configuration.ConfigBinder.configBinder;\n+\n+public class ConfluentModule\n+        extends AbstractConfigurationAwareModule\n+{\n+    @Override\n+    protected void setup(Binder binder)\n+    {\n+        configBinder(binder).bindConfig(ConfluentSchemaRegistryConfig.class);\n+        binder.bind(AvroReaderSupplier.Factory.class).to(ConfluentAvroReaderSupplier.Factory.class).in(Scopes.SINGLETON);\n+        binder.bind(AvroDeserializer.Factory.class).to(AvroBytesDeserializer.Factory.class).in(Scopes.SINGLETON);\n+        binder.bind(ContentSchemaReader.class).to(AvroConfluentContentSchemaReader.class).in(Scopes.SINGLETON);\n+        newSetBinder(binder, new TypeLiteral<List<PropertyMetadata<?>>>() {}, ForKafka.class).addBinding().toProvider(ConfluentSessionProperties.class).in(Scopes.SINGLETON);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTI1MDMxOnYy", "diffSide": "RIGHT", "path": "pom.xml", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxMzoxMDozNlrOH76ZzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMTo0NTozNVrOH9XK_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjU4NDkwOQ==", "bodyText": "I don't see this used.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532584909", "createdAt": "2020-11-30T13:10:36Z", "author": {"login": "kokosing"}, "path": "pom.xml", "diffHunk": "@@ -1311,6 +1347,12 @@\n                 <artifactId>snappy-java</artifactId>\n                 <version>1.1.7.3</version>\n             </dependency>\n+\n+            <dependency>\n+                <groupId>org.yaml</groupId>\n+                <artifactId>snakeyaml</artifactId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzk5NTc2Mg==", "bodyText": "It's a transitive dependency of kafka-avro-serializers and pinot-spi but they depend on different versions.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533995762", "createdAt": "2020-12-02T08:57:05Z", "author": {"login": "elonazoulay"}, "path": "pom.xml", "diffHunk": "@@ -1311,6 +1347,12 @@\n                 <artifactId>snappy-java</artifactId>\n                 <version>1.1.7.3</version>\n             </dependency>\n+\n+            <dependency>\n+                <groupId>org.yaml</groupId>\n+                <artifactId>snakeyaml</artifactId>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjU4NDkwOQ=="}, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEwNDgyOQ==", "bodyText": "Thanks, please add a comment.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534104829", "createdAt": "2020-12-02T11:45:35Z", "author": {"login": "kokosing"}, "path": "pom.xml", "diffHunk": "@@ -1311,6 +1347,12 @@\n                 <artifactId>snappy-java</artifactId>\n                 <version>1.1.7.3</version>\n             </dependency>\n+\n+            <dependency>\n+                <groupId>org.yaml</groupId>\n+                <artifactId>snakeyaml</artifactId>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjU4NDkwOQ=="}, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTI2NDA3OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxMzoxNDoxOFrOH76iPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxMzoxNDoxOFrOH76iPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjU4NzA2OA==", "bodyText": "combine?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532587068", "createdAt": "2020-11-30T13:14:18Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -166,8 +168,9 @@ private static DistributedQueryRunner createKafkaQueryRunner(\n                     installModuleIf(\n                             KafkaConfig.class,\n                             kafkaConfig -> kafkaConfig.getTableDescriptionSupplier().equalsIgnoreCase(TEST),\n-                            binder -> binder.bind(TableDescriptionSupplier.class)\n-                                    .toInstance(new MapBasedTableDescriptionSupplier(topicDescriptions)))));\n+                            installModules(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjA0NDMwOnYy", "diffSide": "RIGHT", "path": "pom.xml", "isResolved": true, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMToxODoxMFrOH8n_Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwNToyNjowN1rOIITmiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMTc0Ng==", "bodyText": "Is it possible to find a confluent version that would match kafka version that we are using to avoid doing exclusions. Possibly we could update current kafka version if needed.\nCan you please share details of how it fails without exclussions?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533331746", "createdAt": "2020-12-01T11:18:10Z", "author": {"login": "kokosing"}, "path": "pom.xml", "diffHunk": "@@ -1070,6 +1078,34 @@\n                 <version>4.5.1</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-avro-serializer</artifactId>\n+                <version>${dep.confluent.version}</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDgxMjAzNA==", "bodyText": "Here are the failures without exclusions:\n[WARNING] Rule 1: org.apache.maven.plugins.enforcer.RequireUpperBoundDeps failed with message:\nFailed while enforcing RequireUpperBoundDeps. The error(s) are [\nRequire upper bound dependencies error for com.fasterxml.jackson.core:jackson-databind:2.10.5 paths to dependency are:\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-com.fasterxml.jackson.core:jackson-databind:2.10.5\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-org.apache.avro:avro:1.9.2\n    +-com.fasterxml.jackson.core:jackson-databind:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-databind:2.10.2\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-com.fasterxml.jackson.core:jackson-databind:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-databind:2.10.2\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-com.fasterxml.jackson.core:jackson-databind:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-databind:2.11.1\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.10.5 (managed) <-- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.11.1\n          +-com.fasterxml.jackson.core:jackson-databind:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-databind:2.11.1\n, \nRequire upper bound dependencies error for com.fasterxml.jackson.core:jackson-annotations:2.10.5 paths to dependency are:\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-com.fasterxml.jackson.core:jackson-databind:2.10.5\n    +-com.fasterxml.jackson.core:jackson-annotations:2.10.5\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-org.testcontainers:testcontainers:1.15.0\n    +-com.github.docker-java:docker-java-api:3.2.5\n      +-com.fasterxml.jackson.core:jackson-annotations:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-annotations:2.10.3\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-com.fasterxml.jackson.core:jackson-annotations:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-annotations:2.11.1\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-io.swagger:swagger-models:1.6.2\n          +-com.fasterxml.jackson.core:jackson-annotations:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-annotations:2.11.1\n, \nRequire upper bound dependencies error for io.swagger:swagger-annotations:1.5.22 paths to dependency are:\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-annotations:1.5.22\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-io.swagger:swagger-models:1.6.2\n          +-io.swagger:swagger-annotations:1.6.2\n, \nRequire upper bound dependencies error for com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.10.5 paths to dependency are:\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.10.5 (managed) <-- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.11.1\n]", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r540812034", "createdAt": "2020-12-11T09:34:16Z", "author": {"login": "elonazoulay"}, "path": "pom.xml", "diffHunk": "@@ -1070,6 +1078,34 @@\n                 <version>4.5.1</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-avro-serializer</artifactId>\n+                <version>${dep.confluent.version}</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMTc0Ng=="}, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg1ODAwMA==", "bodyText": "If I include swagger-core 1.6.2, I get the following errors:\nFailed while enforcing RequireUpperBoundDeps. The error(s) are [\nRequire upper bound dependencies error for com.fasterxml.jackson.core:jackson-databind:2.10.5 paths to dependency are:\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-com.fasterxml.jackson.core:jackson-databind:2.10.5\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-org.apache.avro:avro:1.9.2\n    +-com.fasterxml.jackson.core:jackson-databind:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-databind:2.10.2\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-com.fasterxml.jackson.core:jackson-databind:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-databind:2.10.2\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-com.fasterxml.jackson.core:jackson-databind:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-databind:2.11.1\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.10.5 (managed) <-- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.11.1\n          +-com.fasterxml.jackson.core:jackson-databind:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-databind:2.11.1\n, \nRequire upper bound dependencies error for com.fasterxml.jackson.core:jackson-annotations:2.10.5 paths to dependency are:\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-com.fasterxml.jackson.core:jackson-databind:2.10.5\n    +-com.fasterxml.jackson.core:jackson-annotations:2.10.5\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-org.testcontainers:testcontainers:1.15.0\n    +-com.github.docker-java:docker-java-api:3.2.5\n      +-com.fasterxml.jackson.core:jackson-annotations:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-annotations:2.10.3\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-com.fasterxml.jackson.core:jackson-annotations:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-annotations:2.11.1\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-io.swagger:swagger-models:1.6.2\n          +-com.fasterxml.jackson.core:jackson-annotations:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-annotations:2.11.1\n, \nRequire upper bound dependencies error for io.swagger:swagger-annotations:1.5.22 paths to dependency are:\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-annotations:1.5.22\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-io.swagger:swagger-models:1.6.2\n          +-io.swagger:swagger-annotations:1.6.2\n, \nRequire upper bound dependencies error for com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.10.5 paths to dependency are:\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.10.5 (managed) <-- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.11.1", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r540858000", "createdAt": "2020-12-11T10:48:04Z", "author": {"login": "elonazoulay"}, "path": "pom.xml", "diffHunk": "@@ -1070,6 +1078,34 @@\n                 <version>4.5.1</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-avro-serializer</artifactId>\n+                <version>${dep.confluent.version}</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMTc0Ng=="}, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg1OTIxMg==", "bodyText": "And if I do not exclude jackson-annotations:\nRequire upper bound dependencies error for com.fasterxml.jackson.core:jackson-annotations:2.10.5 paths to dependency are:\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-com.fasterxml.jackson.core:jackson-databind:2.10.5\n    +-com.fasterxml.jackson.core:jackson-annotations:2.10.5\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-org.testcontainers:testcontainers:1.15.0\n    +-com.github.docker-java:docker-java-api:3.2.5\n      +-com.fasterxml.jackson.core:jackson-annotations:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-annotations:2.10.3\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-com.fasterxml.jackson.core:jackson-annotations:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-annotations:2.11.1\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-io.swagger:swagger-models:1.6.2\n          +-com.fasterxml.jackson.core:jackson-annotations:2.10.5 (managed) <-- com.fasterxml.jackson.core:jackson-annotations:2.11.1\n, \nRequire upper bound dependencies error for io.swagger:swagger-annotations:1.5.22 paths to dependency are:\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-annotations:1.5.22\nand\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-io.swagger:swagger-models:1.6.2\n          +-io.swagger:swagger-annotations:1.6.2\n, \nRequire upper bound dependencies error for com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.10.5 paths to dependency are:\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.10.5 (managed) <-- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.11.1\n]", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r540859212", "createdAt": "2020-12-11T10:50:04Z", "author": {"login": "elonazoulay"}, "path": "pom.xml", "diffHunk": "@@ -1070,6 +1078,34 @@\n                 <version>4.5.1</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-avro-serializer</artifactId>\n+                <version>${dep.confluent.version}</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMTc0Ng=="}, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2MTg4MQ==", "bodyText": "And finally, if I do not exclude jackson-dataformat-yaml:\nFailed while enforcing RequireUpperBoundDeps. The error(s) are [\nRequire upper bound dependencies error for com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.10.5 paths to dependency are:\n+-io.prestosql:presto-testing-kafka:348-SNAPSHOT\n  +-io.confluent:kafka-avro-serializer:5.4.3\n    +-io.confluent:kafka-schema-registry-client:5.4.3\n      +-io.swagger:swagger-core:1.6.2\n        +-com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.10.5 (managed) <-- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.11.1\n]", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r540861881", "createdAt": "2020-12-11T10:54:27Z", "author": {"login": "elonazoulay"}, "path": "pom.xml", "diffHunk": "@@ -1070,6 +1078,34 @@\n                 <version>4.5.1</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-avro-serializer</artifactId>\n+                <version>${dep.confluent.version}</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMTc0Ng=="}, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwMjQwMA==", "bodyText": "All of these fail with: RequireUpperBoundDeps. That means that your kafka has a conflict in transitive dependencies. In Presto in such case use the newest version used, for example if you declare com.fasterxml.jackson.core:jackson-databind:2.11.1 in kafka the issue will be fixed. Please apply whenever possible and we will see what will be left.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r543302400", "createdAt": "2020-12-15T12:31:18Z", "author": {"login": "kokosing"}, "path": "pom.xml", "diffHunk": "@@ -1070,6 +1078,34 @@\n                 <version>4.5.1</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-avro-serializer</artifactId>\n+                <version>${dep.confluent.version}</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMTc0Ng=="}, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4MDY4MQ==", "bodyText": "Is Kafka Avro Serializer is used only in test scope ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r545580681", "createdAt": "2020-12-18T05:26:07Z", "author": {"login": "Praveen2112"}, "path": "pom.xml", "diffHunk": "@@ -1070,6 +1078,34 @@\n                 <version>4.5.1</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-avro-serializer</artifactId>\n+                <version>${dep.confluent.version}</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMTc0Ng=="}, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjA0NjAzOnYy", "diffSide": "RIGHT", "path": "presto-testing-kafka/src/main/java/io/prestosql/testing/kafka/TestingKafkaWithSchemaRegistry.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMToxODo0MVrOH8oAQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMToxODo0MVrOH8oAQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMjAzMg==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533332032", "createdAt": "2020-12-01T11:18:41Z", "author": {"login": "kokosing"}, "path": "presto-testing-kafka/src/main/java/io/prestosql/testing/kafka/TestingKafkaWithSchemaRegistry.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.testing.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.io.Closer;\n+import io.confluent.kafka.serializers.KafkaAvroSerializer;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.testcontainers.containers.GenericContainer;\n+import org.testcontainers.containers.Network;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class TestingKafkaWithSchemaRegistry\n+        implements Closeable\n+{\n+    public static final int SCHEMA_REGISTRY_PORT = 8081;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjA0Njc2OnYy", "diffSide": "RIGHT", "path": "presto-testing-kafka/src/main/java/io/prestosql/testing/kafka/TestingKafkaWithSchemaRegistry.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMToxODo1NlrOH8oAvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMToxODo1NlrOH8oAvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMjE1Nw==", "bodyText": "it is not used", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533332157", "createdAt": "2020-12-01T11:18:56Z", "author": {"login": "kokosing"}, "path": "presto-testing-kafka/src/main/java/io/prestosql/testing/kafka/TestingKafkaWithSchemaRegistry.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.testing.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.io.Closer;\n+import io.confluent.kafka.serializers.KafkaAvroSerializer;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.testcontainers.containers.GenericContainer;\n+import org.testcontainers.containers.Network;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class TestingKafkaWithSchemaRegistry\n+        implements Closeable\n+{\n+    public static final int SCHEMA_REGISTRY_PORT = 8081;\n+    private final TestingKafka testingKafka;\n+    private final GenericContainer<?> schemaRegistryContainer;\n+\n+    @SuppressWarnings(\"resource\")\n+    private final Closer closer = Closer.create();\n+\n+    public TestingKafkaWithSchemaRegistry(TestingKafka testingKafka)\n+    {\n+        this.testingKafka = requireNonNull(testingKafka, \"testingKafka is null\");\n+        schemaRegistryContainer = new GenericContainer<>(\"confluentinc/cp-schema-registry:5.4.1\")\n+                .withNetwork(Network.SHARED)\n+                .withEnv(\"SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS\", \"PLAINTEXT://kafka:9092\")\n+                .withEnv(\"SCHEMA_REGISTRY_HOST_NAME\", \"0.0.0.0\")\n+                .withEnv(\"SCHEMA_REGISTRY_LISTENERS\", format(\"http://0.0.0.0:%s\", SCHEMA_REGISTRY_PORT))\n+                .withExposedPorts(SCHEMA_REGISTRY_PORT);\n+        closer.register(testingKafka);\n+        closer.register(schemaRegistryContainer::stop);\n+    }\n+\n+    public void start()\n+    {\n+        testingKafka.start();\n+        try {\n+            schemaRegistryContainer.start();\n+        }\n+        catch (Throwable e) {\n+            testingKafka.close();\n+            throw e;\n+        }\n+    }\n+\n+    @Override\n+    public void close()\n+            throws IOException\n+    {\n+        closer.close();\n+    }\n+\n+    public void createTopic(String topic)\n+    {\n+        testingKafka.createTopic(topic);\n+    }\n+\n+    public String getConnectString()\n+    {\n+        return testingKafka.getConnectString();\n+    }\n+\n+    public String getSchemaRegistryConnectString()\n+    {\n+        return \"http://\" + schemaRegistryContainer.getContainerIpAddress() + \":\" + schemaRegistryContainer.getMappedPort(SCHEMA_REGISTRY_PORT);\n+    }\n+\n+    public KafkaProducer<Long, Object> createProducer()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjA1MDM4OnYy", "diffSide": "RIGHT", "path": "pom.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMToxOTo0OVrOH8oCzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMToxNjowMlrOID0XBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMjY4Ng==", "bodyText": "Same comment as in previous commit.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533332686", "createdAt": "2020-12-01T11:19:49Z", "author": {"login": "kokosing"}, "path": "pom.xml", "diffHunk": "@@ -1106,6 +1106,22 @@\n                 </exclusions>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-schema-registry-client</artifactId>\n+                <version>${dep.confluent.version}</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg3NDUwMQ==", "bodyText": "These have the same exclusion errors as the kafka-avro-serializers.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r540874501", "createdAt": "2020-12-11T11:16:02Z", "author": {"login": "elonazoulay"}, "path": "pom.xml", "diffHunk": "@@ -1106,6 +1106,22 @@\n                 </exclusions>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-schema-registry-client</artifactId>\n+                <version>${dep.confluent.version}</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMjY4Ng=="}, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjA3NTgxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentAvroReaderSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMToyNjo0M1rOH8oSDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMToyNjo0M1rOH8oSDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzNjU5MQ==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533336591", "createdAt": "2020-12-01T11:26:43Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentAvroReaderSupplier.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.io.DatumReader;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentAvroReaderSupplier<T>\n+        implements AvroReaderSupplier<T>\n+{\n+    private final Schema targetSchema;\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final LoadingCache<Integer, GenericDatumReader<T>> avroRecordReaderCache;\n+\n+    public ConfluentAvroReaderSupplier(Schema targetSchema, SchemaRegistryClient schemaRegistryClient)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjA4MDQ1OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentAvroReaderSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMToyNzo1OVrOH8oU4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMToyNzo1OVrOH8oU4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzNzMxMw==", "bodyText": "Can sourceSchema be null? Maybe use requireNonNull?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533337313", "createdAt": "2020-12-01T11:27:59Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentAvroReaderSupplier.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.io.DatumReader;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentAvroReaderSupplier<T>\n+        implements AvroReaderSupplier<T>\n+{\n+    private final Schema targetSchema;\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final LoadingCache<Integer, GenericDatumReader<T>> avroRecordReaderCache;\n+\n+    public ConfluentAvroReaderSupplier(Schema targetSchema, SchemaRegistryClient schemaRegistryClient)\n+    {\n+        this.targetSchema = requireNonNull(targetSchema, \"targetSchema is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        avroRecordReaderCache = CacheBuilder.newBuilder()\n+                .build(CacheLoader.from(this::lookupReader));\n+    }\n+\n+    private GenericDatumReader<T> lookupReader(int id)\n+    {\n+        try {\n+            Schema sourceSchema = schemaRegistryClient.getById(id);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjA5MDA4OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentAvroReaderSupplier.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTozMDozMFrOH8oawQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMToyMDoxMFrOID0fow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzODgxNw==", "bodyText": "So schema id is going to change every time it is changed (like new field added)? And so we can indefinitely cache all schemas?\nI think it would make sense to make it possible to configure maximum size of this cache. Maybe confluentConfig.getConfluentSchemaRegistryClientCacheSize()? WDYT?\nAlso do you need this caching at all if you are using CachedSchemaRegistryClient?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533338817", "createdAt": "2020-12-01T11:30:30Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentAvroReaderSupplier.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.io.DatumReader;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentAvroReaderSupplier<T>\n+        implements AvroReaderSupplier<T>\n+{\n+    private final Schema targetSchema;\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final LoadingCache<Integer, GenericDatumReader<T>> avroRecordReaderCache;\n+\n+    public ConfluentAvroReaderSupplier(Schema targetSchema, SchemaRegistryClient schemaRegistryClient)\n+    {\n+        this.targetSchema = requireNonNull(targetSchema, \"targetSchema is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        avroRecordReaderCache = CacheBuilder.newBuilder()\n+                .build(CacheLoader.from(this::lookupReader));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDA0NDA5OA==", "bodyText": "This is to avoid creating a new reader for every record, definitely makes sense to limit the size. Does that sound good?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534044098", "createdAt": "2020-12-02T10:07:38Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentAvroReaderSupplier.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.io.DatumReader;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentAvroReaderSupplier<T>\n+        implements AvroReaderSupplier<T>\n+{\n+    private final Schema targetSchema;\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final LoadingCache<Integer, GenericDatumReader<T>> avroRecordReaderCache;\n+\n+    public ConfluentAvroReaderSupplier(Schema targetSchema, SchemaRegistryClient schemaRegistryClient)\n+    {\n+        this.targetSchema = requireNonNull(targetSchema, \"targetSchema is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        avroRecordReaderCache = CacheBuilder.newBuilder()\n+                .build(CacheLoader.from(this::lookupReader));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzODgxNw=="}, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEwNTE3Mg==", "bodyText": "definitely makes sense to limit the size\n\nPlease do, I am not sure if has to be configurable. But not. WDYT?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534105172", "createdAt": "2020-12-02T11:46:12Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentAvroReaderSupplier.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.io.DatumReader;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentAvroReaderSupplier<T>\n+        implements AvroReaderSupplier<T>\n+{\n+    private final Schema targetSchema;\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final LoadingCache<Integer, GenericDatumReader<T>> avroRecordReaderCache;\n+\n+    public ConfluentAvroReaderSupplier(Schema targetSchema, SchemaRegistryClient schemaRegistryClient)\n+    {\n+        this.targetSchema = requireNonNull(targetSchema, \"targetSchema is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        avroRecordReaderCache = CacheBuilder.newBuilder()\n+                .build(CacheLoader.from(this::lookupReader));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzODgxNw=="}, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg3NjcwNw==", "bodyText": "I set it to the default schema registry client cache size.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r540876707", "createdAt": "2020-12-11T11:20:10Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentAvroReaderSupplier.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.io.DatumReader;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentAvroReaderSupplier<T>\n+        implements AvroReaderSupplier<T>\n+{\n+    private final Schema targetSchema;\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final LoadingCache<Integer, GenericDatumReader<T>> avroRecordReaderCache;\n+\n+    public ConfluentAvroReaderSupplier(Schema targetSchema, SchemaRegistryClient schemaRegistryClient)\n+    {\n+        this.targetSchema = requireNonNull(targetSchema, \"targetSchema is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        avroRecordReaderCache = CacheBuilder.newBuilder()\n+                .build(CacheLoader.from(this::lookupReader));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzODgxNw=="}, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjE1NTUwOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo0Nzo1MVrOH8pA6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo0Nzo1MVrOH8pA6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM0ODU4Nw==", "bodyText": "Why do you need it? I would prefer to avoid hacking guice if it is not needed. Best if we would be able to reuse production guice binding, by using actual schema registry.\nIf it is really needed, then please pass this module as parameter to createKafkaQueryRunner so it is clear what tests need that, and we could allow tests to modify kafka connector according to their needs. That way we would avoid hacking all things together when kafkaConfig.getTableDescriptionSupplier().equalsIgnoreCase(TEST).\nIn other words can we simply use io.prestosql.plugin.kafka.KafkaQueryRunner.Builder#setExtension for that module?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533348587", "createdAt": "2020-12-01T11:47:51Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -166,8 +168,9 @@ private static DistributedQueryRunner createKafkaQueryRunner(\n                     installModuleIf(\n                             KafkaConfig.class,\n                             kafkaConfig -> kafkaConfig.getTableDescriptionSupplier().equalsIgnoreCase(TEST),\n-                            binder -> binder.bind(TableDescriptionSupplier.class)\n-                                    .toInstance(new MapBasedTableDescriptionSupplier(topicDescriptions)))));\n+                            installModules(\n+                                    binder -> binder.bind(TableDescriptionSupplier.class).toInstance(new MapBasedTableDescriptionSupplier(topicDescriptions)),\n+                                    new AvroDecoderModule()))));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjE2NzI0OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1MToyMFrOH8pH9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1MToyMFrOH8pH9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1MDM5MQ==", "bodyText": "use simple concatenation. format makes it just obscure. Same goes to below places like that:\n        testRow(rowDecoder, generateRecord(initialSchema, Arrays.asList(3, format(\"string-%s\", 3))), 1);\n        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(4, format(\"string-%s\", 4), 4L)), 2);\n        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(5, format(\"string-%s\", 5), null)), 2);", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533350391", "createdAt": "2020-12-01T11:51:20Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .name(\"col3\").type().optional().longType()\n+                .endRecord();\n+\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), initialSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjE3Mzg3OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1MzoxMlrOH8pMEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1MzoxMlrOH8pMEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1MTQ0MA==", "bodyText": "How does noDefault works, if there is no value then is it going to return null or fail? Can you please add few tests with org.apache.avro.SchemaBuilder.FieldDefault#usingDefault being used?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533351440", "createdAt": "2020-12-01T11:53:12Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjE4MzQ2OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1NToyOFrOH8pRhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1NToyOFrOH8pRhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1MjgzOA==", "bodyText": "Can you please tests how non optional fields work with null values:", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533352838", "createdAt": "2020-12-01T11:55:28Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .name(\"col3\").type().optional().longType()\n+                .endRecord();\n+\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), initialSchema);\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), evolvedSchema);\n+\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", INTEGER, \"col1\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col2\", VARCHAR, \"col2\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, evolvedSchema);\n+        testRow(rowDecoder, generateRecord(initialSchema, Arrays.asList(3, format(\"string-%s\", 3))), 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjE4NTM2OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1NTo1OVrOH8pSpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1NTo1OVrOH8pSpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1MzEyNw==", "bodyText": "Can you please test empty string?\nCan you please test UTF8 characters?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533353127", "createdAt": "2020-12-01T11:55:59Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .name(\"col3\").type().optional().longType()\n+                .endRecord();\n+\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), initialSchema);\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), evolvedSchema);\n+\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", INTEGER, \"col1\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col2\", VARCHAR, \"col2\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, evolvedSchema);\n+        testRow(rowDecoder, generateRecord(initialSchema, Arrays.asList(3, format(\"string-%s\", 3))), 1);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(4, format(\"string-%s\", 4), 4L)), 2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjE5MDk0OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1NzoyNlrOH8pWGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1NzoyNlrOH8pWGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1NDAwOQ==", "bodyText": "remove final", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533354009", "createdAt": "2020-12-01T11:57:26Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .name(\"col3\").type().optional().longType()\n+                .endRecord();\n+\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), initialSchema);\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), evolvedSchema);\n+\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", INTEGER, \"col1\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col2\", VARCHAR, \"col2\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, evolvedSchema);\n+        testRow(rowDecoder, generateRecord(initialSchema, Arrays.asList(3, format(\"string-%s\", 3))), 1);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(4, format(\"string-%s\", 4), 4L)), 2);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(5, format(\"string-%s\", 5), null)), 2);\n+    }\n+\n+    @Test\n+    public void testSingleValueRow()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema schema = Schema.create(Schema.Type.LONG);\n+        mockSchemaRegistryClient.register(format(\"%s-key\", TOPIC), schema);\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, schema);\n+        testSingleValueRow(rowDecoder, 3L, schema, 1);\n+    }\n+\n+    private void testRow(RowDecoder rowDecoder, GenericRecord record, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(record, record.getSchema(), schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        assertRowsAreEqual(decodedRow, record);\n+    }\n+\n+    private void testSingleValueRow(RowDecoder rowDecoder, Object value, Schema schema, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(value, schema, schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        checkState(decodedRow.isPresent(), \"decodedRow is not present\");\n+        Map.Entry<DecoderColumnHandle, FieldValueProvider> entry = getOnlyElement(decodedRow.get().entrySet());\n+        assertValuesAreEqual(entry.getValue(), value, schema.getType());\n+    }\n+\n+    private byte[] serializeRecord(Object record, Schema schema, int schemaId)\n+    {\n+        try {\n+            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n+            outputStream.write(0);\n+            outputStream.write(ByteBuffer.allocate(4).putInt(schemaId).array());\n+            BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(outputStream, null);\n+            GenericDatumWriter<Object> avroRecordWriter = new GenericDatumWriter<>(schema);\n+            avroRecordWriter.write(record, encoder);\n+            encoder.flush();\n+            byte[] serializedRecord = outputStream.toByteArray();\n+            outputStream.close();\n+            return serializedRecord;\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private RowDecoder getRowDecoder(SchemaRegistryClient schemaRegistryClient, Set<DecoderColumnHandle> columnHandles, Schema schema)\n+    {\n+        ImmutableMap<String, String> decoderParams = ImmutableMap.<String, String>builder()\n+                .put(DATA_SCHEMA, schema.toString())\n+                .build();\n+        return getAvroRowDecoderyFactory(schemaRegistryClient).create(decoderParams, columnHandles);\n+    }\n+\n+    public static final AvroRowDecoderFactory getAvroRowDecoderyFactory(SchemaRegistryClient schemaRegistryClient)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 149}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjE5MjE4OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1Nzo0OFrOH8pW5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1Nzo0OFrOH8pW5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1NDIxNQ==", "bodyText": "static", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533354215", "createdAt": "2020-12-01T11:57:48Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .name(\"col3\").type().optional().longType()\n+                .endRecord();\n+\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), initialSchema);\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), evolvedSchema);\n+\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", INTEGER, \"col1\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col2\", VARCHAR, \"col2\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, evolvedSchema);\n+        testRow(rowDecoder, generateRecord(initialSchema, Arrays.asList(3, format(\"string-%s\", 3))), 1);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(4, format(\"string-%s\", 4), 4L)), 2);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(5, format(\"string-%s\", 5), null)), 2);\n+    }\n+\n+    @Test\n+    public void testSingleValueRow()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema schema = Schema.create(Schema.Type.LONG);\n+        mockSchemaRegistryClient.register(format(\"%s-key\", TOPIC), schema);\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, schema);\n+        testSingleValueRow(rowDecoder, 3L, schema, 1);\n+    }\n+\n+    private void testRow(RowDecoder rowDecoder, GenericRecord record, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(record, record.getSchema(), schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        assertRowsAreEqual(decodedRow, record);\n+    }\n+\n+    private void testSingleValueRow(RowDecoder rowDecoder, Object value, Schema schema, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(value, schema, schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        checkState(decodedRow.isPresent(), \"decodedRow is not present\");\n+        Map.Entry<DecoderColumnHandle, FieldValueProvider> entry = getOnlyElement(decodedRow.get().entrySet());\n+        assertValuesAreEqual(entry.getValue(), value, schema.getType());\n+    }\n+\n+    private byte[] serializeRecord(Object record, Schema schema, int schemaId)\n+    {\n+        try {\n+            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n+            outputStream.write(0);\n+            outputStream.write(ByteBuffer.allocate(4).putInt(schemaId).array());\n+            BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(outputStream, null);\n+            GenericDatumWriter<Object> avroRecordWriter = new GenericDatumWriter<>(schema);\n+            avroRecordWriter.write(record, encoder);\n+            encoder.flush();\n+            byte[] serializedRecord = outputStream.toByteArray();\n+            outputStream.close();\n+            return serializedRecord;\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private RowDecoder getRowDecoder(SchemaRegistryClient schemaRegistryClient, Set<DecoderColumnHandle> columnHandles, Schema schema)\n+    {\n+        ImmutableMap<String, String> decoderParams = ImmutableMap.<String, String>builder()\n+                .put(DATA_SCHEMA, schema.toString())\n+                .build();\n+        return getAvroRowDecoderyFactory(schemaRegistryClient).create(decoderParams, columnHandles);\n+    }\n+\n+    public static final AvroRowDecoderFactory getAvroRowDecoderyFactory(SchemaRegistryClient schemaRegistryClient)\n+    {\n+        return new AvroRowDecoderFactory(new ConfluentAvroReaderSupplier.Factory(schemaRegistryClient), new AvroBytesDeserializer.Factory());\n+    }\n+\n+    private void assertRowsAreEqual(Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow, GenericRecord expected)\n+    {\n+        checkState(decodedRow.isPresent(), \"decoded row is not present\");\n+        for (Map.Entry<DecoderColumnHandle, FieldValueProvider> entry : decodedRow.get().entrySet()) {\n+            String columnName = entry.getKey().getName();\n+            assertValuesAreEqual(entry.getValue(), expected.get(columnName), expected.getSchema().getField(columnName).schema().getType());\n+        }\n+    }\n+\n+    private void assertValuesAreEqual(FieldValueProvider actual, Object expected, Schema.Type avroType)\n+    {\n+        if (actual.isNull()) {\n+            assertNull(expected);\n+        }\n+        else {\n+            switch (avroType) {\n+                case INT:\n+                case LONG:\n+                    assertEquals(actual.getLong(), ((Number) expected).longValue());\n+                    break;\n+                case STRING:\n+                    assertEquals(actual.getSlice().toStringUtf8(), expected);\n+                    break;\n+                default:\n+                    throw new IllegalStateException(\"Unexpected type\");\n+            }\n+        }\n+    }\n+\n+    private GenericRecord generateRecord(Schema schema, List<Object> values)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 183}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjE5MjQzOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1Nzo1M1rOH8pXCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1Nzo1M1rOH8pXCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1NDI1MA==", "bodyText": "static", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533354250", "createdAt": "2020-12-01T11:57:53Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .name(\"col3\").type().optional().longType()\n+                .endRecord();\n+\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), initialSchema);\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), evolvedSchema);\n+\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", INTEGER, \"col1\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col2\", VARCHAR, \"col2\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, evolvedSchema);\n+        testRow(rowDecoder, generateRecord(initialSchema, Arrays.asList(3, format(\"string-%s\", 3))), 1);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(4, format(\"string-%s\", 4), 4L)), 2);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(5, format(\"string-%s\", 5), null)), 2);\n+    }\n+\n+    @Test\n+    public void testSingleValueRow()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema schema = Schema.create(Schema.Type.LONG);\n+        mockSchemaRegistryClient.register(format(\"%s-key\", TOPIC), schema);\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, schema);\n+        testSingleValueRow(rowDecoder, 3L, schema, 1);\n+    }\n+\n+    private void testRow(RowDecoder rowDecoder, GenericRecord record, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(record, record.getSchema(), schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        assertRowsAreEqual(decodedRow, record);\n+    }\n+\n+    private void testSingleValueRow(RowDecoder rowDecoder, Object value, Schema schema, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(value, schema, schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        checkState(decodedRow.isPresent(), \"decodedRow is not present\");\n+        Map.Entry<DecoderColumnHandle, FieldValueProvider> entry = getOnlyElement(decodedRow.get().entrySet());\n+        assertValuesAreEqual(entry.getValue(), value, schema.getType());\n+    }\n+\n+    private byte[] serializeRecord(Object record, Schema schema, int schemaId)\n+    {\n+        try {\n+            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n+            outputStream.write(0);\n+            outputStream.write(ByteBuffer.allocate(4).putInt(schemaId).array());\n+            BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(outputStream, null);\n+            GenericDatumWriter<Object> avroRecordWriter = new GenericDatumWriter<>(schema);\n+            avroRecordWriter.write(record, encoder);\n+            encoder.flush();\n+            byte[] serializedRecord = outputStream.toByteArray();\n+            outputStream.close();\n+            return serializedRecord;\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private RowDecoder getRowDecoder(SchemaRegistryClient schemaRegistryClient, Set<DecoderColumnHandle> columnHandles, Schema schema)\n+    {\n+        ImmutableMap<String, String> decoderParams = ImmutableMap.<String, String>builder()\n+                .put(DATA_SCHEMA, schema.toString())\n+                .build();\n+        return getAvroRowDecoderyFactory(schemaRegistryClient).create(decoderParams, columnHandles);\n+    }\n+\n+    public static final AvroRowDecoderFactory getAvroRowDecoderyFactory(SchemaRegistryClient schemaRegistryClient)\n+    {\n+        return new AvroRowDecoderFactory(new ConfluentAvroReaderSupplier.Factory(schemaRegistryClient), new AvroBytesDeserializer.Factory());\n+    }\n+\n+    private void assertRowsAreEqual(Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow, GenericRecord expected)\n+    {\n+        checkState(decodedRow.isPresent(), \"decoded row is not present\");\n+        for (Map.Entry<DecoderColumnHandle, FieldValueProvider> entry : decodedRow.get().entrySet()) {\n+            String columnName = entry.getKey().getName();\n+            assertValuesAreEqual(entry.getValue(), expected.get(columnName), expected.getSchema().getField(columnName).schema().getType());\n+        }\n+    }\n+\n+    private void assertValuesAreEqual(FieldValueProvider actual, Object expected, Schema.Type avroType)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 163}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjE5MzA1OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1ODowNVrOH8pXfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1ODowNVrOH8pXfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1NDM2Ng==", "bodyText": "static", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533354366", "createdAt": "2020-12-01T11:58:05Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .name(\"col3\").type().optional().longType()\n+                .endRecord();\n+\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), initialSchema);\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), evolvedSchema);\n+\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", INTEGER, \"col1\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col2\", VARCHAR, \"col2\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, evolvedSchema);\n+        testRow(rowDecoder, generateRecord(initialSchema, Arrays.asList(3, format(\"string-%s\", 3))), 1);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(4, format(\"string-%s\", 4), 4L)), 2);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(5, format(\"string-%s\", 5), null)), 2);\n+    }\n+\n+    @Test\n+    public void testSingleValueRow()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema schema = Schema.create(Schema.Type.LONG);\n+        mockSchemaRegistryClient.register(format(\"%s-key\", TOPIC), schema);\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, schema);\n+        testSingleValueRow(rowDecoder, 3L, schema, 1);\n+    }\n+\n+    private void testRow(RowDecoder rowDecoder, GenericRecord record, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(record, record.getSchema(), schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        assertRowsAreEqual(decodedRow, record);\n+    }\n+\n+    private void testSingleValueRow(RowDecoder rowDecoder, Object value, Schema schema, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(value, schema, schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        checkState(decodedRow.isPresent(), \"decodedRow is not present\");\n+        Map.Entry<DecoderColumnHandle, FieldValueProvider> entry = getOnlyElement(decodedRow.get().entrySet());\n+        assertValuesAreEqual(entry.getValue(), value, schema.getType());\n+    }\n+\n+    private byte[] serializeRecord(Object record, Schema schema, int schemaId)\n+    {\n+        try {\n+            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n+            outputStream.write(0);\n+            outputStream.write(ByteBuffer.allocate(4).putInt(schemaId).array());\n+            BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(outputStream, null);\n+            GenericDatumWriter<Object> avroRecordWriter = new GenericDatumWriter<>(schema);\n+            avroRecordWriter.write(record, encoder);\n+            encoder.flush();\n+            byte[] serializedRecord = outputStream.toByteArray();\n+            outputStream.close();\n+            return serializedRecord;\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private RowDecoder getRowDecoder(SchemaRegistryClient schemaRegistryClient, Set<DecoderColumnHandle> columnHandles, Schema schema)\n+    {\n+        ImmutableMap<String, String> decoderParams = ImmutableMap.<String, String>builder()\n+                .put(DATA_SCHEMA, schema.toString())\n+                .build();\n+        return getAvroRowDecoderyFactory(schemaRegistryClient).create(decoderParams, columnHandles);\n+    }\n+\n+    public static final AvroRowDecoderFactory getAvroRowDecoderyFactory(SchemaRegistryClient schemaRegistryClient)\n+    {\n+        return new AvroRowDecoderFactory(new ConfluentAvroReaderSupplier.Factory(schemaRegistryClient), new AvroBytesDeserializer.Factory());\n+    }\n+\n+    private void assertRowsAreEqual(Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow, GenericRecord expected)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjE5NzIzOnYy", "diffSide": "RIGHT", "path": "presto-kinesis/src/main/java/io/prestosql/plugin/kinesis/KinesisModule.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1OToxMlrOH8paCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMTo1OToxMlrOH8paCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1NTAxNg==", "bodyText": "Extracting AvroDecoderModule should go in separate commit. If you like you can do this as separate pull request so we can merge it sooner.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533355016", "createdAt": "2020-12-01T11:59:12Z", "author": {"login": "kokosing"}, "path": "presto-kinesis/src/main/java/io/prestosql/plugin/kinesis/KinesisModule.java", "diffHunk": "@@ -54,6 +55,7 @@ public void configure(Binder binder)\n         jsonCodecBinder(binder).bindJsonCodec(KinesisStreamDescription.class);\n \n         binder.install(new DecoderModule());\n+        binder.install(new AvroDecoderModule());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjM1NzMxOnYy", "diffSide": "LEFT", "path": "presto-record-decoder/src/main/java/io/prestosql/decoder/avro/AvroRowDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo0NDowNlrOH8q7rQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo0NDowNlrOH8q7rQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4MDAxMw==", "bodyText": "It would be nice to separate refactor of this class to separate commit, so it is known which class it was turned into.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533380013", "createdAt": "2020-12-01T12:44:06Z", "author": {"login": "kokosing"}, "path": "presto-record-decoder/src/main/java/io/prestosql/decoder/avro/AvroRowDecoder.java", "diffHunk": "@@ -1,95 +0,0 @@\n-/*\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package io.prestosql.decoder.avro;\n-\n-import io.prestosql.decoder.DecoderColumnHandle;\n-import io.prestosql.decoder.FieldValueProvider;\n-import io.prestosql.decoder.RowDecoder;\n-import io.prestosql.spi.PrestoException;\n-import org.apache.avro.file.DataFileStream;\n-import org.apache.avro.generic.GenericRecord;\n-import org.apache.avro.io.DatumReader;\n-\n-import java.io.ByteArrayInputStream;\n-import java.io.IOException;\n-import java.util.Map;\n-import java.util.Optional;\n-import java.util.Set;\n-\n-import static com.google.common.base.Functions.identity;\n-import static com.google.common.collect.ImmutableMap.toImmutableMap;\n-import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n-import static java.util.Objects.requireNonNull;\n-\n-public class AvroRowDecoder", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjM2MzYyOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/AvroSchemaConverter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo0NTo1M1rOH8q_kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMzo0NjozNlrOID5bkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4MTAxMQ==", "bodyText": "What is FIXED?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533381011", "createdAt": "2020-12-01T12:45:53Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/AvroSchemaConverter.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.spi.type.ArrayType;\n+import io.prestosql.spi.type.BigintType;\n+import io.prestosql.spi.type.BooleanType;\n+import io.prestosql.spi.type.DoubleType;\n+import io.prestosql.spi.type.IntegerType;\n+import io.prestosql.spi.type.MapType;\n+import io.prestosql.spi.type.RealType;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import io.prestosql.spi.type.VarbinaryType;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+import static org.apache.avro.Schema.Type.ARRAY;\n+import static org.apache.avro.Schema.Type.BYTES;\n+import static org.apache.avro.Schema.Type.DOUBLE;\n+import static org.apache.avro.Schema.Type.ENUM;\n+import static org.apache.avro.Schema.Type.FIXED;\n+import static org.apache.avro.Schema.Type.FLOAT;\n+import static org.apache.avro.Schema.Type.INT;\n+import static org.apache.avro.Schema.Type.LONG;\n+import static org.apache.avro.Schema.Type.MAP;\n+import static org.apache.avro.Schema.Type.NULL;\n+import static org.apache.avro.Schema.Type.RECORD;\n+import static org.apache.avro.Schema.Type.STRING;\n+import static org.apache.avro.Schema.Type.UNION;\n+\n+public class AvroSchemaConverter\n+{\n+    public static final String DUMMY_FIELD_NAME = \"dummy\";\n+\n+    public enum EmptyFieldStrategy\n+    {\n+        IGNORE,\n+        ADD_DUMMY,\n+        FAIL,\n+    }\n+\n+    private static final Set<Schema.Type> INTEGRAL_TYPES = ImmutableSet.of(INT, LONG);\n+    private static final Set<Schema.Type> DECIMAL_TYPES = ImmutableSet.of(FLOAT, DOUBLE);\n+    private static final Set<Schema.Type> STRING_TYPES = ImmutableSet.of(STRING, ENUM);\n+    private static final Set<Schema.Type> BINARY_TYPES = ImmutableSet.of(BYTES, FIXED);\n+\n+    private final TypeManager typeManager;\n+    private final EmptyFieldStrategy emptyFieldStrategy;\n+\n+    public AvroSchemaConverter(TypeManager typeManager, EmptyFieldStrategy emptyFieldStrategy)\n+    {\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.emptyFieldStrategy = requireNonNull(emptyFieldStrategy, \"emptyFieldStrategy is null\");\n+    }\n+\n+    public List<Type> convertAvroSchema(Schema schema)\n+    {\n+        requireNonNull(schema, \"schema is null\");\n+        List<Type> types;\n+        if (schema.getType().equals(RECORD)) {\n+            types = convertRecordSchema(schema);\n+        }\n+        else {\n+            types = convertSimpleSchema(schema);\n+        }\n+        checkState(!types.isEmpty(), \"Schema has no valid fields: '%s'\", schema);\n+        return types;\n+    }\n+\n+    private List<Type> convertRecordSchema(Schema schema)\n+    {\n+        checkState(schema.getType().equals(RECORD), \"schema is not an avro record\");\n+        ImmutableList.Builder<Type> builder = ImmutableList.builder();\n+        for (Field field : schema.getFields()) {\n+            convert(field.schema()).ifPresent(builder::add);\n+        }\n+        return builder.build();\n+    }\n+\n+    private List<Type> convertSimpleSchema(Schema schema)\n+    {\n+        checkState(!schema.getType().equals(RECORD), \"Unexpected type for simple schema, cannot be a record\");\n+        return convert(schema).stream()\n+                .collect(toImmutableList());\n+    }\n+\n+    private Optional<Type> convert(Schema schema)\n+    {\n+        switch (schema.getType()) {\n+            case INT:\n+                return Optional.of(IntegerType.INTEGER);\n+            case LONG:\n+                return Optional.of(BigintType.BIGINT);\n+            case BOOLEAN:\n+                return Optional.of(BooleanType.BOOLEAN);\n+            case FLOAT:\n+                return Optional.of(RealType.REAL);\n+            case DOUBLE:\n+                return Optional.of(DoubleType.DOUBLE);\n+            case ENUM:\n+            case STRING:\n+                return Optional.of(VarcharType.VARCHAR);\n+            case BYTES:\n+            case FIXED:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "157a72680de83c8673c229ef17e6ac271d9600a1"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk1NzU4NA==", "bodyText": "It is the avro enum for the GenericFixed type. This java type is a byte array of fixed size.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r540957584", "createdAt": "2020-12-11T13:46:36Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/AvroSchemaConverter.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.spi.type.ArrayType;\n+import io.prestosql.spi.type.BigintType;\n+import io.prestosql.spi.type.BooleanType;\n+import io.prestosql.spi.type.DoubleType;\n+import io.prestosql.spi.type.IntegerType;\n+import io.prestosql.spi.type.MapType;\n+import io.prestosql.spi.type.RealType;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import io.prestosql.spi.type.VarbinaryType;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+import static org.apache.avro.Schema.Type.ARRAY;\n+import static org.apache.avro.Schema.Type.BYTES;\n+import static org.apache.avro.Schema.Type.DOUBLE;\n+import static org.apache.avro.Schema.Type.ENUM;\n+import static org.apache.avro.Schema.Type.FIXED;\n+import static org.apache.avro.Schema.Type.FLOAT;\n+import static org.apache.avro.Schema.Type.INT;\n+import static org.apache.avro.Schema.Type.LONG;\n+import static org.apache.avro.Schema.Type.MAP;\n+import static org.apache.avro.Schema.Type.NULL;\n+import static org.apache.avro.Schema.Type.RECORD;\n+import static org.apache.avro.Schema.Type.STRING;\n+import static org.apache.avro.Schema.Type.UNION;\n+\n+public class AvroSchemaConverter\n+{\n+    public static final String DUMMY_FIELD_NAME = \"dummy\";\n+\n+    public enum EmptyFieldStrategy\n+    {\n+        IGNORE,\n+        ADD_DUMMY,\n+        FAIL,\n+    }\n+\n+    private static final Set<Schema.Type> INTEGRAL_TYPES = ImmutableSet.of(INT, LONG);\n+    private static final Set<Schema.Type> DECIMAL_TYPES = ImmutableSet.of(FLOAT, DOUBLE);\n+    private static final Set<Schema.Type> STRING_TYPES = ImmutableSet.of(STRING, ENUM);\n+    private static final Set<Schema.Type> BINARY_TYPES = ImmutableSet.of(BYTES, FIXED);\n+\n+    private final TypeManager typeManager;\n+    private final EmptyFieldStrategy emptyFieldStrategy;\n+\n+    public AvroSchemaConverter(TypeManager typeManager, EmptyFieldStrategy emptyFieldStrategy)\n+    {\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.emptyFieldStrategy = requireNonNull(emptyFieldStrategy, \"emptyFieldStrategy is null\");\n+    }\n+\n+    public List<Type> convertAvroSchema(Schema schema)\n+    {\n+        requireNonNull(schema, \"schema is null\");\n+        List<Type> types;\n+        if (schema.getType().equals(RECORD)) {\n+            types = convertRecordSchema(schema);\n+        }\n+        else {\n+            types = convertSimpleSchema(schema);\n+        }\n+        checkState(!types.isEmpty(), \"Schema has no valid fields: '%s'\", schema);\n+        return types;\n+    }\n+\n+    private List<Type> convertRecordSchema(Schema schema)\n+    {\n+        checkState(schema.getType().equals(RECORD), \"schema is not an avro record\");\n+        ImmutableList.Builder<Type> builder = ImmutableList.builder();\n+        for (Field field : schema.getFields()) {\n+            convert(field.schema()).ifPresent(builder::add);\n+        }\n+        return builder.build();\n+    }\n+\n+    private List<Type> convertSimpleSchema(Schema schema)\n+    {\n+        checkState(!schema.getType().equals(RECORD), \"Unexpected type for simple schema, cannot be a record\");\n+        return convert(schema).stream()\n+                .collect(toImmutableList());\n+    }\n+\n+    private Optional<Type> convert(Schema schema)\n+    {\n+        switch (schema.getType()) {\n+            case INT:\n+                return Optional.of(IntegerType.INTEGER);\n+            case LONG:\n+                return Optional.of(BigintType.BIGINT);\n+            case BOOLEAN:\n+                return Optional.of(BooleanType.BOOLEAN);\n+            case FLOAT:\n+                return Optional.of(RealType.REAL);\n+            case DOUBLE:\n+                return Optional.of(DoubleType.DOUBLE);\n+            case ENUM:\n+            case STRING:\n+                return Optional.of(VarcharType.VARCHAR);\n+            case BYTES:\n+            case FIXED:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4MTAxMQ=="}, "originalCommit": {"oid": "157a72680de83c8673c229ef17e6ac271d9600a1"}, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjM3MTkxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroSchemaConverter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo0Nzo1NlrOH8rEXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo0Nzo1NlrOH8rEXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4MjIzNg==", "bodyText": "can you please add tests for usingDefault as well?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533382236", "createdAt": "2020-12-01T12:47:56Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroSchemaConverter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.type.ArrayType;\n+import io.prestosql.spi.type.MapType;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.TestingTypeManager;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.DUMMY_FIELD_NAME;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.ADD_DUMMY;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.FAIL;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.IGNORE;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestAvroSchemaConverter\n+{\n+    private static final String RECORD_NAME = \"test\";\n+\n+    private static final TypeManager TYPE_MANAGER = new TestingTypeManager();\n+\n+    @Test\n+    public void testConvertSchema()\n+    {\n+        Schema schema = SchemaBuilder.record(RECORD_NAME)\n+                .fields()\n+                .name(\"bool_col\").type().booleanType().noDefault()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "157a72680de83c8673c229ef17e6ac271d9600a1"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjM3NTA5OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroSchemaConverter.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo0ODo1MFrOH8rGWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMlQwODozMTowNlrOIEcgxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4Mjc0Nw==", "bodyText": "Instead of testing all fields at once, maybe we could have a parametric tests where each test could verify single type?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533382747", "createdAt": "2020-12-01T12:48:50Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroSchemaConverter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.type.ArrayType;\n+import io.prestosql.spi.type.MapType;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.TestingTypeManager;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.DUMMY_FIELD_NAME;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.ADD_DUMMY;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.FAIL;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.IGNORE;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestAvroSchemaConverter\n+{\n+    private static final String RECORD_NAME = \"test\";\n+\n+    private static final TypeManager TYPE_MANAGER = new TestingTypeManager();\n+\n+    @Test\n+    public void testConvertSchema()\n+    {\n+        Schema schema = SchemaBuilder.record(RECORD_NAME)\n+                .fields()\n+                .name(\"bool_col\").type().booleanType().noDefault()\n+                .name(\"int_col\").type().intType().noDefault()\n+                .name(\"long_col\").type().longType().noDefault()\n+                .name(\"float_col\").type().floatType().noDefault()\n+                .name(\"double_col\").type().doubleType().noDefault()\n+                .name(\"string_col\").type().stringType().noDefault()\n+                .name(\"enum_col\").type().enumeration(\"colors\").symbols(\"blue\", \"red\", \"yellow\").noDefault()\n+                .name(\"bytes_col\").type().bytesType().noDefault()\n+                .name(\"fixed_col\").type().fixed(\"fixed\").size(5).noDefault()\n+                .name(\"union_col\").type().unionOf().nullType().and().floatType().and().doubleType().endUnion().noDefault()\n+                .name(\"union_col2\").type().unionOf().nullType().and().intType().and().longType().endUnion().noDefault()\n+                .name(\"union_col3\").type().unionOf().nullType().and().bytesType().and().type(\"fixed\").endUnion().noDefault()\n+                .name(\"union_col4\").type().unionOf().nullType().and().type(\"colors\").and().stringType().endUnion().noDefault()\n+                .name(\"list_col\").type().array().items().intType().noDefault()\n+                .name(\"map_col\").type().map().values().intType().noDefault()\n+                .name(\"record_col\").type().record(\"record_col\")\n+                .fields()\n+                .name(\"nested_list\").type().array().items().map().values().stringType().noDefault()\n+                .name(\"nested_map\").type().map().values().array().items().stringType().noDefault()\n+                .endRecord()\n+                .noDefault()\n+                .endRecord();\n+        AvroSchemaConverter avroSchemaConverter = new AvroSchemaConverter(new TestingTypeManager(), IGNORE);\n+        List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+        List<Type> expected = ImmutableList.<Type>builder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "157a72680de83c8673c229ef17e6ac271d9600a1"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk1OTE1OA==", "bodyText": "Added a test for single types with default values.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r540959158", "createdAt": "2020-12-11T13:49:00Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroSchemaConverter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.type.ArrayType;\n+import io.prestosql.spi.type.MapType;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.TestingTypeManager;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.DUMMY_FIELD_NAME;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.ADD_DUMMY;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.FAIL;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.IGNORE;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestAvroSchemaConverter\n+{\n+    private static final String RECORD_NAME = \"test\";\n+\n+    private static final TypeManager TYPE_MANAGER = new TestingTypeManager();\n+\n+    @Test\n+    public void testConvertSchema()\n+    {\n+        Schema schema = SchemaBuilder.record(RECORD_NAME)\n+                .fields()\n+                .name(\"bool_col\").type().booleanType().noDefault()\n+                .name(\"int_col\").type().intType().noDefault()\n+                .name(\"long_col\").type().longType().noDefault()\n+                .name(\"float_col\").type().floatType().noDefault()\n+                .name(\"double_col\").type().doubleType().noDefault()\n+                .name(\"string_col\").type().stringType().noDefault()\n+                .name(\"enum_col\").type().enumeration(\"colors\").symbols(\"blue\", \"red\", \"yellow\").noDefault()\n+                .name(\"bytes_col\").type().bytesType().noDefault()\n+                .name(\"fixed_col\").type().fixed(\"fixed\").size(5).noDefault()\n+                .name(\"union_col\").type().unionOf().nullType().and().floatType().and().doubleType().endUnion().noDefault()\n+                .name(\"union_col2\").type().unionOf().nullType().and().intType().and().longType().endUnion().noDefault()\n+                .name(\"union_col3\").type().unionOf().nullType().and().bytesType().and().type(\"fixed\").endUnion().noDefault()\n+                .name(\"union_col4\").type().unionOf().nullType().and().type(\"colors\").and().stringType().endUnion().noDefault()\n+                .name(\"list_col\").type().array().items().intType().noDefault()\n+                .name(\"map_col\").type().map().values().intType().noDefault()\n+                .name(\"record_col\").type().record(\"record_col\")\n+                .fields()\n+                .name(\"nested_list\").type().array().items().map().values().stringType().noDefault()\n+                .name(\"nested_map\").type().map().values().array().items().stringType().noDefault()\n+                .endRecord()\n+                .noDefault()\n+                .endRecord();\n+        AvroSchemaConverter avroSchemaConverter = new AvroSchemaConverter(new TestingTypeManager(), IGNORE);\n+        List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+        List<Type> expected = ImmutableList.<Type>builder()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4Mjc0Nw=="}, "originalCommit": {"oid": "157a72680de83c8673c229ef17e6ac271d9600a1"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTUzMjM1OQ==", "bodyText": "Thought about it more, I added tests for coverage anyway, but defaults wouldn't change the type of the column.\nThe nullable columns do change the type from the base type to union so I added those.\nI'm not sure that having a test per type vs one table with all types would add any more coverge.\nlmk what you think, I can always change it.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r541532359", "createdAt": "2020-12-12T08:31:06Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroSchemaConverter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.type.ArrayType;\n+import io.prestosql.spi.type.MapType;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.TestingTypeManager;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.DUMMY_FIELD_NAME;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.ADD_DUMMY;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.FAIL;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.IGNORE;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestAvroSchemaConverter\n+{\n+    private static final String RECORD_NAME = \"test\";\n+\n+    private static final TypeManager TYPE_MANAGER = new TestingTypeManager();\n+\n+    @Test\n+    public void testConvertSchema()\n+    {\n+        Schema schema = SchemaBuilder.record(RECORD_NAME)\n+                .fields()\n+                .name(\"bool_col\").type().booleanType().noDefault()\n+                .name(\"int_col\").type().intType().noDefault()\n+                .name(\"long_col\").type().longType().noDefault()\n+                .name(\"float_col\").type().floatType().noDefault()\n+                .name(\"double_col\").type().doubleType().noDefault()\n+                .name(\"string_col\").type().stringType().noDefault()\n+                .name(\"enum_col\").type().enumeration(\"colors\").symbols(\"blue\", \"red\", \"yellow\").noDefault()\n+                .name(\"bytes_col\").type().bytesType().noDefault()\n+                .name(\"fixed_col\").type().fixed(\"fixed\").size(5).noDefault()\n+                .name(\"union_col\").type().unionOf().nullType().and().floatType().and().doubleType().endUnion().noDefault()\n+                .name(\"union_col2\").type().unionOf().nullType().and().intType().and().longType().endUnion().noDefault()\n+                .name(\"union_col3\").type().unionOf().nullType().and().bytesType().and().type(\"fixed\").endUnion().noDefault()\n+                .name(\"union_col4\").type().unionOf().nullType().and().type(\"colors\").and().stringType().endUnion().noDefault()\n+                .name(\"list_col\").type().array().items().intType().noDefault()\n+                .name(\"map_col\").type().map().values().intType().noDefault()\n+                .name(\"record_col\").type().record(\"record_col\")\n+                .fields()\n+                .name(\"nested_list\").type().array().items().map().values().stringType().noDefault()\n+                .name(\"nested_map\").type().map().values().array().items().stringType().noDefault()\n+                .endRecord()\n+                .noDefault()\n+                .endRecord();\n+        AvroSchemaConverter avroSchemaConverter = new AvroSchemaConverter(new TestingTypeManager(), IGNORE);\n+        List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+        List<Type> expected = ImmutableList.<Type>builder()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4Mjc0Nw=="}, "originalCommit": {"oid": "157a72680de83c8673c229ef17e6ac271d9600a1"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjM5ODQwOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/AbstractContentSchemaReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1NTowOVrOH8rUKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1NTowOVrOH8rUKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NjI4MA==", "bodyText": "final", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533386280", "createdAt": "2020-12-01T12:55:09Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/AbstractContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema;\n+\n+import io.prestosql.plugin.kafka.KafkaTableHandle;\n+\n+import java.util.Optional;\n+\n+public abstract class AbstractContentSchemaReader\n+        implements ContentSchemaReader\n+{\n+    @Override\n+    public Optional<String> readKeyContentSchema(KafkaTableHandle tableHandle)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjM5ODgzOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/AbstractContentSchemaReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1NToxNFrOH8rUWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1NToxNFrOH8rUWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NjMzMA==", "bodyText": "final", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533386330", "createdAt": "2020-12-01T12:55:14Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/AbstractContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema;\n+\n+import io.prestosql.plugin.kafka.KafkaTableHandle;\n+\n+import java.util.Optional;\n+\n+public abstract class AbstractContentSchemaReader\n+        implements ContentSchemaReader\n+{\n+    @Override\n+    public Optional<String> readKeyContentSchema(KafkaTableHandle tableHandle)\n+    {\n+        return readSchema(tableHandle.getKeyDataSchemaLocation(), tableHandle.getKeySubject());\n+    }\n+\n+    @Override\n+    public Optional<String> readValueContentSchema(KafkaTableHandle tableHandle)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjQwMDQxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/ContentSchemaReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1NTo0MVrOH8rVSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1NTo0MVrOH8rVSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NjU3MQ==", "bodyText": "it is not used", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533386571", "createdAt": "2020-12-01T12:55:41Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/ContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,27 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema;\n+\n+import io.prestosql.plugin.kafka.KafkaTableHandle;\n+\n+import java.util.Optional;\n+\n+public interface ContentSchemaReader\n+{\n+    String SCHEMA_READER = \"schemaReader\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjQwMzI0OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/AvroConfluentContentSchemaReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1NjoyNFrOH8rW6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMlQwOToxNjoxMlrOIEc5_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4Njk4NQ==", "bodyText": "So what is going to happen then? Why not to fail? Do you have tests coverage for that?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533386985", "createdAt": "2020-12-01T12:56:24Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/AvroConfluentContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.plugin.kafka.schema.AbstractContentSchemaReader;\n+import io.prestosql.spi.PrestoException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroConfluentContentSchemaReader\n+        extends AbstractContentSchemaReader\n+{\n+    private final SchemaRegistryClient schemaRegistryClient;\n+\n+    @Inject\n+    public AvroConfluentContentSchemaReader(SchemaRegistryClient schemaRegistryClient)\n+    {\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+    }\n+\n+    @Override\n+    protected Optional<String> readSchema(Optional<String> dataSchemaLocation, Optional<String> subject)\n+    {\n+        if (subject.isPresent()) {\n+            try {\n+                return Optional.of(schemaRegistryClient.getLatestSchemaMetadata(subject.get()).getSchema());\n+            }\n+            catch (IOException | RestClientException e) {\n+                throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Could not resolve schema for the '%s' subject\", subject.get()), e);\n+            }\n+        }\n+        return Optional.empty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTUzODgxNQ==", "bodyText": "This can occur if the key or message is not defined. If the key and message are not defined than the dummy decoder will be used.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r541538815", "createdAt": "2020-12-12T09:16:12Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/AvroConfluentContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.plugin.kafka.schema.AbstractContentSchemaReader;\n+import io.prestosql.spi.PrestoException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroConfluentContentSchemaReader\n+        extends AbstractContentSchemaReader\n+{\n+    private final SchemaRegistryClient schemaRegistryClient;\n+\n+    @Inject\n+    public AvroConfluentContentSchemaReader(SchemaRegistryClient schemaRegistryClient)\n+    {\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+    }\n+\n+    @Override\n+    protected Optional<String> readSchema(Optional<String> dataSchemaLocation, Optional<String> subject)\n+    {\n+        if (subject.isPresent()) {\n+            try {\n+                return Optional.of(schemaRegistryClient.getLatestSchemaMetadata(subject.get()).getSchema());\n+            }\n+            catch (IOException | RestClientException e) {\n+                throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Could not resolve schema for the '%s' subject\", subject.get()), e);\n+            }\n+        }\n+        return Optional.empty();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4Njk4NQ=="}, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjQwMzg2OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/AvroConfluentContentSchemaReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1NjozNVrOH8rXUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1NjozNVrOH8rXUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NzA4OA==", "bodyText": "invert condition", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533387088", "createdAt": "2020-12-01T12:56:35Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/AvroConfluentContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.plugin.kafka.schema.AbstractContentSchemaReader;\n+import io.prestosql.spi.PrestoException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroConfluentContentSchemaReader\n+        extends AbstractContentSchemaReader\n+{\n+    private final SchemaRegistryClient schemaRegistryClient;\n+\n+    @Inject\n+    public AvroConfluentContentSchemaReader(SchemaRegistryClient schemaRegistryClient)\n+    {\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+    }\n+\n+    @Override\n+    protected Optional<String> readSchema(Optional<String> dataSchemaLocation, Optional<String> subject)\n+    {\n+        if (subject.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjQwNzE5OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/AvroConfluentContentSchemaReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1NzoyNlrOH8rZRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1NzoyNlrOH8rZRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NzU4OA==", "bodyText": "what if dataSchemaLocation is set? Should we ignore it? Should we fails as this is unexpected?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533387588", "createdAt": "2020-12-01T12:57:26Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/AvroConfluentContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.plugin.kafka.schema.AbstractContentSchemaReader;\n+import io.prestosql.spi.PrestoException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroConfluentContentSchemaReader\n+        extends AbstractContentSchemaReader\n+{\n+    private final SchemaRegistryClient schemaRegistryClient;\n+\n+    @Inject\n+    public AvroConfluentContentSchemaReader(SchemaRegistryClient schemaRegistryClient)\n+    {\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+    }\n+\n+    @Override\n+    protected Optional<String> readSchema(Optional<String> dataSchemaLocation, Optional<String> subject)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjQwODY1OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/file/FileContentSchemaReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1Nzo1MlrOH8raKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1Nzo1MlrOH8raKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NzgxOQ==", "bodyText": "verify that subject is not set?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533387819", "createdAt": "2020-12-01T12:57:52Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/file/FileContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.file;\n+\n+import com.google.common.io.CharStreams;\n+import io.prestosql.plugin.kafka.schema.AbstractContentSchemaReader;\n+import io.prestosql.spi.PrestoException;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.Optional;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.util.Locale.ENGLISH;\n+\n+public class FileContentSchemaReader\n+        extends AbstractContentSchemaReader\n+{\n+    @Override\n+    protected Optional<String> readSchema(Optional<String> dataSchemaLocation, Optional<String> subject)\n+    {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjQwOTY1OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/file/FileContentSchemaReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1ODowOFrOH8raxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMlQwOToyODowN1rOIEdBAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4Nzk3NA==", "bodyText": "do we have tests for that case?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533387974", "createdAt": "2020-12-01T12:58:08Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/file/FileContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.file;\n+\n+import com.google.common.io.CharStreams;\n+import io.prestosql.plugin.kafka.schema.AbstractContentSchemaReader;\n+import io.prestosql.spi.PrestoException;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.Optional;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.util.Locale.ENGLISH;\n+\n+public class FileContentSchemaReader\n+        extends AbstractContentSchemaReader\n+{\n+    @Override\n+    protected Optional<String> readSchema(Optional<String> dataSchemaLocation, Optional<String> subject)\n+    {\n+        if (!dataSchemaLocation.isPresent()) {\n+            return Optional.empty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU0MDYwOQ==", "bodyText": "This is the default content schema used, there are tests where the key is not defined.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r541540609", "createdAt": "2020-12-12T09:28:07Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/file/FileContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.file;\n+\n+import com.google.common.io.CharStreams;\n+import io.prestosql.plugin.kafka.schema.AbstractContentSchemaReader;\n+import io.prestosql.spi.PrestoException;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.Optional;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.util.Locale.ENGLISH;\n+\n+public class FileContentSchemaReader\n+        extends AbstractContentSchemaReader\n+{\n+    @Override\n+    protected Optional<String> readSchema(Optional<String> dataSchemaLocation, Optional<String> subject)\n+    {\n+        if (!dataSchemaLocation.isPresent()) {\n+            return Optional.empty();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4Nzk3NA=="}, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjQxNTIyOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentSchemaReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1OTo0MlrOH8reNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1OTo0MlrOH8reNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4ODg1Mg==", "bodyText": "update test name to TestAvroConfluentContentSchemaReader", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533388852", "createdAt": "2020-12-01T12:59:42Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentSchemaReader.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaTableHandle;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.testng.annotations.Test;\n+\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestAvroConfluentSchemaReader", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTA3NDM0OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMTo1MTo0N1rOH9XYiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMTo1MTo0N1rOH9XYiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEwODI5Nw==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534108297", "createdAt": "2020-12-02T11:51:47Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTA3NDYwOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMTo1MTo1M1rOH9XYuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMTo1MTo1M1rOH9XYuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEwODM0NA==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534108344", "createdAt": "2020-12-02T11:51:53Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTA3ODExOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMTo1Mjo1OVrOH9Xa_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMlQwOTo0OToxOFrOIEdNHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEwODkyNQ==", "bodyText": "it is not used", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534108925", "createdAt": "2020-12-02T11:52:59Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU0MzcwOQ==", "bodyText": "It is referenced in @PreDestroy method, to shut it down. Is that necessary or can it be removed since they're daemon threads?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r541543709", "createdAt": "2020-12-12T09:49:18Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEwODkyNQ=="}, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTA4MDQ4OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMTo1Mzo0M1rOH9Xcfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMTo1Mzo0M1rOH9Xcfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEwOTMxMQ==", "bodyText": "It is Kafka error, not Presto generic internal error", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534109311", "createdAt": "2020-12-02T11:53:43Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 155}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTA5MDUzOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMTo1NjowNVrOH9XiPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMTo1NjowNVrOH9XiPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExMDc4MQ==", "bodyText": "SetMulitmap?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534110781", "createdAt": "2020-12-02T11:56:05Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 138}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTA5NDI0OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMTo1NzowNFrOH9XkeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMTo1NzowNFrOH9XkeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExMTM1Mw==", "bodyText": "move topic to the next line", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534111353", "createdAt": "2020-12-02T11:57:04Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 147}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTExNTYxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowMzowMlrOH9Xx2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMlQwOTo1OToyMlrOIEdTMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNDc3OA==", "bodyText": "isEmpty", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534114778", "createdAt": "2020-12-02T12:03:02Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 175}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU0NTI2Ng==", "bodyText": "We can use java11 in the code now? There are more places where this can be used then, that would be great.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r541545266", "createdAt": "2020-12-12T09:59:22Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNDc3OA=="}, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 175}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTExNjAxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowMzoxMFrOH9XyFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowMzoxMFrOH9XyFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNDgzOQ==", "bodyText": "isEmpty", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534114839", "createdAt": "2020-12-02T12:03:10Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 178}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTExNjUyOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowMzoxOVrOH9Xyaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowMzoxOVrOH9Xyaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNDkyMw==", "bodyText": "isEmpty", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534114923", "createdAt": "2020-12-02T12:03:19Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 182}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTEyNTA1OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowNToyOFrOH9X3hA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMlQxMTowNjozOFrOIEd5jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNjIyOA==", "bodyText": "Why messageSubject and not valueSubject?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534116228", "createdAt": "2020-12-02T12:05:28Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);\n+        }\n+    }\n+\n+    // If the default subject naming strategy is not used, the key and message subjects\n+    // can be specified by adding them to the table name as follows:\n+    //  <tablename>&key-subject=<key subject>&message-subject=<value subject>\n+    // ex. kafka.default.\"mytable&key-subject=foo&message-subject=bar\"\n+    @VisibleForTesting\n+    static TopicAndSubjects parseTopicAndSubjects(SchemaTableName encodedSchemaTableName)\n+    {\n+        String encodedTableName = encodedSchemaTableName.getTableName();\n+        List<String> parts = Splitter.on(DELIMITER).trimResults().splitToList(encodedTableName);\n+        checkState(!parts.isEmpty() && parts.size() <= 3, \"Unexpected format for encodedTableName. Expected format is <tableName>[&keySubject=<key subject>][&messageSubject=<message subject>]\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 240}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU1NTA4Ng==", "bodyText": "To keep naming consistency with the KafkaTableHandle, there is key/message data format and key/message schema location. But here it makes sense - trying it out. lmk if it looks more confusing this way, I can change it back.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r541555086", "createdAt": "2020-12-12T11:06:38Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);\n+        }\n+    }\n+\n+    // If the default subject naming strategy is not used, the key and message subjects\n+    // can be specified by adding them to the table name as follows:\n+    //  <tablename>&key-subject=<key subject>&message-subject=<value subject>\n+    // ex. kafka.default.\"mytable&key-subject=foo&message-subject=bar\"\n+    @VisibleForTesting\n+    static TopicAndSubjects parseTopicAndSubjects(SchemaTableName encodedSchemaTableName)\n+    {\n+        String encodedTableName = encodedSchemaTableName.getTableName();\n+        List<String> parts = Splitter.on(DELIMITER).trimResults().splitToList(encodedTableName);\n+        checkState(!parts.isEmpty() && parts.size() <= 3, \"Unexpected format for encodedTableName. Expected format is <tableName>[&keySubject=<key subject>][&messageSubject=<message subject>]\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNjIyOA=="}, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 240}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTEzMTkwOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowNzoyNVrOH9X7qw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowNzoyNVrOH9X7qw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNzI5MQ==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534117291", "createdAt": "2020-12-02T12:07:25Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);\n+        }\n+    }\n+\n+    // If the default subject naming strategy is not used, the key and message subjects\n+    // can be specified by adding them to the table name as follows:\n+    //  <tablename>&key-subject=<key subject>&message-subject=<value subject>\n+    // ex. kafka.default.\"mytable&key-subject=foo&message-subject=bar\"\n+    @VisibleForTesting\n+    static TopicAndSubjects parseTopicAndSubjects(SchemaTableName encodedSchemaTableName)\n+    {\n+        String encodedTableName = encodedSchemaTableName.getTableName();\n+        List<String> parts = Splitter.on(DELIMITER).trimResults().splitToList(encodedTableName);\n+        checkState(!parts.isEmpty() && parts.size() <= 3, \"Unexpected format for encodedTableName. Expected format is <tableName>[&keySubject=<key subject>][&messageSubject=<message subject>]\");\n+        String tableName = parts.get(0);\n+        Optional<String> keySubject = Optional.empty();\n+        Optional<String> valueSubject = Optional.empty();\n+        for (int part = 1; part < parts.size(); part++) {\n+            List<String> subjectKeyValue = Splitter.on(SEPARATOR).trimResults().splitToList(parts.get(part));\n+            checkState(subjectKeyValue.size() == 2 && (subjectKeyValue.get(0).equals(KEY_SUBJECT) || subjectKeyValue.get(0).equals(MESSAGE_SUBJECT)), \"Unexpected parameter '%s', should be %s=<key subject>' or %s=<message subject>\", parts.get(part), KEY_SUBJECT, MESSAGE_SUBJECT);\n+            if (subjectKeyValue.get(0).equals(KEY_SUBJECT)) {\n+                checkState(!keySubject.isPresent(), \"Key subject already defined\");\n+                keySubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+            else {\n+                checkState(!valueSubject.isPresent(), \"Value subject already defined\");\n+                valueSubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+        }\n+        return new TopicAndSubjects(tableName, keySubject, valueSubject);\n+    }\n+\n+    @Override\n+    public Set<SchemaTableName> listTables()\n+    {\n+        try {\n+            ImmutableSet.Builder<SchemaTableName> schemaTableNameBuilder = ImmutableSet.builder();\n+            schemaRegistryClient.getAllSubjects().stream()\n+                    .filter(ConfluentSchemaRegistryTableDescriptionSupplier::isValidSubject)\n+                    .forEach(subject -> schemaTableNameBuilder.add(new SchemaTableName(defaultSchema, extractTopicFromSubject(subject))));\n+            return schemaTableNameBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Unable to list tables\", e);\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    static boolean isValidSubject(String subject)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 275}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTEzMjQzOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowNzozNlrOH9X8Ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowNzozNlrOH9X8Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNzM3OA==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534117378", "createdAt": "2020-12-02T12:07:36Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);\n+        }\n+    }\n+\n+    // If the default subject naming strategy is not used, the key and message subjects\n+    // can be specified by adding them to the table name as follows:\n+    //  <tablename>&key-subject=<key subject>&message-subject=<value subject>\n+    // ex. kafka.default.\"mytable&key-subject=foo&message-subject=bar\"\n+    @VisibleForTesting\n+    static TopicAndSubjects parseTopicAndSubjects(SchemaTableName encodedSchemaTableName)\n+    {\n+        String encodedTableName = encodedSchemaTableName.getTableName();\n+        List<String> parts = Splitter.on(DELIMITER).trimResults().splitToList(encodedTableName);\n+        checkState(!parts.isEmpty() && parts.size() <= 3, \"Unexpected format for encodedTableName. Expected format is <tableName>[&keySubject=<key subject>][&messageSubject=<message subject>]\");\n+        String tableName = parts.get(0);\n+        Optional<String> keySubject = Optional.empty();\n+        Optional<String> valueSubject = Optional.empty();\n+        for (int part = 1; part < parts.size(); part++) {\n+            List<String> subjectKeyValue = Splitter.on(SEPARATOR).trimResults().splitToList(parts.get(part));\n+            checkState(subjectKeyValue.size() == 2 && (subjectKeyValue.get(0).equals(KEY_SUBJECT) || subjectKeyValue.get(0).equals(MESSAGE_SUBJECT)), \"Unexpected parameter '%s', should be %s=<key subject>' or %s=<message subject>\", parts.get(part), KEY_SUBJECT, MESSAGE_SUBJECT);\n+            if (subjectKeyValue.get(0).equals(KEY_SUBJECT)) {\n+                checkState(!keySubject.isPresent(), \"Key subject already defined\");\n+                keySubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+            else {\n+                checkState(!valueSubject.isPresent(), \"Value subject already defined\");\n+                valueSubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+        }\n+        return new TopicAndSubjects(tableName, keySubject, valueSubject);\n+    }\n+\n+    @Override\n+    public Set<SchemaTableName> listTables()\n+    {\n+        try {\n+            ImmutableSet.Builder<SchemaTableName> schemaTableNameBuilder = ImmutableSet.builder();\n+            schemaRegistryClient.getAllSubjects().stream()\n+                    .filter(ConfluentSchemaRegistryTableDescriptionSupplier::isValidSubject)\n+                    .forEach(subject -> schemaTableNameBuilder.add(new SchemaTableName(defaultSchema, extractTopicFromSubject(subject))));\n+            return schemaTableNameBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Unable to list tables\", e);\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    static boolean isValidSubject(String subject)\n+    {\n+        requireNonNull(subject, \"subject is null\");\n+        return subject.endsWith(VALUE_SUFFIX) || subject.endsWith(KEY_SUFFIX);\n+    }\n+\n+    @VisibleForTesting\n+    static String extractTopicFromSubject(String subject)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 282}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTEzMzAxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowNzo0OVrOH9X8YA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowNzo0OVrOH9X8YA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNzQ3Mg==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534117472", "createdAt": "2020-12-02T12:07:49Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);\n+        }\n+    }\n+\n+    // If the default subject naming strategy is not used, the key and message subjects\n+    // can be specified by adding them to the table name as follows:\n+    //  <tablename>&key-subject=<key subject>&message-subject=<value subject>\n+    // ex. kafka.default.\"mytable&key-subject=foo&message-subject=bar\"\n+    @VisibleForTesting\n+    static TopicAndSubjects parseTopicAndSubjects(SchemaTableName encodedSchemaTableName)\n+    {\n+        String encodedTableName = encodedSchemaTableName.getTableName();\n+        List<String> parts = Splitter.on(DELIMITER).trimResults().splitToList(encodedTableName);\n+        checkState(!parts.isEmpty() && parts.size() <= 3, \"Unexpected format for encodedTableName. Expected format is <tableName>[&keySubject=<key subject>][&messageSubject=<message subject>]\");\n+        String tableName = parts.get(0);\n+        Optional<String> keySubject = Optional.empty();\n+        Optional<String> valueSubject = Optional.empty();\n+        for (int part = 1; part < parts.size(); part++) {\n+            List<String> subjectKeyValue = Splitter.on(SEPARATOR).trimResults().splitToList(parts.get(part));\n+            checkState(subjectKeyValue.size() == 2 && (subjectKeyValue.get(0).equals(KEY_SUBJECT) || subjectKeyValue.get(0).equals(MESSAGE_SUBJECT)), \"Unexpected parameter '%s', should be %s=<key subject>' or %s=<message subject>\", parts.get(part), KEY_SUBJECT, MESSAGE_SUBJECT);\n+            if (subjectKeyValue.get(0).equals(KEY_SUBJECT)) {\n+                checkState(!keySubject.isPresent(), \"Key subject already defined\");\n+                keySubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+            else {\n+                checkState(!valueSubject.isPresent(), \"Value subject already defined\");\n+                valueSubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+        }\n+        return new TopicAndSubjects(tableName, keySubject, valueSubject);\n+    }\n+\n+    @Override\n+    public Set<SchemaTableName> listTables()\n+    {\n+        try {\n+            ImmutableSet.Builder<SchemaTableName> schemaTableNameBuilder = ImmutableSet.builder();\n+            schemaRegistryClient.getAllSubjects().stream()\n+                    .filter(ConfluentSchemaRegistryTableDescriptionSupplier::isValidSubject)\n+                    .forEach(subject -> schemaTableNameBuilder.add(new SchemaTableName(defaultSchema, extractTopicFromSubject(subject))));\n+            return schemaTableNameBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Unable to list tables\", e);\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    static boolean isValidSubject(String subject)\n+    {\n+        requireNonNull(subject, \"subject is null\");\n+        return subject.endsWith(VALUE_SUFFIX) || subject.endsWith(KEY_SUFFIX);\n+    }\n+\n+    @VisibleForTesting\n+    static String extractTopicFromSubject(String subject)\n+    {\n+        requireNonNull(subject, \"subject is null\");\n+        if (subject.endsWith(VALUE_SUFFIX)) {\n+            return subject.substring(0, subject.length() - VALUE_SUFFIX.length());\n+        }\n+        checkState(subject.endsWith(KEY_SUFFIX), \"Unexpected subject name %s\", subject);\n+        return subject.substring(0, subject.length() - KEY_SUFFIX.length());\n+    }\n+\n+    @VisibleForTesting\n+    static Optional<String> getKeySubjectFromTopic(String topic, Set<String> subjectsForTopic)\n+    {\n+        String keySubject = topic + KEY_SUFFIX;\n+        if (subjectsForTopic.contains(keySubject)) {\n+            return Optional.of(keySubject);\n+        }\n+        return Optional.empty();\n+    }\n+\n+    @VisibleForTesting\n+    static Optional<String> getValueSubjectFromTopic(String topic, Set<String> subjectsForTopic)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 303}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTEzMzE3OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowNzo1M1rOH9X8eA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowNzo1M1rOH9X8eA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNzQ5Ng==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534117496", "createdAt": "2020-12-02T12:07:53Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);\n+        }\n+    }\n+\n+    // If the default subject naming strategy is not used, the key and message subjects\n+    // can be specified by adding them to the table name as follows:\n+    //  <tablename>&key-subject=<key subject>&message-subject=<value subject>\n+    // ex. kafka.default.\"mytable&key-subject=foo&message-subject=bar\"\n+    @VisibleForTesting\n+    static TopicAndSubjects parseTopicAndSubjects(SchemaTableName encodedSchemaTableName)\n+    {\n+        String encodedTableName = encodedSchemaTableName.getTableName();\n+        List<String> parts = Splitter.on(DELIMITER).trimResults().splitToList(encodedTableName);\n+        checkState(!parts.isEmpty() && parts.size() <= 3, \"Unexpected format for encodedTableName. Expected format is <tableName>[&keySubject=<key subject>][&messageSubject=<message subject>]\");\n+        String tableName = parts.get(0);\n+        Optional<String> keySubject = Optional.empty();\n+        Optional<String> valueSubject = Optional.empty();\n+        for (int part = 1; part < parts.size(); part++) {\n+            List<String> subjectKeyValue = Splitter.on(SEPARATOR).trimResults().splitToList(parts.get(part));\n+            checkState(subjectKeyValue.size() == 2 && (subjectKeyValue.get(0).equals(KEY_SUBJECT) || subjectKeyValue.get(0).equals(MESSAGE_SUBJECT)), \"Unexpected parameter '%s', should be %s=<key subject>' or %s=<message subject>\", parts.get(part), KEY_SUBJECT, MESSAGE_SUBJECT);\n+            if (subjectKeyValue.get(0).equals(KEY_SUBJECT)) {\n+                checkState(!keySubject.isPresent(), \"Key subject already defined\");\n+                keySubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+            else {\n+                checkState(!valueSubject.isPresent(), \"Value subject already defined\");\n+                valueSubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+        }\n+        return new TopicAndSubjects(tableName, keySubject, valueSubject);\n+    }\n+\n+    @Override\n+    public Set<SchemaTableName> listTables()\n+    {\n+        try {\n+            ImmutableSet.Builder<SchemaTableName> schemaTableNameBuilder = ImmutableSet.builder();\n+            schemaRegistryClient.getAllSubjects().stream()\n+                    .filter(ConfluentSchemaRegistryTableDescriptionSupplier::isValidSubject)\n+                    .forEach(subject -> schemaTableNameBuilder.add(new SchemaTableName(defaultSchema, extractTopicFromSubject(subject))));\n+            return schemaTableNameBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Unable to list tables\", e);\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    static boolean isValidSubject(String subject)\n+    {\n+        requireNonNull(subject, \"subject is null\");\n+        return subject.endsWith(VALUE_SUFFIX) || subject.endsWith(KEY_SUFFIX);\n+    }\n+\n+    @VisibleForTesting\n+    static String extractTopicFromSubject(String subject)\n+    {\n+        requireNonNull(subject, \"subject is null\");\n+        if (subject.endsWith(VALUE_SUFFIX)) {\n+            return subject.substring(0, subject.length() - VALUE_SUFFIX.length());\n+        }\n+        checkState(subject.endsWith(KEY_SUFFIX), \"Unexpected subject name %s\", subject);\n+        return subject.substring(0, subject.length() - KEY_SUFFIX.length());\n+    }\n+\n+    @VisibleForTesting\n+    static Optional<String> getKeySubjectFromTopic(String topic, Set<String> subjectsForTopic)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 293}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTE0MDUyOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxMDowN1rOH9YBHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxMDowN1rOH9YBHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExODY4Ng==", "bodyText": "Move this up, then you can:\n        Optional<String> keySubject = topicAndSubjects.getKeySubject()\n                .or(topicAndSubjectsFromCache::getKeySubject);\n\nSame couldbe applied to value. Mind that topicAndSubjectsFromCache is nullable.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534118686", "createdAt": "2020-12-02T12:10:07Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 170}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTE0NjM4OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxMTo0NlrOH9YEwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxMTo0NlrOH9YEwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExOTYxOA==", "bodyText": "It is kafka error, not Presto generic internal error.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534119618", "createdAt": "2020-12-02T12:11:46Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 227}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTE0ODQzOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxMjoyN1rOH9YGMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxMjoyN1rOH9YGMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExOTk4Nw==", "bodyText": "use topicAndSubjectsSupplier to list tables.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534119987", "createdAt": "2020-12-02T12:12:27Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);\n+        }\n+    }\n+\n+    // If the default subject naming strategy is not used, the key and message subjects\n+    // can be specified by adding them to the table name as follows:\n+    //  <tablename>&key-subject=<key subject>&message-subject=<value subject>\n+    // ex. kafka.default.\"mytable&key-subject=foo&message-subject=bar\"\n+    @VisibleForTesting\n+    static TopicAndSubjects parseTopicAndSubjects(SchemaTableName encodedSchemaTableName)\n+    {\n+        String encodedTableName = encodedSchemaTableName.getTableName();\n+        List<String> parts = Splitter.on(DELIMITER).trimResults().splitToList(encodedTableName);\n+        checkState(!parts.isEmpty() && parts.size() <= 3, \"Unexpected format for encodedTableName. Expected format is <tableName>[&keySubject=<key subject>][&messageSubject=<message subject>]\");\n+        String tableName = parts.get(0);\n+        Optional<String> keySubject = Optional.empty();\n+        Optional<String> valueSubject = Optional.empty();\n+        for (int part = 1; part < parts.size(); part++) {\n+            List<String> subjectKeyValue = Splitter.on(SEPARATOR).trimResults().splitToList(parts.get(part));\n+            checkState(subjectKeyValue.size() == 2 && (subjectKeyValue.get(0).equals(KEY_SUBJECT) || subjectKeyValue.get(0).equals(MESSAGE_SUBJECT)), \"Unexpected parameter '%s', should be %s=<key subject>' or %s=<message subject>\", parts.get(part), KEY_SUBJECT, MESSAGE_SUBJECT);\n+            if (subjectKeyValue.get(0).equals(KEY_SUBJECT)) {\n+                checkState(!keySubject.isPresent(), \"Key subject already defined\");\n+                keySubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+            else {\n+                checkState(!valueSubject.isPresent(), \"Value subject already defined\");\n+                valueSubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+        }\n+        return new TopicAndSubjects(tableName, keySubject, valueSubject);\n+    }\n+\n+    @Override\n+    public Set<SchemaTableName> listTables()\n+    {\n+        try {\n+            ImmutableSet.Builder<SchemaTableName> schemaTableNameBuilder = ImmutableSet.builder();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 263}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTE1MDMxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSessionProperties.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxMzowMFrOH9YHaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxMzowMFrOH9YHaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyMDI5OQ==", "bodyText": "private final?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534120299", "createdAt": "2020-12-02T12:13:00Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSessionProperties.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.session.PropertyMetadata;\n+\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.util.List;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentSessionProperties\n+        implements Provider<List<PropertyMetadata<?>>>\n+{\n+    public static final String EMPTY_FIELD_STRATEGY = \"empty_field_strategy\";\n+\n+    List<PropertyMetadata<?>> sessionProperties;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTE2NzYwOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/KafkaWithConfluentSchemaRegistryQueryRunner.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxNzozMVrOH9YRvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxNzozMVrOH9YRvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyMjk0MQ==", "bodyText": "Why can't you use simply KafkaQueryRunner? Let's make TestingKafkaWithSchemaRegistry  and TestingKafka to share common interface (maybe interface could be called TestingKafka and implementation BasicTestingKafka, wdyt?), are there any other issues?\nYou can keep this class for main method and some utils if you like. BTW I like the main method here ;)", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534122941", "createdAt": "2020-12-02T12:17:31Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/KafkaWithConfluentSchemaRegistryQueryRunner.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Module;\n+import io.airlift.log.Level;\n+import io.airlift.log.Logger;\n+import io.airlift.log.Logging;\n+import io.prestosql.plugin.kafka.KafkaPlugin;\n+import io.prestosql.plugin.kafka.KafkaQueryRunner;\n+import io.prestosql.plugin.tpch.TpchPlugin;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import io.prestosql.testing.kafka.TestingKafkaWithSchemaRegistry;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.airlift.testing.Closeables.closeAllSuppress;\n+import static io.prestosql.plugin.kafka.KafkaPlugin.DEFAULT_EXTENSION;\n+import static io.prestosql.testing.TestingSession.testSessionBuilder;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaWithConfluentSchemaRegistryQueryRunner", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTE3NTQxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestKafkaWithConfluentSchemaRegistryMinimalFunctionality.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxOTozOFrOH9YWOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxOTozOFrOH9YWOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyNDA5MA==", "bodyText": "Use failsafe.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534124090", "createdAt": "2020-12-02T12:19:38Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestKafkaWithConfluentSchemaRegistryMinimalFunctionality.java", "diffHunk": "@@ -0,0 +1,278 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.confluent.kafka.serializers.subject.RecordNameStrategy;\n+import io.confluent.kafka.serializers.subject.TopicRecordNameStrategy;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import io.prestosql.testing.kafka.TestingKafkaWithSchemaRegistry;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+import java.util.function.Supplier;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.VALUE_SUBJECT_NAME_STRATEGY;\n+import static java.lang.Math.multiplyExact;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static org.testng.Assert.assertTrue;\n+\n+@Test(singleThreaded = true)\n+public class TestKafkaWithConfluentSchemaRegistryMinimalFunctionality\n+        extends AbstractTestQueryFramework\n+{\n+    private static final String RECORD_NAME = \"test_record\";\n+    private static final int MESSAGE_COUNT = 100;\n+    private static final Schema INITIAL_SCHEMA = SchemaBuilder.record(RECORD_NAME)\n+            .fields()\n+            .name(\"col_1\").type().longType().noDefault()\n+            .name(\"col_2\").type().stringType().noDefault()\n+            .endRecord();\n+    private static final Schema EVOLVED_SCHEMA = SchemaBuilder.record(RECORD_NAME)\n+            .fields()\n+            .name(\"col_1\").type().longType().noDefault()\n+            .name(\"col_2\").type().stringType().noDefault()\n+            .name(\"col_3\").type().optional().doubleType()\n+            .endRecord();\n+\n+    private TestingKafkaWithSchemaRegistry testingKafkaWithSchemaRegistry;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        testingKafkaWithSchemaRegistry = new TestingKafkaWithSchemaRegistry(new TestingKafka());\n+        return KafkaWithConfluentSchemaRegistryQueryRunner.builder(testingKafkaWithSchemaRegistry)\n+                .setExtraKafkaProperties(ImmutableMap.<String, String>builder()\n+                        .put(\"kafka.confluent-subjects-cache-refresh-interval\", \"1ms\")\n+                        .build())\n+                .build();\n+    }\n+\n+    @Test\n+    public void testBasicTopic()\n+    {\n+        String topic = \"topic-basic-MixedCase\";\n+        assertTopic(topic,\n+                format(\"SELECT col_1, col_2 FROM %s\", toDoubleQuoted(topic)),\n+                format(\"SELECT col_1, col_2, col_3 FROM %s\", toDoubleQuoted(topic)),\n+                false,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducerWithLongKeys());\n+    }\n+\n+    @Test\n+    public void testTopicWithKeySubject()\n+    {\n+        String topic = \"topic-key-subject\";\n+        assertTopic(topic,\n+                format(\"SELECT \\\"%s-key\\\", col_1, col_2 FROM %s\", topic, toDoubleQuoted(topic)),\n+                format(\"SELECT \\\"%s-key\\\", col_1, col_2, col_3 FROM %s\", topic, toDoubleQuoted(topic)),\n+                true,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducer());\n+    }\n+\n+    @Test\n+    public void testTopicWithRecordNameStrategy()\n+    {\n+        String topic = \"topic-record-name-strategy\";\n+        assertTopic(topic,\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2 FROM \\\"%1$s&message-subject=%2$s\\\"\", topic, RECORD_NAME),\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2, col_3 FROM \\\"%1$s&message-subject=%2$s\\\"\", topic, RECORD_NAME),\n+                true,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducer(ImmutableMap.<String, String>builder()\n+                        .put(VALUE_SUBJECT_NAME_STRATEGY, RecordNameStrategy.class.getName())\n+                        .build()));\n+    }\n+\n+    @Test\n+    public void testTopicWithTopicRecordNameStrategy()\n+    {\n+        String topic = \"topic-topic-record-name-strategy\";\n+        assertTopic(topic,\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2 FROM \\\"%1$s&message-subject=%1$s-%2$s\\\"\", topic, RECORD_NAME),\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2, col_3 FROM \\\"%1$s&message-subject=%1$s-%2$s\\\"\", topic, RECORD_NAME),\n+                true,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducer(ImmutableMap.<String, String>builder()\n+                        .put(VALUE_SUBJECT_NAME_STRATEGY, TopicRecordNameStrategy.class.getName())\n+                        .build()));\n+    }\n+\n+    private void assertTopic(String topicName, String initialQuery, String evolvedQuery, boolean isKeyIncluded, Supplier<KafkaProducer<Long, GenericRecord>> producerSupplier)\n+    {\n+        testingKafkaWithSchemaRegistry.createTopic(topicName);\n+\n+        assertNotExists(topicName);\n+\n+        List<ProducerRecord<Long, GenericRecord>> messages = createMessages(topicName, MESSAGE_COUNT, true);\n+        sendMessages(messages, producerSupplier);\n+\n+        waitUntilTableExists(topicName);\n+        assertCount(topicName, MESSAGE_COUNT);\n+\n+        assertQuery(initialQuery, getExpectedValues(messages, INITIAL_SCHEMA, isKeyIncluded));\n+\n+        List<ProducerRecord<Long, GenericRecord>> newMessages = createMessages(topicName, MESSAGE_COUNT, false);\n+        sendMessages(newMessages, producerSupplier);\n+\n+        List<ProducerRecord<Long, GenericRecord>> allMessages = ImmutableList.<ProducerRecord<Long, GenericRecord>>builder()\n+                .addAll(messages)\n+                .addAll(newMessages)\n+                .build();\n+        assertCount(topicName, allMessages.size());\n+        assertQuery(evolvedQuery, getExpectedValues(allMessages, EVOLVED_SCHEMA, isKeyIncluded));\n+    }\n+\n+    private String getExpectedValues(List<ProducerRecord<Long, GenericRecord>> messages, Schema schema, boolean isKeyIncluded)\n+    {\n+        StringBuilder valuesBuilder = new StringBuilder(\"VALUES \");\n+        ImmutableList.Builder<String> rowsBuilder = ImmutableList.builder();\n+        for (ProducerRecord<Long, GenericRecord> message : messages) {\n+            ImmutableList.Builder<String> columnsBuilder = ImmutableList.builder();\n+\n+            if (isKeyIncluded) {\n+                columnsBuilder.add(String.valueOf(message.key()));\n+            }\n+\n+            addExpectedColumns(schema, message.value(), columnsBuilder);\n+\n+            rowsBuilder.add(format(\"(%s)\", String.join(\", \", columnsBuilder.build())));\n+        }\n+        valuesBuilder.append(String.join(\", \", rowsBuilder.build()));\n+        return valuesBuilder.toString();\n+    }\n+\n+    private void addExpectedColumns(Schema schema, GenericRecord record, ImmutableList.Builder<String> columnsBuilder)\n+    {\n+        for (Schema.Field field : schema.getFields()) {\n+            Object value = record.get(field.name());\n+            if (value == null) {\n+                columnsBuilder.add(\"null\");\n+            }\n+            else if (field.schema().getType().equals(Schema.Type.STRING)) {\n+                columnsBuilder.add(toSingleQuoted(value));\n+            }\n+            else {\n+                columnsBuilder.add(String.valueOf(value));\n+            }\n+        }\n+    }\n+\n+    private void assertNotExists(String tableName)\n+    {\n+        if (schemaExists()) {\n+            assertQueryReturnsEmptyResult(format(\"SHOW TABLES LIKE '%s'\", tableName));\n+        }\n+    }\n+\n+    private void waitUntilTableExists(String tableName)\n+    {\n+        int attempts = 10;\n+        int waitMs = 100;\n+        for (int attempt = 0; !schemaExists() && attempt < attempts; attempt++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 193}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MTE3NjAzOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestKafkaWithConfluentSchemaRegistryMinimalFunctionality.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxOTo0N1rOH9YWjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxOTo0N1rOH9YWjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyNDE3NA==", "bodyText": "failsafe", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534124174", "createdAt": "2020-12-02T12:19:47Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestKafkaWithConfluentSchemaRegistryMinimalFunctionality.java", "diffHunk": "@@ -0,0 +1,278 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.confluent.kafka.serializers.subject.RecordNameStrategy;\n+import io.confluent.kafka.serializers.subject.TopicRecordNameStrategy;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import io.prestosql.testing.kafka.TestingKafkaWithSchemaRegistry;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+import java.util.function.Supplier;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.VALUE_SUBJECT_NAME_STRATEGY;\n+import static java.lang.Math.multiplyExact;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static org.testng.Assert.assertTrue;\n+\n+@Test(singleThreaded = true)\n+public class TestKafkaWithConfluentSchemaRegistryMinimalFunctionality\n+        extends AbstractTestQueryFramework\n+{\n+    private static final String RECORD_NAME = \"test_record\";\n+    private static final int MESSAGE_COUNT = 100;\n+    private static final Schema INITIAL_SCHEMA = SchemaBuilder.record(RECORD_NAME)\n+            .fields()\n+            .name(\"col_1\").type().longType().noDefault()\n+            .name(\"col_2\").type().stringType().noDefault()\n+            .endRecord();\n+    private static final Schema EVOLVED_SCHEMA = SchemaBuilder.record(RECORD_NAME)\n+            .fields()\n+            .name(\"col_1\").type().longType().noDefault()\n+            .name(\"col_2\").type().stringType().noDefault()\n+            .name(\"col_3\").type().optional().doubleType()\n+            .endRecord();\n+\n+    private TestingKafkaWithSchemaRegistry testingKafkaWithSchemaRegistry;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        testingKafkaWithSchemaRegistry = new TestingKafkaWithSchemaRegistry(new TestingKafka());\n+        return KafkaWithConfluentSchemaRegistryQueryRunner.builder(testingKafkaWithSchemaRegistry)\n+                .setExtraKafkaProperties(ImmutableMap.<String, String>builder()\n+                        .put(\"kafka.confluent-subjects-cache-refresh-interval\", \"1ms\")\n+                        .build())\n+                .build();\n+    }\n+\n+    @Test\n+    public void testBasicTopic()\n+    {\n+        String topic = \"topic-basic-MixedCase\";\n+        assertTopic(topic,\n+                format(\"SELECT col_1, col_2 FROM %s\", toDoubleQuoted(topic)),\n+                format(\"SELECT col_1, col_2, col_3 FROM %s\", toDoubleQuoted(topic)),\n+                false,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducerWithLongKeys());\n+    }\n+\n+    @Test\n+    public void testTopicWithKeySubject()\n+    {\n+        String topic = \"topic-key-subject\";\n+        assertTopic(topic,\n+                format(\"SELECT \\\"%s-key\\\", col_1, col_2 FROM %s\", topic, toDoubleQuoted(topic)),\n+                format(\"SELECT \\\"%s-key\\\", col_1, col_2, col_3 FROM %s\", topic, toDoubleQuoted(topic)),\n+                true,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducer());\n+    }\n+\n+    @Test\n+    public void testTopicWithRecordNameStrategy()\n+    {\n+        String topic = \"topic-record-name-strategy\";\n+        assertTopic(topic,\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2 FROM \\\"%1$s&message-subject=%2$s\\\"\", topic, RECORD_NAME),\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2, col_3 FROM \\\"%1$s&message-subject=%2$s\\\"\", topic, RECORD_NAME),\n+                true,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducer(ImmutableMap.<String, String>builder()\n+                        .put(VALUE_SUBJECT_NAME_STRATEGY, RecordNameStrategy.class.getName())\n+                        .build()));\n+    }\n+\n+    @Test\n+    public void testTopicWithTopicRecordNameStrategy()\n+    {\n+        String topic = \"topic-topic-record-name-strategy\";\n+        assertTopic(topic,\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2 FROM \\\"%1$s&message-subject=%1$s-%2$s\\\"\", topic, RECORD_NAME),\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2, col_3 FROM \\\"%1$s&message-subject=%1$s-%2$s\\\"\", topic, RECORD_NAME),\n+                true,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducer(ImmutableMap.<String, String>builder()\n+                        .put(VALUE_SUBJECT_NAME_STRATEGY, TopicRecordNameStrategy.class.getName())\n+                        .build()));\n+    }\n+\n+    private void assertTopic(String topicName, String initialQuery, String evolvedQuery, boolean isKeyIncluded, Supplier<KafkaProducer<Long, GenericRecord>> producerSupplier)\n+    {\n+        testingKafkaWithSchemaRegistry.createTopic(topicName);\n+\n+        assertNotExists(topicName);\n+\n+        List<ProducerRecord<Long, GenericRecord>> messages = createMessages(topicName, MESSAGE_COUNT, true);\n+        sendMessages(messages, producerSupplier);\n+\n+        waitUntilTableExists(topicName);\n+        assertCount(topicName, MESSAGE_COUNT);\n+\n+        assertQuery(initialQuery, getExpectedValues(messages, INITIAL_SCHEMA, isKeyIncluded));\n+\n+        List<ProducerRecord<Long, GenericRecord>> newMessages = createMessages(topicName, MESSAGE_COUNT, false);\n+        sendMessages(newMessages, producerSupplier);\n+\n+        List<ProducerRecord<Long, GenericRecord>> allMessages = ImmutableList.<ProducerRecord<Long, GenericRecord>>builder()\n+                .addAll(messages)\n+                .addAll(newMessages)\n+                .build();\n+        assertCount(topicName, allMessages.size());\n+        assertQuery(evolvedQuery, getExpectedValues(allMessages, EVOLVED_SCHEMA, isKeyIncluded));\n+    }\n+\n+    private String getExpectedValues(List<ProducerRecord<Long, GenericRecord>> messages, Schema schema, boolean isKeyIncluded)\n+    {\n+        StringBuilder valuesBuilder = new StringBuilder(\"VALUES \");\n+        ImmutableList.Builder<String> rowsBuilder = ImmutableList.builder();\n+        for (ProducerRecord<Long, GenericRecord> message : messages) {\n+            ImmutableList.Builder<String> columnsBuilder = ImmutableList.builder();\n+\n+            if (isKeyIncluded) {\n+                columnsBuilder.add(String.valueOf(message.key()));\n+            }\n+\n+            addExpectedColumns(schema, message.value(), columnsBuilder);\n+\n+            rowsBuilder.add(format(\"(%s)\", String.join(\", \", columnsBuilder.build())));\n+        }\n+        valuesBuilder.append(String.join(\", \", rowsBuilder.build()));\n+        return valuesBuilder.toString();\n+    }\n+\n+    private void addExpectedColumns(Schema schema, GenericRecord record, ImmutableList.Builder<String> columnsBuilder)\n+    {\n+        for (Schema.Field field : schema.getFields()) {\n+            Object value = record.get(field.name());\n+            if (value == null) {\n+                columnsBuilder.add(\"null\");\n+            }\n+            else if (field.schema().getType().equals(Schema.Type.STRING)) {\n+                columnsBuilder.add(toSingleQuoted(value));\n+            }\n+            else {\n+                columnsBuilder.add(String.valueOf(value));\n+            }\n+        }\n+    }\n+\n+    private void assertNotExists(String tableName)\n+    {\n+        if (schemaExists()) {\n+            assertQueryReturnsEmptyResult(format(\"SHOW TABLES LIKE '%s'\", tableName));\n+        }\n+    }\n+\n+    private void waitUntilTableExists(String tableName)\n+    {\n+        int attempts = 10;\n+        int waitMs = 100;\n+        for (int attempt = 0; !schemaExists() && attempt < attempts; attempt++) {\n+            try {\n+                Thread.sleep(waitMs);\n+            }\n+            catch (InterruptedException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        assertTrue(schemaExists());\n+        for (int attempt = 0; !tableExists(tableName) && attempt < attempts; attempt++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 202}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzc4NTUyOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentModule.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjozMzoxM1rOIGInmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjozMzoxM1rOIGInmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwMzU3Ng==", "bodyText": "extact static inner class for this module", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r543303576", "createdAt": "2020-12-15T12:33:13Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentModule.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.inject.Binder;\n+import com.google.inject.Provides;\n+import com.google.inject.Scopes;\n+import io.airlift.configuration.AbstractConfigurationAwareModule;\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderModule;\n+import io.prestosql.decoder.RowDecoderFactory;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroDeserializer;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+\n+import javax.inject.Singleton;\n+\n+import static com.google.inject.multibindings.MapBinder.newMapBinder;\n+import static io.airlift.configuration.ConfigBinder.configBinder;\n+\n+public class ConfluentModule\n+        extends AbstractConfigurationAwareModule\n+{\n+    @Override\n+    protected void setup(Binder binder)\n+    {\n+        configBinder(binder).bindConfig(ConfluentSchemaRegistryConfig.class);\n+        install(new DecoderModule(decoderBinder -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzc5Nzc2OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSessionProperties.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjozNjoyM1rOIGIu4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjozNjoyM1rOIGIu4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwNTQ0Mg==", "bodyText": "undo this refactor or separate commit", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r543305442", "createdAt": "2020-12-15T12:36:23Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSessionProperties.java", "diffHunk": "@@ -22,19 +22,23 @@\n import java.util.List;\n \n public final class KafkaSessionProperties\n+        implements SessionPropertiesProvider\n {\n     private static final String TIMESTAMP_UPPER_BOUND_FORCE_PUSH_DOWN_ENABLED = \"timestamp_upper_bound_force_push_down_enabled\";\n     private final List<PropertyMetadata<?>> sessionProperties;\n \n     @Inject\n     public KafkaSessionProperties(KafkaConfig kafkaConfig)\n     {\n-        sessionProperties = ImmutableList.of(PropertyMetadata.booleanProperty(\n-                TIMESTAMP_UPPER_BOUND_FORCE_PUSH_DOWN_ENABLED,\n-                \"Enable or disable timestamp upper bound push down for topic createTime mode\",\n-                kafkaConfig.isTimestampUpperBoundPushDownEnabled(), false));\n+        sessionProperties = ImmutableList.<PropertyMetadata<?>>builder()\n+                .add(PropertyMetadata.booleanProperty(\n+                        TIMESTAMP_UPPER_BOUND_FORCE_PUSH_DOWN_ENABLED,\n+                        \"Enable or disable timestamp upper bound push down for topic createTime mode\",\n+                        kafkaConfig.isTimestampUpperBoundPushDownEnabled(), false))\n+                .build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzgxMzY1OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjo0MDowM1rOIGI4GA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNDo0MTo0NlrOIHInBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwNzgwMA==", "bodyText": "I think user custom data is more important that cached. So let's fallback to cached data if it is missing in schemaTableName. What do you think?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r543307800", "createdAt": "2020-12-15T12:40:03Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,350 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSetMultimap;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.function.UnaryOperator.identity;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    private static final String KEY_SUBJECT = \"key-subject\";\n+\n+    private static final String VALUE_SUBJECT = \"value-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final Supplier<Map<String, String>> subjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::getTopicAndSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+        subjectsSupplier = memoizeWithExpiration(this::getAllSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    private String resolveSubject(String candidate)\n+    {\n+        String subject = subjectsSupplier.get().get(candidate);\n+        checkState(subject != null, \"Subject '%s' not found\", candidate);\n+        return subject;\n+    }\n+\n+    private Map<String, String> getAllSubjects()\n+    {\n+        try {\n+            return schemaRegistryClient.getAllSubjects().stream()\n+                    .collect(toImmutableMap(subject -> subject.toLowerCase(ENGLISH), identity()));\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new RuntimeException(\"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // *Note: that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and value subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&value-subject=<value subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&value-subject=bar\"\n+    private Map<String, TopicAndSubjects> getTopicAndSubjects()\n+    {\n+        ImmutableSetMultimap.Builder<String, String> topicToSubjectsBuilder = ImmutableSetMultimap.builder();\n+        for (String subject : subjectsSupplier.get().values()) {\n+            if (isValidSubject(subject)) {\n+                topicToSubjectsBuilder.put(extractTopicFromSubject(subject), subject);\n+            }\n+        }\n+        ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+        for (Map.Entry<String, Collection<String>> entry : topicToSubjectsBuilder.build().asMap().entrySet()) {\n+            String topic = entry.getKey();\n+            TopicAndSubjects topicAndSubjects = new TopicAndSubjects(\n+                    topic,\n+                    getKeySubjectFromTopic(topic, entry.getValue()),\n+                    getValueSubjectFromTopic(topic, entry.getValue()));\n+            topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+        }\n+        return topicSubjectsCacheBuilder.build();\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        Optional<TopicAndSubjects> topicAndSubjectsFromCache = Optional.ofNullable(topicAndSubjectsSupplier.get().get(tableName));\n+\n+        // Use the topic from cache, if present, in case the topic is mixed case\n+        String topic = topicAndSubjectsFromCache.map(TopicAndSubjects::getTopic).orElse(topicAndSubjects.getTopic());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDM1MjAwNg==", "bodyText": "The cached data will only override user supplied data if the actual topic is mixed case and the custom data equalsIgnoreCase the actual topic, otherwise tests will fail.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544352006", "createdAt": "2020-12-16T14:41:46Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,350 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSetMultimap;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.function.UnaryOperator.identity;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    private static final String KEY_SUBJECT = \"key-subject\";\n+\n+    private static final String VALUE_SUBJECT = \"value-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final Supplier<Map<String, String>> subjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::getTopicAndSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+        subjectsSupplier = memoizeWithExpiration(this::getAllSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    private String resolveSubject(String candidate)\n+    {\n+        String subject = subjectsSupplier.get().get(candidate);\n+        checkState(subject != null, \"Subject '%s' not found\", candidate);\n+        return subject;\n+    }\n+\n+    private Map<String, String> getAllSubjects()\n+    {\n+        try {\n+            return schemaRegistryClient.getAllSubjects().stream()\n+                    .collect(toImmutableMap(subject -> subject.toLowerCase(ENGLISH), identity()));\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new RuntimeException(\"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // *Note: that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and value subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&value-subject=<value subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&value-subject=bar\"\n+    private Map<String, TopicAndSubjects> getTopicAndSubjects()\n+    {\n+        ImmutableSetMultimap.Builder<String, String> topicToSubjectsBuilder = ImmutableSetMultimap.builder();\n+        for (String subject : subjectsSupplier.get().values()) {\n+            if (isValidSubject(subject)) {\n+                topicToSubjectsBuilder.put(extractTopicFromSubject(subject), subject);\n+            }\n+        }\n+        ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+        for (Map.Entry<String, Collection<String>> entry : topicToSubjectsBuilder.build().asMap().entrySet()) {\n+            String topic = entry.getKey();\n+            TopicAndSubjects topicAndSubjects = new TopicAndSubjects(\n+                    topic,\n+                    getKeySubjectFromTopic(topic, entry.getValue()),\n+                    getValueSubjectFromTopic(topic, entry.getValue()));\n+            topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+        }\n+        return topicSubjectsCacheBuilder.build();\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        Optional<TopicAndSubjects> topicAndSubjectsFromCache = Optional.ofNullable(topicAndSubjectsSupplier.get().get(tableName));\n+\n+        // Use the topic from cache, if present, in case the topic is mixed case\n+        String topic = topicAndSubjectsFromCache.map(TopicAndSubjects::getTopic).orElse(topicAndSubjects.getTopic());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwNzgwMA=="}, "originalCommit": null, "originalPosition": 170}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzgyNTA2OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjo0MjozN1rOIGI-ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjo0MjozN1rOIGI-ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwOTQ3NQ==", "bodyText": "DELIMITER and SEPARATOR are synonyms. Maybe simply inline them? Otherwise I suggest KEY_VALUE_PAIR_DELIMITER and KEY_VALUE_DELIMITER", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r543309475", "createdAt": "2020-12-15T12:42:37Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,350 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSetMultimap;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.function.UnaryOperator.identity;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzgyNzgyOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSessionProperties.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjo0MzoxM1rOIGJAHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjo0MzoxM1rOIGJAHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwOTg1Mw==", "bodyText": "static import", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r543309853", "createdAt": "2020-12-15T12:43:13Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSessionProperties.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.kafka.SessionPropertiesProvider;\n+import io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.session.PropertyMetadata;\n+\n+import javax.inject.Inject;\n+\n+import java.util.List;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentSessionProperties\n+        implements SessionPropertiesProvider\n+{\n+    public static final String EMPTY_FIELD_STRATEGY = \"empty_field_strategy\";\n+\n+    private final List<PropertyMetadata<?>> sessionProperties;\n+\n+    @Inject\n+    public ConfluentSessionProperties(ConfluentSchemaRegistryConfig config)\n+    {\n+        requireNonNull(config, \"config is null\");\n+        sessionProperties = ImmutableList.<PropertyMetadata<?>>builder()\n+                .add(PropertyMetadata.enumProperty(EMPTY_FIELD_STRATEGY,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzgzNTg1OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjo0NTowMVrOIGJE6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwNjo0NDoxOFrOIIVAtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMxMTA4Mw==", "bodyText": "Instead of these methods you can inject Consumer<DistributedQueryRunner> objects that could be called in certain points.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r543311083", "createdAt": "2020-12-15T12:45:01Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -115,36 +97,14 @@ public Builder setExtension(Module extension)\n         }\n \n         @Override\n-        public DistributedQueryRunner build()\n+        public void preInit(DistributedQueryRunner queryRunner)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDM1NTgxNg==", "bodyText": "The consumers would also depend on every other member of the builder, i.e. properties, tables, etc.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544355816", "createdAt": "2020-12-16T14:46:23Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -115,36 +97,14 @@ public Builder setExtension(Module extension)\n         }\n \n         @Override\n-        public DistributedQueryRunner build()\n+        public void preInit(DistributedQueryRunner queryRunner)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMxMTA4Mw=="}, "originalCommit": null, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU0NTE3Ng==", "bodyText": "I could inject a BiConsumer of KafkaQueryRunnerBuilder, DistributedQueryRunner, lmk.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r545545176", "createdAt": "2020-12-18T03:10:18Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -115,36 +97,14 @@ public Builder setExtension(Module extension)\n         }\n \n         @Override\n-        public DistributedQueryRunner build()\n+        public void preInit(DistributedQueryRunner queryRunner)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMxMTA4Mw=="}, "originalCommit": null, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTYwMzc2Nw==", "bodyText": "Let's leave it as is. BiConsumer is not much better.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r545603767", "createdAt": "2020-12-18T06:44:18Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -115,36 +97,14 @@ public Builder setExtension(Module extension)\n         }\n \n         @Override\n-        public DistributedQueryRunner build()\n+        public void preInit(DistributedQueryRunner queryRunner)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMxMTA4Mw=="}, "originalCommit": null, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNDE1Mjc1OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/TableDescriptionSupplier.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNjo1MDo1OVrOIHnG8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwODowNToxNVrOIIW37g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1MTY5OA==", "bodyText": "Can we capture this as a separate commit and apply Confluent schema registry on top of it ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544851698", "createdAt": "2020-12-17T06:50:59Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/TableDescriptionSupplier.java", "diffHunk": "@@ -24,5 +25,5 @@\n {\n     Set<SchemaTableName> listTables();\n \n-    Optional<KafkaTopicDescription> getTopicDescription(SchemaTableName schemaTableName);\n+    Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTYzNDI4Ng==", "bodyText": "I did this originally but there was a suggestion from @kokosing in the previous version of this pull request to squash it. lmk, I can always extract it.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r545634286", "createdAt": "2020-12-18T08:05:15Z", "author": {"login": "elonazoulay"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/TableDescriptionSupplier.java", "diffHunk": "@@ -24,5 +25,5 @@\n {\n     Set<SchemaTableName> listTables();\n \n-    Optional<KafkaTopicDescription> getTopicDescription(SchemaTableName schemaTableName);\n+    Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1MTY5OA=="}, "originalCommit": null, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNDE1Mzc5OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNjo1MTozMlrOIHnHog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNjo1MTozMlrOIHnHog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1MTg3NA==", "bodyText": "Any validation for Min/Max value ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544851874", "createdAt": "2020-12-17T06:51:32Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryConfig.java", "diffHunk": "@@ -70,4 +73,17 @@ public ConfluentSchemaRegistryConfig setEmptyFieldStrategy(EmptyFieldStrategy em\n         this.emptyFieldStrategy = emptyFieldStrategy;\n         return this;\n     }\n+\n+    public Duration getConfluentSubjectsCacheRefreshInterval()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNDE1Njg1OnYy", "diffSide": "RIGHT", "path": "presto-kafka/pom.xml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNjo1Mjo0NlrOIHnJVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNjo1Mjo0NlrOIHnJVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1MjMxMA==", "bodyText": "Do we use it outside test scope ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544852310", "createdAt": "2020-12-17T06:52:46Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/pom.xml", "diffHunk": "@@ -92,6 +92,11 @@\n             <artifactId>joda-time</artifactId>\n         </dependency>\n \n+        <dependency>\n+            <groupId>net.jodah</groupId>\n+            <artifactId>failsafe</artifactId>\n+        </dependency>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNDE2MTIxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/SessionPropertiesProvider.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNjo1NDozMFrOIHnLvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNjo1NDozMFrOIHnLvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1MjkyNQ==", "bodyText": "How about having them as a separate commit", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544852925", "createdAt": "2020-12-17T06:54:30Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/SessionPropertiesProvider.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.session.PropertyMetadata;\n+\n+import java.util.List;\n+\n+public interface SessionPropertiesProvider", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNDE3MDc5OnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSessionProperties.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNjo1ODowNVrOIHnQ6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNjo1ODowNVrOIHnQ6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1NDI0OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            .add(enumProperty(EMPTY_FIELD_STRATEGY,\n          \n          \n            \n                                    \"Strategy for handling struct types with no fields: IGNORE (default), FAIL, and ADD_DUMMY to add a boolean field named 'dummy'\",\n          \n          \n            \n                                    EmptyFieldStrategy.class, config.getEmptyFieldStrategy(), false))\n          \n          \n            \n                            .add(enumProperty(\n          \n          \n            \n                                    EMPTY_FIELD_STRATEGY,\n          \n          \n            \n                                    \"Strategy for handling struct types with no fields: IGNORE (default), FAIL, and ADD_DUMMY to add a boolean field named 'dummy'\",\n          \n          \n            \n                                    EmptyFieldStrategy.class, \n          \n          \n            \n                                    config.getEmptyFieldStrategy(), \n          \n          \n            \n                                    false))", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544854248", "createdAt": "2020-12-17T06:58:05Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSessionProperties.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.kafka.SessionPropertiesProvider;\n+import io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.session.PropertyMetadata;\n+\n+import javax.inject.Inject;\n+\n+import java.util.List;\n+\n+import static io.prestosql.spi.session.PropertyMetadata.enumProperty;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentSessionProperties\n+        implements SessionPropertiesProvider\n+{\n+    public static final String EMPTY_FIELD_STRATEGY = \"empty_field_strategy\";\n+\n+    private final List<PropertyMetadata<?>> sessionProperties;\n+\n+    @Inject\n+    public ConfluentSessionProperties(ConfluentSchemaRegistryConfig config)\n+    {\n+        requireNonNull(config, \"config is null\");\n+        sessionProperties = ImmutableList.<PropertyMetadata<?>>builder()\n+                .add(enumProperty(EMPTY_FIELD_STRATEGY,\n+                        \"Strategy for handling struct types with no fields: IGNORE (default), FAIL, and ADD_DUMMY to add a boolean field named 'dummy'\",\n+                        EmptyFieldStrategy.class, config.getEmptyFieldStrategy(), false))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNDY2MjUxOnYy", "diffSide": "RIGHT", "path": "pom.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOTowOToyNFrOIHrl-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwNjo1NToyM1rOIIVQQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkyNTE3Ng==", "bodyText": "Let's move this comment to presto-kafka/pom.xml. Same for belows.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544925176", "createdAt": "2020-12-17T09:09:24Z", "author": {"login": "kokosing"}, "path": "pom.xml", "diffHunk": "@@ -1077,12 +1086,39 @@\n                 <version>4.5.1</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-avro-serializer</artifactId>\n+                <version>${dep.confluent.version}</version>\n+                <exclusions>\n+                    <exclusion>\n+                        <groupId>org.apache.kafka</groupId>\n+                        <artifactId>kafka-clients</artifactId>\n+                    </exclusion>\n+                </exclusions>\n+            </dependency>\n+\n             <dependency>\n                 <groupId>io.jsonwebtoken</groupId>\n                 <artifactId>jjwt</artifactId>\n                 <version>0.9.0</version>\n             </dependency>\n \n+\n+            <!-- io.confluent:kafka-avro-serializer uses multiple versions of this transitive dependency -->", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTYwNzc0Ng==", "bodyText": "The dependencies are transitive, they are not included directly anywhere.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r545607746", "createdAt": "2020-12-18T06:55:23Z", "author": {"login": "elonazoulay"}, "path": "pom.xml", "diffHunk": "@@ -1077,12 +1086,39 @@\n                 <version>4.5.1</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-avro-serializer</artifactId>\n+                <version>${dep.confluent.version}</version>\n+                <exclusions>\n+                    <exclusion>\n+                        <groupId>org.apache.kafka</groupId>\n+                        <artifactId>kafka-clients</artifactId>\n+                    </exclusion>\n+                </exclusions>\n+            </dependency>\n+\n             <dependency>\n                 <groupId>io.jsonwebtoken</groupId>\n                 <artifactId>jjwt</artifactId>\n                 <version>0.9.0</version>\n             </dependency>\n \n+\n+            <!-- io.confluent:kafka-avro-serializer uses multiple versions of this transitive dependency -->", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkyNTE3Ng=="}, "originalCommit": null, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNDc0MTY4OnYy", "diffSide": "RIGHT", "path": "presto-plugin-toolkit/src/test/java/io/prestosql/plugin/base/util/TestJsonUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOToyNzoxNVrOIHsUgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOToyNzoxNVrOIHsUgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkzNzA5MA==", "bodyText": "Please squash this change with the commit that broke this test.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544937090", "createdAt": "2020-12-17T09:27:15Z", "author": {"login": "kokosing"}, "path": "presto-plugin-toolkit/src/test/java/io/prestosql/plugin/base/util/TestJsonUtils.java", "diffHunk": "@@ -34,7 +34,7 @@\n     public static class TestObject\n     {\n         @JsonProperty\n-        private TestEnum testEnum;\n+        public TestEnum testEnum;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyOTEwNjgxOnYy", "diffSide": "RIGHT", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentModule.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwNTozMjoyN1rOIITtpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwNTozMjoyN1rOIITtpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4MjUwMw==", "bodyText": "Can we provide a way to inject custom List to the SchemaRegistry", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r545582503", "createdAt": "2020-12-18T05:32:27Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentModule.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.inject.Binder;\n+import com.google.inject.Module;\n+import com.google.inject.Provides;\n+import com.google.inject.Scopes;\n+import io.airlift.configuration.AbstractConfigurationAwareModule;\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderModule;\n+import io.prestosql.decoder.RowDecoderFactory;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroDeserializer;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.SessionPropertiesProvider;\n+import io.prestosql.plugin.kafka.schema.ContentSchemaReader;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+\n+import javax.inject.Singleton;\n+\n+import static com.google.inject.multibindings.MapBinder.newMapBinder;\n+import static com.google.inject.multibindings.Multibinder.newSetBinder;\n+import static io.airlift.configuration.ConfigBinder.configBinder;\n+\n+public class ConfluentModule\n+        extends AbstractConfigurationAwareModule\n+{\n+    @Override\n+    protected void setup(Binder binder)\n+    {\n+        configBinder(binder).bindConfig(ConfluentSchemaRegistryConfig.class);\n+        install(new DecoderModule(new ConfluentAvroModule()));\n+        binder.bind(ContentSchemaReader.class).to(AvroConfluentContentSchemaReader.class).in(Scopes.SINGLETON);\n+        newSetBinder(binder, SessionPropertiesProvider.class).addBinding().to(ConfluentSessionProperties.class).in(Scopes.SINGLETON);\n+        binder.bind(TableDescriptionSupplier.class).toProvider(ConfluentSchemaRegistryTableDescriptionSupplier.Factory.class).in(Scopes.SINGLETON);\n+    }\n+\n+    @Provides\n+    @Singleton\n+    public SchemaRegistryClient getSchemaRegistryClient(ConfluentSchemaRegistryConfig confluentConfig)\n+    {\n+        return new CachedSchemaRegistryClient(confluentConfig.getConfluentSchemaRegistryUrl(), confluentConfig.getConfluentSchemaRegistryClientCacheSize());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4707, "cost": 1, "resetAt": "2021-11-12T12:57:47Z"}}}