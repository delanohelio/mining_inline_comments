{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAxNjM3ODQz", "number": 5528, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxNzozMTo1NFrOEsuEQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1MjowNlrOEtYiNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MzI3NTUzOnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxNzozMTo1NFrOHgHaXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxNzozNzoxMlrOHgHj_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQzNzkxOA==", "bodyText": "its not clear what belongs to TestReader and what to TestParquetReader (same package)", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503437918", "createdAt": "2020-10-12T17:31:54Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ0MDM4MA==", "bodyText": "Couldn't come up with a better name, and TestParquetReader is taken (it's the full blown test with all the combinations of readers and writers). This is a targeted test for one type. I could rename it to TestTimestampReader for now -- I'll have to revamp the TestParquetReader for the new parquet reader I'm working on, anyway, as the current one is too rigid to be able to implement the tests I need.", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503440380", "createdAt": "2020-10-12T17:37:12Z", "author": {"login": "martint"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQzNzkxOA=="}, "originalCommit": null, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NTM5MDM3OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetTester.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwODoxOTo0MlrOHga6_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNTo0Mjo1NVrOHgtUjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzc1NzU2NA==", "bodyText": "make it package-private?", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503757564", "createdAt": "2020-10-13T08:19:42Z", "author": {"login": "sopel39"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetTester.java", "diffHunk": "@@ -569,7 +569,7 @@ private static FileFormat getFileFormat()\n         return OPTIMIZED ? FileFormat.PRESTO_PARQUET : FileFormat.HIVE_PARQUET;\n     }\n \n-    private static void writeParquetColumn(\n+    public static void writeParquetColumn(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA1OTAyMA==", "bodyText": "It's now part of the public API of this class, so package private doesn't seem appropriate.", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r504059020", "createdAt": "2020-10-13T15:42:55Z", "author": {"login": "martint"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetTester.java", "diffHunk": "@@ -569,7 +569,7 @@ private static FileFormat getFileFormat()\n         return OPTIMIZED ? FileFormat.PRESTO_PARQUET : FileFormat.HIVE_PARQUET;\n     }\n \n-    private static void writeParquetColumn(\n+    public static void writeParquetColumn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzc1NzU2NA=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NTcxNDE2OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTozNDoyMFrOHgeAMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTozNDoyMFrOHgeAMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwODA0OQ==", "bodyText": "None of these is needed", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503808049", "createdAt": "2020-10-13T09:34:20Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NTcxNzMyOnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTozNTowNVrOHgeCGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTozNTowNVrOHgeCGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwODUzOA==", "bodyText": "seems unrelated?", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503808538", "createdAt": "2020-10-13T09:35:05Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NTcyMDg4OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTozNTo0OFrOHgeELA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTozNTo0OFrOHgeELA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwOTA2OA==", "bodyText": "assert firstPage.getPositionCount() > 0", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503809068", "createdAt": "2020-10-13T09:35:48Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+                for (int i = 0; i < firstPage.getPositionCount(); i++) {\n+                    expectedValues.next();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NTcyMzcwOnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTozNjozMVrOHgeGEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNTo1Mzo0NlrOHgtz8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwOTU1Mg==", "bodyText": "We should document what null return value means. \"This method is allowed to return null\" is not sufficient.", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503809552", "createdAt": "2020-10-13T09:36:31Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+                for (int i = 0; i < firstPage.getPositionCount(); i++) {\n+                    expectedValues.next();\n+                }\n+\n+                while (!pageSource.isFinished()) {\n+                    Page page = pageSource.getNextPage();\n+                    if (page == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA2NzA1Nw==", "bodyText": "Agreed, but unrelated to this PR", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r504067057", "createdAt": "2020-10-13T15:53:46Z", "author": {"login": "martint"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+                for (int i = 0; i < firstPage.getPositionCount(); i++) {\n+                    expectedValues.next();\n+                }\n+\n+                while (!pageSource.isFinished()) {\n+                    Page page = pageSource.getNextPage();\n+                    if (page == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwOTU1Mg=="}, "originalCommit": null, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NTcyNzk4OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTozNzozM1rOHgeIqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTozNzozM1rOHgeIqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgxMDIxNg==", "bodyText": "add a message, othewise exception coming from here will be hard to understand", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503810216", "createdAt": "2020-10-13T09:37:33Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+                for (int i = 0; i < firstPage.getPositionCount(); i++) {\n+                    expectedValues.next();\n+                }\n+\n+                while (!pageSource.isFinished()) {\n+                    Page page = pageSource.getNextPage();\n+                    if (page == null) {\n+                        continue;\n+                    }\n+                    Block block = page.getBlock(0);\n+\n+                    for (int i = 0; i < block.getPositionCount(); i++) {\n+                        assertEquals(TIMESTAMP_MILLIS.getObjectValue(session, block, i), expectedValues.next());\n+                    }\n+                }\n+            }\n+\n+            assertFalse(expectedValues.hasNext());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NTczMTA0OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTozODoxOFrOHgeKrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNTo1MzoyM1rOHgtyqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgxMDczMg==", "bodyText": "Is negative value any special?\nis the range length (2000) special? Can it be eg 20?", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503810732", "createdAt": "2020-10-13T09:38:18Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA2NjczMQ==", "bodyText": "No, nothing special about it, other than that it has to be large enough to produce multiple pages. I added some assertions to ensure the assumptions of the test hold.", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r504066731", "createdAt": "2020-10-13T15:53:23Z", "author": {"login": "martint"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgxMDczMg=="}, "originalCommit": null, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDIyODM5OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestTimestampReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1MDo1MlrOHhI3lQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1MDo1MlrOHhI3lQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxMDM1Nw==", "bodyText": "the shorter, intermediate version that you had\nassertTrue(firstPage.getPositionCount() > 0, \"Expected first page to have at least 1 row\");\n\nproduces equally informative exception message (assuming page.positionCount is non-negative)", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r504510357", "createdAt": "2020-10-14T08:50:52Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestTimestampReader.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestTimestampReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig());\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+\n+                assertThat(firstPage.getPositionCount())\n+                        .withFailMessage(\"Expected first page to have at least 1 row\")\n+                        .isGreaterThan(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDIzMzUwOnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestTimestampReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1MjowNlrOHhI6tg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1NDo0OFrOHhJB5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxMTE1OA==", "bodyText": "TestTimestampColumnReader ?", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r504511158", "createdAt": "2020-10-14T08:52:06Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestTimestampReader.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestTimestampReader", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxMjk5OQ==", "bodyText": "or, if  you reserve that name for unit tests of that class, than maybe TestTimestamp?", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r504512999", "createdAt": "2020-10-14T08:54:48Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestTimestampReader.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestTimestampReader", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxMTE1OA=="}, "originalCommit": null, "originalPosition": 48}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4686, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}