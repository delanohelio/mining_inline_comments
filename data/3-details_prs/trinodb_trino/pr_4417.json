{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3NTA3OTg0", "number": 4417, "title": "Add Kafka raw encoder", "bodyText": "Add RawRowEncoder and RawRowEncoderFactory\nAdd test case in io.prestosql.plugin.kafka.TestKafkaIntegrationSmokeTest#testRoundTripAllFormats", "createdAt": "2020-07-10T15:36:32Z", "url": "https://github.com/trinodb/trino/pull/4417", "merged": true, "mergeCommit": {"oid": "8472b6273a75f0ae04c497e2602a276f7a4c17c7"}, "closed": true, "closedAt": "2020-07-22T11:01:57Z", "author": {"login": "charlesjmorgan"}, "timelineItems": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABczl0MZgBqjM1MzQ0ODcxNzQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc10qJXABqjM1NTgzNzA3NDc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8069f747393f980e6514d46df688619124b6da48", "author": {"user": {"login": "charlesjmorgan", "name": "Charles Morgan"}}, "url": "https://github.com/trinodb/trino/commit/8069f747393f980e6514d46df688619124b6da48", "committedDate": "2020-07-10T15:06:20Z", "message": "Add raw encoder and raw roundtrip test"}, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NTM5ODU4", "url": "https://github.com/trinodb/trino/pull/4417#pullrequestreview-446539858", "createdAt": "2020-07-10T16:33:18Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQxNjozMzoxOFrOGv94_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQxNjo0MjowNlrOGv-Jvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MDI2OQ==", "bodyText": "nit: possibly catch NumberFormatException and wrap in exception with clear message. It can throw if value provided does not fit in int.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r452950269", "createdAt": "2020-07-10T16:33:18Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+    private int currentBufferPosition;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(columnHandle, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+        this.currentBufferPosition = 0;\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String mapping = Optional.ofNullable(columnHandle.getMapping()).orElse(\"0\");\n+        Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping);\n+        if (!mappingMatcher.matches()) {\n+            throw new IllegalArgumentException(format(\"Invalid mapping format '%s' for column '%s'\", mapping, columnHandle.getName()));\n+        }\n+        int start = parseInt(mappingMatcher.group(1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MDY0Mw==", "bodyText": "can we make it @VisibleForTesting and add unit test for it?", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r452950643", "createdAt": "2020-07-10T16:34:09Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+    private int currentBufferPosition;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(columnHandle, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+        this.currentBufferPosition = 0;\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(EncoderColumnHandle columnHandle, FieldType fieldType)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDE2NA==", "bodyText": "it does not seem to handle case where we are dealing with varchar and end is not explicitly specified.\nWould lenght be 1 then?", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r452954164", "createdAt": "2020-07-10T16:41:15Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+    private int currentBufferPosition;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(columnHandle, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+        this.currentBufferPosition = 0;\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String mapping = Optional.ofNullable(columnHandle.getMapping()).orElse(\"0\");\n+        Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping);\n+        if (!mappingMatcher.matches()) {\n+            throw new IllegalArgumentException(format(\"Invalid mapping format '%s' for column '%s'\", mapping, columnHandle.getName()));\n+        }\n+        int start = parseInt(mappingMatcher.group(1));\n+        OptionalInt end;\n+        if (mappingMatcher.group(2) != null) {\n+            end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+        }\n+        else {\n+            if (!isVarcharType(columnHandle.getType())) {\n+                end = OptionalInt.of(start + fieldType.getSize());\n+            }\n+            else {\n+                end = OptionalInt.empty();\n+            }\n+        }\n+\n+        checkArgument(start >= 0, \"Start offset %s for column '%s' must be greater or equal 0\", start, columnHandle.getName());\n+        end.ifPresent(endValue -> {\n+            checkArgument(endValue >= 0, \"End offset %s for column '%s' must be greater or equal 0\", endValue, columnHandle.getName());\n+            checkArgument(endValue > start, \"End offset %s for column '%s' must greater than start offset\", endValue, columnHandle.getName());\n+        });\n+\n+        int length = end.isPresent() ? end.getAsInt() - start : fieldType.getSize();\n+\n+        if (!isVarcharType(columnHandle.getType())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDU1OQ==", "bodyText": "this can throw value.getBytes() may return shorter array than valueLengths[currentColumnIndex]", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r452954559", "createdAt": "2020-07-10T16:42:06Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+    private int currentBufferPosition;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(columnHandle, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+        this.currentBufferPosition = 0;\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String mapping = Optional.ofNullable(columnHandle.getMapping()).orElse(\"0\");\n+        Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping);\n+        if (!mappingMatcher.matches()) {\n+            throw new IllegalArgumentException(format(\"Invalid mapping format '%s' for column '%s'\", mapping, columnHandle.getName()));\n+        }\n+        int start = parseInt(mappingMatcher.group(1));\n+        OptionalInt end;\n+        if (mappingMatcher.group(2) != null) {\n+            end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+        }\n+        else {\n+            if (!isVarcharType(columnHandle.getType())) {\n+                end = OptionalInt.of(start + fieldType.getSize());\n+            }\n+            else {\n+                end = OptionalInt.empty();\n+            }\n+        }\n+\n+        checkArgument(start >= 0, \"Start offset %s for column '%s' must be greater or equal 0\", start, columnHandle.getName());\n+        end.ifPresent(endValue -> {\n+            checkArgument(endValue >= 0, \"End offset %s for column '%s' must be greater or equal 0\", endValue, columnHandle.getName());\n+            checkArgument(endValue > start, \"End offset %s for column '%s' must greater than start offset\", endValue, columnHandle.getName());\n+        });\n+\n+        int length = end.isPresent() ? end.getAsInt() - start : fieldType.getSize();\n+\n+        if (!isVarcharType(columnHandle.getType())) {\n+            checkArgument(!end.isPresent() || end.getAsInt() - start == length,\n+                    \"Bytes mapping for column '%s' does not match dataFormat '%s'; expected %s bytes but got %s\",\n+                    columnHandle.getName(),\n+                    length,\n+                    end.getAsInt() - start);\n+        }\n+\n+        return length;\n+    }\n+\n+    private static boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        buffer.put(new byte[valueLengths[currentColumnIndex]], currentBufferPosition, valueLengths[currentColumnIndex]);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        buffer.putLong(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        buffer.putInt(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        buffer.putShort(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        buffer.put(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        buffer.putDouble(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        buffer.putFloat(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        buffer.put(currentBufferPosition, (byte) (value ? 1 : 0));\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        buffer.put(value.getBytes(StandardCharsets.UTF_8), currentBufferPosition, valueLengths[currentColumnIndex]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 261}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NjUzMTA0", "url": "https://github.com/trinodb/trino/pull/4417#pullrequestreview-446653104", "createdAt": "2020-07-10T19:30:53Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQxOTozMDo1M1rOGwDVwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQxOTo1NzoyNlrOGwEAcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAzOTU1Mw==", "bodyText": "This check is unnecessary, since it's covered by the one below.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453039553", "createdAt": "2020-07-10T19:30:53Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+    private int currentBufferPosition;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(columnHandle, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+        this.currentBufferPosition = 0;\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String mapping = Optional.ofNullable(columnHandle.getMapping()).orElse(\"0\");\n+        Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping);\n+        if (!mappingMatcher.matches()) {\n+            throw new IllegalArgumentException(format(\"Invalid mapping format '%s' for column '%s'\", mapping, columnHandle.getName()));\n+        }\n+        int start = parseInt(mappingMatcher.group(1));\n+        OptionalInt end;\n+        if (mappingMatcher.group(2) != null) {\n+            end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+        }\n+        else {\n+            if (!isVarcharType(columnHandle.getType())) {\n+                end = OptionalInt.of(start + fieldType.getSize());\n+            }\n+            else {\n+                end = OptionalInt.empty();\n+            }\n+        }\n+\n+        checkArgument(start >= 0, \"Start offset %s for column '%s' must be greater or equal 0\", start, columnHandle.getName());\n+        end.ifPresent(endValue -> {\n+            checkArgument(endValue >= 0, \"End offset %s for column '%s' must be greater or equal 0\", endValue, columnHandle.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA0NjA3MA==", "bodyText": "The documentation we have for the raw mapping states that when the end position is missing,\n\nWhen VARCHAR value is decoded all bytes from start position till the end of the message will be used.\n\nI have not checked the code.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453046070", "createdAt": "2020-07-10T19:46:49Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+    private int currentBufferPosition;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(columnHandle, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+        this.currentBufferPosition = 0;\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String mapping = Optional.ofNullable(columnHandle.getMapping()).orElse(\"0\");\n+        Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping);\n+        if (!mappingMatcher.matches()) {\n+            throw new IllegalArgumentException(format(\"Invalid mapping format '%s' for column '%s'\", mapping, columnHandle.getName()));\n+        }\n+        int start = parseInt(mappingMatcher.group(1));\n+        OptionalInt end;\n+        if (mappingMatcher.group(2) != null) {\n+            end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+        }\n+        else {\n+            if (!isVarcharType(columnHandle.getType())) {\n+                end = OptionalInt.of(start + fieldType.getSize());\n+            }\n+            else {\n+                end = OptionalInt.empty();\n+            }\n+        }\n+\n+        checkArgument(start >= 0, \"Start offset %s for column '%s' must be greater or equal 0\", start, columnHandle.getName());\n+        end.ifPresent(endValue -> {\n+            checkArgument(endValue >= 0, \"End offset %s for column '%s' must be greater or equal 0\", endValue, columnHandle.getName());\n+            checkArgument(endValue > start, \"End offset %s for column '%s' must greater than start offset\", endValue, columnHandle.getName());\n+        });\n+\n+        int length = end.isPresent() ? end.getAsInt() - start : fieldType.getSize();\n+\n+        if (!isVarcharType(columnHandle.getType())) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDE2NA=="}, "originalCommit": null, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA1MDQ4MQ==", "bodyText": "I recall from previous troubleshooting that the encoding for VARCHAR is kind of iffy.  Basically, this mapping represents a fixed length string.  Writing a shorter string will lead the consumer to read garbage, because we have no marker for the end of the string.  Also, when writing a longer string we truncate and end up with data loss - again not an ideal situation.\nI think we need to throw an exception if the lengths do not match and perhaps update the documentation to state the unpleasant reality.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453050481", "createdAt": "2020-07-10T19:57:26Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+    private int currentBufferPosition;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(columnHandle, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+        this.currentBufferPosition = 0;\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String mapping = Optional.ofNullable(columnHandle.getMapping()).orElse(\"0\");\n+        Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping);\n+        if (!mappingMatcher.matches()) {\n+            throw new IllegalArgumentException(format(\"Invalid mapping format '%s' for column '%s'\", mapping, columnHandle.getName()));\n+        }\n+        int start = parseInt(mappingMatcher.group(1));\n+        OptionalInt end;\n+        if (mappingMatcher.group(2) != null) {\n+            end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+        }\n+        else {\n+            if (!isVarcharType(columnHandle.getType())) {\n+                end = OptionalInt.of(start + fieldType.getSize());\n+            }\n+            else {\n+                end = OptionalInt.empty();\n+            }\n+        }\n+\n+        checkArgument(start >= 0, \"Start offset %s for column '%s' must be greater or equal 0\", start, columnHandle.getName());\n+        end.ifPresent(endValue -> {\n+            checkArgument(endValue >= 0, \"End offset %s for column '%s' must be greater or equal 0\", endValue, columnHandle.getName());\n+            checkArgument(endValue > start, \"End offset %s for column '%s' must greater than start offset\", endValue, columnHandle.getName());\n+        });\n+\n+        int length = end.isPresent() ? end.getAsInt() - start : fieldType.getSize();\n+\n+        if (!isVarcharType(columnHandle.getType())) {\n+            checkArgument(!end.isPresent() || end.getAsInt() - start == length,\n+                    \"Bytes mapping for column '%s' does not match dataFormat '%s'; expected %s bytes but got %s\",\n+                    columnHandle.getName(),\n+                    length,\n+                    end.getAsInt() - start);\n+        }\n+\n+        return length;\n+    }\n+\n+    private static boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        buffer.put(new byte[valueLengths[currentColumnIndex]], currentBufferPosition, valueLengths[currentColumnIndex]);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        buffer.putLong(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        buffer.putInt(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        buffer.putShort(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        buffer.put(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        buffer.putDouble(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        buffer.putFloat(currentBufferPosition, value);\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        buffer.put(currentBufferPosition, (byte) (value ? 1 : 0));\n+        currentBufferPosition += valueLengths[currentColumnIndex];\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        buffer.put(value.getBytes(StandardCharsets.UTF_8), currentBufferPosition, valueLengths[currentColumnIndex]);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NDU1OQ=="}, "originalCommit": null, "originalPosition": 261}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NDc0ODEy", "url": "https://github.com/trinodb/trino/pull/4417#pullrequestreview-447474812", "createdAt": "2020-07-13T17:56:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNzo1NjozOVrOGwzf9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNzo1NzoyMFrOGwzhuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzgyODU5Ng==", "bodyText": "This is over-complex for me :)\nI would suggest to simplify it by going doing parsing mulitpass:\n\npass1: go over all fields and parse mapping to tuple: Optionalnt start, OptionalInt end; just look at textual definition.\npass2: for each field if end == empty(), set it to start+field.length(); for varchar columns leave it as empty().\npass3: for each field verify that end-start matches field.length; fail otherwise\npass4; verify that no field mappings overlap each other; treat end==empty() as till the end of buffer; fail otherwise\n\nInstead having parseMapping just have a few loops over all column handles one by one, each doing simple thing. I have a suspicion it will be much easier to follow.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453828596", "createdAt": "2020-07-13T17:56:39Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(i, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(int index, FieldType fieldType)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzgyOTA0OQ==", "bodyText": "using valueLengths internally while building it based on result of this function is antipattern and make it much harder to follow.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r453829049", "createdAt": "2020-07-13T17:57:20Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final int[] valueLengths;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        valueLengths = new int[this.columnHandles.size()];\n+        int capacity = 0;\n+        EncoderColumnHandle columnHandle;\n+\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnHandle = this.columnHandles.get(i);\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+\n+            FieldType fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, fieldType);\n+\n+            valueLengths[i] = parseMapping(i, fieldType);\n+            capacity += valueLengths[i];\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(capacity);\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        if (!Arrays.asList(allowedFieldTypes).contains(declaredFieldType)) {\n+            throw new IllegalArgumentException(format(\n+                    \"Wrong dataFormat '%s' specified for column '%s'; %s type implies use of %s\",\n+                    declaredFieldType.name(),\n+                    columnName,\n+                    columnType.getDisplayName(),\n+                    Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+    }\n+\n+    private int parseMapping(int index, FieldType fieldType)\n+    {\n+        EncoderColumnHandle columnHandle = columnHandles.get(index);\n+        String mapping = Optional.ofNullable(columnHandle.getMapping()).orElse(\"0\");\n+\n+        int start = 0;\n+        for (int i = 0; i < index; i++) {\n+            start += valueLengths[i];", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 163}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ4ODY5NDE2", "url": "https://github.com/trinodb/trino/pull/4417#pullrequestreview-448869416", "createdAt": "2020-07-15T11:37:21Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMTozNzoyMVrOGx6JIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMjoxNzozNlrOGx7aTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk4NjAxOA==", "bodyText": "This one non-final field is a glitch.\nCan we rename this class to ColumnMappingBuilder. And create separate ColumnMapping class with just getters and all fields final.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454986018", "createdAt": "2020-07-15T11:37:21Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap\n+        int position = 0;\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            checkArgument(columnMapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    columnMapping.getStart(),\n+                    columnMapping.getName(),\n+                    position));\n+            checkArgument(columnMapping.getEnd().isPresent() && columnMapping.getEnd().getAsInt() > columnMapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    columnMapping.getEnd().getAsInt(),\n+                    columnMapping.getName(),\n+                    columnMapping.getStart()));\n+            position += columnMapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private OptionalInt end;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk4NzU0NA==", "bodyText": "Parametrize with EncoderColumnHandler instead of index. Then you should be able to make this class static. Some helper function it calls will need to be made static too, but it should not be a problem.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454987544", "createdAt": "2020-07-15T11:40:24Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap\n+        int position = 0;\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            checkArgument(columnMapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    columnMapping.getStart(),\n+                    columnMapping.getName(),\n+                    position));\n+            checkArgument(columnMapping.getEnd().isPresent() && columnMapping.getEnd().getAsInt() > columnMapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    columnMapping.getEnd().getAsInt(),\n+                    columnMapping.getName(),\n+                    columnMapping.getStart()));\n+            position += columnMapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private OptionalInt end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(int index)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk4OTQwOQ==", "bodyText": "I would compute end to be set here and just call out to columnMapping.setEnd(end). Let's keep logic here and make ColumnMapping (or ColumnMappingBuilder if you follow my advice below) dumb.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454989409", "createdAt": "2020-07-15T11:44:12Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5MDI3OA==", "bodyText": "make it else if { and drop one indentation level.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454990278", "createdAt": "2020-07-15T11:45:55Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5MDc4Mg==", "bodyText": "you can make it\n            if (columnMapping.getEnd().isPresent()) {\n                continue;\n            }\n\nto drop one indentation level", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454990782", "createdAt": "2020-07-15T11:47:02Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5MTgwMQ==", "bodyText": "combine conditions with &&", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454991801", "createdAt": "2020-07-15T11:48:58Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5MjE3OA==", "bodyText": "name variable handle for brevity?", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454992178", "createdAt": "2020-07-15T11:49:43Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5MjMyMw==", "bodyText": "Name variable mapping for brevity? (here and below)", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454992323", "createdAt": "2020-07-15T11:50:00Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5NDkzMQ==", "bodyText": "May be personal taste but linear search hurts me.\nWhat about\nif (columnMapping.getName().equals(getLast(columnHandles).getName()))\n?", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454994931", "createdAt": "2020-07-15T11:54:55Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5NjY2OQ==", "bodyText": "This actually checks something stronger. We are not allowing gaps between fields too. I think this is fine though.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454996669", "createdAt": "2020-07-15T11:58:12Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5ODU0OA==", "bodyText": "I do not believe you are calling this method ever with end non-set. Add precondition that end.isPresent() and drop if.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r454998548", "createdAt": "2020-07-15T12:01:56Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap\n+        int position = 0;\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            checkArgument(columnMapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    columnMapping.getStart(),\n+                    columnMapping.getName(),\n+                    position));\n+            checkArgument(columnMapping.getEnd().isPresent() && columnMapping.getEnd().getAsInt() > columnMapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    columnMapping.getEnd().getAsInt(),\n+                    columnMapping.getName(),\n+                    columnMapping.getStart()));\n+            position += columnMapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private OptionalInt end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(int index)\n+        {\n+            EncoderColumnHandle columnHandle = columnHandles.get(index);\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else {\n+                    this.start = parseInt(mappingMatcher.group(1));\n+                    if (mappingMatcher.group(2) != null) {\n+                        try {\n+                            this.end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+                        }\n+                        catch (NumberFormatException e) {\n+                            throw new IllegalArgumentException(format(\"The end index mapping value for column '%s' is too large, must be 4 bytes\", this.name), e);\n+                        }\n+                    }\n+                    else {\n+                        this.end = OptionalInt.empty();\n+                    }\n+                }\n+            }\n+            else {\n+                throw new IllegalArgumentException(format(\"No mapping defined for column '%s'\", this.name));\n+            }\n+        }\n+\n+        public String getName()\n+        {\n+            return name;\n+        }\n+\n+        public Type getType()\n+        {\n+            return type;\n+        }\n+\n+        public int getStart()\n+        {\n+            return start;\n+        }\n+\n+        public OptionalInt getEnd()\n+        {\n+            return end;\n+        }\n+\n+        public FieldType getFieldType()\n+        {\n+            return fieldType;\n+        }\n+\n+        public int getLength()\n+        {\n+            if (end.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 219}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAwMDE3Nw==", "bodyText": "I would initialize this.end with OptionalInt.empty() and drop else sth like:\n                    this.start = parseInt(mappingMatcher.group(1));\n                    this.end = OptionalInt.empty();\n                    if (mappingMatcher.group(2) != null) {\n...", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455000177", "createdAt": "2020-07-15T12:05:06Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap\n+        int position = 0;\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            checkArgument(columnMapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    columnMapping.getStart(),\n+                    columnMapping.getName(),\n+                    position));\n+            checkArgument(columnMapping.getEnd().isPresent() && columnMapping.getEnd().getAsInt() > columnMapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    columnMapping.getEnd().getAsInt(),\n+                    columnMapping.getName(),\n+                    columnMapping.getStart()));\n+            position += columnMapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private OptionalInt end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(int index)\n+        {\n+            EncoderColumnHandle columnHandle = columnHandles.get(index);\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else {\n+                    this.start = parseInt(mappingMatcher.group(1));\n+                    if (mappingMatcher.group(2) != null) {\n+                        try {\n+                            this.end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+                        }\n+                        catch (NumberFormatException e) {\n+                            throw new IllegalArgumentException(format(\"The end index mapping value for column '%s' is too large, must be 4 bytes\", this.name), e);\n+                        }\n+                    }\n+                    else {\n+                        this.end = OptionalInt.empty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAwMTg1Nw==", "bodyText": "wrap with try\nMaybe extract method parseOffset() which does exception handling and call it like:\nparseOffset(mappingMatcher.group(1), \"start\", this.name)\nparseOffset(mappingMatcher.group(2), \"end\", this.name)", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455001857", "createdAt": "2020-07-15T12:08:25Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap\n+        int position = 0;\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            checkArgument(columnMapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    columnMapping.getStart(),\n+                    columnMapping.getName(),\n+                    position));\n+            checkArgument(columnMapping.getEnd().isPresent() && columnMapping.getEnd().getAsInt() > columnMapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    columnMapping.getEnd().getAsInt(),\n+                    columnMapping.getName(),\n+                    columnMapping.getStart()));\n+            position += columnMapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private OptionalInt end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(int index)\n+        {\n+            EncoderColumnHandle columnHandle = columnHandles.get(index);\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else {\n+                    this.start = parseInt(mappingMatcher.group(1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAwNDY2NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    for (int i = 0; i < columnMappings.get(currentColumnIndex).getLength(); i++) {\n          \n          \n            \n                        buffer.put(valueBytes[i]);\n          \n          \n            \n                    }\n          \n          \n            \n                    buffer.put(valueBytes, 0, valueBytes.length)", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455004664", "createdAt": "2020-07-15T12:13:45Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap\n+        int position = 0;\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            checkArgument(columnMapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    columnMapping.getStart(),\n+                    columnMapping.getName(),\n+                    position));\n+            checkArgument(columnMapping.getEnd().isPresent() && columnMapping.getEnd().getAsInt() > columnMapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    columnMapping.getEnd().getAsInt(),\n+                    columnMapping.getName(),\n+                    columnMapping.getStart()));\n+            position += columnMapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private OptionalInt end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(int index)\n+        {\n+            EncoderColumnHandle columnHandle = columnHandles.get(index);\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else {\n+                    this.start = parseInt(mappingMatcher.group(1));\n+                    if (mappingMatcher.group(2) != null) {\n+                        try {\n+                            this.end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+                        }\n+                        catch (NumberFormatException e) {\n+                            throw new IllegalArgumentException(format(\"The end index mapping value for column '%s' is too large, must be 4 bytes\", this.name), e);\n+                        }\n+                    }\n+                    else {\n+                        this.end = OptionalInt.empty();\n+                    }\n+                }\n+            }\n+            else {\n+                throw new IllegalArgumentException(format(\"No mapping defined for column '%s'\", this.name));\n+            }\n+        }\n+\n+        public String getName()\n+        {\n+            return name;\n+        }\n+\n+        public Type getType()\n+        {\n+            return type;\n+        }\n+\n+        public int getStart()\n+        {\n+            return start;\n+        }\n+\n+        public OptionalInt getEnd()\n+        {\n+            return end;\n+        }\n+\n+        public FieldType getFieldType()\n+        {\n+            return fieldType;\n+        }\n+\n+        public int getLength()\n+        {\n+            if (end.isPresent()) {\n+                return end.getAsInt() - start;\n+            }\n+            else {\n+                return fieldType.getSize();\n+            }\n+        }\n+\n+        public void setEnd()\n+        {\n+            this.end = OptionalInt.of(start + fieldType.getSize());\n+        }\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        checkArgument(Arrays.asList(allowedFieldTypes).contains(declaredFieldType),\n+                format(\"Wrong dataformat '%s' specified for column '%s'; %s type implies use of %s\",\n+                        declaredFieldType.name(),\n+                        columnName,\n+                        columnType,\n+                        Joiner.on(\"/\").join(allowedFieldTypes)));\n+    }\n+\n+    private static boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        for (int i = 0; i < columnMappings.get(currentColumnIndex).getLength(); i++) {\n+            buffer.put((byte) 0);\n+        }\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        buffer.putLong(value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        buffer.putInt(value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        buffer.putShort(value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        buffer.put(value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        buffer.putDouble(value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        buffer.putFloat(value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        buffer.put((byte) (value ? 1 : 0));\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        byte[] valueBytes = value.getBytes(StandardCharsets.UTF_8);\n+        checkArgument(valueBytes.length == columnMappings.get(currentColumnIndex).getLength(), format(\n+                \"length of message '%s' for column '%s' does not equal expected length '%s'\",\n+                valueBytes.length,\n+                columnHandles.get(currentColumnIndex),\n+                columnMappings.get(currentColumnIndex).getLength()));\n+        for (int i = 0; i < columnMappings.get(currentColumnIndex).getLength(); i++) {\n+            buffer.put(valueBytes[i]);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 348}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAwNTY4Ng==", "bodyText": "This is a strong constraint. Maybe allowing for shorter strings with some user-defined padding character would give better ux?\n@findepi @aalbu WDYT? (can be relaxed as a followup)", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455005686", "createdAt": "2020-07-15T12:15:32Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.OptionalInt;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final ImmutableList<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType().getDisplayName(), columnHandle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingBuilder = ImmutableList.builder();\n+        for (int i = 0; i < this.columnHandles.size(); i++) {\n+            columnMappingBuilder.add(new ColumnMapping(i));\n+        }\n+        this.columnMappings = columnMappingBuilder.build();\n+\n+        // make sure an end value is set for each column,\n+        // without one it won't be possible to allocate the right size ByteBuffer\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getEnd().isEmpty()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    columnMapping.setEnd();\n+                }\n+                else {\n+                    if (this.columnMappings.indexOf(columnMapping) == this.columnMappings.size() - 1) {\n+                        columnMapping.setEnd();\n+                    }\n+                    else {\n+                        throw new IndexOutOfBoundsException(format(\n+                                \"No end mapping defined for column '%s' of type '%s'\",\n+                                columnMapping.getName(),\n+                                columnMapping.getType().getDisplayName()));\n+                    }\n+                }\n+            }\n+        }\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            if (columnMapping.getLength() != columnMapping.getFieldType().getSize()) {\n+                if (!isVarcharType(columnMapping.getType())) {\n+                    throw new IndexOutOfBoundsException(format(\n+                            \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                            columnMapping.getLength(),\n+                            columnMapping.getFieldType().getSize(),\n+                            columnMapping.getName()));\n+                }\n+            }\n+        }\n+\n+        // check that column mappings don't overlap\n+        int position = 0;\n+        for (ColumnMapping columnMapping : this.columnMappings) {\n+            checkArgument(columnMapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    columnMapping.getStart(),\n+                    columnMapping.getName(),\n+                    position));\n+            checkArgument(columnMapping.getEnd().isPresent() && columnMapping.getEnd().getAsInt() > columnMapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    columnMapping.getEnd().getAsInt(),\n+                    columnMapping.getName(),\n+                    columnMapping.getStart()));\n+            position += columnMapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private OptionalInt end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(int index)\n+        {\n+            EncoderColumnHandle columnHandle = columnHandles.get(index);\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle);\n+            checkFieldType(columnHandle, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else {\n+                    this.start = parseInt(mappingMatcher.group(1));\n+                    if (mappingMatcher.group(2) != null) {\n+                        try {\n+                            this.end = OptionalInt.of(parseInt(mappingMatcher.group(2)));\n+                        }\n+                        catch (NumberFormatException e) {\n+                            throw new IllegalArgumentException(format(\"The end index mapping value for column '%s' is too large, must be 4 bytes\", this.name), e);\n+                        }\n+                    }\n+                    else {\n+                        this.end = OptionalInt.empty();\n+                    }\n+                }\n+            }\n+            else {\n+                throw new IllegalArgumentException(format(\"No mapping defined for column '%s'\", this.name));\n+            }\n+        }\n+\n+        public String getName()\n+        {\n+            return name;\n+        }\n+\n+        public Type getType()\n+        {\n+            return type;\n+        }\n+\n+        public int getStart()\n+        {\n+            return start;\n+        }\n+\n+        public OptionalInt getEnd()\n+        {\n+            return end;\n+        }\n+\n+        public FieldType getFieldType()\n+        {\n+            return fieldType;\n+        }\n+\n+        public int getLength()\n+        {\n+            if (end.isPresent()) {\n+                return end.getAsInt() - start;\n+            }\n+            else {\n+                return fieldType.getSize();\n+            }\n+        }\n+\n+        public void setEnd()\n+        {\n+            this.end = OptionalInt.of(start + fieldType.getSize());\n+        }\n+    }\n+\n+    private FieldType parseFieldType(EncoderColumnHandle columnHandle)\n+    {\n+        try {\n+            return Optional.ofNullable(columnHandle.getDataFormat())\n+                    .map(dataFormat -> FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH)))\n+                    .orElse(FieldType.BYTE);\n+        }\n+        catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName()));\n+        }\n+    }\n+\n+    private void checkFieldType(EncoderColumnHandle columnHandle, FieldType fieldType)\n+    {\n+        String columnName = columnHandle.getName();\n+        Type columnType = columnHandle.getType();\n+        if (columnType == BIGINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == INTEGER) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+        }\n+        else if (columnType == SMALLINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+        }\n+        else if (columnType == TINYINT) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+        else if (columnType == BOOLEAN) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+        }\n+        else if (columnType == DOUBLE) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+        }\n+        else if (isVarcharType(columnType)) {\n+            checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+        }\n+    }\n+\n+    private void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+    {\n+        checkArgument(Arrays.asList(allowedFieldTypes).contains(declaredFieldType),\n+                format(\"Wrong dataformat '%s' specified for column '%s'; %s type implies use of %s\",\n+                        declaredFieldType.name(),\n+                        columnName,\n+                        columnType,\n+                        Joiner.on(\"/\").join(allowedFieldTypes)));\n+    }\n+\n+    private static boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        for (int i = 0; i < columnMappings.get(currentColumnIndex).getLength(); i++) {\n+            buffer.put((byte) 0);\n+        }\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        buffer.putLong(value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        buffer.putInt(value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        buffer.putShort(value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        buffer.put(value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        buffer.putDouble(value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        buffer.putFloat(value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        buffer.put((byte) (value ? 1 : 0));\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        byte[] valueBytes = value.getBytes(StandardCharsets.UTF_8);\n+        checkArgument(valueBytes.length == columnMappings.get(currentColumnIndex).getLength(), format(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 341}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAwNjc5OA==", "bodyText": "I know that internally RoundTripTestCase now sends and receives two rows. But IMO it would be nicer if each row would be different, not same one repeated twice. Can we improve on that?", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455006798", "createdAt": "2020-07-15T12:17:36Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -149,6 +149,10 @@ public void testRoundTripAllFormats(RoundTripTestCase testCase)\n                         \"all_datatypes_csv\",\n                         ImmutableList.of(\"f_bigint\", \"f_int\", \"f_smallint\", \"f_tinyint\", \"f_double\", \"f_boolean\", \"f_varchar\"),\n                         ImmutableList.of(100000, 1000, 100, 10, 1000.001, true, \"'test'\")))\n+                .add(new RoundTripTestCase(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ5MTQ1NDg3", "url": "https://github.com/trinodb/trino/pull/4417#pullrequestreview-449145487", "createdAt": "2020-07-15T17:03:45Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxNzowMzo0NVrOGyHKjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxNzoyNToyNVrOGyIAOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE5OTM3NQ==", "bodyText": "A functional approach would make this shorter and more readable.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455199375", "createdAt": "2020-07-15T17:03:45Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingsBuilder = ImmutableList.builder();\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            columnMappingsBuilder.add(new ColumnMapping(handle));\n+        }\n+        this.columnMappings = columnMappingsBuilder.build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIwMDgwMA==", "bodyText": "else is unnecessary.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455200800", "createdAt": "2020-07-15T17:06:09Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingsBuilder = ImmutableList.builder();\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            columnMappingsBuilder.add(new ColumnMapping(handle));\n+        }\n+        this.columnMappings = columnMappingsBuilder.build();\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            if (mapping.getLength() != mapping.getFieldType().getSize() && !isVarcharType(mapping.getType())) {\n+                throw new IndexOutOfBoundsException(format(\n+                        \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                        mapping.getLength(),\n+                        mapping.getFieldType().getSize(),\n+                        mapping.getName()));\n+            }\n+        }\n+\n+        // check that column mappings don't overlap and that there are no gaps\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkArgument(mapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    mapping.getStart(),\n+                    mapping.getName(),\n+                    position));\n+            checkArgument(mapping.getEnd() > mapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    mapping.getEnd(),\n+                    mapping.getName(),\n+                    mapping.getStart()));\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private static class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private final int end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(EncoderColumnHandle columnHandle)\n+        {\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle.getDataFormat(), this.name);\n+            checkFieldType(this.name, this.type, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else if (mappingMatcher.group(2) != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIwNzEyOQ==", "bodyText": "It doesn't look like the method throws NumberFormatException.  I also don't get the exception message.  But it doesn't look like you need to catch here.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455207129", "createdAt": "2020-07-15T17:16:20Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingsBuilder = ImmutableList.builder();\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            columnMappingsBuilder.add(new ColumnMapping(handle));\n+        }\n+        this.columnMappings = columnMappingsBuilder.build();\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            if (mapping.getLength() != mapping.getFieldType().getSize() && !isVarcharType(mapping.getType())) {\n+                throw new IndexOutOfBoundsException(format(\n+                        \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                        mapping.getLength(),\n+                        mapping.getFieldType().getSize(),\n+                        mapping.getName()));\n+            }\n+        }\n+\n+        // check that column mappings don't overlap and that there are no gaps\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkArgument(mapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    mapping.getStart(),\n+                    mapping.getName(),\n+                    position));\n+            checkArgument(mapping.getEnd() > mapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    mapping.getEnd(),\n+                    mapping.getName(),\n+                    mapping.getStart()));\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private static class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private final int end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(EncoderColumnHandle columnHandle)\n+        {\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle.getDataFormat(), this.name);\n+            checkFieldType(this.name, this.type, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else if (mappingMatcher.group(2) != null) {\n+                    this.start = parseOffset(mappingMatcher.group(1), \"start\", this.name);\n+                    try {\n+                        this.end = parseOffset(mappingMatcher.group(2), \"end\", this.name);\n+                    }\n+                    catch (NumberFormatException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIxMzExNQ==", "bodyText": "This is another missing part in the spec, right?  It doesn't say how nulls should be encoded.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455213115", "createdAt": "2020-07-15T17:25:25Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        // parse column mappings from column handles\n+        ImmutableList.Builder<ColumnMapping> columnMappingsBuilder = ImmutableList.builder();\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            columnMappingsBuilder.add(new ColumnMapping(handle));\n+        }\n+        this.columnMappings = columnMappingsBuilder.build();\n+\n+        // make sure the actual/expected lengths match\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            if (mapping.getLength() != mapping.getFieldType().getSize() && !isVarcharType(mapping.getType())) {\n+                throw new IndexOutOfBoundsException(format(\n+                        \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                        mapping.getLength(),\n+                        mapping.getFieldType().getSize(),\n+                        mapping.getName()));\n+            }\n+        }\n+\n+        // check that column mappings don't overlap and that there are no gaps\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkArgument(mapping.getStart() == position, format(\n+                    \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                    mapping.getStart(),\n+                    mapping.getName(),\n+                    position));\n+            checkArgument(mapping.getEnd() > mapping.getStart(), format(\n+                    \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                    mapping.getEnd(),\n+                    mapping.getName(),\n+                    mapping.getStart()));\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    private static class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final int start;\n+        private final int end;\n+        private final FieldType fieldType;\n+\n+        public ColumnMapping(EncoderColumnHandle columnHandle)\n+        {\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+            this.fieldType = parseFieldType(columnHandle.getDataFormat(), this.name);\n+            checkFieldType(this.name, this.type, this.fieldType);\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+                else if (mappingMatcher.group(2) != null) {\n+                    this.start = parseOffset(mappingMatcher.group(1), \"start\", this.name);\n+                    try {\n+                        this.end = parseOffset(mappingMatcher.group(2), \"end\", this.name);\n+                    }\n+                    catch (NumberFormatException e) {\n+                        throw new IllegalArgumentException(format(\"The end index mapping value for column '%s' is too large, must be 4 bytes\", this.name), e);\n+                    }\n+                }\n+                else {\n+                    this.start = parseOffset(mappingMatcher.group(1), \"start\", this.name);\n+                    this.end = this.start + this.fieldType.getSize();\n+                }\n+            }\n+            else {\n+                throw new IllegalArgumentException(format(\"No mapping defined for column '%s'\", this.name));\n+            }\n+        }\n+\n+        private static int parseOffset(String group, String offsetName, String columnName)\n+        {\n+            try {\n+                return parseInt(group);\n+            }\n+            catch (NumberFormatException e) {\n+                throw new IllegalArgumentException(format(\"Unable to parse int '%s' for column '%s'\", offsetName, columnName), e);\n+            }\n+        }\n+\n+        private static FieldType parseFieldType(String dataFormat, String columnName)\n+        {\n+            try {\n+                if (!dataFormat.equals(\"\")) {\n+                    return FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH));\n+                }\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", dataFormat, columnName));\n+            }\n+            return FieldType.BYTE;\n+        }\n+\n+        private static void checkFieldType(String columnName, Type columnType, FieldType fieldType)\n+        {\n+            if (columnType == BIGINT) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+            }\n+            else if (columnType == INTEGER) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+            }\n+            else if (columnType == SMALLINT) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+            }\n+            else if (columnType == TINYINT) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+            }\n+            else if (columnType == BOOLEAN) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+            }\n+            else if (columnType == DOUBLE) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+            }\n+            else if (isVarcharType(columnType)) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+            }\n+        }\n+\n+        private static void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+        {\n+            checkArgument(Arrays.asList(allowedFieldTypes).contains(declaredFieldType),\n+                    format(\"Wrong dataformat '%s' specified for column '%s'; %s type implies use of %s\",\n+                            declaredFieldType.name(),\n+                            columnName,\n+                            columnType,\n+                            Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+\n+        public String getName()\n+        {\n+            return name;\n+        }\n+\n+        public Type getType()\n+        {\n+            return type;\n+        }\n+\n+        public int getStart()\n+        {\n+            return start;\n+        }\n+\n+        public int getEnd()\n+        {\n+            return end;\n+        }\n+\n+        public FieldType getFieldType()\n+        {\n+            return fieldType;\n+        }\n+\n+        public int getLength()\n+        {\n+            return end - start;\n+        }\n+    }\n+\n+    private static boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        for (int i = 0; i < columnMappings.get(currentColumnIndex).getLength(); i++) {\n+            buffer.put((byte) 0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 264}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ5NjQ4MDc4", "url": "https://github.com/trinodb/trino/pull/4417#pullrequestreview-449648078", "createdAt": "2020-07-16T08:50:59Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQwODo1MDo1OVrOGyhXZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQwOTowMzoyOVrOGyh1pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYyODY0NQ==", "bodyText": "can not hardcode it to 2 values, just generate the VALUES (..), (..), (...) for that many rows as present in input list?", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455628645", "createdAt": "2020-07-16T08:50:59Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -125,10 +125,10 @@ public void testRoundTripAllFormats(RoundTripTestCase testCase)\n     {\n         assertUpdate(\"INSERT into write_test.\" + testCase.getTableName() +\n                 \" (\" + testCase.getFieldNames() + \")\" +\n-                \" VALUES (\" + testCase.getFieldValues() + \"), (\" + testCase.getFieldValues() + \")\", 2);\n+                \" VALUES (\" + testCase.getRowValues(0) + \"), (\" + testCase.getRowValues(1) + \")\", 2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYzMTIzOQ==", "bodyText": "nit: could be done with stream...collect(toImmutableList)...", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455631239", "createdAt": "2020-07-16T08:55:07Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        this.columnMappings = buildMappings(this.columnHandles);\n+\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingLengths(mapping);\n+        }\n+\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingOffsets(mapping, position);\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    // parse column mappings from column handles\n+    private static List<ColumnMapping> buildMappings(List<EncoderColumnHandle> columnHandles)\n+    {\n+        ImmutableList.Builder<ColumnMapping> columnMappingsBuilder = ImmutableList.builder();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYzMjQ5OQ==", "bodyText": "rename to checkMappingMatchesTypeSize and drop comment", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455632499", "createdAt": "2020-07-16T08:57:07Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        this.columnMappings = buildMappings(this.columnHandles);\n+\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingLengths(mapping);\n+        }\n+\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingOffsets(mapping, position);\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    // parse column mappings from column handles\n+    private static List<ColumnMapping> buildMappings(List<EncoderColumnHandle> columnHandles)\n+    {\n+        ImmutableList.Builder<ColumnMapping> columnMappingsBuilder = ImmutableList.builder();\n+        for (EncoderColumnHandle handle : columnHandles) {\n+            columnMappingsBuilder.add(new ColumnMapping(handle));\n+        }\n+        return columnMappingsBuilder.build();\n+    }\n+\n+    // make sure the actual/expected lengths match\n+    private static void checkMappingLengths(ColumnMapping mapping)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYzNjM5MQ==", "bodyText": "first clear() and then array(). I am supprised it works. I think we should just mark intention to clear here and do actual clearing on next append.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r455636391", "createdAt": "2020-07-16T09:03:29Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        this.columnMappings = buildMappings(this.columnHandles);\n+\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingLengths(mapping);\n+        }\n+\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingOffsets(mapping, position);\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    // parse column mappings from column handles\n+    private static List<ColumnMapping> buildMappings(List<EncoderColumnHandle> columnHandles)\n+    {\n+        ImmutableList.Builder<ColumnMapping> columnMappingsBuilder = ImmutableList.builder();\n+        for (EncoderColumnHandle handle : columnHandles) {\n+            columnMappingsBuilder.add(new ColumnMapping(handle));\n+        }\n+        return columnMappingsBuilder.build();\n+    }\n+\n+    // make sure the actual/expected lengths match\n+    private static void checkMappingLengths(ColumnMapping mapping)\n+    {\n+        if (mapping.getLength() != mapping.getFieldType().getSize() && !isVarcharType(mapping.getType())) {\n+            throw new IndexOutOfBoundsException(format(\n+                    \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                    mapping.getLength(),\n+                    mapping.getFieldType().getSize(),\n+                    mapping.getName()));\n+        }\n+    }\n+\n+    // check that column mappings don't overlap and that there are no gaps\n+    private static void checkMappingOffsets(ColumnMapping mapping, int expectedPosition)\n+    {\n+        checkArgument(mapping.getStart() == expectedPosition, format(\n+                \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                mapping.getStart(),\n+                mapping.getName(),\n+                expectedPosition));\n+        checkArgument(mapping.getEnd() > mapping.getStart(), format(\n+                \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                mapping.getEnd(),\n+                mapping.getName(),\n+                mapping.getStart()));\n+    }\n+\n+    private static class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final FieldType fieldType;\n+        private final int start;\n+        private final int end;\n+\n+        public ColumnMapping(EncoderColumnHandle columnHandle)\n+        {\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+\n+            this.fieldType = parseFieldType(columnHandle.getDataFormat(), this.name);\n+            checkFieldType(this.name, this.type, this.fieldType);\n+\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+\n+                if (mappingMatcher.group(2) != null) {\n+                    this.start = parseOffset(mappingMatcher.group(1), \"start\", this.name);\n+                    this.end = parseOffset(mappingMatcher.group(2), \"end\", this.name);\n+                }\n+                else {\n+                    this.start = parseOffset(mappingMatcher.group(1), \"start\", this.name);\n+                    this.end = this.start + this.fieldType.getSize();\n+                }\n+            }\n+            else {\n+                throw new IllegalArgumentException(format(\"No mapping defined for column '%s'\", this.name));\n+            }\n+        }\n+\n+        private static int parseOffset(String group, String offsetName, String columnName)\n+        {\n+            try {\n+                return parseInt(group);\n+            }\n+            catch (NumberFormatException e) {\n+                throw new IllegalArgumentException(format(\"Unable to parse '%s' offset for column '%s'\", offsetName, columnName), e);\n+            }\n+        }\n+\n+        private static FieldType parseFieldType(String dataFormat, String columnName)\n+        {\n+            try {\n+                if (!dataFormat.equals(\"\")) {\n+                    return FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH));\n+                }\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", dataFormat, columnName));\n+            }\n+            return FieldType.BYTE;\n+        }\n+\n+        private static void checkFieldType(String columnName, Type columnType, FieldType fieldType)\n+        {\n+            if (columnType == BIGINT) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+            }\n+            else if (columnType == INTEGER) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT);\n+            }\n+            else if (columnType == SMALLINT) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT);\n+            }\n+            else if (columnType == TINYINT) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+            }\n+            else if (columnType == BOOLEAN) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE, FieldType.SHORT, FieldType.INT, FieldType.LONG);\n+            }\n+            else if (columnType == DOUBLE) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.DOUBLE, FieldType.FLOAT);\n+            }\n+            else if (isVarcharType(columnType)) {\n+                checkFieldTypeOneOf(fieldType, columnName, columnType, FieldType.BYTE);\n+            }\n+        }\n+\n+        private static void checkFieldTypeOneOf(FieldType declaredFieldType, String columnName, Type columnType, FieldType... allowedFieldTypes)\n+        {\n+            checkArgument(Arrays.asList(allowedFieldTypes).contains(declaredFieldType),\n+                    format(\"Wrong dataformat '%s' specified for column '%s'; %s type implies use of %s\",\n+                            declaredFieldType.name(),\n+                            columnName,\n+                            columnType,\n+                            Joiner.on(\"/\").join(allowedFieldTypes)));\n+        }\n+\n+        public String getName()\n+        {\n+            return name;\n+        }\n+\n+        public Type getType()\n+        {\n+            return type;\n+        }\n+\n+        public int getStart()\n+        {\n+            return start;\n+        }\n+\n+        public int getEnd()\n+        {\n+            return end;\n+        }\n+\n+        public FieldType getFieldType()\n+        {\n+            return fieldType;\n+        }\n+\n+        public int getLength()\n+        {\n+            return end - start;\n+        }\n+    }\n+\n+    private static boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        for (int i = 0; i < columnMappings.get(currentColumnIndex).getLength(); i++) {\n+            buffer.put((byte) 0);\n+        }\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        buffer.putLong(value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        buffer.putInt(value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        buffer.putShort(value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        buffer.put(value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        buffer.putDouble(value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        buffer.putFloat(value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        buffer.put((byte) (value ? 1 : 0));\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        byte[] valueBytes = value.getBytes(StandardCharsets.UTF_8);\n+        checkArgument(valueBytes.length == columnMappings.get(currentColumnIndex).getLength(), format(\n+                \"length '%s' of message '%s' for column '%s' does not equal expected length '%s'\",\n+                valueBytes.length,\n+                value,\n+                columnHandles.get(currentColumnIndex).getName(),\n+                columnMappings.get(currentColumnIndex).getLength()));\n+        buffer.put(valueBytes, 0, valueBytes.length);\n+    }\n+\n+    @Override\n+    protected void appendByteBuffer(ByteBuffer value)\n+    {\n+        byte[] valueBytes = value.array();\n+        checkArgument(valueBytes.length == columnMappings.get(currentColumnIndex).getLength(), format(\n+                \"length '%s' of message for column '%s' does not equal expected length '%s'\",\n+                valueBytes.length,\n+                columnHandles.get(currentColumnIndex).getName(),\n+                columnMappings.get(currentColumnIndex).getLength()));\n+        buffer.put(valueBytes, 0, valueBytes.length);\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        // make sure entire row has been updated with new values\n+        checkArgument(currentColumnIndex == columnHandles.size(), format(\"Missing %d columns\", columnHandles.size() - currentColumnIndex + 1));\n+\n+        resetColumnIndex(); // reset currentColumnIndex to prepare for next row\n+        buffer.clear(); // reset buffer position to prepare for next row", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 354}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwMzE4ODA0", "url": "https://github.com/trinodb/trino/pull/4417#pullrequestreview-450318804", "createdAt": "2020-07-17T01:32:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMTozMjozOVrOGzCV1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMTo0Mzo0MFrOGzChGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE2ODkxNw==", "bodyText": "You can inline this.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r456168917", "createdAt": "2020-07-17T01:32:39Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        this.columnMappings = buildMappings(this.columnHandles);\n+\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingMatchesTypeSize(mapping);\n+        }\n+\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingOffsets(mapping, position);\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    // parse column mappings from column handles\n+    private static List<ColumnMapping> buildMappings(List<EncoderColumnHandle> columnHandles)\n+    {\n+        return columnHandles.stream().map(ColumnMapping::new).collect(toImmutableList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE3MDg2Mw==", "bodyText": "checkMappingOffsets() is validating (in part) that the encoder itself is constructed properly.  The constructor of ColumnMapping is responsible for making sure that those objects are properly constructed.", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r456170863", "createdAt": "2020-07-17T01:39:59Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        this.columnMappings = buildMappings(this.columnHandles);\n+\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingMatchesTypeSize(mapping);\n+        }\n+\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingOffsets(mapping, position);\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    // parse column mappings from column handles\n+    private static List<ColumnMapping> buildMappings(List<EncoderColumnHandle> columnHandles)\n+    {\n+        return columnHandles.stream().map(ColumnMapping::new).collect(toImmutableList());\n+    }\n+\n+    private static void checkMappingMatchesTypeSize(ColumnMapping mapping)\n+    {\n+        if (mapping.getLength() != mapping.getFieldType().getSize() && !isVarcharType(mapping.getType())) {\n+            throw new IndexOutOfBoundsException(format(\n+                    \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                    mapping.getLength(),\n+                    mapping.getFieldType().getSize(),\n+                    mapping.getName()));\n+        }\n+    }\n+\n+    // check that column mappings don't overlap and that there are no gaps\n+    private static void checkMappingOffsets(ColumnMapping mapping, int expectedPosition)\n+    {\n+        checkArgument(mapping.getStart() == expectedPosition, format(\n+                \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                mapping.getStart(),\n+                mapping.getName(),\n+                expectedPosition));\n+        checkArgument(mapping.getEnd() > mapping.getStart(), format(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE3MTgwMw==", "bodyText": "Can you move this statement right after the if?", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r456171803", "createdAt": "2020-07-17T01:43:40Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/raw/RawRowEncoder.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.raw;\n+\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.SmallintType.SMALLINT;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+\n+public class RawRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private enum FieldType\n+    {\n+        BYTE(Byte.SIZE),\n+        SHORT(Short.SIZE),\n+        INT(Integer.SIZE),\n+        LONG(Long.SIZE),\n+        FLOAT(Float.SIZE),\n+        DOUBLE(Double.SIZE);\n+\n+        private final int size;\n+\n+        FieldType(int bitSize)\n+        {\n+            this.size = bitSize / 8;\n+        }\n+\n+        public int getSize()\n+        {\n+            return size;\n+        }\n+    }\n+\n+    private static final Pattern MAPPING_PATTERN = Pattern.compile(\"(\\\\d+)(?::(\\\\d+))?\");\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BIGINT, INTEGER, SMALLINT, TINYINT, DOUBLE, REAL, BOOLEAN);\n+\n+    public static final String NAME = \"raw\";\n+\n+    private final List<ColumnMapping> columnMappings;\n+    private final ByteBuffer buffer;\n+\n+    public RawRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles)\n+    {\n+        super(session, columnHandles);\n+\n+        for (EncoderColumnHandle handle : this.columnHandles) {\n+            checkArgument(handle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", handle.getFormatHint(), handle.getName());\n+            checkArgument(isSupportedType(handle.getType()), \"Unsupported column type '%s' for column '%s'\", handle.getType().getDisplayName(), handle.getName());\n+        }\n+\n+        this.columnMappings = buildMappings(this.columnHandles);\n+\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingMatchesTypeSize(mapping);\n+        }\n+\n+        int position = 0;\n+        for (ColumnMapping mapping : this.columnMappings) {\n+            checkMappingOffsets(mapping, position);\n+            position += mapping.getLength();\n+        }\n+\n+        this.buffer = ByteBuffer.allocate(position);\n+    }\n+\n+    // parse column mappings from column handles\n+    private static List<ColumnMapping> buildMappings(List<EncoderColumnHandle> columnHandles)\n+    {\n+        return columnHandles.stream().map(ColumnMapping::new).collect(toImmutableList());\n+    }\n+\n+    private static void checkMappingMatchesTypeSize(ColumnMapping mapping)\n+    {\n+        if (mapping.getLength() != mapping.getFieldType().getSize() && !isVarcharType(mapping.getType())) {\n+            throw new IndexOutOfBoundsException(format(\n+                    \"Mapping length '%s' is not equal to expected length '%s' for column '%s'\",\n+                    mapping.getLength(),\n+                    mapping.getFieldType().getSize(),\n+                    mapping.getName()));\n+        }\n+    }\n+\n+    // check that column mappings don't overlap and that there are no gaps\n+    private static void checkMappingOffsets(ColumnMapping mapping, int expectedPosition)\n+    {\n+        checkArgument(mapping.getStart() == expectedPosition, format(\n+                \"Start mapping '%s' for column '%s' does not equal expected mapping '%s'\",\n+                mapping.getStart(),\n+                mapping.getName(),\n+                expectedPosition));\n+        checkArgument(mapping.getEnd() > mapping.getStart(), format(\n+                \"End mapping '%s' for column '%s' is less than or equal to start '%s'\",\n+                mapping.getEnd(),\n+                mapping.getName(),\n+                mapping.getStart()));\n+    }\n+\n+    private static class ColumnMapping\n+    {\n+        private final String name;\n+        private final Type type;\n+        private final FieldType fieldType;\n+        private final int start;\n+        private final int end;\n+\n+        public ColumnMapping(EncoderColumnHandle columnHandle)\n+        {\n+            this.name = columnHandle.getName();\n+            this.type = columnHandle.getType();\n+\n+            this.fieldType = parseFieldType(columnHandle.getDataFormat(), this.name);\n+            checkFieldType(this.name, this.type, this.fieldType);\n+\n+            Optional<String> mapping = Optional.ofNullable(columnHandle.getMapping());\n+            if (mapping.isPresent()) {\n+                Matcher mappingMatcher = MAPPING_PATTERN.matcher(mapping.get());\n+                if (!mappingMatcher.matches()) {\n+                    throw new IllegalArgumentException(format(\"Invalid mapping for column '%s'\", this.name));\n+                }\n+\n+                if (mappingMatcher.group(2) != null) {\n+                    this.start = parseOffset(mappingMatcher.group(1), \"start\", this.name);\n+                    this.end = parseOffset(mappingMatcher.group(2), \"end\", this.name);\n+                }\n+                else {\n+                    this.start = parseOffset(mappingMatcher.group(1), \"start\", this.name);\n+                    this.end = this.start + this.fieldType.getSize();\n+                }\n+            }\n+            else {\n+                throw new IllegalArgumentException(format(\"No mapping defined for column '%s'\", this.name));\n+            }\n+        }\n+\n+        private static int parseOffset(String group, String offsetName, String columnName)\n+        {\n+            try {\n+                return parseInt(group);\n+            }\n+            catch (NumberFormatException e) {\n+                throw new IllegalArgumentException(format(\"Unable to parse '%s' offset for column '%s'\", offsetName, columnName), e);\n+            }\n+        }\n+\n+        private static FieldType parseFieldType(String dataFormat, String columnName)\n+        {\n+            try {\n+                if (!dataFormat.equals(\"\")) {\n+                    return FieldType.valueOf(dataFormat.toUpperCase(Locale.ENGLISH));\n+                }\n+            }\n+            catch (IllegalArgumentException e) {\n+                throw new IllegalArgumentException(format(\"Invalid dataFormat '%s' for column '%s'\", dataFormat, columnName));\n+            }\n+            return FieldType.BYTE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 193}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwNDQ4NzE3", "url": "https://github.com/trinodb/trino/pull/4417#pullrequestreview-450448717", "createdAt": "2020-07-17T07:55:44Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwNzo1NTo0NFrOGzJL4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwNzo1NTo0NFrOGzJL4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjI4MTA1Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            rows[i] = \"(\" + rowValues.get(i).stream().map(Object::toString).collect(Collectors.joining(\", \")) + \")\";\n          \n          \n            \n                            rows[i] = rowValues.get(i).stream().map(Object::toString).collect(Collectors.joining(\", \", \"(\", \")\"));", "url": "https://github.com/trinodb/trino/pull/4417#discussion_r456281057", "createdAt": "2020-07-17T07:55:44Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -144,62 +144,64 @@ public void testRoundTripAllFormats(RoundTripTestCase testCase)\n                 .add(new RoundTripTestCase(\n                         \"all_datatypes_avro\",\n                         ImmutableList.of(\"f_bigint\", \"f_double\", \"f_boolean\", \"f_varchar\"),\n-                        ImmutableList.of(100000, 1000.001, true, \"'test'\")))\n+                        ImmutableList.of(\n+                                ImmutableList.of(100000, 1000.001, true, \"'test'\"),\n+                                ImmutableList.of(123456, 1234.123, false, \"'abcd'\"))))\n                 .add(new RoundTripTestCase(\n                         \"all_datatypes_csv\",\n                         ImmutableList.of(\"f_bigint\", \"f_int\", \"f_smallint\", \"f_tinyint\", \"f_double\", \"f_boolean\", \"f_varchar\"),\n-                        ImmutableList.of(100000, 1000, 100, 10, 1000.001, true, \"'test'\")))\n+                        ImmutableList.of(\n+                                ImmutableList.of(100000, 1000, 100, 10, 1000.001, true, \"'test'\"),\n+                                ImmutableList.of(123456, 1234, 123, 12, 12345.123, false, \"'abcd'\"))))\n+                .add(new RoundTripTestCase(\n+                        \"all_datatypes_raw\",\n+                        ImmutableList.of(\"kafka_key\", \"f_varchar\", \"f_bigint\", \"f_int\", \"f_smallint\", \"f_tinyint\", \"f_double\", \"f_boolean\"),\n+                        ImmutableList.of(\n+                                ImmutableList.of(1, \"'test'\", 100000, 1000, 100, 10, 1000.001, true),\n+                                ImmutableList.of(1, \"'abcd'\", 123456, 1234, 123, 12, 12345.123, false))))\n                 .build();\n     }\n \n     protected static final class RoundTripTestCase\n     {\n         private final String tableName;\n         private final List<String> fieldNames;\n-        private final List<Object> fieldValues;\n-        private final int length;\n+        private final List<List<Object>> rowValues;\n+        private final int numRows;\n \n-        public RoundTripTestCase(String tableName, List<String> fieldNames, List<Object> fieldValues)\n+        public RoundTripTestCase(String tableName, List<String> fieldNames, List<List<Object>> rowValues)\n         {\n-            checkArgument(fieldNames.size() == fieldValues.size(), \"sizes of fieldNames and fieldValues are not equal\");\n+            for (List<Object> row : rowValues) {\n+                checkArgument(fieldNames.size() == row.size(), \"sizes of fieldNames and rowValues are not equal\");\n+            }\n             this.tableName = requireNonNull(tableName, \"tableName is null\");\n             this.fieldNames = ImmutableList.copyOf(fieldNames);\n-            this.fieldValues = ImmutableList.copyOf(fieldValues);\n-            this.length = fieldNames.size();\n+            this.rowValues = ImmutableList.copyOf(rowValues);\n+            this.numRows = this.rowValues.size();\n         }\n \n         public String getTableName()\n         {\n             return tableName;\n         }\n \n-        private int getIndex(String fieldName)\n-        {\n-            return fieldNames.indexOf(fieldName);\n-        }\n-\n-        public String getFieldName(String fieldName)\n-        {\n-            int index = getIndex(fieldName);\n-            checkArgument(index >= 0 && index < length, \"index out of bounds\");\n-            return fieldNames.get(index);\n-        }\n-\n         public String getFieldNames()\n         {\n             return String.join(\", \", fieldNames);\n         }\n \n-        public Object getFieldValue(String fieldName)\n+        public String getRowValues()\n         {\n-            int index = getIndex(fieldName);\n-            checkArgument(index >= 0 && index < length, \"index out of bounds\");\n-            return fieldValues.get(index);\n+            String[] rows = new String[numRows];\n+            for (int i = 0; i < numRows; i++) {\n+                rows[i] = \"(\" + rowValues.get(i).stream().map(Object::toString).collect(Collectors.joining(\", \")) + \")\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 92}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b9ffd9d861d5afedb490d637fefa944fc75e268a", "author": {"user": {"login": "charlesjmorgan", "name": "Charles Morgan"}}, "url": "https://github.com/trinodb/trino/commit/b9ffd9d861d5afedb490d637fefa944fc75e268a", "committedDate": "2020-07-17T14:24:24Z", "message": "Improve Kafka round trip test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "856d21c46c6280942b3397b44c4bb5db4e7de5e5", "author": {"user": {"login": "charlesjmorgan", "name": "Charles Morgan"}}, "url": "https://github.com/trinodb/trino/commit/856d21c46c6280942b3397b44c4bb5db4e7de5e5", "committedDate": "2020-07-17T14:24:27Z", "message": "Add Kafka raw encoder"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "856d21c46c6280942b3397b44c4bb5db4e7de5e5", "author": {"user": {"login": "charlesjmorgan", "name": "Charles Morgan"}}, "url": "https://github.com/trinodb/trino/commit/856d21c46c6280942b3397b44c4bb5db4e7de5e5", "committedDate": "2020-07-17T14:24:27Z", "message": "Add Kafka raw encoder"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 66, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}