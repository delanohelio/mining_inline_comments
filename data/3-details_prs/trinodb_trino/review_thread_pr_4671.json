{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYxOTAyMjQ1", "number": 4671, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMTo0MzoxNVrOEUkpag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNjowNDo0N1rOEUqjcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMDA3NDAyOnYy", "diffSide": "RIGHT", "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMTo0MzoxNVrOG62Vqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxODo0MDoyM1rOG7Em2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM2MDg3NQ==", "bodyText": "I think we should use the original name of your test (TestSparkCompatibility) or something similar.", "url": "https://github.com/trinodb/trino/pull/4671#discussion_r464360875", "createdAt": "2020-08-03T11:43:15Z", "author": {"login": "aalbu"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "diffHunk": "@@ -30,30 +29,177 @@\n public class TestIcebergBasic", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU5NDY1MQ==", "bodyText": "Good point.  I'll change it back to that name.", "url": "https://github.com/trinodb/trino/pull/4671#discussion_r464594651", "createdAt": "2020-08-03T18:40:23Z", "author": {"login": "djsstarburst"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "diffHunk": "@@ -30,30 +29,177 @@\n public class TestIcebergBasic", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM2MDg3NQ=="}, "originalCommit": null, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMDA4OTY4OnYy", "diffSide": "RIGHT", "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMTo0ODozNlrOG62enw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMDoxNTo0NlrOHB5Yjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM2MzE2Nw==", "bodyText": "I think the equivalent Presto semantics would be to treat this as a timestamp (without timezone), so we could cast(_timestamp as TIMESTAMP).  In addition, we need non-legacy semantics to preserve wall clock time, so we'd need to execute SET SESSION legacy_timestamp=FALSE.  Though this area is tricky and I don't fully understand it.", "url": "https://github.com/trinodb/trino/pull/4671#discussion_r464363167", "createdAt": "2020-08-03T11:48:36Z", "author": {"login": "aalbu"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "diffHunk": "@@ -30,30 +29,177 @@\n public class TestIcebergBasic\n         extends ProductTest\n {\n+    // TODO: Spark SQL doesn't yet support decimal.  When it does add it to the test.\n+    // TODO: Spark SQL only stores TIMESTAMP WITH TIME ZONE, and Iceberg only supports\n+    // TIMESTAMP with no time zone.  The Spark writes/Presto reads test can pass by\n+    // stripping off the UTC.  However, I haven't been able to get the\n+    // Presto writes/Spark reads test TIMESTAMPs to match.\n+\n     // see spark-defaults.conf\n     private static final String SPARK_CATALOG = \"iceberg_test\";\n     private static final String PRESTO_CATALOG = \"iceberg\";\n-    private static final String TABLE_NAME = \"test_iceberg_basic\";\n-    private static final String SPARK_TABLE_NAME = format(\"%s.default.%s\", SPARK_CATALOG, TABLE_NAME);\n-    private static final String PRESTO_TABLE_NAME = format(\"%s.default.%s\", PRESTO_CATALOG, TABLE_NAME);\n \n-    @BeforeTestWithContext\n-    @AfterTestWithContext\n-    public void dropTestTables()\n+    @Test(groups = {ICEBERG, PROFILE_SPECIFIC_TESTS})\n+    public void testPrestoReadingSparkData()\n+    {\n+        String baseTableName = \"test_presto_reading_primitive_types\";\n+        String sparkTableName = sparkTableName(baseTableName);\n+\n+        String sparkTableDefinition =\n+                \"CREATE TABLE %s (\" +\n+                        \"  _string STRING\" +\n+                        \", _bigint BIGINT\" +\n+                        \", _integer INTEGER\" +\n+                        \", _real REAL\" +\n+                        \", _double DOUBLE\" +\n+                        \", _boolean BOOLEAN\" +\n+                        \", _timestamp TIMESTAMP\" +\n+                        \", _date DATE\" +\n+                        \") USING ICEBERG\";\n+        onSpark().executeQuery(format(sparkTableDefinition, sparkTableName));\n+\n+        String values = \"VALUES (\" +\n+                \"'a_string'\" +\n+                \", 1000000000000000\" +\n+                \", 1000000000\" +\n+                \", 10000000.123\" +\n+                \", 100000000000.123\" +\n+                \", true\" +\n+                \", TIMESTAMP '2020-06-28 14:16:00.456'\" +\n+                \", DATE '1950-06-28'\" +\n+                \")\";\n+        String insert = format(\"INSERT INTO %s %s\", sparkTableName, values);\n+        onSpark().executeQuery(insert);\n+\n+        Row row = row(\n+                \"a_string\",\n+                1000000000000000L,\n+                1000000000,\n+                10000000.123F,\n+                100000000000.123,\n+                true,\n+                \"2020-06-28 14:16:00.456\",\n+                \"1950-06-28\");\n+\n+        String startOfSelect = \"SELECT _string, _bigint, _integer, _real, _double, _boolean\";\n+        QueryResult sparkSelect = onSpark().executeQuery(format(\"%s, CAST(_timestamp AS STRING), CAST(_date AS STRING) FROM %s\", startOfSelect, sparkTableName));\n+        assertThat(sparkSelect).containsOnly(row);\n+\n+        QueryResult prestoSelect = onPresto().executeQuery(format(\"%s, REPLACE(CAST(_timestamp AS VARCHAR), ' UTC'), CAST(_date AS VARCHAR) FROM %s\", startOfSelect, prestoTableName(baseTableName)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU5NTEzNQ==", "bodyText": "Yes, @electrum and I spent a few minutes this morning trying to understand the best course.  @electrum is going to think about it and get back to me.", "url": "https://github.com/trinodb/trino/pull/4671#discussion_r464595135", "createdAt": "2020-08-03T18:41:30Z", "author": {"login": "djsstarburst"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "diffHunk": "@@ -30,30 +29,177 @@\n public class TestIcebergBasic\n         extends ProductTest\n {\n+    // TODO: Spark SQL doesn't yet support decimal.  When it does add it to the test.\n+    // TODO: Spark SQL only stores TIMESTAMP WITH TIME ZONE, and Iceberg only supports\n+    // TIMESTAMP with no time zone.  The Spark writes/Presto reads test can pass by\n+    // stripping off the UTC.  However, I haven't been able to get the\n+    // Presto writes/Spark reads test TIMESTAMPs to match.\n+\n     // see spark-defaults.conf\n     private static final String SPARK_CATALOG = \"iceberg_test\";\n     private static final String PRESTO_CATALOG = \"iceberg\";\n-    private static final String TABLE_NAME = \"test_iceberg_basic\";\n-    private static final String SPARK_TABLE_NAME = format(\"%s.default.%s\", SPARK_CATALOG, TABLE_NAME);\n-    private static final String PRESTO_TABLE_NAME = format(\"%s.default.%s\", PRESTO_CATALOG, TABLE_NAME);\n \n-    @BeforeTestWithContext\n-    @AfterTestWithContext\n-    public void dropTestTables()\n+    @Test(groups = {ICEBERG, PROFILE_SPECIFIC_TESTS})\n+    public void testPrestoReadingSparkData()\n+    {\n+        String baseTableName = \"test_presto_reading_primitive_types\";\n+        String sparkTableName = sparkTableName(baseTableName);\n+\n+        String sparkTableDefinition =\n+                \"CREATE TABLE %s (\" +\n+                        \"  _string STRING\" +\n+                        \", _bigint BIGINT\" +\n+                        \", _integer INTEGER\" +\n+                        \", _real REAL\" +\n+                        \", _double DOUBLE\" +\n+                        \", _boolean BOOLEAN\" +\n+                        \", _timestamp TIMESTAMP\" +\n+                        \", _date DATE\" +\n+                        \") USING ICEBERG\";\n+        onSpark().executeQuery(format(sparkTableDefinition, sparkTableName));\n+\n+        String values = \"VALUES (\" +\n+                \"'a_string'\" +\n+                \", 1000000000000000\" +\n+                \", 1000000000\" +\n+                \", 10000000.123\" +\n+                \", 100000000000.123\" +\n+                \", true\" +\n+                \", TIMESTAMP '2020-06-28 14:16:00.456'\" +\n+                \", DATE '1950-06-28'\" +\n+                \")\";\n+        String insert = format(\"INSERT INTO %s %s\", sparkTableName, values);\n+        onSpark().executeQuery(insert);\n+\n+        Row row = row(\n+                \"a_string\",\n+                1000000000000000L,\n+                1000000000,\n+                10000000.123F,\n+                100000000000.123,\n+                true,\n+                \"2020-06-28 14:16:00.456\",\n+                \"1950-06-28\");\n+\n+        String startOfSelect = \"SELECT _string, _bigint, _integer, _real, _double, _boolean\";\n+        QueryResult sparkSelect = onSpark().executeQuery(format(\"%s, CAST(_timestamp AS STRING), CAST(_date AS STRING) FROM %s\", startOfSelect, sparkTableName));\n+        assertThat(sparkSelect).containsOnly(row);\n+\n+        QueryResult prestoSelect = onPresto().executeQuery(format(\"%s, REPLACE(CAST(_timestamp AS VARCHAR), ' UTC'), CAST(_date AS VARCHAR) FROM %s\", startOfSelect, prestoTableName(baseTableName)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM2MzE2Nw=="}, "originalCommit": null, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTc1MDc5OQ==", "bodyText": "@electrum, I just rebased on tip master, and re-ran the tests, and the fact Iceberg doesn't yet support TIMESTAMP WITH TIME ZONE and Spark only supports TIMESTAMP WITH TIME ZONE is still a problem, causing testSparkReadingPrestoData() to fail if the timestamp-related fields are uncommented in that test.", "url": "https://github.com/trinodb/trino/pull/4671#discussion_r471750799", "createdAt": "2020-08-17T20:15:46Z", "author": {"login": "djsstarburst"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "diffHunk": "@@ -30,30 +29,177 @@\n public class TestIcebergBasic\n         extends ProductTest\n {\n+    // TODO: Spark SQL doesn't yet support decimal.  When it does add it to the test.\n+    // TODO: Spark SQL only stores TIMESTAMP WITH TIME ZONE, and Iceberg only supports\n+    // TIMESTAMP with no time zone.  The Spark writes/Presto reads test can pass by\n+    // stripping off the UTC.  However, I haven't been able to get the\n+    // Presto writes/Spark reads test TIMESTAMPs to match.\n+\n     // see spark-defaults.conf\n     private static final String SPARK_CATALOG = \"iceberg_test\";\n     private static final String PRESTO_CATALOG = \"iceberg\";\n-    private static final String TABLE_NAME = \"test_iceberg_basic\";\n-    private static final String SPARK_TABLE_NAME = format(\"%s.default.%s\", SPARK_CATALOG, TABLE_NAME);\n-    private static final String PRESTO_TABLE_NAME = format(\"%s.default.%s\", PRESTO_CATALOG, TABLE_NAME);\n \n-    @BeforeTestWithContext\n-    @AfterTestWithContext\n-    public void dropTestTables()\n+    @Test(groups = {ICEBERG, PROFILE_SPECIFIC_TESTS})\n+    public void testPrestoReadingSparkData()\n+    {\n+        String baseTableName = \"test_presto_reading_primitive_types\";\n+        String sparkTableName = sparkTableName(baseTableName);\n+\n+        String sparkTableDefinition =\n+                \"CREATE TABLE %s (\" +\n+                        \"  _string STRING\" +\n+                        \", _bigint BIGINT\" +\n+                        \", _integer INTEGER\" +\n+                        \", _real REAL\" +\n+                        \", _double DOUBLE\" +\n+                        \", _boolean BOOLEAN\" +\n+                        \", _timestamp TIMESTAMP\" +\n+                        \", _date DATE\" +\n+                        \") USING ICEBERG\";\n+        onSpark().executeQuery(format(sparkTableDefinition, sparkTableName));\n+\n+        String values = \"VALUES (\" +\n+                \"'a_string'\" +\n+                \", 1000000000000000\" +\n+                \", 1000000000\" +\n+                \", 10000000.123\" +\n+                \", 100000000000.123\" +\n+                \", true\" +\n+                \", TIMESTAMP '2020-06-28 14:16:00.456'\" +\n+                \", DATE '1950-06-28'\" +\n+                \")\";\n+        String insert = format(\"INSERT INTO %s %s\", sparkTableName, values);\n+        onSpark().executeQuery(insert);\n+\n+        Row row = row(\n+                \"a_string\",\n+                1000000000000000L,\n+                1000000000,\n+                10000000.123F,\n+                100000000000.123,\n+                true,\n+                \"2020-06-28 14:16:00.456\",\n+                \"1950-06-28\");\n+\n+        String startOfSelect = \"SELECT _string, _bigint, _integer, _real, _double, _boolean\";\n+        QueryResult sparkSelect = onSpark().executeQuery(format(\"%s, CAST(_timestamp AS STRING), CAST(_date AS STRING) FROM %s\", startOfSelect, sparkTableName));\n+        assertThat(sparkSelect).containsOnly(row);\n+\n+        QueryResult prestoSelect = onPresto().executeQuery(format(\"%s, REPLACE(CAST(_timestamp AS VARCHAR), ' UTC'), CAST(_date AS VARCHAR) FROM %s\", startOfSelect, prestoTableName(baseTableName)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM2MzE2Nw=="}, "originalCommit": null, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMTAwNTg2OnYy", "diffSide": "RIGHT", "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNTo1NToxMVrOG6_HDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxODo0NDozNVrOG7EuTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUwNDU5MA==", "bodyText": "If we use java.sql types in the expected value, we could drop the casts to STRING.", "url": "https://github.com/trinodb/trino/pull/4671#discussion_r464504590", "createdAt": "2020-08-03T15:55:11Z", "author": {"login": "aalbu"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "diffHunk": "@@ -30,30 +29,177 @@\n public class TestIcebergBasic\n         extends ProductTest\n {\n+    // TODO: Spark SQL doesn't yet support decimal.  When it does add it to the test.\n+    // TODO: Spark SQL only stores TIMESTAMP WITH TIME ZONE, and Iceberg only supports\n+    // TIMESTAMP with no time zone.  The Spark writes/Presto reads test can pass by\n+    // stripping off the UTC.  However, I haven't been able to get the\n+    // Presto writes/Spark reads test TIMESTAMPs to match.\n+\n     // see spark-defaults.conf\n     private static final String SPARK_CATALOG = \"iceberg_test\";\n     private static final String PRESTO_CATALOG = \"iceberg\";\n-    private static final String TABLE_NAME = \"test_iceberg_basic\";\n-    private static final String SPARK_TABLE_NAME = format(\"%s.default.%s\", SPARK_CATALOG, TABLE_NAME);\n-    private static final String PRESTO_TABLE_NAME = format(\"%s.default.%s\", PRESTO_CATALOG, TABLE_NAME);\n \n-    @BeforeTestWithContext\n-    @AfterTestWithContext\n-    public void dropTestTables()\n+    @Test(groups = {ICEBERG, PROFILE_SPECIFIC_TESTS})\n+    public void testPrestoReadingSparkData()\n+    {\n+        String baseTableName = \"test_presto_reading_primitive_types\";\n+        String sparkTableName = sparkTableName(baseTableName);\n+\n+        String sparkTableDefinition =\n+                \"CREATE TABLE %s (\" +\n+                        \"  _string STRING\" +\n+                        \", _bigint BIGINT\" +\n+                        \", _integer INTEGER\" +\n+                        \", _real REAL\" +\n+                        \", _double DOUBLE\" +\n+                        \", _boolean BOOLEAN\" +\n+                        \", _timestamp TIMESTAMP\" +\n+                        \", _date DATE\" +\n+                        \") USING ICEBERG\";\n+        onSpark().executeQuery(format(sparkTableDefinition, sparkTableName));\n+\n+        String values = \"VALUES (\" +\n+                \"'a_string'\" +\n+                \", 1000000000000000\" +\n+                \", 1000000000\" +\n+                \", 10000000.123\" +\n+                \", 100000000000.123\" +\n+                \", true\" +\n+                \", TIMESTAMP '2020-06-28 14:16:00.456'\" +\n+                \", DATE '1950-06-28'\" +\n+                \")\";\n+        String insert = format(\"INSERT INTO %s %s\", sparkTableName, values);\n+        onSpark().executeQuery(insert);\n+\n+        Row row = row(\n+                \"a_string\",\n+                1000000000000000L,\n+                1000000000,\n+                10000000.123F,\n+                100000000000.123,\n+                true,\n+                \"2020-06-28 14:16:00.456\",\n+                \"1950-06-28\");\n+\n+        String startOfSelect = \"SELECT _string, _bigint, _integer, _real, _double, _boolean\";\n+        QueryResult sparkSelect = onSpark().executeQuery(format(\"%s, CAST(_timestamp AS STRING), CAST(_date AS STRING) FROM %s\", startOfSelect, sparkTableName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU5NjU1OA==", "bodyText": "That's what I tried the first time but it didn't work.  Sadly, the value types returned by Spark are Hive value class instances, and the ones returned by Presto are Presto value class instances.  The agree for primitive types like Long and Double, but not for TIMESTAMP, TIME and DATE.", "url": "https://github.com/trinodb/trino/pull/4671#discussion_r464596558", "createdAt": "2020-08-03T18:44:35Z", "author": {"login": "djsstarburst"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "diffHunk": "@@ -30,30 +29,177 @@\n public class TestIcebergBasic\n         extends ProductTest\n {\n+    // TODO: Spark SQL doesn't yet support decimal.  When it does add it to the test.\n+    // TODO: Spark SQL only stores TIMESTAMP WITH TIME ZONE, and Iceberg only supports\n+    // TIMESTAMP with no time zone.  The Spark writes/Presto reads test can pass by\n+    // stripping off the UTC.  However, I haven't been able to get the\n+    // Presto writes/Spark reads test TIMESTAMPs to match.\n+\n     // see spark-defaults.conf\n     private static final String SPARK_CATALOG = \"iceberg_test\";\n     private static final String PRESTO_CATALOG = \"iceberg\";\n-    private static final String TABLE_NAME = \"test_iceberg_basic\";\n-    private static final String SPARK_TABLE_NAME = format(\"%s.default.%s\", SPARK_CATALOG, TABLE_NAME);\n-    private static final String PRESTO_TABLE_NAME = format(\"%s.default.%s\", PRESTO_CATALOG, TABLE_NAME);\n \n-    @BeforeTestWithContext\n-    @AfterTestWithContext\n-    public void dropTestTables()\n+    @Test(groups = {ICEBERG, PROFILE_SPECIFIC_TESTS})\n+    public void testPrestoReadingSparkData()\n+    {\n+        String baseTableName = \"test_presto_reading_primitive_types\";\n+        String sparkTableName = sparkTableName(baseTableName);\n+\n+        String sparkTableDefinition =\n+                \"CREATE TABLE %s (\" +\n+                        \"  _string STRING\" +\n+                        \", _bigint BIGINT\" +\n+                        \", _integer INTEGER\" +\n+                        \", _real REAL\" +\n+                        \", _double DOUBLE\" +\n+                        \", _boolean BOOLEAN\" +\n+                        \", _timestamp TIMESTAMP\" +\n+                        \", _date DATE\" +\n+                        \") USING ICEBERG\";\n+        onSpark().executeQuery(format(sparkTableDefinition, sparkTableName));\n+\n+        String values = \"VALUES (\" +\n+                \"'a_string'\" +\n+                \", 1000000000000000\" +\n+                \", 1000000000\" +\n+                \", 10000000.123\" +\n+                \", 100000000000.123\" +\n+                \", true\" +\n+                \", TIMESTAMP '2020-06-28 14:16:00.456'\" +\n+                \", DATE '1950-06-28'\" +\n+                \")\";\n+        String insert = format(\"INSERT INTO %s %s\", sparkTableName, values);\n+        onSpark().executeQuery(insert);\n+\n+        Row row = row(\n+                \"a_string\",\n+                1000000000000000L,\n+                1000000000,\n+                10000000.123F,\n+                100000000000.123,\n+                true,\n+                \"2020-06-28 14:16:00.456\",\n+                \"1950-06-28\");\n+\n+        String startOfSelect = \"SELECT _string, _bigint, _integer, _real, _double, _boolean\";\n+        QueryResult sparkSelect = onSpark().executeQuery(format(\"%s, CAST(_timestamp AS STRING), CAST(_date AS STRING) FROM %s\", startOfSelect, sparkTableName));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUwNDU5MA=="}, "originalCommit": null, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMTA0MTc4OnYy", "diffSide": "RIGHT", "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNjowNDo0N1rOG6_dKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxODo0NDo1OFrOG7EvIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUxMDI0OQ==", "bodyText": "We could use java.sql types:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            \"2020-06-28 14:16:00.456\",\n          \n          \n            \n                            \"1950-06-28\");\n          \n          \n            \n                            Timestamp.valueOf(\"2020-06-28 14:16:00.456\"),\n          \n          \n            \n                            Date.valueOf(\"1950-06-28\"));", "url": "https://github.com/trinodb/trino/pull/4671#discussion_r464510249", "createdAt": "2020-08-03T16:04:47Z", "author": {"login": "aalbu"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "diffHunk": "@@ -30,30 +29,177 @@\n public class TestIcebergBasic\n         extends ProductTest\n {\n+    // TODO: Spark SQL doesn't yet support decimal.  When it does add it to the test.\n+    // TODO: Spark SQL only stores TIMESTAMP WITH TIME ZONE, and Iceberg only supports\n+    // TIMESTAMP with no time zone.  The Spark writes/Presto reads test can pass by\n+    // stripping off the UTC.  However, I haven't been able to get the\n+    // Presto writes/Spark reads test TIMESTAMPs to match.\n+\n     // see spark-defaults.conf\n     private static final String SPARK_CATALOG = \"iceberg_test\";\n     private static final String PRESTO_CATALOG = \"iceberg\";\n-    private static final String TABLE_NAME = \"test_iceberg_basic\";\n-    private static final String SPARK_TABLE_NAME = format(\"%s.default.%s\", SPARK_CATALOG, TABLE_NAME);\n-    private static final String PRESTO_TABLE_NAME = format(\"%s.default.%s\", PRESTO_CATALOG, TABLE_NAME);\n \n-    @BeforeTestWithContext\n-    @AfterTestWithContext\n-    public void dropTestTables()\n+    @Test(groups = {ICEBERG, PROFILE_SPECIFIC_TESTS})\n+    public void testPrestoReadingSparkData()\n+    {\n+        String baseTableName = \"test_presto_reading_primitive_types\";\n+        String sparkTableName = sparkTableName(baseTableName);\n+\n+        String sparkTableDefinition =\n+                \"CREATE TABLE %s (\" +\n+                        \"  _string STRING\" +\n+                        \", _bigint BIGINT\" +\n+                        \", _integer INTEGER\" +\n+                        \", _real REAL\" +\n+                        \", _double DOUBLE\" +\n+                        \", _boolean BOOLEAN\" +\n+                        \", _timestamp TIMESTAMP\" +\n+                        \", _date DATE\" +\n+                        \") USING ICEBERG\";\n+        onSpark().executeQuery(format(sparkTableDefinition, sparkTableName));\n+\n+        String values = \"VALUES (\" +\n+                \"'a_string'\" +\n+                \", 1000000000000000\" +\n+                \", 1000000000\" +\n+                \", 10000000.123\" +\n+                \", 100000000000.123\" +\n+                \", true\" +\n+                \", TIMESTAMP '2020-06-28 14:16:00.456'\" +\n+                \", DATE '1950-06-28'\" +\n+                \")\";\n+        String insert = format(\"INSERT INTO %s %s\", sparkTableName, values);\n+        onSpark().executeQuery(insert);\n+\n+        Row row = row(\n+                \"a_string\",\n+                1000000000000000L,\n+                1000000000,\n+                10000000.123F,\n+                100000000000.123,\n+                true,\n+                \"2020-06-28 14:16:00.456\",\n+                \"1950-06-28\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU5Njc2OA==", "bodyText": "I wish it were true :)", "url": "https://github.com/trinodb/trino/pull/4671#discussion_r464596768", "createdAt": "2020-08-03T18:44:58Z", "author": {"login": "djsstarburst"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "diffHunk": "@@ -30,30 +29,177 @@\n public class TestIcebergBasic\n         extends ProductTest\n {\n+    // TODO: Spark SQL doesn't yet support decimal.  When it does add it to the test.\n+    // TODO: Spark SQL only stores TIMESTAMP WITH TIME ZONE, and Iceberg only supports\n+    // TIMESTAMP with no time zone.  The Spark writes/Presto reads test can pass by\n+    // stripping off the UTC.  However, I haven't been able to get the\n+    // Presto writes/Spark reads test TIMESTAMPs to match.\n+\n     // see spark-defaults.conf\n     private static final String SPARK_CATALOG = \"iceberg_test\";\n     private static final String PRESTO_CATALOG = \"iceberg\";\n-    private static final String TABLE_NAME = \"test_iceberg_basic\";\n-    private static final String SPARK_TABLE_NAME = format(\"%s.default.%s\", SPARK_CATALOG, TABLE_NAME);\n-    private static final String PRESTO_TABLE_NAME = format(\"%s.default.%s\", PRESTO_CATALOG, TABLE_NAME);\n \n-    @BeforeTestWithContext\n-    @AfterTestWithContext\n-    public void dropTestTables()\n+    @Test(groups = {ICEBERG, PROFILE_SPECIFIC_TESTS})\n+    public void testPrestoReadingSparkData()\n+    {\n+        String baseTableName = \"test_presto_reading_primitive_types\";\n+        String sparkTableName = sparkTableName(baseTableName);\n+\n+        String sparkTableDefinition =\n+                \"CREATE TABLE %s (\" +\n+                        \"  _string STRING\" +\n+                        \", _bigint BIGINT\" +\n+                        \", _integer INTEGER\" +\n+                        \", _real REAL\" +\n+                        \", _double DOUBLE\" +\n+                        \", _boolean BOOLEAN\" +\n+                        \", _timestamp TIMESTAMP\" +\n+                        \", _date DATE\" +\n+                        \") USING ICEBERG\";\n+        onSpark().executeQuery(format(sparkTableDefinition, sparkTableName));\n+\n+        String values = \"VALUES (\" +\n+                \"'a_string'\" +\n+                \", 1000000000000000\" +\n+                \", 1000000000\" +\n+                \", 10000000.123\" +\n+                \", 100000000000.123\" +\n+                \", true\" +\n+                \", TIMESTAMP '2020-06-28 14:16:00.456'\" +\n+                \", DATE '1950-06-28'\" +\n+                \")\";\n+        String insert = format(\"INSERT INTO %s %s\", sparkTableName, values);\n+        onSpark().executeQuery(insert);\n+\n+        Row row = row(\n+                \"a_string\",\n+                1000000000000000L,\n+                1000000000,\n+                10000000.123F,\n+                100000000000.123,\n+                true,\n+                \"2020-06-28 14:16:00.456\",\n+                \"1950-06-28\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUxMDI0OQ=="}, "originalCommit": null, "originalPosition": 74}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3585, "cost": 1, "resetAt": "2021-11-12T12:57:47Z"}}}