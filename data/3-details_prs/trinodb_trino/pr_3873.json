{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI0NDQ0MDU4", "number": 3873, "title": "Make RubixInitializer an extension point", "bodyText": "", "createdAt": "2020-05-28T12:09:55Z", "url": "https://github.com/trinodb/trino/pull/3873", "merged": true, "mergeCommit": {"oid": "fe8fd54b3291282c447e1a4f79dc5f9d4a3f06fc"}, "closed": true, "closedAt": "2020-05-28T14:15:08Z", "author": {"login": "sopel39"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABclt35qABqjMzODI3ODMyOTk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcluieRABqjMzODI5OTYzMzU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIwMDYyNTA5", "url": "https://github.com/trinodb/trino/pull/3873#pullrequestreview-420062509", "createdAt": "2020-05-28T12:36:46Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxMjozNjo0NlrOGbzFSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNDowODozM1rOGb22mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTgwMTY3NQ==", "bodyText": "Now responsibility is extended. The class also takes care of HDFS configuration updating.", "url": "https://github.com/trinodb/trino/pull/3873#discussion_r431801675", "createdAt": "2020-05-28T12:36:46Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/rubix/DefaultRubixInitializer.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.rubix;\n+\n+import com.codahale.metrics.MetricRegistry;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.io.Closer;\n+import com.qubole.rubix.bookkeeper.BookKeeper;\n+import com.qubole.rubix.bookkeeper.BookKeeperServer;\n+import com.qubole.rubix.bookkeeper.LocalDataTransferServer;\n+import com.qubole.rubix.core.CachingFileSystem;\n+import com.qubole.rubix.prestosql.CachingPrestoAzureBlobFileSystem;\n+import com.qubole.rubix.prestosql.CachingPrestoDistributedFileSystem;\n+import com.qubole.rubix.prestosql.CachingPrestoGoogleHadoopFileSystem;\n+import com.qubole.rubix.prestosql.CachingPrestoNativeAzureFileSystem;\n+import com.qubole.rubix.prestosql.CachingPrestoS3FileSystem;\n+import com.qubole.rubix.prestosql.CachingPrestoSecureAzureBlobFileSystem;\n+import com.qubole.rubix.prestosql.CachingPrestoSecureNativeAzureFileSystem;\n+import com.qubole.rubix.prestosql.PrestoClusterManager;\n+import io.airlift.log.Logger;\n+import io.airlift.units.Duration;\n+import io.prestosql.plugin.base.CatalogName;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.util.RetryDriver;\n+import io.prestosql.spi.HostAddress;\n+import io.prestosql.spi.Node;\n+import io.prestosql.spi.NodeManager;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import javax.annotation.Nullable;\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Throwables.propagateIfPossible;\n+import static com.qubole.rubix.spi.CacheConfig.enableHeartbeat;\n+import static com.qubole.rubix.spi.CacheConfig.setBookKeeperServerPort;\n+import static com.qubole.rubix.spi.CacheConfig.setCacheDataDirPrefix;\n+import static com.qubole.rubix.spi.CacheConfig.setCacheDataEnabled;\n+import static com.qubole.rubix.spi.CacheConfig.setClusterNodeRefreshTime;\n+import static com.qubole.rubix.spi.CacheConfig.setCoordinatorHostName;\n+import static com.qubole.rubix.spi.CacheConfig.setCurrentNodeHostName;\n+import static com.qubole.rubix.spi.CacheConfig.setDataTransferServerPort;\n+import static com.qubole.rubix.spi.CacheConfig.setEmbeddedMode;\n+import static com.qubole.rubix.spi.CacheConfig.setIsParallelWarmupEnabled;\n+import static com.qubole.rubix.spi.CacheConfig.setOnMaster;\n+import static com.qubole.rubix.spi.CacheConfig.setPrestoClusterManager;\n+import static io.prestosql.plugin.hive.DynamicConfigurationProvider.setCacheKey;\n+import static io.prestosql.plugin.hive.util.ConfigurationUtils.getInitialConfiguration;\n+import static io.prestosql.plugin.hive.util.RetryDriver.DEFAULT_SCALE_FACTOR;\n+import static io.prestosql.plugin.hive.util.RetryDriver.retry;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.Integer.MAX_VALUE;\n+import static java.util.concurrent.TimeUnit.MINUTES;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/*\n+ * Responsibilities of this initializer:\n+ * 1. Wait for master and setup RubixConfigurationInitializer with information about master when it becomes available\n+ * 2. Start Rubix Servers.\n+ * 3. Inject BookKeeper object into CachingFileSystem class", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTg2MzQ1MQ==", "bodyText": "nit: extraConfigurationInitializer", "url": "https://github.com/trinodb/trino/pull/3873#discussion_r431863451", "createdAt": "2020-05-28T14:08:33Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/rubix/RubixInitializer.java", "diffHunk": "@@ -108,6 +110,7 @@\n     private final NodeManager nodeManager;\n     private final CatalogName catalogName;\n     private final HdfsConfigurationInitializer hdfsConfigurationInitializer;\n+    private final Optional<ConfigurationInitializer> additionalConfigInitializer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a325001a6d9a7acde771d39aa5c20f80df972bc", "author": {"user": {"login": "sopel39", "name": "Karol Sobczak"}}, "url": "https://github.com/trinodb/trino/commit/7a325001a6d9a7acde771d39aa5c20f80df972bc", "committedDate": "2020-05-28T14:14:08Z", "message": "Make RubixConfigurationInitializer depend on RubixInitializer\n\nThis encapsulates responsibilities of these objects and allows\nfor RubixConfigurationInitializer to drive business logic\nwhen Rubix cache should be enabled. It also makes it be possible\nto provide extension points for both of these classes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e70fa19d5cc47e002f21c367f9b318730538cd0", "author": {"user": {"login": "sopel39", "name": "Karol Sobczak"}}, "url": "https://github.com/trinodb/trino/commit/4e70fa19d5cc47e002f21c367f9b318730538cd0", "committedDate": "2020-05-28T14:14:08Z", "message": "Add extension point for modifying Rubix Hadoop configuration"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "4e70fa19d5cc47e002f21c367f9b318730538cd0", "author": {"user": {"login": "sopel39", "name": "Karol Sobczak"}}, "url": "https://github.com/trinodb/trino/commit/4e70fa19d5cc47e002f21c367f9b318730538cd0", "committedDate": "2020-05-28T14:14:08Z", "message": "Add extension point for modifying Rubix Hadoop configuration"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 520, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}