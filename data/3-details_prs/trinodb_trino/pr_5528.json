{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAxNjM3ODQz", "number": 5528, "title": "Fix skipping in parquet int64 millis decoder", "bodyText": "Fixes #5443", "createdAt": "2020-10-12T15:06:04Z", "url": "https://github.com/trinodb/trino/pull/5528", "merged": true, "mergeCommit": {"oid": "9fd4a811d8c6145b205a186a6caf81e8262cffce"}, "closed": true, "closedAt": "2020-10-14T18:07:57Z", "author": {"login": "martint"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdR1o5NgFqTUwNjcxNjAwNw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdSfglNgBqjM4Nzc0MDI5OTI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2NzE2MDA3", "url": "https://github.com/trinodb/trino/pull/5528#pullrequestreview-506716007", "createdAt": "2020-10-12T15:23:35Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2ODAwOTgx", "url": "https://github.com/trinodb/trino/pull/5528#pullrequestreview-506800981", "createdAt": "2020-10-12T17:31:54Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxNzozMTo1NFrOHgHaXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxNzozMTo1NFrOHgHaXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQzNzkxOA==", "bodyText": "its not clear what belongs to TestReader and what to TestParquetReader (same package)", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503437918", "createdAt": "2020-10-12T17:31:54Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 52}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3MTgyOTAy", "url": "https://github.com/trinodb/trino/pull/5528#pullrequestreview-507182902", "createdAt": "2020-10-13T08:19:42Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwODoxOTo0MlrOHga6_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwODoxOTo0MlrOHga6_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzc1NzU2NA==", "bodyText": "make it package-private?", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503757564", "createdAt": "2020-10-13T08:19:42Z", "author": {"login": "sopel39"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/ParquetTester.java", "diffHunk": "@@ -569,7 +569,7 @@ private static FileFormat getFileFormat()\n         return OPTIMIZED ? FileFormat.PRESTO_PARQUET : FileFormat.HIVE_PARQUET;\n     }\n \n-    private static void writeParquetColumn(\n+    public static void writeParquetColumn(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3MjQ3NjQ5", "url": "https://github.com/trinodb/trino/pull/5528#pullrequestreview-507247649", "createdAt": "2020-10-13T09:34:19Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTozNDoyMFrOHgeAMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTozODoxOFrOHgeKrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwODA0OQ==", "bodyText": "None of these is needed", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503808049", "createdAt": "2020-10-13T09:34:20Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwODUzOA==", "bodyText": "seems unrelated?", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503808538", "createdAt": "2020-10-13T09:35:05Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwOTA2OA==", "bodyText": "assert firstPage.getPositionCount() > 0", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503809068", "createdAt": "2020-10-13T09:35:48Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+                for (int i = 0; i < firstPage.getPositionCount(); i++) {\n+                    expectedValues.next();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgwOTU1Mg==", "bodyText": "We should document what null return value means. \"This method is allowed to return null\" is not sufficient.", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503809552", "createdAt": "2020-10-13T09:36:31Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+                for (int i = 0; i < firstPage.getPositionCount(); i++) {\n+                    expectedValues.next();\n+                }\n+\n+                while (!pageSource.isFinished()) {\n+                    Page page = pageSource.getNextPage();\n+                    if (page == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgxMDIxNg==", "bodyText": "add a message, othewise exception coming from here will be hard to understand", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503810216", "createdAt": "2020-10-13T09:37:33Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig()\n+                .setHiveStorageFormat(HiveStorageFormat.PARQUET)\n+                .setUseParquetColumnNames(true));\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(COMPRESSION, CompressionCodecName.SNAPPY);\n+            jobConf.setBoolean(ENABLE_DICTIONARY, true);\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+                for (int i = 0; i < firstPage.getPositionCount(); i++) {\n+                    expectedValues.next();\n+                }\n+\n+                while (!pageSource.isFinished()) {\n+                    Page page = pageSource.getNextPage();\n+                    if (page == null) {\n+                        continue;\n+                    }\n+                    Block block = page.getBlock(0);\n+\n+                    for (int i = 0; i < block.getPositionCount(); i++) {\n+                        assertEquals(TIMESTAMP_MILLIS.getObjectValue(session, block, i), expectedValues.next());\n+                    }\n+                }\n+            }\n+\n+            assertFalse(expectedValues.hasNext());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgxMDczMg==", "bodyText": "Is negative value any special?\nis the range length (2000) special? Can it be eg 20?", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r503810732", "createdAt": "2020-10-13T09:38:18Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestReader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.HiveStorageFormat;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.COMPRESSION;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+\n+public class TestReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA4MTMwMDgz", "url": "https://github.com/trinodb/trino/pull/5528#pullrequestreview-508130083", "createdAt": "2020-10-14T08:50:52Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1MDo1MlrOHhI3lQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1MDo1MlrOHhI3lQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxMDM1Nw==", "bodyText": "the shorter, intermediate version that you had\nassertTrue(firstPage.getPositionCount() > 0, \"Expected first page to have at least 1 row\");\n\nproduces equally informative exception message (assuming page.positionCount is non-negative)", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r504510357", "createdAt": "2020-10-14T08:50:52Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestTimestampReader.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestTimestampReader\n+{\n+    @Test\n+    public void testTimestampBackedByInt64()\n+            throws Exception\n+    {\n+        MessageType parquetSchema = parseMessageType(\"message hive_timestamp { optional int64 test (TIMESTAMP_MILLIS); }\");\n+        ContiguousSet<Long> epochMillisValues = ContiguousSet.create(Range.closedOpen((long) -1_000, (long) 1_000), DiscreteDomain.longs());\n+        ImmutableList.Builder<SqlTimestamp> timestamps = new ImmutableList.Builder<>();\n+        for (long value : epochMillisValues) {\n+            timestamps.add(SqlTimestamp.fromMillis(3, value));\n+        }\n+\n+        List<ObjectInspector> objectInspectors = singletonList(javaLongObjectInspector);\n+        List<String> columnNames = ImmutableList.of(\"test\");\n+\n+        ConnectorSession session = getHiveSession(new HiveConfig());\n+\n+        try (ParquetTester.TempFile tempFile = new ParquetTester.TempFile(\"test\", \"parquet\")) {\n+            JobConf jobConf = new JobConf();\n+            jobConf.setEnum(WRITER_VERSION, PARQUET_1_0);\n+\n+            ParquetTester.writeParquetColumn(\n+                    jobConf,\n+                    tempFile.getFile(),\n+                    CompressionCodecName.SNAPPY,\n+                    ParquetTester.createTableProperties(columnNames, objectInspectors),\n+                    getStandardStructObjectInspector(columnNames, objectInspectors),\n+                    new Iterator<?>[] {epochMillisValues.iterator()},\n+                    Optional.of(parquetSchema),\n+                    false);\n+\n+            Iterator<SqlTimestamp> expectedValues = timestamps.build().iterator();\n+            try (ConnectorPageSource pageSource = FileFormat.PRESTO_PARQUET.createFileFormatReader(session, HDFS_ENVIRONMENT, tempFile.getFile(), columnNames, ImmutableList.of(TIMESTAMP_MILLIS))) {\n+                // skip a page to exercise the decoder's skip() logic\n+                Page firstPage = pageSource.getNextPage();\n+\n+                assertThat(firstPage.getPositionCount())\n+                        .withFailMessage(\"Expected first page to have at least 1 row\")\n+                        .isGreaterThan(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 87}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA4MTMxMDc5", "url": "https://github.com/trinodb/trino/pull/5528#pullrequestreview-508131079", "createdAt": "2020-10-14T08:52:06Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1MjowNlrOHhI6tg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwODo1MjowNlrOHhI6tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUxMTE1OA==", "bodyText": "TestTimestampColumnReader ?", "url": "https://github.com/trinodb/trino/pull/5528#discussion_r504511158", "createdAt": "2020-10-14T08:52:06Z", "author": {"login": "findepi"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/parquet/TestTimestampReader.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ContiguousSet;\n+import com.google.common.collect.DiscreteDomain;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import io.prestosql.plugin.hive.HiveConfig;\n+import io.prestosql.plugin.hive.benchmark.FileFormat;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.connector.ConnectorPageSource;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.SqlTimestamp;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageType;\n+import org.testng.annotations.Test;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static io.prestosql.plugin.hive.HiveTestUtils.HDFS_ENVIRONMENT;\n+import static io.prestosql.plugin.hive.HiveTestUtils.getHiveSession;\n+import static io.prestosql.spi.type.TimestampType.TIMESTAMP_MILLIS;\n+import static java.util.Collections.singletonList;\n+import static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector;\n+import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.javaLongObjectInspector;\n+import static org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+public class TestTimestampReader", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "87970a6636d483824e76c90e9c72605384890772", "author": {"user": {"login": "martint", "name": "Martin Traverso"}}, "url": "https://github.com/trinodb/trino/commit/87970a6636d483824e76c90e9c72605384890772", "committedDate": "2020-10-14T16:10:18Z", "message": "Fix skipping in parquet int64 millis decoder"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "87970a6636d483824e76c90e9c72605384890772", "author": {"user": {"login": "martint", "name": "Martin Traverso"}}, "url": "https://github.com/trinodb/trino/commit/87970a6636d483824e76c90e9c72605384890772", "committedDate": "2020-10-14T16:10:18Z", "message": "Fix skipping in parquet int64 millis decoder"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3419, "cost": 1, "resetAt": "2021-10-28T20:13:43Z"}}}