{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA1Nzc1MDk2", "number": 3483, "title": "Support reading uniontype as struct from Avro/ORC Hive tables", "bodyText": "Reading uniontypes by converting them into structs. Take type\n\"uniontype<int, double>\" as an example:\n\nIt will be regarded as \"struct<tag int, field0 int, field1 string>\"\nData {1: 'hello'}, {0: 312}, {1: 'world'} will be read as [1, NULL,\n'hello'], [0, 312, NULL], [1, NULL, 'world']\n\nWriting into uniontypes remains unsupported.\n(Note: support for Parquet is not added because Parquet itself doesn't support union types yet.)\nCloses #1751", "createdAt": "2020-04-20T01:22:32Z", "url": "https://github.com/trinodb/trino/pull/3483", "merged": true, "mergeCommit": {"oid": "e3d798db846241aaaad583f3d0fac01f59572b76"}, "closed": true, "closedAt": "2020-06-07T18:16:19Z", "author": {"login": "lxynov"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcZWPztABqjMyNDk4OTY0OTA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABco__QhAFqTQyNTg0MTk1NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2MTc5NTg2", "url": "https://github.com/trinodb/trino/pull/3483#pullrequestreview-396179586", "createdAt": "2020-04-20T06:47:54Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQwNjo0Nzo1NVrOGIFlBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQwNjo0Nzo1NVrOGIFlBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTEzMzE5MQ==", "bodyText": "Add a comment explaining whyh this is storage format dependent", "url": "https://github.com/trinodb/trino/pull/3483#discussion_r411133191", "createdAt": "2020-04-20T06:47:55Z", "author": {"login": "findepi"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveType.java", "diffHunk": "@@ -148,26 +152,32 @@ public String toString()\n         return hiveTypeName.toString();\n     }\n \n-    public boolean isSupportedType()\n+    public boolean isSupportedType(StorageFormat storageFormat)\n     {\n-        return isSupportedType(getTypeInfo());\n+        return isSupportedType(getTypeInfo(), storageFormat);\n     }\n \n-    public static boolean isSupportedType(TypeInfo typeInfo)\n+    public static boolean isSupportedType(TypeInfo typeInfo, StorageFormat storageFormat)\n     {\n         switch (typeInfo.getCategory()) {\n             case PRIMITIVE:\n                 return getPrimitiveType((PrimitiveTypeInfo) typeInfo) != null;\n             case MAP:\n                 MapTypeInfo mapTypeInfo = (MapTypeInfo) typeInfo;\n-                return isSupportedType(mapTypeInfo.getMapKeyTypeInfo()) && isSupportedType(mapTypeInfo.getMapValueTypeInfo());\n+                return isSupportedType(mapTypeInfo.getMapKeyTypeInfo(), storageFormat) && isSupportedType(mapTypeInfo.getMapValueTypeInfo(), storageFormat);\n             case LIST:\n                 ListTypeInfo listTypeInfo = (ListTypeInfo) typeInfo;\n-                return isSupportedType(listTypeInfo.getListElementTypeInfo());\n+                return isSupportedType(listTypeInfo.getListElementTypeInfo(), storageFormat);\n             case STRUCT:\n                 StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;\n                 return structTypeInfo.getAllStructFieldTypeInfos().stream()\n-                        .allMatch(HiveType::isSupportedType);\n+                        .allMatch(fieldTypeInfo -> isSupportedType(fieldTypeInfo, storageFormat));\n+            case UNION:\n+                if (storageFormat.getSerDe().equalsIgnoreCase(AVRO.getSerDe()) || storageFormat.getSerDe().equals(ORC.getSerDe())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA2MTYyMDE3", "url": "https://github.com/trinodb/trino/pull/3483#pullrequestreview-406162017", "createdAt": "2020-05-05T21:17:20Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMToxNzoyMFrOGQ76BA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMToxNzoyMFrOGQ76BA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDQxMTkwOA==", "bodyText": "Instead of copying the data use a dictionary block.", "url": "https://github.com/trinodb/trino/pull/3483#discussion_r420411908", "createdAt": "2020-05-05T21:17:20Z", "author": {"login": "dain"}, "path": "presto-orc/src/main/java/io/prestosql/orc/reader/UnionColumnReader.java", "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.orc.reader;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.io.Closer;\n+import io.prestosql.memory.context.AggregatedMemoryContext;\n+import io.prestosql.orc.OrcBlockFactory;\n+import io.prestosql.orc.OrcColumn;\n+import io.prestosql.orc.OrcCorruptionException;\n+import io.prestosql.orc.metadata.ColumnEncoding;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.stream.BooleanInputStream;\n+import io.prestosql.orc.stream.ByteInputStream;\n+import io.prestosql.orc.stream.InputStreamSource;\n+import io.prestosql.orc.stream.InputStreamSources;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.block.BlockBuilder;\n+import io.prestosql.spi.block.ByteArrayBlock;\n+import io.prestosql.spi.block.LazyBlock;\n+import io.prestosql.spi.block.LazyBlockLoader;\n+import io.prestosql.spi.block.RowBlock;\n+import io.prestosql.spi.block.RunLengthEncodedBlock;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.Type;\n+import org.openjdk.jol.info.ClassLayout;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.ZoneId;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static com.google.common.base.MoreObjects.toStringHelper;\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Verify.verify;\n+import static io.prestosql.orc.metadata.Stream.StreamKind.DATA;\n+import static io.prestosql.orc.metadata.Stream.StreamKind.PRESENT;\n+import static io.prestosql.orc.reader.ColumnReaders.createColumnReader;\n+import static io.prestosql.orc.reader.ReaderUtils.verifyStreamType;\n+import static io.prestosql.orc.stream.MissingInputStreamSource.missingStreamSource;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static java.util.Objects.requireNonNull;\n+\n+// Use row blocks to represent union objects when reading\n+public class UnionColumnReader\n+        implements ColumnReader\n+{\n+    private static final int INSTANCE_SIZE = ClassLayout.parseClass(UnionColumnReader.class).instanceSize();\n+\n+    private final OrcColumn column;\n+    private final OrcBlockFactory blockFactory;\n+\n+    private final RowType type;\n+    private final List<ColumnReader> fieldReaders;\n+\n+    private int readOffset;\n+    private int nextBatchSize;\n+\n+    private InputStreamSource<BooleanInputStream> presentStreamSource = missingStreamSource(BooleanInputStream.class);\n+    private InputStreamSource<ByteInputStream> dataStreamSource = missingStreamSource(ByteInputStream.class);\n+    @Nullable\n+    private BooleanInputStream presentStream;\n+    @Nullable\n+    private ByteInputStream dataStream;\n+\n+    private boolean rowGroupOpen;\n+\n+    UnionColumnReader(Type type, OrcColumn column, AggregatedMemoryContext systemMemoryContext, OrcBlockFactory blockFactory)\n+            throws OrcCorruptionException\n+    {\n+        requireNonNull(type, \"type is null\");\n+        verifyStreamType(column, type, RowType.class::isInstance);\n+        this.type = (RowType) type;\n+\n+        this.column = requireNonNull(column, \"column is null\");\n+        this.blockFactory = requireNonNull(blockFactory, \"blockFactory is null\");\n+\n+        ImmutableList.Builder<ColumnReader> fieldReadersBuilder = ImmutableList.builder();\n+        List<OrcColumn> fields = column.getNestedColumns();\n+        for (int i = 0; i < fields.size(); i++) {\n+            fieldReadersBuilder.add(createColumnReader(type.getTypeParameters().get(i + 1), fields.get(i), systemMemoryContext, blockFactory));\n+        }\n+        fieldReaders = fieldReadersBuilder.build();\n+    }\n+\n+    @Override\n+    public void prepareNextRead(int batchSize)\n+    {\n+        readOffset += nextBatchSize;\n+        nextBatchSize = batchSize;\n+    }\n+\n+    @Override\n+    public Block readBlock()\n+            throws IOException\n+    {\n+        if (!rowGroupOpen) {\n+            openRowGroup();\n+        }\n+\n+        if (readOffset > 0) {\n+            if (presentStream != null) {\n+                readOffset = presentStream.countBitsSet(readOffset);\n+            }\n+            if (readOffset > 0) {\n+                if (dataStream == null) {\n+                    throw new OrcCorruptionException(column.getOrcDataSourceId(), \"Value is not null but data stream is missing\");\n+                }\n+                int[] readOffsets = new int[fieldReaders.size()];\n+                for (byte tag : dataStream.next(readOffset)) {\n+                    readOffsets[tag]++;\n+                }\n+                for (int i = 0; i < fieldReaders.size(); i++) {\n+                    fieldReaders.get(i).prepareNextRead(readOffsets[i]);\n+                }\n+            }\n+        }\n+\n+        boolean[] nullVector = null;\n+        Block[] blocks;\n+\n+        if (presentStream == null) {\n+            blocks = getBlocks(nextBatchSize);\n+        }\n+        else {\n+            nullVector = new boolean[nextBatchSize];\n+            int nullValues = presentStream.getUnsetBits(nextBatchSize, nullVector);\n+            if (nullValues != nextBatchSize) {\n+                blocks = getBlocks(nextBatchSize - nullValues);\n+            }\n+            else {\n+                List<Type> typeParameters = type.getTypeParameters();\n+                blocks = new Block[typeParameters.size() + 1];\n+                blocks[0] = TINYINT.createBlockBuilder(null, 0).build();\n+                for (int i = 0; i < typeParameters.size(); i++) {\n+                    blocks[i + 1] = typeParameters.get(i).createBlockBuilder(null, 0).build();\n+                }\n+            }\n+        }\n+\n+        verify(Arrays.stream(blocks)\n+                .mapToInt(Block::getPositionCount)\n+                .distinct()\n+                .count() == 1);\n+\n+        Block rowBlock = RowBlock.fromFieldBlocks(nextBatchSize, Optional.ofNullable(nullVector), blocks);\n+\n+        readOffset = 0;\n+        nextBatchSize = 0;\n+\n+        return rowBlock;\n+    }\n+\n+    private void openRowGroup()\n+            throws IOException\n+    {\n+        presentStream = presentStreamSource.openStream();\n+        dataStream = dataStreamSource.openStream();\n+\n+        rowGroupOpen = true;\n+    }\n+\n+    @Override\n+    public void startStripe(ZoneId fileTimeZone, ZoneId storageTimeZone, InputStreamSources dictionaryStreamSources, ColumnMetadata<ColumnEncoding> encoding)\n+            throws IOException\n+    {\n+        presentStreamSource = missingStreamSource(BooleanInputStream.class);\n+        dataStreamSource = missingStreamSource(ByteInputStream.class);\n+\n+        readOffset = 0;\n+        nextBatchSize = 0;\n+\n+        presentStream = null;\n+        dataStream = null;\n+\n+        rowGroupOpen = false;\n+\n+        for (ColumnReader fieldReader : fieldReaders) {\n+            fieldReader.startStripe(fileTimeZone, storageTimeZone, dictionaryStreamSources, encoding);\n+        }\n+    }\n+\n+    @Override\n+    public void startRowGroup(InputStreamSources dataStreamSources)\n+            throws IOException\n+    {\n+        presentStreamSource = dataStreamSources.getInputStreamSource(column, PRESENT, BooleanInputStream.class);\n+        dataStreamSource = dataStreamSources.getInputStreamSource(column, DATA, ByteInputStream.class);\n+\n+        readOffset = 0;\n+        nextBatchSize = 0;\n+\n+        presentStream = null;\n+        dataStream = null;\n+\n+        rowGroupOpen = false;\n+\n+        for (ColumnReader fieldReader : fieldReaders) {\n+            fieldReader.startRowGroup(dataStreamSources);\n+        }\n+    }\n+\n+    @Override\n+    public String toString()\n+    {\n+        return toStringHelper(this)\n+                .addValue(column)\n+                .toString();\n+    }\n+\n+    private Block[] getBlocks(int positionCount)\n+            throws IOException\n+    {\n+        if (dataStream == null) {\n+            throw new OrcCorruptionException(column.getOrcDataSourceId(), \"Value is not null but data stream is missing\");\n+        }\n+\n+        Block[] blocks = new Block[fieldReaders.size() + 1];\n+\n+        byte[] tags = dataStream.next(positionCount);\n+        blocks[0] = new ByteArrayBlock(positionCount, Optional.empty(), tags);\n+\n+        boolean[][] valueIsNonNull = new boolean[fieldReaders.size()][positionCount];\n+        int[] nonNullValueCount = new int[fieldReaders.size()];\n+        for (int i = 0; i < positionCount; i++) {\n+            valueIsNonNull[tags[i]][i] = true;\n+            nonNullValueCount[tags[i]]++;\n+        }\n+\n+        for (int i = 0; i < fieldReaders.size(); i++) {\n+            Type fieldType = type.getTypeParameters().get(i + 1);\n+            if (nonNullValueCount[i] > 0) {\n+                ColumnReader reader = fieldReaders.get(i);\n+                reader.prepareNextRead(nonNullValueCount[i]);\n+                Block rawBlock = blockFactory.createBlock(nonNullValueCount[i], reader::readBlock, true);\n+                blocks[i + 1] = new LazyBlock(positionCount, new UnpackLazyBlockLoader(rawBlock, fieldType, valueIsNonNull[i]));\n+            }\n+            else {\n+                blocks[i + 1] = new RunLengthEncodedBlock(\n+                        fieldType.createBlockBuilder(null, 1).appendNull().build(),\n+                        positionCount);\n+            }\n+        }\n+        return blocks;\n+    }\n+\n+    @Override\n+    public void close()\n+    {\n+        try (Closer closer = Closer.create()) {\n+            for (ColumnReader structField : fieldReaders) {\n+                closer.register(structField::close);\n+            }\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    @Override\n+    public long getRetainedSizeInBytes()\n+    {\n+        long retainedSizeInBytes = INSTANCE_SIZE;\n+        for (ColumnReader structField : fieldReaders) {\n+            retainedSizeInBytes += structField.getRetainedSizeInBytes();\n+        }\n+        return retainedSizeInBytes;\n+    }\n+\n+    private static final class UnpackLazyBlockLoader\n+            implements LazyBlockLoader\n+    {\n+        private final Block denseBlock;\n+        private final Type type;\n+        private final boolean[] valueIsNonNull;\n+\n+        public UnpackLazyBlockLoader(Block denseBlock, Type type, boolean[] valueIsNonNull)\n+        {\n+            this.denseBlock = requireNonNull(denseBlock, \"denseBlock is null\");\n+            this.type = requireNonNull(type, \"type is null\");\n+            this.valueIsNonNull = requireNonNull(valueIsNonNull, \"valueIsNonNull\");\n+        }\n+\n+        @Override\n+        public Block load()\n+        {\n+            Block loadedDenseBlock = denseBlock.getLoadedBlock();\n+            BlockBuilder unpackedBlock = type.createBlockBuilder(null, valueIsNonNull.length);\n+\n+            int denseBlockPosition = 0;\n+            for (boolean isNonNull : valueIsNonNull) {\n+                if (isNonNull) {\n+                    type.appendTo(loadedDenseBlock, denseBlockPosition++, unpackedBlock);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 308}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA2MTYzOTkx", "url": "https://github.com/trinodb/trino/pull/3483#pullrequestreview-406163991", "createdAt": "2020-05-05T21:20:27Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2b4f2ad9851e3a0d1c0939274d5deaf16266e11b", "author": {"user": {"login": "lxynov", "name": "Xingyuan Lin"}}, "url": "https://github.com/trinodb/trino/commit/2b4f2ad9851e3a0d1c0939274d5deaf16266e11b", "committedDate": "2020-05-17T20:30:37Z", "message": "Support reading uniontype as struct from Avro/ORC Hive tables\n\nReading uniontypes by converting them into structs. Take type\n\"uniontype<int, double>\" as an example:\n1. It will be regarded as \"struct<tag int, field0 int, field1 string>\"\n2. Data {1: 'hello'}, {0: 312}, {1: 'world'} will be read as [1, NULL,\n   'hello'], [0, 312, NULL], [1, NULL, 'world']\n\nWriting into uniontypes remains unsupported."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "2b4f2ad9851e3a0d1c0939274d5deaf16266e11b", "author": {"user": {"login": "lxynov", "name": "Xingyuan Lin"}}, "url": "https://github.com/trinodb/trino/commit/2b4f2ad9851e3a0d1c0939274d5deaf16266e11b", "committedDate": "2020-05-17T20:30:37Z", "message": "Support reading uniontype as struct from Avro/ORC Hive tables\n\nReading uniontypes by converting them into structs. Take type\n\"uniontype<int, double>\" as an example:\n1. It will be regarded as \"struct<tag int, field0 int, field1 string>\"\n2. Data {1: 'hello'}, {0: 312}, {1: 'world'} will be read as [1, NULL,\n   'hello'], [0, 312, NULL], [1, NULL, 'world']\n\nWriting into uniontypes remains unsupported."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI1ODQxOTU1", "url": "https://github.com/trinodb/trino/pull/3483#pullrequestreview-425841955", "createdAt": "2020-06-07T18:15:38Z", "commit": {"oid": "2b4f2ad9851e3a0d1c0939274d5deaf16266e11b"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QxODoxNTozOFrOGgLCQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QxODoxNTozOFrOGgLCQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjM4ODQxNg==", "bodyText": "Ah, yes.  This is the same problem we had with unnest.  In that case, we scanned the block for a null and if present, we used that; otherwise we copied.  We can leave this for now.", "url": "https://github.com/trinodb/trino/pull/3483#discussion_r436388416", "createdAt": "2020-06-07T18:15:38Z", "author": {"login": "dain"}, "path": "presto-orc/src/main/java/io/prestosql/orc/reader/UnionColumnReader.java", "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.orc.reader;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.io.Closer;\n+import io.prestosql.memory.context.AggregatedMemoryContext;\n+import io.prestosql.orc.OrcBlockFactory;\n+import io.prestosql.orc.OrcColumn;\n+import io.prestosql.orc.OrcCorruptionException;\n+import io.prestosql.orc.metadata.ColumnEncoding;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.stream.BooleanInputStream;\n+import io.prestosql.orc.stream.ByteInputStream;\n+import io.prestosql.orc.stream.InputStreamSource;\n+import io.prestosql.orc.stream.InputStreamSources;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.block.BlockBuilder;\n+import io.prestosql.spi.block.ByteArrayBlock;\n+import io.prestosql.spi.block.LazyBlock;\n+import io.prestosql.spi.block.LazyBlockLoader;\n+import io.prestosql.spi.block.RowBlock;\n+import io.prestosql.spi.block.RunLengthEncodedBlock;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.Type;\n+import org.openjdk.jol.info.ClassLayout;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.ZoneId;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static com.google.common.base.MoreObjects.toStringHelper;\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Verify.verify;\n+import static io.prestosql.orc.metadata.Stream.StreamKind.DATA;\n+import static io.prestosql.orc.metadata.Stream.StreamKind.PRESENT;\n+import static io.prestosql.orc.reader.ColumnReaders.createColumnReader;\n+import static io.prestosql.orc.reader.ReaderUtils.verifyStreamType;\n+import static io.prestosql.orc.stream.MissingInputStreamSource.missingStreamSource;\n+import static io.prestosql.spi.type.TinyintType.TINYINT;\n+import static java.util.Objects.requireNonNull;\n+\n+// Use row blocks to represent union objects when reading\n+public class UnionColumnReader\n+        implements ColumnReader\n+{\n+    private static final int INSTANCE_SIZE = ClassLayout.parseClass(UnionColumnReader.class).instanceSize();\n+\n+    private final OrcColumn column;\n+    private final OrcBlockFactory blockFactory;\n+\n+    private final RowType type;\n+    private final List<ColumnReader> fieldReaders;\n+\n+    private int readOffset;\n+    private int nextBatchSize;\n+\n+    private InputStreamSource<BooleanInputStream> presentStreamSource = missingStreamSource(BooleanInputStream.class);\n+    private InputStreamSource<ByteInputStream> dataStreamSource = missingStreamSource(ByteInputStream.class);\n+    @Nullable\n+    private BooleanInputStream presentStream;\n+    @Nullable\n+    private ByteInputStream dataStream;\n+\n+    private boolean rowGroupOpen;\n+\n+    UnionColumnReader(Type type, OrcColumn column, AggregatedMemoryContext systemMemoryContext, OrcBlockFactory blockFactory)\n+            throws OrcCorruptionException\n+    {\n+        requireNonNull(type, \"type is null\");\n+        verifyStreamType(column, type, RowType.class::isInstance);\n+        this.type = (RowType) type;\n+\n+        this.column = requireNonNull(column, \"column is null\");\n+        this.blockFactory = requireNonNull(blockFactory, \"blockFactory is null\");\n+\n+        ImmutableList.Builder<ColumnReader> fieldReadersBuilder = ImmutableList.builder();\n+        List<OrcColumn> fields = column.getNestedColumns();\n+        for (int i = 0; i < fields.size(); i++) {\n+            fieldReadersBuilder.add(createColumnReader(type.getTypeParameters().get(i + 1), fields.get(i), systemMemoryContext, blockFactory));\n+        }\n+        fieldReaders = fieldReadersBuilder.build();\n+    }\n+\n+    @Override\n+    public void prepareNextRead(int batchSize)\n+    {\n+        readOffset += nextBatchSize;\n+        nextBatchSize = batchSize;\n+    }\n+\n+    @Override\n+    public Block readBlock()\n+            throws IOException\n+    {\n+        if (!rowGroupOpen) {\n+            openRowGroup();\n+        }\n+\n+        if (readOffset > 0) {\n+            if (presentStream != null) {\n+                readOffset = presentStream.countBitsSet(readOffset);\n+            }\n+            if (readOffset > 0) {\n+                if (dataStream == null) {\n+                    throw new OrcCorruptionException(column.getOrcDataSourceId(), \"Value is not null but data stream is missing\");\n+                }\n+                int[] readOffsets = new int[fieldReaders.size()];\n+                for (byte tag : dataStream.next(readOffset)) {\n+                    readOffsets[tag]++;\n+                }\n+                for (int i = 0; i < fieldReaders.size(); i++) {\n+                    fieldReaders.get(i).prepareNextRead(readOffsets[i]);\n+                }\n+            }\n+        }\n+\n+        boolean[] nullVector = null;\n+        Block[] blocks;\n+\n+        if (presentStream == null) {\n+            blocks = getBlocks(nextBatchSize);\n+        }\n+        else {\n+            nullVector = new boolean[nextBatchSize];\n+            int nullValues = presentStream.getUnsetBits(nextBatchSize, nullVector);\n+            if (nullValues != nextBatchSize) {\n+                blocks = getBlocks(nextBatchSize - nullValues);\n+            }\n+            else {\n+                List<Type> typeParameters = type.getTypeParameters();\n+                blocks = new Block[typeParameters.size() + 1];\n+                blocks[0] = TINYINT.createBlockBuilder(null, 0).build();\n+                for (int i = 0; i < typeParameters.size(); i++) {\n+                    blocks[i + 1] = typeParameters.get(i).createBlockBuilder(null, 0).build();\n+                }\n+            }\n+        }\n+\n+        verify(Arrays.stream(blocks)\n+                .mapToInt(Block::getPositionCount)\n+                .distinct()\n+                .count() == 1);\n+\n+        Block rowBlock = RowBlock.fromFieldBlocks(nextBatchSize, Optional.ofNullable(nullVector), blocks);\n+\n+        readOffset = 0;\n+        nextBatchSize = 0;\n+\n+        return rowBlock;\n+    }\n+\n+    private void openRowGroup()\n+            throws IOException\n+    {\n+        presentStream = presentStreamSource.openStream();\n+        dataStream = dataStreamSource.openStream();\n+\n+        rowGroupOpen = true;\n+    }\n+\n+    @Override\n+    public void startStripe(ZoneId fileTimeZone, ZoneId storageTimeZone, InputStreamSources dictionaryStreamSources, ColumnMetadata<ColumnEncoding> encoding)\n+            throws IOException\n+    {\n+        presentStreamSource = missingStreamSource(BooleanInputStream.class);\n+        dataStreamSource = missingStreamSource(ByteInputStream.class);\n+\n+        readOffset = 0;\n+        nextBatchSize = 0;\n+\n+        presentStream = null;\n+        dataStream = null;\n+\n+        rowGroupOpen = false;\n+\n+        for (ColumnReader fieldReader : fieldReaders) {\n+            fieldReader.startStripe(fileTimeZone, storageTimeZone, dictionaryStreamSources, encoding);\n+        }\n+    }\n+\n+    @Override\n+    public void startRowGroup(InputStreamSources dataStreamSources)\n+            throws IOException\n+    {\n+        presentStreamSource = dataStreamSources.getInputStreamSource(column, PRESENT, BooleanInputStream.class);\n+        dataStreamSource = dataStreamSources.getInputStreamSource(column, DATA, ByteInputStream.class);\n+\n+        readOffset = 0;\n+        nextBatchSize = 0;\n+\n+        presentStream = null;\n+        dataStream = null;\n+\n+        rowGroupOpen = false;\n+\n+        for (ColumnReader fieldReader : fieldReaders) {\n+            fieldReader.startRowGroup(dataStreamSources);\n+        }\n+    }\n+\n+    @Override\n+    public String toString()\n+    {\n+        return toStringHelper(this)\n+                .addValue(column)\n+                .toString();\n+    }\n+\n+    private Block[] getBlocks(int positionCount)\n+            throws IOException\n+    {\n+        if (dataStream == null) {\n+            throw new OrcCorruptionException(column.getOrcDataSourceId(), \"Value is not null but data stream is missing\");\n+        }\n+\n+        Block[] blocks = new Block[fieldReaders.size() + 1];\n+\n+        byte[] tags = dataStream.next(positionCount);\n+        blocks[0] = new ByteArrayBlock(positionCount, Optional.empty(), tags);\n+\n+        boolean[][] valueIsNonNull = new boolean[fieldReaders.size()][positionCount];\n+        int[] nonNullValueCount = new int[fieldReaders.size()];\n+        for (int i = 0; i < positionCount; i++) {\n+            valueIsNonNull[tags[i]][i] = true;\n+            nonNullValueCount[tags[i]]++;\n+        }\n+\n+        for (int i = 0; i < fieldReaders.size(); i++) {\n+            Type fieldType = type.getTypeParameters().get(i + 1);\n+            if (nonNullValueCount[i] > 0) {\n+                ColumnReader reader = fieldReaders.get(i);\n+                reader.prepareNextRead(nonNullValueCount[i]);\n+                Block rawBlock = blockFactory.createBlock(nonNullValueCount[i], reader::readBlock, true);\n+                blocks[i + 1] = new LazyBlock(positionCount, new UnpackLazyBlockLoader(rawBlock, fieldType, valueIsNonNull[i]));\n+            }\n+            else {\n+                blocks[i + 1] = new RunLengthEncodedBlock(\n+                        fieldType.createBlockBuilder(null, 1).appendNull().build(),\n+                        positionCount);\n+            }\n+        }\n+        return blocks;\n+    }\n+\n+    @Override\n+    public void close()\n+    {\n+        try (Closer closer = Closer.create()) {\n+            for (ColumnReader structField : fieldReaders) {\n+                closer.register(structField::close);\n+            }\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    @Override\n+    public long getRetainedSizeInBytes()\n+    {\n+        long retainedSizeInBytes = INSTANCE_SIZE;\n+        for (ColumnReader structField : fieldReaders) {\n+            retainedSizeInBytes += structField.getRetainedSizeInBytes();\n+        }\n+        return retainedSizeInBytes;\n+    }\n+\n+    private static final class UnpackLazyBlockLoader\n+            implements LazyBlockLoader\n+    {\n+        private final Block denseBlock;\n+        private final Type type;\n+        private final boolean[] valueIsNonNull;\n+\n+        public UnpackLazyBlockLoader(Block denseBlock, Type type, boolean[] valueIsNonNull)\n+        {\n+            this.denseBlock = requireNonNull(denseBlock, \"denseBlock is null\");\n+            this.type = requireNonNull(type, \"type is null\");\n+            this.valueIsNonNull = requireNonNull(valueIsNonNull, \"valueIsNonNull\");\n+        }\n+\n+        @Override\n+        public Block load()\n+        {\n+            Block loadedDenseBlock = denseBlock.getLoadedBlock();\n+            BlockBuilder unpackedBlock = type.createBlockBuilder(null, valueIsNonNull.length);\n+\n+            int denseBlockPosition = 0;\n+            for (boolean isNonNull : valueIsNonNull) {\n+                if (isNonNull) {\n+                    type.appendTo(loadedDenseBlock, denseBlockPosition++, unpackedBlock);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDQxMTkwOA=="}, "originalCommit": null, "originalPosition": 308}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1584, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}