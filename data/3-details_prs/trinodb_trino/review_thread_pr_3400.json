{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAxNzM3MDIx", "number": 3400, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQyMDowNjo0MlrOD5JTMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDo0Nzo1OVrOD5kqnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMjQ3NzkzOnYy", "diffSide": "RIGHT", "path": "presto-parquet/src/main/java/io/prestosql/parquet/writer/ParquetWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQyMDowNjo0MlrOGQQNUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNFQwMzo0MjoyNVrOGZseIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY5NTk1Mg==", "bodyText": "Consider caching this value if it is expensive to calculate.  In the ORC writer, we update the cached value after each write operation is processed.", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r419695952", "createdAt": "2020-05-04T20:06:42Z", "author": {"login": "dain"}, "path": "presto-parquet/src/main/java/io/prestosql/parquet/writer/ParquetWriter.java", "diffHunk": "@@ -85,6 +90,16 @@ public ParquetWriter(OutputStream outputStream, List<String> columnNames, List<T\n         this.chunkMaxLogicalBytes = max(1, writerOption.getMaxBlockSize() / 2);\n     }\n \n+    public long getWrittenBytes()\n+    {\n+        return outputStream.size();\n+    }\n+\n+    public long getBufferedBytes()\n+    {\n+        return columnWriters.stream().mapToLong(ColumnWriter::getBufferedBytes).sum();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5NjE5Mg==", "bodyText": "I resolved this. Also, I don't quite understand why in OrcWriter:\n    @Override\n    public long getWrittenBytes()\n    {\n        return orcWriter.getWrittenBytes() + orcWriter.getBufferedBytes();\n    }\n\n\nwhy bufferedBytes is part of written bytes?", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r429596192", "createdAt": "2020-05-24T03:42:25Z", "author": {"login": "qqibrow"}, "path": "presto-parquet/src/main/java/io/prestosql/parquet/writer/ParquetWriter.java", "diffHunk": "@@ -85,6 +90,16 @@ public ParquetWriter(OutputStream outputStream, List<String> columnNames, List<T\n         this.chunkMaxLogicalBytes = max(1, writerOption.getMaxBlockSize() / 2);\n     }\n \n+    public long getWrittenBytes()\n+    {\n+        return outputStream.size();\n+    }\n+\n+    public long getBufferedBytes()\n+    {\n+        return columnWriters.stream().mapToLong(ColumnWriter::getBufferedBytes).sum();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY5NTk1Mg=="}, "originalCommit": null, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxNjg0MDUxOnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveModule.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDoxMjoxMlrOGQ5yWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDoxMjoxMlrOGQ5yWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM3NzE3Nw==", "bodyText": "The commit message has a typo Add paruqet writer in hive module", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420377177", "createdAt": "2020-05-05T20:12:12Z", "author": {"login": "dain"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveModule.java", "diffHunk": "@@ -27,6 +27,7 @@\n import io.prestosql.plugin.hive.orc.OrcPageSourceFactory;\n import io.prestosql.plugin.hive.orc.OrcReaderConfig;\n import io.prestosql.plugin.hive.orc.OrcWriterConfig;\n+import io.prestosql.plugin.hive.parquet.ParquetFileWriterFactory;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxNjg4OTc3OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDoyNjo0M1rOGQ6QiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDoyNjo0M1rOGQ6QiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM4NDkwNA==", "bodyText": "Consider adding a toString for debugging.", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420384904", "createdAt": "2020-05-05T20:26:43Z", "author": {"login": "dain"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetFileWriter.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.parquet;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.parquet.writer.ParquetWriter;\n+import io.prestosql.parquet.writer.ParquetWriterOptions;\n+import io.prestosql.plugin.hive.FileWriter;\n+import io.prestosql.spi.Page;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.block.Block;\n+import io.prestosql.spi.block.BlockBuilder;\n+import io.prestosql.spi.block.RunLengthEncodedBlock;\n+import io.prestosql.spi.type.Type;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.openjdk.jol.info.ClassLayout;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+\n+import static io.prestosql.plugin.hive.HiveErrorCode.HIVE_WRITER_CLOSE_ERROR;\n+import static io.prestosql.plugin.hive.HiveErrorCode.HIVE_WRITER_DATA_ERROR;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ParquetFileWriter\n+        implements FileWriter\n+{\n+    private static final int INSTANCE_SIZE = ClassLayout.parseClass(ParquetFileWriter.class).instanceSize();\n+\n+    private final ParquetWriter parquetWriter;\n+    private final Callable<Void> rollbackAction;\n+    private final int[] fileInputColumnIndexes;\n+    private final List<Block> nullBlocks;\n+\n+    public ParquetFileWriter(\n+            OutputStream outputStream,\n+            Callable<Void> rollbackAction,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ParquetWriterOptions parquetWriterOptions,\n+            int[] fileInputColumnIndexes,\n+            CompressionCodecName compressionCodecName)\n+    {\n+        requireNonNull(outputStream, \"outputStream is null\");\n+\n+        this.parquetWriter = new ParquetWriter(\n+                outputStream,\n+                columnNames,\n+                fileColumnTypes,\n+                parquetWriterOptions,\n+                compressionCodecName);\n+\n+        this.rollbackAction = requireNonNull(rollbackAction, \"rollbackAction is null\");\n+        this.fileInputColumnIndexes = requireNonNull(fileInputColumnIndexes, \"fileInputColumnIndexes is null\");\n+\n+        ImmutableList.Builder<Block> nullBlocks = ImmutableList.builder();\n+        for (Type fileColumnType : fileColumnTypes) {\n+            BlockBuilder blockBuilder = fileColumnType.createBlockBuilder(null, 1, 0);\n+            blockBuilder.appendNull();\n+            nullBlocks.add(blockBuilder.build());\n+        }\n+        this.nullBlocks = nullBlocks.build();\n+    }\n+\n+    @Override\n+    public long getWrittenBytes()\n+    {\n+        return parquetWriter.getWrittenBytes();\n+    }\n+\n+    @Override\n+    public long getSystemMemoryUsage()\n+    {\n+        return INSTANCE_SIZE + parquetWriter.getRetainedBytes();\n+    }\n+\n+    @Override\n+    public void appendRows(Page dataPage)\n+    {\n+        Block[] blocks = new Block[fileInputColumnIndexes.length];\n+        for (int i = 0; i < fileInputColumnIndexes.length; i++) {\n+            int inputColumnIndex = fileInputColumnIndexes[i];\n+            if (inputColumnIndex < 0) {\n+                blocks[i] = new RunLengthEncodedBlock(nullBlocks.get(i), dataPage.getPositionCount());\n+            }\n+            else {\n+                blocks[i] = dataPage.getBlock(inputColumnIndex);\n+            }\n+        }\n+        Page page = new Page(dataPage.getPositionCount(), blocks);\n+        try {\n+            parquetWriter.write(page);\n+        }\n+        catch (IOException | UncheckedIOException e) {\n+            throw new PrestoException(HIVE_WRITER_DATA_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public void commit()\n+    {\n+        try {\n+            parquetWriter.close();\n+        }\n+        catch (IOException | UncheckedIOException e) {\n+            try {\n+                rollbackAction.call();\n+            }\n+            catch (Exception ignored) {\n+                // ignore\n+            }\n+            throw new PrestoException(HIVE_WRITER_CLOSE_ERROR, \"Error committing write parquet to Hive\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void rollback()\n+    {\n+        try {\n+            try {\n+                parquetWriter.close();\n+            }\n+            finally {\n+                rollbackAction.call();\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(HIVE_WRITER_CLOSE_ERROR, \"Error rolling back write parquet to Hive\", e);\n+        }\n+    }\n+\n+    @Override\n+    public long getValidationCpuNanos()\n+    {\n+        return 0;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxNjg5NDM1OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetFileWriterFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDoyODoxNFrOGQ6Teg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDoyODoxNFrOGQ6Teg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM4NTY1OA==", "bodyText": "This should be in the previous commit", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420385658", "createdAt": "2020-05-05T20:28:14Z", "author": {"login": "dain"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -18,6 +18,7 @@\n import io.prestosql.plugin.hive.HdfsEnvironment;\n import io.prestosql.plugin.hive.HiveConfig;\n import io.prestosql.plugin.hive.HiveFileWriterFactory;\n+import io.prestosql.plugin.hive.HiveSessionProperties;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxNjg5NzA4OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetWriterConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDoyODo1N1rOGQ6VHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDoyODo1N1rOGQ6VHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM4NjA3OA==", "bodyText": "Capitalize Parquet in description", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420386078", "createdAt": "2020-05-05T20:28:57Z", "author": {"login": "dain"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetWriterConfig.java", "diffHunk": "@@ -47,6 +50,19 @@ public ParquetWriterConfig setPageSize(DataSize pageSize)\n         return this;\n     }\n \n+    public boolean isParquetOptimizedWriterEnabled()\n+    {\n+        return parquetOptimizedWriterEnabled;\n+    }\n+\n+    @Config(\"hive.parquet.optimized-writer.enabled\")\n+    @ConfigDescription(\"Enable optimized parquet writer\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxNjkwMDQxOnYy", "diffSide": "RIGHT", "path": "presto-parquet/pom.xml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDoyOTo1MVrOGQ6XEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDoyOTo1MVrOGQ6XEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM4NjU3Nw==", "bodyText": "This should be in the commit Support getRetainedBytes in ParquetWriter", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420386577", "createdAt": "2020-05-05T20:29:51Z", "author": {"login": "dain"}, "path": "presto-parquet/pom.xml", "diffHunk": "@@ -82,6 +82,12 @@\n             <artifactId>slice</artifactId>\n         </dependency>\n \n+        <dependency>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxNjkwNTA1OnYy", "diffSide": "RIGHT", "path": "presto-parquet/pom.xml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDozMTowOFrOGQ6Z8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDozMTowOFrOGQ6Z8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM4NzMxNA==", "bodyText": "I don't think this scope is correct.  In the ORC module it not provided.", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420387314", "createdAt": "2020-05-05T20:31:08Z", "author": {"login": "dain"}, "path": "presto-parquet/pom.xml", "diffHunk": "@@ -82,6 +82,12 @@\n             <artifactId>slice</artifactId>\n         </dependency>\n \n+        <dependency>\n+            <groupId>org.openjdk.jol</groupId>\n+            <artifactId>jol-core</artifactId>\n+            <scope>provided</scope>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxNjk0MzU1OnYy", "diffSide": "RIGHT", "path": "presto-parquet/src/main/java/io/prestosql/parquet/writer/PrimitiveColumnWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDo0Mjo0NlrOGQ6yHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDo0Mjo0NlrOGQ6yHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM5MzUwMA==", "bodyText": "I would squash this into the commit that added getBufferedBytes.  I think it was Expose written bytes and buffered bytes in ParquetWriter", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420393500", "createdAt": "2020-05-05T20:42:46Z", "author": {"login": "dain"}, "path": "presto-parquet/src/main/java/io/prestosql/parquet/writer/PrimitiveColumnWriter.java", "diffHunk": "@@ -296,7 +296,10 @@ private void flushCurrentPageToBuffer()\n     @Override\n     public long getBufferedBytes()\n     {\n-        return pageBuffer.stream().mapToLong(ParquetDataOutput::size).sum() + definitionLevelEncoder.getBufferedSize() + repetitionLevelEncoder.getBufferedSize();\n+        return pageBuffer.stream().mapToLong(ParquetDataOutput::size).sum() +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxNjk2MTU5OnYy", "diffSide": "RIGHT", "path": "presto-parquet/src/main/java/io/prestosql/parquet/writer/PrimitiveColumnWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQyMDo0Nzo1OVrOGQ69IQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNFQwMzo1ODowN1rOGZsgnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM5NjMyMQ==", "bodyText": "Are there statistics for non-primitive types?", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r420396321", "createdAt": "2020-05-05T20:47:59Z", "author": {"login": "dain"}, "path": "presto-parquet/src/main/java/io/prestosql/parquet/writer/PrimitiveColumnWriter.java", "diffHunk": "@@ -102,6 +103,8 @@ public PrimitiveColumnWriter(Type type, ColumnDescriptor columnDescriptor, Primi\n         this.compressionCodec = requireNonNull(compressionCodecName, \"compressionCodecName is null\");\n         this.compressor = getCompressor(compressionCodecName);\n         this.pageSizeThreshold = pageSizeThreshold;\n+\n+        this.columnStatistics = Statistics.createStats(columnDescriptor.getPrimitiveType());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5NjgzMQ==", "bodyText": "No. This aims to fill the stats part for primitive columns https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift#L740 . There is no support for this on non-primitive types.", "url": "https://github.com/trinodb/trino/pull/3400#discussion_r429596831", "createdAt": "2020-05-24T03:58:07Z", "author": {"login": "qqibrow"}, "path": "presto-parquet/src/main/java/io/prestosql/parquet/writer/PrimitiveColumnWriter.java", "diffHunk": "@@ -102,6 +103,8 @@ public PrimitiveColumnWriter(Type type, ColumnDescriptor columnDescriptor, Primi\n         this.compressionCodec = requireNonNull(compressionCodecName, \"compressionCodecName is null\");\n         this.compressor = getCompressor(compressionCodecName);\n         this.pageSizeThreshold = pageSizeThreshold;\n+\n+        this.columnStatistics = Statistics.createStats(columnDescriptor.getPrimitiveType());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM5NjMyMQ=="}, "originalCommit": null, "originalPosition": 13}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 390, "cost": 1, "resetAt": "2021-11-13T12:10:21Z"}}}