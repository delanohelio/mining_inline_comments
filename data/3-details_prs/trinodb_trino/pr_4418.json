{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3NTA4MzY2", "number": 4418, "title": "Add Kafka Avro encoder", "bodyText": "Add AvroRowEncoder and AvroRowEncoderFactory\nAdd test case in io.prestosql.plugin.kafka.TestKafkaIntegrationSmokeTest#testRoundTripAllFormats", "createdAt": "2020-07-10T15:37:12Z", "url": "https://github.com/trinodb/trino/pull/4418", "merged": true, "mergeCommit": {"oid": "2aa3d75a5925a1c3326495452a870ef2f9778883"}, "closed": true, "closedAt": "2020-07-14T18:55:45Z", "author": {"login": "charlesjmorgan"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABczlzlVgBqjM1MzQ0ODQ5Mjc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc027zGAFqTQ0ODE2MjYyMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "64fb3324c583f01ee3948e02c30d28e54b9a76f1", "author": {"user": {"login": "charlesjmorgan", "name": "Charles Morgan"}}, "url": "https://github.com/trinodb/trino/commit/64fb3324c583f01ee3948e02c30d28e54b9a76f1", "committedDate": "2020-07-10T15:32:42Z", "message": "Add Kafka avro encoder and avro roundtrip test"}, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NTQ3NjE3", "url": "https://github.com/trinodb/trino/pull/4418#pullrequestreview-446547617", "createdAt": "2020-07-10T16:45:25Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQxNjo0NToyNVrOGv-QJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQxNjo1MTo1MlrOGv-ccQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NjE5OQ==", "bodyText": "Extract making RowEncoder Closable to separate commit.", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r452956199", "createdAt": "2020-07-10T16:45:25Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -15,7 +15,10 @@\n \n import io.prestosql.spi.block.Block;\n \n+import java.io.Closeable;\n+\n public interface RowEncoder\n+        extends Closeable", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1Njk0OQ==", "bodyText": "rename to byteArrayOutputStream", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r452956949", "createdAt": "2020-07-10T16:46:54Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/avro/AvroRowEncoder.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.avro;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"avro\";\n+\n+    private final ByteArrayOutputStream byteArrayOuts;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1ODU3NQ==", "bodyText": "do we need to call this one for each row?\nAnd if should we ignore DataFileWriter it returns?", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r452958575", "createdAt": "2020-07-10T16:50:19Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/avro/AvroRowEncoder.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.avro;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"avro\";\n+\n+    private final ByteArrayOutputStream byteArrayOuts;\n+    private final Schema parsedSchema;\n+    private final DataFileWriter<GenericRecord> dataFileWriter;\n+    private final GenericData.Record record;\n+\n+    public AvroRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles, Schema parsedSchema)\n+    {\n+        super(session, columnHandles);\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(columnHandle.getDataFormat() == null, \"Unexpected data format '%s' defined for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName());\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType(), columnHandle.getName());\n+        }\n+        this.byteArrayOuts = new ByteArrayOutputStream();\n+        this.parsedSchema = requireNonNull(parsedSchema, \"parsedSchema is null\");\n+        this.dataFileWriter = new DataFileWriter<>(new GenericDatumWriter<>(this.parsedSchema));\n+        this.record = new GenericData.Record(this.parsedSchema);\n+    }\n+\n+    private boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), null);\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        // make sure entire row has been updated with new values\n+        checkArgument(currentColumnIndex == columnHandles.size(), format(\"Missing %d columns\", columnHandles.size() - currentColumnIndex + 1));\n+\n+        try {\n+            byteArrayOuts.reset();\n+            dataFileWriter.create(parsedSchema, byteArrayOuts);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1OTM0NQ==", "bodyText": "can we write more than single row in the test?\n(this actually applies to other formats too)", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r452959345", "createdAt": "2020-07-10T16:51:52Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -141,6 +141,10 @@ public void testRoundTripAllFormats(RoundTripTestCase testCase)\n     private List<RoundTripTestCase> testRoundTripAllFormatsData()\n     {\n         return ImmutableList.<RoundTripTestCase>builder()\n+                .add(new RoundTripTestCase(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NjgzNzQy", "url": "https://github.com/trinodb/trino/pull/4418#pullrequestreview-446683742", "createdAt": "2020-07-10T20:31:02Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMDozMTowM1rOGwEzcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMDozNToyN1rOGwE57w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA2MzUzNg==", "bodyText": "This is not a pretty API.  It looks like this call gets the writer ready to write to a new output stream with a new schema.  Feels like a factory method, but it just resets the instance.\nAnd without the schema registry integration, writing Avro messages are quite heavyweight (each message contains the schema).", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r453063536", "createdAt": "2020-07-10T20:31:03Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/avro/AvroRowEncoder.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.avro;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"avro\";\n+\n+    private final ByteArrayOutputStream byteArrayOuts;\n+    private final Schema parsedSchema;\n+    private final DataFileWriter<GenericRecord> dataFileWriter;\n+    private final GenericData.Record record;\n+\n+    public AvroRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles, Schema parsedSchema)\n+    {\n+        super(session, columnHandles);\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(columnHandle.getDataFormat() == null, \"Unexpected data format '%s' defined for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName());\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType(), columnHandle.getName());\n+        }\n+        this.byteArrayOuts = new ByteArrayOutputStream();\n+        this.parsedSchema = requireNonNull(parsedSchema, \"parsedSchema is null\");\n+        this.dataFileWriter = new DataFileWriter<>(new GenericDatumWriter<>(this.parsedSchema));\n+        this.record = new GenericData.Record(this.parsedSchema);\n+    }\n+\n+    private boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), null);\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        // make sure entire row has been updated with new values\n+        checkArgument(currentColumnIndex == columnHandles.size(), format(\"Missing %d columns\", columnHandles.size() - currentColumnIndex + 1));\n+\n+        try {\n+            byteArrayOuts.reset();\n+            dataFileWriter.create(parsedSchema, byteArrayOuts);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1ODU3NQ=="}, "originalCommit": null, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA2NTE5OQ==", "bodyText": "Nothing wrong with this, but it didn't seem necessary.  ByteArrayOutputStream.close() is really a no-op and I don't think the Avro writer holds on to any resources.", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r453065199", "createdAt": "2020-07-10T20:35:27Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/avro/AvroRowEncoder.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.avro;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"avro\";\n+\n+    private final ByteArrayOutputStream byteArrayOuts;\n+    private final Schema parsedSchema;\n+    private final DataFileWriter<GenericRecord> dataFileWriter;\n+    private final GenericData.Record record;\n+\n+    public AvroRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles, Schema parsedSchema)\n+    {\n+        super(session, columnHandles);\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(columnHandle.getDataFormat() == null, \"Unexpected data format '%s' defined for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName());\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType(), columnHandle.getName());\n+        }\n+        this.byteArrayOuts = new ByteArrayOutputStream();\n+        this.parsedSchema = requireNonNull(parsedSchema, \"parsedSchema is null\");\n+        this.dataFileWriter = new DataFileWriter<>(new GenericDatumWriter<>(this.parsedSchema));\n+        this.record = new GenericData.Record(this.parsedSchema);\n+    }\n+\n+    private boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), null);\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        // make sure entire row has been updated with new values\n+        checkArgument(currentColumnIndex == columnHandles.size(), format(\"Missing %d columns\", columnHandles.size() - currentColumnIndex + 1));\n+\n+        try {\n+            byteArrayOuts.reset();\n+            dataFileWriter.create(parsedSchema, byteArrayOuts);\n+            dataFileWriter.append(record);\n+            dataFileWriter.flush();\n+\n+            resetColumnIndex(); // reset currentColumnIndex to prepare for next row\n+            return byteArrayOuts.toByteArray();\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(\"Failed to append record\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void close()\n+    {\n+        try {\n+            byteArrayOuts.close();\n+            dataFileWriter.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 155}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3OTQ3MjI4", "url": "https://github.com/trinodb/trino/pull/4418#pullrequestreview-447947228", "createdAt": "2020-07-14T09:35:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwOTozNTozOVrOGxMAsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwOTo0MDo0MVrOGxML0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDIzMDE5Mg==", "bodyText": "Sorry for not being precise.\nI meant extracting the interface chage to separate commit which is before one which add Avro encoder.\nThen AvroRowEncoder.close() can be added together with rest of Avro encoder code.", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r454230192", "createdAt": "2020-07-14T09:35:39Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -15,7 +15,10 @@\n \n import io.prestosql.spi.block.Block;\n \n+import java.io.Closeable;\n+\n public interface RowEncoder\n+        extends Closeable", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NjE5OQ=="}, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDIzMzA0Mg==", "bodyText": "s/avro/Avro/", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r454233042", "createdAt": "2020-07-14T09:40:41Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/avro/AvroRowEncoderFactory.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.avro;\n+\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.plugin.kafka.encoder.RowEncoderFactory;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import org.apache.avro.Schema;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroRowEncoderFactory\n+        implements RowEncoderFactory\n+{\n+    @Override\n+    public RowEncoder create(ConnectorSession session, Optional<String> dataSchema, List<EncoderColumnHandle> columnHandles)\n+    {\n+        checkArgument(dataSchema.isPresent(), \"dataSchema for avro format is not present\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "88a95f1362f1135eb6a5eb9f9ab310472060ce22", "author": {"user": {"login": "charlesjmorgan", "name": "Charles Morgan"}}, "url": "https://github.com/trinodb/trino/commit/88a95f1362f1135eb6a5eb9f9ab310472060ce22", "committedDate": "2020-07-14T14:24:48Z", "message": "Make Kafka RowEncoder Closeable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "37ccd7bcd37d6e9c791459210f2c6eb9bd165b05", "author": {"user": {"login": "charlesjmorgan", "name": "Charles Morgan"}}, "url": "https://github.com/trinodb/trino/commit/37ccd7bcd37d6e9c791459210f2c6eb9bd165b05", "committedDate": "2020-07-14T14:26:58Z", "message": "Add Kafka Avro encoder"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7d2b2fde14166235feaaf37a661fd09633ff3372", "author": {"user": {"login": "charlesjmorgan", "name": "Charles Morgan"}}, "url": "https://github.com/trinodb/trino/commit/7d2b2fde14166235feaaf37a661fd09633ff3372", "committedDate": "2020-07-14T14:27:04Z", "message": "Add row to Kafka round trip test"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "7d2b2fde14166235feaaf37a661fd09633ff3372", "author": {"user": {"login": "charlesjmorgan", "name": "Charles Morgan"}}, "url": "https://github.com/trinodb/trino/commit/7d2b2fde14166235feaaf37a661fd09633ff3372", "committedDate": "2020-07-14T14:27:04Z", "message": "Add row to Kafka round trip test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ4MTYyNjIw", "url": "https://github.com/trinodb/trino/pull/4418#pullrequestreview-448162620", "createdAt": "2020-07-14T14:30:20Z", "commit": {"oid": "7d2b2fde14166235feaaf37a661fd09633ff3372"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 71, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}