{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY3NzkzMzcw", "number": 4832, "title": "Iceberg connector support for Materialized Views", "bodyText": "This PR is for Iceberg connector changes to support materialized views. Note that only the top commit (\n25678de) needs to be reviewed as a part of this PR. Other commits are being reviewed here: #3283", "createdAt": "2020-08-14T06:12:55Z", "url": "https://github.com/trinodb/trino/pull/4832", "merged": true, "mergeCommit": {"oid": "a8f94f02eb8842ea68cc975e7ff31ab95c3efb8f"}, "closed": true, "closedAt": "2020-11-09T21:42:01Z", "author": {"login": "anjalinorwood"}, "timelineItems": {"totalCount": 33, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc_UNJFABqjM2NTg5NjU0MTM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdYUP_TgBqjM5NDU0NTc0NTE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1MzYwMTYx", "url": "https://github.com/trinodb/trino/pull/4832#pullrequestreview-485360161", "createdAt": "2020-09-09T19:59:27Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxOTo1OToyN1rOHPYJcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMDoxMzo1NFrOHPYsLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg4NjMyMQ==", "bodyText": "This sequence of handling where the insert (including the table scans of source tables is planned), insert performed and then the metadata (snapshots of tables) is captured can lead to metadata and data getting out of sync. e.g. if an insert was performed on the source table in between the insert on target table (say using snapshot S1 of source table) and getting snapshot id (say, returns S2 since an insert happened on the source table), the metadata is fresh (shows S2) and will indicate the materialized view (MV) is fresh while the data in MV is actually stale (has data corresponding to S1). Here is how we can keep the data and metadata in sync.\n\nMake transaction metadata object available to IcebergMetadata\nIn BeginRefreshMaterializedView, get snapshot-ids of all source tables and store them in transaction metadata. (In our problem example, this is S1)\nPopulate TableHandle for each source table using the snapshot-ids from transaction metadata in 'getSplits'. This way, GetTableHandle will return the table as of the given snapshot to planner for planning. (S1)\nFinishRefreshMaterializedView writes the snapshot-ids from transaction metadata as MV metadata (S1) ensuring the data and the metadata correspond to the same snapshot (S1).\n\nNote that the algorithm above does not ensure that the end of refresh the MV has the latest data/metadata. MV may still be stale. Future refresh can update the MV to fresh state.\nAlso note that keeping metadata and data in sync ensures that incremental refresh (especially row-level) would work correctly.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r485886321", "createdAt": "2020-09-09T19:59:27Z", "author": {"login": "anjalinorwood"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +697,332 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+            appendFiles.commit();\n+        }\n+\n+        // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+        UpdateProperties updateProperties = transaction.updateProperties();\n+        Map<String, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle.toString(), token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle.toString(), (long) -1);\n+            }\n+        }\n+        String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(tableSnapshotIdMap);\n+        updateProperties.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+        updateProperties.commit();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 320}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5NTIxMw==", "bodyText": "This code stores the dependsOnTables metadata in the table properties of the Iceberg storage table. This solution works since the data insert and updating the table property is done in a transaction.\nAn alternative way is to store the dependsOnTables metadata as snapshot metadata. The snapshot data and metadata are written at the same time. An additional advantage is that this leaves enough bread-crumbs such that if the materialized view is rolled back to a particular snapshot,  snapshot metadata has information on which snapshots of source tables were used to compute that snapshot of MV. This can be useful for debugging purposes.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r485895213", "createdAt": "2020-09-09T20:13:54Z", "author": {"login": "anjalinorwood"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +697,332 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+            appendFiles.commit();\n+        }\n+\n+        // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+        UpdateProperties updateProperties = transaction.updateProperties();\n+        Map<String, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle.toString(), token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle.toString(), (long) -1);\n+            }\n+        }\n+        String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(tableSnapshotIdMap);\n+        updateProperties.set(DEPENDS_ON_TABLES, tableSnapshotIDString);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 319}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg4MjE5Mzc2", "url": "https://github.com/trinodb/trino/pull/4832#pullrequestreview-488219376", "createdAt": "2020-09-14T23:20:17Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 26, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQyMzoyMDoxN1rOHRq5Lg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwMDozNjoxNVrOHRsQoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI5MDYwNg==", "bodyText": "Why is this in the SPI? It's not used by anything", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488290606", "createdAt": "2020-09-14T23:20:17Z", "author": {"login": "electrum"}, "path": "presto-spi/src/main/java/io/prestosql/spi/connector/TableToken.java", "diffHunk": "@@ -0,0 +1,18 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.spi.connector;\n+\n+public interface TableToken", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI5MTY5Nw==", "bodyText": "st_ might be better here. Or perhaps z_ so that they sort to the end?", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488291697", "createdAt": "2020-09-14T23:24:01Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 162}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI5Mzg4MA==", "bodyText": "Why is replace special? If we're going to store the comment, it seems like we should always do it.\nFor Presto views in Hive, we always use \"Presto View\" since these comments are not useful to non-Presto users. The same logic probably applies to Iceberg. Having a dummy comment in the Hive view will discourage users from touching it or thinking they can change the comment there.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488293880", "createdAt": "2020-09-14T23:30:42Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI5NDAzNA==", "bodyText": "Use an ImmutableMap builder here.\nNaming nit: we avoid abbreviating variables, so spell out \"properties\". This could be called propertyBuilder", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488294034", "createdAt": "2020-09-14T23:31:12Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwMzIxNQ==", "bodyText": "Pass transaction to the create() call so that the field can be final", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488303215", "createdAt": "2020-09-15T00:01:50Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergConnector.java", "diffHunk": "@@ -180,9 +179,11 @@ public ConnectorAccessControl getAccessControl()\n     public ConnectorTransactionHandle beginTransaction(IsolationLevel isolationLevel, boolean readOnly)\n     {\n         checkConnectorSupports(READ_COMMITTED, isolationLevel);\n-        ConnectorTransactionHandle transaction = new HiveTransactionHandle();\n+        IcebergTransactionHandle transaction = new IcebergTransactionHandle();\n         try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(getClass().getClassLoader())) {\n-            transactionManager.put(transaction, metadataFactory.create());\n+            IcebergMetadata icebergMetadata = metadataFactory.create();\n+            icebergMetadata.setTransactionHandle(transaction);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwMzUxOA==", "bodyText": "Drop the \"A\" from the method name to match the common naming convention", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488303518", "createdAt": "2020-09-15T00:02:57Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 346}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwMzYzNA==", "bodyText": "As a general note, this can be moved to the field declaration so the field can be final\nprivate final Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488303634", "createdAt": "2020-09-15T00:03:22Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergTransactionHandle.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import io.prestosql.spi.connector.ConnectorTableHandle;\n+import io.prestosql.spi.connector.ConnectorTransactionHandle;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.UUID;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergTransactionHandle\n+        implements ConnectorTransactionHandle\n+{\n+    private final UUID uuid;\n+    private Map<ConnectorTableHandle, Long> tableSnapshotIdMap; // Used for refresh of a materialized view\n+\n+    public IcebergTransactionHandle()\n+    {\n+        this(UUID.randomUUID());\n+        this.tableSnapshotIdMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNTAzOA==", "bodyText": "The purpose of the transaction handle is so that the engine can associate the beginTransaction(), getMetadata() and commit() calls. However, it's just a handle and shouldn't the store.\nInstead, it's expected that the transaction state is stored inside IcebergMetadata. That's why IcebergConnector creates the metadata object once and stores it in a map using the handle.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488305038", "createdAt": "2020-09-15T00:08:17Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergTransactionHandle.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import io.prestosql.spi.connector.ConnectorTableHandle;\n+import io.prestosql.spi.connector.ConnectorTransactionHandle;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.UUID;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergTransactionHandle\n+        implements ConnectorTransactionHandle\n+{\n+    private final UUID uuid;\n+    private Map<ConnectorTableHandle, Long> tableSnapshotIdMap; // Used for refresh of a materialized view", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNTMwNg==", "bodyText": "Actually, if we move the state into IcebergMetadata, we shouldn't need the handle at all there (see other comment).", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488305306", "createdAt": "2020-09-15T00:09:17Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergConnector.java", "diffHunk": "@@ -180,9 +179,11 @@ public ConnectorAccessControl getAccessControl()\n     public ConnectorTransactionHandle beginTransaction(IsolationLevel isolationLevel, boolean readOnly)\n     {\n         checkConnectorSupports(READ_COMMITTED, isolationLevel);\n-        ConnectorTransactionHandle transaction = new HiveTransactionHandle();\n+        IcebergTransactionHandle transaction = new IcebergTransactionHandle();\n         try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(getClass().getClassLoader())) {\n-            transactionManager.put(transaction, metadataFactory.create());\n+            IcebergMetadata icebergMetadata = metadataFactory.create();\n+            icebergMetadata.setTransactionHandle(transaction);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwMzIxNQ=="}, "originalCommit": null, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNTQ0NQ==", "bodyText": "I think this should be MaterializedViewAlreadyExistsException", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488305445", "createdAt": "2020-09-15T00:09:52Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNTY0OA==", "bodyText": "This seems to be missing the \"not\" word in \"clause is specified\"\n// create command where materialized view already exists and IF NOT EXISTS clause is not specified", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488305648", "createdAt": "2020-09-15T00:10:29Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 195}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNzQ3MA==", "bodyText": "This logic seems wrong. If ignoreExisting is true, we will still throw MaterializedViewAlreadyExistsException below. Should we return here instead? Something like\nif (!replace && existing.isPresent()) {\n    if (ignoreExisting) {\n        return;\n    }\n    throw new MaterializedViewAlreadyExistsException(viewName);\n}", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488307470", "createdAt": "2020-09-15T00:16:44Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 196}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNzY5Nw==", "bodyText": "This can be removed if we handle it above", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488307697", "createdAt": "2020-09-15T00:17:27Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 222}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwODIwNg==", "bodyText": "How is OR REPLACE supposed to work in combination with IF NOT EXISTS? Why do we have both of these flags?\nWe don't seem to have any tests for IF NOT EXISTS (both with and without OR REPLACE)", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488308206", "createdAt": "2020-09-15T00:19:16Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNzQ3MA=="}, "originalCommit": null, "originalPosition": 196}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwOTY0MA==", "bodyText": "This should use the file format from the table properties\ngetFileFormat(definition.getProperties())", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488309640", "createdAt": "2020-09-15T00:24:10Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMDAxNw==", "bodyText": "Why modify the table properties here at all, rather than just passing them through?", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488310017", "createdAt": "2020-09-15T00:25:19Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwOTY0MA=="}, "originalCommit": null, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMDUzMQ==", "bodyText": "This can be chained\n.withStorage(storage ->\n        .setStorageFormat(VIEW_STORAGE_FORMAT)\n        .setLocation(\"\"))", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488310531", "createdAt": "2020-09-15T00:26:59Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 188}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMDc4NA==", "bodyText": "If we move the entire Table.Builder part down here, we could directly chain the whole thing\nTable table = Table.builder()\n        ...\n        .build();", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488310784", "createdAt": "2020-09-15T00:27:54Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMTIxMg==", "bodyText": "We shouldn't catch arbitrary checked exceptions. This should probably just be PrestoException.\nWhy do we want to ignore errors when dropping the storage table? It seems like we should log a warning, at a minimum.\ncatch (PrestoException e) {\n    log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s', storageTableName, viewName);\n}", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488311212", "createdAt": "2020-09-15T00:29:44Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 251}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMTk4NQ==", "bodyText": "This needs to use the correct Path for the file we are going to open. The purpose of HdfsEnvironment.getConfiguration() is to create a Configuration object with the right values for the file system we are going to use. If we don't", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488311985", "createdAt": "2020-09-15T00:32:29Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 260}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjMxNA==", "bodyText": "Use HdfsInputFile instead of HadoopInputFile. See finishInsert()", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488312314", "createdAt": "2020-09-15T00:33:46Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 322}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjYwOA==", "bodyText": "Nit: we only capitalize the first letter of words, even acronyms, so tableSnapshotIdString", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488312608", "createdAt": "2020-09-15T00:34:53Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 336}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjY5Mw==", "bodyText": "Same, drop the \"A\"", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488312693", "createdAt": "2020-09-15T00:35:10Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 356}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjc1Mw==", "bodyText": "Nit: wrap all arguments or none", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488312753", "createdAt": "2020-09-15T00:35:22Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 366}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjg1Mg==", "bodyText": "Use Guava Splitter", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488312852", "createdAt": "2020-09-15T00:35:42Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,\n+            List<io.prestosql.plugin.hive.metastore.Column> schema)\n+    {\n+        List<ConnectorMaterializedViewDefinition.Column> columns = new ArrayList<>();\n+        for (io.prestosql.plugin.hive.metastore.Column field : schema) {\n+            ConnectorMaterializedViewDefinition.Column column =\n+                    new ConnectorMaterializedViewDefinition.Column(field.getName(), field.getType().getType(typeManager).getTypeId());\n+            columns.add(column);\n+        }\n+        return columns;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        if (!isAMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Optional<Table> matViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!matViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        Table matView = matViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(matView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = matView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(matView.getOwner().replaceAll(\"@.*\", \"\")),\n+            new HashMap<>(matView.getParameters())));\n+    }\n+\n+    public Optional<io.prestosql.plugin.iceberg.TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        if (icebergTable.currentSnapshot() == null) {\n+            return Optional.empty();\n+        }\n+        else {\n+            return Optional.of(new io.prestosql.plugin.iceberg.TableToken(icebergTable.currentSnapshot().snapshotId()));\n+        }\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<io.prestosql.plugin.iceberg.TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<io.prestosql.plugin.iceberg.TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!currentToken.isPresent() && tableToken.isPresent() && tableToken.get().getSnapshotId() == -1) {\n+            return true;\n+        }\n+\n+        if (tableToken.isPresent() && currentToken.isPresent() &&\n+                tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId()) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public MaterializedViewFreshness getMaterializedViewFreshness(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        Map<String, Optional<io.prestosql.plugin.iceberg.TableToken>> refreshStateMap = getMaterializedViewToken(session, tableHandle);\n+        if (refreshStateMap.size() == 0) {\n+            return new MaterializedViewFreshness(false);\n+        }\n+\n+        for (Map.Entry<String, Optional<io.prestosql.plugin.iceberg.TableToken>> entry : refreshStateMap.entrySet()) {\n+            String[] strings = entry.getKey().split(\"\\\\.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 443}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjk5Mg==", "bodyText": "Use isEmpty() for collections instead of size() == 0", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r488312992", "createdAt": "2020-09-15T00:36:15Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,\n+            List<io.prestosql.plugin.hive.metastore.Column> schema)\n+    {\n+        List<ConnectorMaterializedViewDefinition.Column> columns = new ArrayList<>();\n+        for (io.prestosql.plugin.hive.metastore.Column field : schema) {\n+            ConnectorMaterializedViewDefinition.Column column =\n+                    new ConnectorMaterializedViewDefinition.Column(field.getName(), field.getType().getType(typeManager).getTypeId());\n+            columns.add(column);\n+        }\n+        return columns;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        if (!isAMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Optional<Table> matViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!matViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        Table matView = matViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(matView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = matView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(matView.getOwner().replaceAll(\"@.*\", \"\")),\n+            new HashMap<>(matView.getParameters())));\n+    }\n+\n+    public Optional<io.prestosql.plugin.iceberg.TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        if (icebergTable.currentSnapshot() == null) {\n+            return Optional.empty();\n+        }\n+        else {\n+            return Optional.of(new io.prestosql.plugin.iceberg.TableToken(icebergTable.currentSnapshot().snapshotId()));\n+        }\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<io.prestosql.plugin.iceberg.TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<io.prestosql.plugin.iceberg.TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!currentToken.isPresent() && tableToken.isPresent() && tableToken.get().getSnapshotId() == -1) {\n+            return true;\n+        }\n+\n+        if (tableToken.isPresent() && currentToken.isPresent() &&\n+                tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId()) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public MaterializedViewFreshness getMaterializedViewFreshness(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        Map<String, Optional<io.prestosql.plugin.iceberg.TableToken>> refreshStateMap = getMaterializedViewToken(session, tableHandle);\n+        if (refreshStateMap.size() == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 438}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkxMDI2MjMy", "url": "https://github.com/trinodb/trino/pull/4832#pullrequestreview-491026232", "createdAt": "2020-09-17T22:16:25Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMjoxNjoyNVrOHT3YgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMjoyMDo0OFrOHT3eyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5MjM4NA==", "bodyText": "Nit: the indentation is off here. Continuation indent should be 8 spaces", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490592384", "createdAt": "2020-09-17T22:16:25Z", "author": {"login": "electrum"}, "path": "presto-main/src/main/java/io/prestosql/sql/planner/optimizations/BeginTableWrite.java", "diffHunk": "@@ -189,7 +189,8 @@ private WriterTarget createWriterTarget(WriterTarget target)\n             }\n             if (target instanceof TableWriterNode.RefreshMaterializedViewReference) {\n                 TableWriterNode.RefreshMaterializedViewReference refreshMV = (TableWriterNode.RefreshMaterializedViewReference) target;\n-                return new TableWriterNode.RefreshMaterializedViewTarget(metadata.beginRefreshMaterializedView(session, refreshMV.getStorageTableHandle()),\n+                return new TableWriterNode.RefreshMaterializedViewTarget(\n+                    metadata.beginRefreshMaterializedView(session, refreshMV.getStorageTableHandle(), refreshMV.getSourceTableHandles()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5MzQ5MA==", "bodyText": "From what I can tell, the query for a materialized view can reference any connector. Passing arbitrary arbitrary table handles from other connectors is not useful (the connector can't do anything with them). It's also expected that a connector only gets their own table handles. Maybe we should filter this list to the connector and a boolean indicating that there are source tables for other connectors.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490593490", "createdAt": "2020-09-17T22:19:30Z", "author": {"login": "electrum"}, "path": "presto-spi/src/main/java/io/prestosql/spi/connector/ConnectorMetadata.java", "diffHunk": "@@ -454,7 +454,7 @@ default boolean supportsMissingColumnsOnInsert()\n     /**\n      * Begin materialized view query\n      */\n-    default ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    default ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5Mzk5NQ==", "bodyText": "This type of operation can be done with a stream (though we'll need to change it anyway so we filter out tables from other connectors)", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490593995", "createdAt": "2020-09-17T22:20:48Z", "author": {"login": "electrum"}, "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -847,13 +847,18 @@ public boolean supportsMissingColumnsOnInsert(Session session, TableHandle table\n     }\n \n     @Override\n-    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle)\n+    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle, List<TableHandle> sourceTableHandles)\n     {\n         CatalogName catalogName = tableHandle.getCatalogName();\n         CatalogMetadata catalogMetadata = getCatalogMetadataForWrite(session, catalogName);\n         ConnectorMetadata metadata = catalogMetadata.getMetadata();\n         ConnectorTransactionHandle transactionHandle = catalogMetadata.getTransactionHandleFor(catalogName);\n-        ConnectorInsertTableHandle handle = metadata.beginRefreshMaterializedView(session.toConnectorSession(catalogName), tableHandle.getConnectorHandle());\n+        List<ConnectorTableHandle> sourceConnectorHandles = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkxMDM5ODA0", "url": "https://github.com/trinodb/trino/pull/4832#pullrequestreview-491039804", "createdAt": "2020-09-17T22:47:31Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMjo0NzozMVrOHT4E3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMzowOTowNFrOHT4fjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwMzc0Mw==", "bodyText": "Don't abbreviate variable names", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490603743", "createdAt": "2020-09-17T22:47:31Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,\n+            List<io.prestosql.plugin.hive.metastore.Column> schema)\n+    {\n+        List<ConnectorMaterializedViewDefinition.Column> columns = new ArrayList<>();\n+        for (io.prestosql.plugin.hive.metastore.Column field : schema) {\n+            ConnectorMaterializedViewDefinition.Column column =\n+                    new ConnectorMaterializedViewDefinition.Column(field.getName(), field.getType().getType(typeManager).getTypeId());\n+            columns.add(column);\n+        }\n+        return columns;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        if (!isAMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Optional<Table> matViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!matViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        Table matView = matViewOptional.get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 389}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwMzg2Ng==", "bodyText": "Why? We shouldn't change the owner stored in the view", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490603866", "createdAt": "2020-09-17T22:48:01Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,\n+            List<io.prestosql.plugin.hive.metastore.Column> schema)\n+    {\n+        List<ConnectorMaterializedViewDefinition.Column> columns = new ArrayList<>();\n+        for (io.prestosql.plugin.hive.metastore.Column field : schema) {\n+            ConnectorMaterializedViewDefinition.Column column =\n+                    new ConnectorMaterializedViewDefinition.Column(field.getName(), field.getType().getType(typeManager).getTypeId());\n+            columns.add(column);\n+        }\n+        return columns;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        if (!isAMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Optional<Table> matViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!matViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        Table matView = matViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(matView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = matView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(matView.getOwner().replaceAll(\"@.*\", \"\")),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 402}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwNDk4NQ==", "bodyText": "This can be\nreturn Optional.ofNullable(icebergTable.currentSnapshot())\n        .map(snapshot -> new TableToken(snapshot.snapshotId()));", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490604985", "createdAt": "2020-09-17T22:51:26Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,\n+            List<io.prestosql.plugin.hive.metastore.Column> schema)\n+    {\n+        List<ConnectorMaterializedViewDefinition.Column> columns = new ArrayList<>();\n+        for (io.prestosql.plugin.hive.metastore.Column field : schema) {\n+            ConnectorMaterializedViewDefinition.Column column =\n+                    new ConnectorMaterializedViewDefinition.Column(field.getName(), field.getType().getType(typeManager).getTypeId());\n+            columns.add(column);\n+        }\n+        return columns;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        if (!isAMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Optional<Table> matViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!matViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        Table matView = matViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(matView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = matView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(matView.getOwner().replaceAll(\"@.*\", \"\")),\n+            new HashMap<>(matView.getParameters())));\n+    }\n+\n+    public Optional<io.prestosql.plugin.iceberg.TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        if (icebergTable.currentSnapshot() == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 410}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwNjM3Mw==", "bodyText": "We can restructure this a bit to make the various cases more clear\nif (!tableToken.isPresent()) {\n    return false;\n}\n\nif (!currentToken.isPresent()) {\n    return tableToken.get().getSnapshotId() == -1;\n}\n\nreturn tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId();", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490606373", "createdAt": "2020-09-17T22:55:35Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,\n+            List<io.prestosql.plugin.hive.metastore.Column> schema)\n+    {\n+        List<ConnectorMaterializedViewDefinition.Column> columns = new ArrayList<>();\n+        for (io.prestosql.plugin.hive.metastore.Column field : schema) {\n+            ConnectorMaterializedViewDefinition.Column column =\n+                    new ConnectorMaterializedViewDefinition.Column(field.getName(), field.getType().getType(typeManager).getTypeId());\n+            columns.add(column);\n+        }\n+        return columns;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        if (!isAMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Optional<Table> matViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!matViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        Table matView = matViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(matView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = matView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(matView.getOwner().replaceAll(\"@.*\", \"\")),\n+            new HashMap<>(matView.getParameters())));\n+    }\n+\n+    public Optional<io.prestosql.plugin.iceberg.TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        if (icebergTable.currentSnapshot() == null) {\n+            return Optional.empty();\n+        }\n+        else {\n+            return Optional.of(new io.prestosql.plugin.iceberg.TableToken(icebergTable.currentSnapshot().snapshotId()));\n+        }\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<io.prestosql.plugin.iceberg.TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<io.prestosql.plugin.iceberg.TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!currentToken.isPresent() && tableToken.isPresent() && tableToken.get().getSnapshotId() == -1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 423}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwODE2Ng==", "bodyText": "This should probably start with presto since it is Presto specific", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490608166", "createdAt": "2020-09-17T23:01:10Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -124,10 +145,12 @@\n public class IcebergMetadata\n         implements ConnectorMetadata\n {\n+    public static final String DEPENDS_ON_TABLES = \"dependsOnTables\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwOTI0OA==", "bodyText": "We shouldn't depend on the toString() method of IcebergTableHandle as the serialization format, since it could change and it's not designed to be serializable. Using SchemaTableName directly and serializing to a JSON list seems better, since that handles names with special characters, etc.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490609248", "createdAt": "2020-09-17T23:04:45Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMxMjYwOA=="}, "originalCommit": null, "originalPosition": 336}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYwOTk1Ng==", "bodyText": "This is inherently racey since the definition of the table might change between the time that we check it and when we use it later. We should fetch the table metadata, then check if it's a materialized view.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490609956", "createdAt": "2020-09-17T23:07:00Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -662,4 +702,335 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n         org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n         return TableStatisticsMaker.getTableStatistics(typeManager, session, constraint, handle, icebergTable);\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        Map<String, String> properties;\n+\n+        Map buildProps = new HashMap();\n+        buildProps.put(PRESTO_QUERY_ID_NAME, session.getQueryId());\n+\n+        // Generate a storage table name\n+        String storageTableName = \"st\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        buildProps.put(STORAGE_TABLE, storageTableName);\n+        buildProps.put(PRESTO_VIEW_FLAG, \"true\");\n+        if (replace && definition.getComment().isPresent()) {\n+            buildProps.put(TABLE_COMMENT, definition.getComment().get());\n+        }\n+        else {\n+            // Add a default table comment when the view is being created\n+            buildProps.put(TABLE_COMMENT, \"Presto Materialized View\");\n+        }\n+        properties = ImmutableMap.<String, String>builder()\n+            .putAll(buildProps)\n+            .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(properties);\n+        tableBuilder.getStorageBuilder()\n+                .setStorageFormat(VIEW_STORAGE_FORMAT)\n+                .setLocation(\"\");\n+\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new TableAlreadyExistsException(viewName);\n+        }\n+\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+\n+        Map<String, Object> finalProperties = new HashMap<>();\n+        finalProperties.put(FILE_FORMAT_PROPERTY, FileFormat.PARQUET);\n+        if (definition.getProperties().containsKey(PARTITIONING_PROPERTY)) {\n+            finalProperties.put(PARTITIONING_PROPERTY, definition.getProperties().get(PARTITIONING_PROPERTY));\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, finalProperties, Optional.empty());\n+\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        tableBuilder.setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent()) {\n+            if (!replace) {\n+                throw new MaterializedViewAlreadyExistsException(viewName);\n+            }\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String currentStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), currentStorageTable, true);\n+            }\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (Exception e) {\n+                // ignore the exception\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    private Configuration getConfiguration(ConnectorSession session, String schemaName)\n+    {\n+        return hdfsEnvironment.getConfiguration(new HdfsContext(session, schemaName), new Path(\"file:///tmp\"));\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        // Grab the snapshot-ids of the source table. The table scans for insert are planned and\n+        // executed as of these snapshots and these snapshot ids are written to storage table\n+        // as 'dependsOnTables' property. This 'as-of' processing ensures that the metadata of the\n+        // materialized view and the data are in sync.\n+        Map<ConnectorTableHandle, Long> tableSnapshotIdMap = new HashMap<>();\n+        for (ConnectorTableHandle handle : sourceTableHandles) {\n+            Optional<io.prestosql.plugin.iceberg.TableToken> token = getTableToken(session, handle);\n+            if (token.isPresent()) {\n+                tableSnapshotIdMap.put(handle, token.get().getSnapshotId());\n+            }\n+            else {\n+                tableSnapshotIdMap.put(handle, (long) -1);\n+            }\n+        }\n+        transactionHandle.setTableSnapshotIdMap(tableSnapshotIdMap);\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(HadoopInputFile.fromLocation(task.getPath(), getConfiguration(session, table.getSchemaName())))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            // Update the map that tracks tables on which the materialized view depends and the corresponding snapshot ids of the table\n+            String tableSnapshotIDString = Joiner.on(\",\").withKeyValueSeparator(\"=\").join(transactionHandle.getTableSnapshotIdMap());\n+            appendFiles.set(DEPENDS_ON_TABLES, tableSnapshotIDString);\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isAMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isAMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isAMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    public static List<ConnectorMaterializedViewDefinition.Column> icebergSchemaToMaterializedViewCols(TypeManager typeManager,\n+            List<io.prestosql.plugin.hive.metastore.Column> schema)\n+    {\n+        List<ConnectorMaterializedViewDefinition.Column> columns = new ArrayList<>();\n+        for (io.prestosql.plugin.hive.metastore.Column field : schema) {\n+            ConnectorMaterializedViewDefinition.Column column =\n+                    new ConnectorMaterializedViewDefinition.Column(field.getName(), field.getType().getType(typeManager).getTypeId());\n+            columns.add(column);\n+        }\n+        return columns;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        if (!isAMaterializedView(session, viewName)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 381}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYxMDU3NQ==", "bodyText": "Handles should be immutable. Update this by creating a new object with the updated value.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r490610575", "createdAt": "2020-09-17T23:09:04Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergTableHandle.java", "diffHunk": "@@ -84,6 +84,11 @@ public TableType getTableType()\n         return snapshotId;\n     }\n \n+    public void setSnapshotId(Optional<Long> snapshotId)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA5ODIxMjA5", "url": "https://github.com/trinodb/trino/pull/4832#pullrequestreview-509821209", "createdAt": "2020-10-15T21:26:05Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 39, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQyMToyNjowNVrOHibrDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQwNToxMjoxOVrOHinL2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg2NzAyMw==", "bodyText": "This should be NOT_SUPPORTED", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505867023", "createdAt": "2020-10-15T21:26:05Z", "author": {"login": "electrum"}, "path": "presto-main/src/main/java/io/prestosql/sql/analyzer/StatementAnalyzer.java", "diffHunk": "@@ -942,6 +943,10 @@ protected Scope visitCreateMaterializedView(CreateMaterializedView node, Optiona\n             QualifiedObjectName viewName = createQualifiedObjectName(session, node, node.getName());\n             analysis.setUpdateType(\"CREATE MATERIALIZED VIEW\", viewName);\n \n+            if (node.isReplace() && node.isNotExists()) {\n+                throw semanticException(GENERIC_USER_ERROR, node, \"'CREATE OR REPLACE' and 'IF NOT EXISTS' clauses can not be used together\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg2NzMxOA==", "bodyText": "Nit: uppercase SQL keywords", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505867318", "createdAt": "2020-10-15T21:26:26Z", "author": {"login": "electrum"}, "path": "presto-main/src/test/java/io/prestosql/sql/analyzer/TestAnalyzer.java", "diffHunk": "@@ -2497,6 +2498,13 @@ public void testNullTreatment()\n         analyze(\"SELECT lag(1) IGNORE NULLS OVER (ORDER BY x) FROM (VALUES 1) t(x)\");\n     }\n \n+    @Test\n+    public void testCreateOrReplaceMaterializedView()\n+    {\n+        assertFails(\"create or replace materialized view if not exists mv1 as select * from tab1\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg3MjIzMA==", "bodyText": "This should be indented as\n.stream()\n.flatMap(schema -> metastore.getAllViews(schema).stream()\n        .map(table -> new SchemaTableName(schema, table))\n        .collect(toList())\n        .stream())\n.collect(toList());", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505872230", "createdAt": "2020-10-15T21:32:37Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -256,14 +280,25 @@ public ConnectorTableMetadata getTableMetadata(ConnectorSession session, Connect\n     @Override\n     public List<SchemaTableName> listTables(ConnectorSession session, Optional<String> schemaName)\n     {\n-        return schemaName.map(Collections::singletonList)\n+        List<SchemaTableName> tablesList = schemaName.map(Collections::singletonList)\n                 .orElseGet(metastore::getAllDatabases)\n                 .stream()\n                 .flatMap(schema -> metastore.getTablesWithParameter(schema, TABLE_TYPE_PROP, ICEBERG_TABLE_TYPE_VALUE).stream()\n                         .map(table -> new SchemaTableName(schema, table))\n                         .collect(toList())\n                         .stream())\n                 .collect(toList());\n+\n+        List<SchemaTableName> viewsList = schemaName.map(Collections::singletonList)\n+                .orElseGet(metastore::getAllDatabases)\n+                .stream()\n+                .flatMap(schema -> metastore.getAllViews(schema).stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg3MjYxOQ==", "bodyText": "You can remove the viewsList variable by replacing this with\n.forEach(tablesList::add);", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505872619", "createdAt": "2020-10-15T21:33:09Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -256,14 +280,25 @@ public ConnectorTableMetadata getTableMetadata(ConnectorSession session, Connect\n     @Override\n     public List<SchemaTableName> listTables(ConnectorSession session, Optional<String> schemaName)\n     {\n-        return schemaName.map(Collections::singletonList)\n+        List<SchemaTableName> tablesList = schemaName.map(Collections::singletonList)\n                 .orElseGet(metastore::getAllDatabases)\n                 .stream()\n                 .flatMap(schema -> metastore.getTablesWithParameter(schema, TABLE_TYPE_PROP, ICEBERG_TABLE_TYPE_VALUE).stream()\n                         .map(table -> new SchemaTableName(schema, table))\n                         .collect(toList())\n                         .stream())\n                 .collect(toList());\n+\n+        List<SchemaTableName> viewsList = schemaName.map(Collections::singletonList)\n+                .orElseGet(metastore::getAllDatabases)\n+                .stream()\n+                .flatMap(schema -> metastore.getAllViews(schema).stream()\n+                .map(table -> new SchemaTableName(schema, table))\n+                .collect(toList())\n+                .stream())\n+                .collect(toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg3OTk5Ng==", "bodyText": "Nit: we don't use the Immutable* types in declarations. Instead, declare as Map", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505879996", "createdAt": "2020-10-15T21:43:29Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg4Njg2OQ==", "bodyText": "Nit: might look nicer to write this as\n .withStorage(storage -> storage.setStorageFormat(VIEW_STORAGE_FORMAT))\n .withStorage(storage -> storage.setLocation(\"\"))", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505886869", "createdAt": "2020-10-15T21:54:36Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 172}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg5NzcxMQ==", "bodyText": "Nit: we avoid abbreviations. Name this column\nAlso, I think this can be done as a stream", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505897711", "createdAt": "2020-10-15T22:22:57Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg5OTM1Ng==", "bodyText": "This can use Map.putIfAbsent()", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505899356", "createdAt": "2020-10-15T22:27:07Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkwMDY2Ng==", "bodyText": "I think we need to return here if ignoreExisting is present and the view already exists\nHow do we treat these flags when the view exists but storage table does not, or vice versa?", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505900666", "createdAt": "2020-10-15T22:30:49Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkwNzc4Mw==", "bodyText": "I don't think this needs to be fully qualified", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505907783", "createdAt": "2020-10-15T22:50:57Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkwODA5NQ==", "bodyText": "We could static import VIRTUAL_VIEW", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505908095", "createdAt": "2020-10-15T22:51:52Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkwOTE4NQ==", "bodyText": "What does it mean when the STORAGE_TABLE parameter is not present?", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505909185", "createdAt": "2020-10-15T22:55:03Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 181}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkwOTg1Mw==", "bodyText": "This can be\nTable view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName())\n        .orElseThrow(() -> new MaterializedViewNotFoundException(viewName));", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505909853", "createdAt": "2020-10-15T22:56:57Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxMDE0MQ==", "bodyText": "The canonical way to write this is\nString oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\nif (oldStorageTable != null) {\n    ...\n}", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505910141", "createdAt": "2020-10-15T22:57:47Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkwOTE4NQ=="}, "originalCommit": null, "originalPosition": 181}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxMDI0OQ==", "bodyText": "Same comment as above. Do get() and check for null", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505910249", "createdAt": "2020-10-15T22:58:04Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxMDUwOA==", "bodyText": "If any of the subsequent operations fail, we leave the storage table behind", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505910508", "createdAt": "2020-10-15T22:58:47Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxMzYwNQ==", "bodyText": "This could be ImmutableList.of() since we don't modify it", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505913605", "createdAt": "2020-10-15T23:07:55Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 243}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxNTIyOA==", "bodyText": "Do we need this check? We don't have it for finishInsert(). It seems like we should still update the dependsOnTables even if the final output is empty, since we still updated the materialized view based on those input tables.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505915228", "createdAt": "2020-10-15T23:12:40Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 245}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxNTY1OA==", "bodyText": "This can be collected directly into a string\nString dependencies = ...\n        .collect(joining(\",\"));", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505915658", "createdAt": "2020-10-15T23:14:01Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            List<String> listOfStrings = sourceTableHandles.stream()\n+                    .map(handle -> (IcebergTableHandle) handle)\n+                    .filter(handle -> handle.getSnapshotId().isPresent())\n+                    .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                    .collect(Collectors.toUnmodifiableList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 276}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxNTc1OQ==", "bodyText": "This indentation is off. Please run the code formatter.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505915759", "createdAt": "2020-10-15T23:14:19Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            List<String> listOfStrings = sourceTableHandles.stream()\n+                    .map(handle -> (IcebergTableHandle) handle)\n+                    .filter(handle -> handle.getSnapshotId().isPresent())\n+                    .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                    .collect(Collectors.toUnmodifiableList());\n+\n+            // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+            appendFiles.set(DEPENDS_ON_TABLES, Joiner.on(\",\").join(listOfStrings));\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 284}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkxNjcyMA==", "bodyText": "I'm not sure we should be using PRESTO_VIEW_FLAG here, since these are materialized views for Iceberg. They can't be queried as a view in the Hive connector. Thoughts?", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505916720", "createdAt": "2020-10-15T23:17:05Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMjUxMQ==", "bodyText": "This defensive copy should be done in ConnectorMaterializedViewDefinition (although we don't need it since the map returned from Table is immutable)", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505922511", "createdAt": "2020-10-15T23:35:41Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            List<String> listOfStrings = sourceTableHandles.stream()\n+                    .map(handle -> (IcebergTableHandle) handle)\n+                    .filter(handle -> handle.getSnapshotId().isPresent())\n+                    .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                    .collect(Collectors.toUnmodifiableList());\n+\n+            // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+            appendFiles.set(DEPENDS_ON_TABLES, Joiner.on(\",\").join(listOfStrings));\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        Optional<Table> materializedViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!materializedViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        if (!isMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Table materializedView = materializedViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(materializedView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = materializedView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(materializedView.getOwner()),\n+            new HashMap<>(materializedView.getParameters())));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 333}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMjcyNA==", "bodyText": "We shouldn't need to fully qualify here", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505922724", "createdAt": "2020-10-15T23:36:28Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            List<String> listOfStrings = sourceTableHandles.stream()\n+                    .map(handle -> (IcebergTableHandle) handle)\n+                    .filter(handle -> handle.getSnapshotId().isPresent())\n+                    .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                    .collect(Collectors.toUnmodifiableList());\n+\n+            // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+            appendFiles.set(DEPENDS_ON_TABLES, Joiner.on(\",\").join(listOfStrings));\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        Optional<Table> materializedViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!materializedViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        if (!isMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Table materializedView = materializedViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(materializedView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = materializedView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(materializedView.getOwner()),\n+            new HashMap<>(materializedView.getParameters())));\n+    }\n+\n+    public Optional<io.prestosql.plugin.iceberg.TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        return Optional.ofNullable(icebergTable.currentSnapshot())\n+            .map(snapshot -> new TableToken(snapshot.snapshotId()));\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<io.prestosql.plugin.iceberg.TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<io.prestosql.plugin.iceberg.TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!tableToken.isPresent() || !currentToken.isPresent()) {\n+            return false;\n+        }\n+\n+        return tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId();\n+    }\n+\n+    @Override\n+    public MaterializedViewFreshness getMaterializedViewFreshness(ConnectorSession session, SchemaTableName materializedViewName)\n+    {\n+        Map<String, Optional<io.prestosql.plugin.iceberg.TableToken>> refreshStateMap = getMaterializedViewToken(session, materializedViewName);\n+        if (refreshStateMap.isEmpty()) {\n+            return new MaterializedViewFreshness(false);\n+        }\n+\n+        for (Map.Entry<String, Optional<io.prestosql.plugin.iceberg.TableToken>> entry : refreshStateMap.entrySet()) {\n+            List<String> strings = Splitter.on(\".\").splitToList(entry.getKey());\n+            Preconditions.checkState(strings.size() >= 2);\n+            String schema;\n+            String name;\n+            if (strings.size() == 3) {\n+                schema = strings.get(1);\n+                name = strings.get(2);\n+            }\n+            else {\n+                schema = strings.get(0);\n+                name = strings.get(1);\n+            }\n+            SchemaTableName schemaTableName = new SchemaTableName(schema, name);\n+            if (!isTableCurrent(session, getTableHandle(session, schemaTableName), entry.getValue())) {\n+                return new MaterializedViewFreshness(false);\n+            }\n+        }\n+        return new MaterializedViewFreshness(true);\n+    }\n+\n+    public Map<String, Optional<io.prestosql.plugin.iceberg.TableToken>> getMaterializedViewToken(ConnectorSession session, SchemaTableName name)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 385}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMjgyMQ==", "bodyText": "I think this method can be private", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505922821", "createdAt": "2020-10-15T23:36:44Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            List<String> listOfStrings = sourceTableHandles.stream()\n+                    .map(handle -> (IcebergTableHandle) handle)\n+                    .filter(handle -> handle.getSnapshotId().isPresent())\n+                    .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                    .collect(Collectors.toUnmodifiableList());\n+\n+            // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+            appendFiles.set(DEPENDS_ON_TABLES, Joiner.on(\",\").join(listOfStrings));\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        Optional<Table> materializedViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!materializedViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        if (!isMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Table materializedView = materializedViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(materializedView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = materializedView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(materializedView.getOwner()),\n+            new HashMap<>(materializedView.getParameters())));\n+    }\n+\n+    public Optional<io.prestosql.plugin.iceberg.TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        return Optional.ofNullable(icebergTable.currentSnapshot())\n+            .map(snapshot -> new TableToken(snapshot.snapshotId()));\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<io.prestosql.plugin.iceberg.TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<io.prestosql.plugin.iceberg.TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!tableToken.isPresent() || !currentToken.isPresent()) {\n+            return false;\n+        }\n+\n+        return tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId();\n+    }\n+\n+    @Override\n+    public MaterializedViewFreshness getMaterializedViewFreshness(ConnectorSession session, SchemaTableName materializedViewName)\n+    {\n+        Map<String, Optional<io.prestosql.plugin.iceberg.TableToken>> refreshStateMap = getMaterializedViewToken(session, materializedViewName);\n+        if (refreshStateMap.isEmpty()) {\n+            return new MaterializedViewFreshness(false);\n+        }\n+\n+        for (Map.Entry<String, Optional<io.prestosql.plugin.iceberg.TableToken>> entry : refreshStateMap.entrySet()) {\n+            List<String> strings = Splitter.on(\".\").splitToList(entry.getKey());\n+            Preconditions.checkState(strings.size() >= 2);\n+            String schema;\n+            String name;\n+            if (strings.size() == 3) {\n+                schema = strings.get(1);\n+                name = strings.get(2);\n+            }\n+            else {\n+                schema = strings.get(0);\n+                name = strings.get(1);\n+            }\n+            SchemaTableName schemaTableName = new SchemaTableName(schema, name);\n+            if (!isTableCurrent(session, getTableHandle(session, schemaTableName), entry.getValue())) {\n+                return new MaterializedViewFreshness(false);\n+            }\n+        }\n+        return new MaterializedViewFreshness(true);\n+    }\n+\n+    public Map<String, Optional<io.prestosql.plugin.iceberg.TableToken>> getMaterializedViewToken(ConnectorSession session, SchemaTableName name)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMjcyNA=="}, "originalCommit": null, "originalPosition": 385}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMzI5MQ==", "bodyText": "We could simplify to\nif (strings.size() == 3) {\n    strings = strings.subList(1, 3);\n}\nString schema = strings.get(0);\nString name = strings.get(1);", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505923291", "createdAt": "2020-10-15T23:38:25Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +731,285 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent() && !ignoreExisting) {\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        if (!storageTableProperties.containsKey(FILE_FORMAT_PROPERTY)) {\n+            storageTableProperties.put(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+        }\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = new ArrayList<>();\n+        for (ConnectorMaterializedViewDefinition.Column col : definition.getColumns()) {\n+            columns.add(new ColumnMetadata(col.getName(), typeManager.getType(col.getType())));\n+        }\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        ImmutableMap<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        io.prestosql.plugin.hive.metastore.Column dummyColumn = new io.prestosql.plugin.hive.metastore.Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage ->\n+                    storage.setStorageFormat(VIEW_STORAGE_FORMAT)\n+                        .setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            if (existing.get().getParameters().containsKey(STORAGE_TABLE)) {\n+                String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+        if (!view.isPresent()) {\n+            throw new MaterializedViewNotFoundException(viewName);\n+        }\n+\n+        if (view.get().getParameters().containsKey(STORAGE_TABLE)) {\n+            String storageTableName = view.get().getParameters().get(STORAGE_TABLE);\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = new ArrayList<>();\n+\n+        if (!fragments.isEmpty()) {\n+            commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+            Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                    icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+            AppendFiles appendFiles = transaction.newFastAppend();\n+            for (CommitTaskData task : commitTasks) {\n+                HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+                DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                        .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                        .withFormat(table.getFileFormat())\n+                        .withMetrics(task.getMetrics().metrics());\n+\n+                if (!icebergTable.spec().fields().isEmpty()) {\n+                    String partitionDataJson = task.getPartitionDataJson()\n+                            .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                    builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+                }\n+\n+                appendFiles.appendFile(builder.build());\n+            }\n+\n+            List<String> listOfStrings = sourceTableHandles.stream()\n+                    .map(handle -> (IcebergTableHandle) handle)\n+                    .filter(handle -> handle.getSnapshotId().isPresent())\n+                    .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                    .collect(Collectors.toUnmodifiableList());\n+\n+            // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+            appendFiles.set(DEPENDS_ON_TABLES, Joiner.on(\",\").join(listOfStrings));\n+            appendFiles.commit();\n+        }\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(org.apache.hadoop.hive.metastore.TableType.VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        Optional<Table> materializedViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!materializedViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        if (!isMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Table materializedView = materializedViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(materializedView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = materializedView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(materializedView.getOwner()),\n+            new HashMap<>(materializedView.getParameters())));\n+    }\n+\n+    public Optional<io.prestosql.plugin.iceberg.TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        return Optional.ofNullable(icebergTable.currentSnapshot())\n+            .map(snapshot -> new TableToken(snapshot.snapshotId()));\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<io.prestosql.plugin.iceberg.TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<io.prestosql.plugin.iceberg.TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!tableToken.isPresent() || !currentToken.isPresent()) {\n+            return false;\n+        }\n+\n+        return tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId();\n+    }\n+\n+    @Override\n+    public MaterializedViewFreshness getMaterializedViewFreshness(ConnectorSession session, SchemaTableName materializedViewName)\n+    {\n+        Map<String, Optional<io.prestosql.plugin.iceberg.TableToken>> refreshStateMap = getMaterializedViewToken(session, materializedViewName);\n+        if (refreshStateMap.isEmpty()) {\n+            return new MaterializedViewFreshness(false);\n+        }\n+\n+        for (Map.Entry<String, Optional<io.prestosql.plugin.iceberg.TableToken>> entry : refreshStateMap.entrySet()) {\n+            List<String> strings = Splitter.on(\".\").splitToList(entry.getKey());\n+            Preconditions.checkState(strings.size() >= 2);\n+            String schema;\n+            String name;\n+            if (strings.size() == 3) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 369}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMzkwNQ==", "bodyText": "I think we can eliminate this class for now since we are only using it as a return type for the internal getMaterializedViewToken() method", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505923905", "createdAt": "2020-10-15T23:40:30Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/TableToken.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+public class TableToken", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyNDQ3Mw==", "bodyText": "Can we use createIcebergQueryRunner(ImmutableMap.of(), false) from IcebergQueryRunner for this? The only difference seems to be that this one doesn't install the TPCH catalog, but that shouldn't be a concern.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505924473", "createdAt": "2020-10-15T23:42:25Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/test/java/io/prestosql/plugin/iceberg/TestIcebergMaterializedViews.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.Session;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfiguration;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.plugin.hive.metastore.HiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastoreConfig;\n+import io.prestosql.sql.tree.ExplainType;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+\n+import static io.prestosql.testing.TestingSession.testSessionBuilder;\n+import static io.prestosql.testing.assertions.Assert.assertEquals;\n+import static io.prestosql.testing.assertions.Assert.assertFalse;\n+import static io.prestosql.testing.assertions.Assert.assertTrue;\n+\n+public class TestIcebergMaterializedViews\n+        extends AbstractTestQueryFramework\n+{\n+    @Override\n+    protected DistributedQueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        Session session = testSessionBuilder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyNDgwNA==", "bodyText": "We should shorten this to test since there is no problem with collisions. A shorter name makes the tests easier to read. Or if we use createIcebergQueryRunner(), just use the default session schema and remove the schema qualification everywhere.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r505924804", "createdAt": "2020-10-15T23:43:34Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/test/java/io/prestosql/plugin/iceberg/TestIcebergMaterializedViews.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.Session;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfiguration;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.plugin.hive.metastore.HiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastoreConfig;\n+import io.prestosql.sql.tree.ExplainType;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+\n+import static io.prestosql.testing.TestingSession.testSessionBuilder;\n+import static io.prestosql.testing.assertions.Assert.assertEquals;\n+import static io.prestosql.testing.assertions.Assert.assertFalse;\n+import static io.prestosql.testing.assertions.Assert.assertTrue;\n+\n+public class TestIcebergMaterializedViews\n+        extends AbstractTestQueryFramework\n+{\n+    @Override\n+    protected DistributedQueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        Session session = testSessionBuilder()\n+                .setCatalog(\"iceberg\")\n+                .build();\n+        DistributedQueryRunner queryRunner = DistributedQueryRunner.builder(session).build();\n+\n+        File baseDir = queryRunner.getCoordinator().getBaseDataDir().resolve(\"iceberg_data\").toFile();\n+\n+        HdfsConfig hdfsConfig = new HdfsConfig();\n+        HdfsConfiguration hdfsConfiguration = new HiveHdfsConfiguration(new HdfsConfigurationInitializer(hdfsConfig), ImmutableSet.of());\n+        HdfsEnvironment hdfsEnvironment = new HdfsEnvironment(hdfsConfiguration, hdfsConfig, new NoHdfsAuthentication());\n+\n+        HiveMetastore metastore = new FileHiveMetastore(\n+                hdfsEnvironment,\n+                new FileHiveMetastoreConfig()\n+                    .setCatalogDirectory(baseDir.toURI().toString())\n+                    .setMetastoreUser(\"test\"));\n+        queryRunner.installPlugin(new TestingIcebergPlugin(metastore));\n+        queryRunner.createCatalog(\"iceberg\", \"iceberg\");\n+\n+        return queryRunner;\n+    }\n+\n+    @BeforeClass\n+    public void setUp()\n+    {\n+        assertUpdate(\"CREATE SCHEMA test_materialized_views\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1MjMwMQ==", "bodyText": "Use the date constructor instead of a cast\nDATE '2019-09-09'", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506052301", "createdAt": "2020-10-16T04:59:22Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/test/java/io/prestosql/plugin/iceberg/TestIcebergMaterializedViews.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.Session;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfiguration;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.plugin.hive.metastore.HiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastoreConfig;\n+import io.prestosql.sql.tree.ExplainType;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+\n+import static io.prestosql.testing.TestingSession.testSessionBuilder;\n+import static io.prestosql.testing.assertions.Assert.assertEquals;\n+import static io.prestosql.testing.assertions.Assert.assertFalse;\n+import static io.prestosql.testing.assertions.Assert.assertTrue;\n+\n+public class TestIcebergMaterializedViews\n+        extends AbstractTestQueryFramework\n+{\n+    @Override\n+    protected DistributedQueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        Session session = testSessionBuilder()\n+                .setCatalog(\"iceberg\")\n+                .build();\n+        DistributedQueryRunner queryRunner = DistributedQueryRunner.builder(session).build();\n+\n+        File baseDir = queryRunner.getCoordinator().getBaseDataDir().resolve(\"iceberg_data\").toFile();\n+\n+        HdfsConfig hdfsConfig = new HdfsConfig();\n+        HdfsConfiguration hdfsConfiguration = new HiveHdfsConfiguration(new HdfsConfigurationInitializer(hdfsConfig), ImmutableSet.of());\n+        HdfsEnvironment hdfsEnvironment = new HdfsEnvironment(hdfsConfiguration, hdfsConfig, new NoHdfsAuthentication());\n+\n+        HiveMetastore metastore = new FileHiveMetastore(\n+                hdfsEnvironment,\n+                new FileHiveMetastoreConfig()\n+                    .setCatalogDirectory(baseDir.toURI().toString())\n+                    .setMetastoreUser(\"test\"));\n+        queryRunner.installPlugin(new TestingIcebergPlugin(metastore));\n+        queryRunner.createCatalog(\"iceberg\", \"iceberg\");\n+\n+        return queryRunner;\n+    }\n+\n+    @BeforeClass\n+    public void setUp()\n+    {\n+        assertUpdate(\"CREATE SCHEMA test_materialized_views\");\n+        assertUpdate(\"CREATE TABLE test_materialized_views.base_table1(_bigint BIGINT, _date DATE) WITH (partitioning = ARRAY['_date'])\");\n+        assertUpdate(\"INSERT INTO test_materialized_views.base_table1 VALUES (0, CAST('2019-09-08' AS DATE)), (1, CAST('2019-09-09' AS DATE)), (2, CAST('2019-09-09' AS DATE))\", 3);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1MzM1MA==", "bodyText": "We prefer to do the initialization close to the tests, since that makes the tests self contained and thus easier to read. Shared resources are also problematic because multiple tests could modify them (even accidentally), and tests run multithreaded in arbitrary order, leading to hard to debug failures.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506053350", "createdAt": "2020-10-16T05:03:37Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/test/java/io/prestosql/plugin/iceberg/TestIcebergMaterializedViews.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.Session;\n+import io.prestosql.plugin.hive.HdfsConfig;\n+import io.prestosql.plugin.hive.HdfsConfiguration;\n+import io.prestosql.plugin.hive.HdfsConfigurationInitializer;\n+import io.prestosql.plugin.hive.HdfsEnvironment;\n+import io.prestosql.plugin.hive.HiveHdfsConfiguration;\n+import io.prestosql.plugin.hive.authentication.NoHdfsAuthentication;\n+import io.prestosql.plugin.hive.metastore.HiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastore;\n+import io.prestosql.plugin.hive.metastore.file.FileHiveMetastoreConfig;\n+import io.prestosql.sql.tree.ExplainType;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.MaterializedRow;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+\n+import static io.prestosql.testing.TestingSession.testSessionBuilder;\n+import static io.prestosql.testing.assertions.Assert.assertEquals;\n+import static io.prestosql.testing.assertions.Assert.assertFalse;\n+import static io.prestosql.testing.assertions.Assert.assertTrue;\n+\n+public class TestIcebergMaterializedViews\n+        extends AbstractTestQueryFramework\n+{\n+    @Override\n+    protected DistributedQueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        Session session = testSessionBuilder()\n+                .setCatalog(\"iceberg\")\n+                .build();\n+        DistributedQueryRunner queryRunner = DistributedQueryRunner.builder(session).build();\n+\n+        File baseDir = queryRunner.getCoordinator().getBaseDataDir().resolve(\"iceberg_data\").toFile();\n+\n+        HdfsConfig hdfsConfig = new HdfsConfig();\n+        HdfsConfiguration hdfsConfiguration = new HiveHdfsConfiguration(new HdfsConfigurationInitializer(hdfsConfig), ImmutableSet.of());\n+        HdfsEnvironment hdfsEnvironment = new HdfsEnvironment(hdfsConfiguration, hdfsConfig, new NoHdfsAuthentication());\n+\n+        HiveMetastore metastore = new FileHiveMetastore(\n+                hdfsEnvironment,\n+                new FileHiveMetastoreConfig()\n+                    .setCatalogDirectory(baseDir.toURI().toString())\n+                    .setMetastoreUser(\"test\"));\n+        queryRunner.installPlugin(new TestingIcebergPlugin(metastore));\n+        queryRunner.createCatalog(\"iceberg\", \"iceberg\");\n+\n+        return queryRunner;\n+    }\n+\n+    @BeforeClass\n+    public void setUp()\n+    {\n+        assertUpdate(\"CREATE SCHEMA test_materialized_views\");\n+        assertUpdate(\"CREATE TABLE test_materialized_views.base_table1(_bigint BIGINT, _date DATE) WITH (partitioning = ARRAY['_date'])\");\n+        assertUpdate(\"INSERT INTO test_materialized_views.base_table1 VALUES (0, CAST('2019-09-08' AS DATE)), (1, CAST('2019-09-09' AS DATE)), (2, CAST('2019-09-09' AS DATE))\", 3);\n+        assertUpdate(\"INSERT INTO test_materialized_views.base_table1 VALUES (3, CAST('2019-09-09' AS DATE)), (4, CAST('2019-09-10' AS DATE)), (5, CAST('2019-09-10' AS DATE))\", 3);\n+        assertQuery(\"SELECT count(*) FROM test_materialized_views.base_table1\", \"VALUES 6\");\n+\n+        assertUpdate(\"CREATE TABLE test_materialized_views.base_table2 (_varchar VARCHAR, _bigint BIGINT, _date DATE) WITH (partitioning = ARRAY['_bigint', '_date'])\");\n+        assertUpdate(\"INSERT INTO test_materialized_views.base_table2 VALUES ('a', 0, CAST('2019-09-08' AS DATE)), ('a', 1, CAST('2019-09-08' AS DATE)), ('a', 0, CAST('2019-09-09' AS DATE))\", 3);\n+        assertQuery(\"SELECT count(*) FROM test_materialized_views.base_table2\", \"VALUES 3\");\n+\n+        // A very simple non-partitioned materialized view\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_no_part as select * from test_materialized_views.base_table1\");\n+        // A non-partitioned materialized view with grouping and aggregation\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_agg as select _date, count(_date) as num_dates from test_materialized_views.base_table1 group by 1\");\n+        // A partitioned materialized view with grouping and aggregation\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_part WITH (partitioning = ARRAY['_date']) as select _date, count(_date) as num_dates from test_materialized_views.base_table1 group by 1\");\n+        // A non-partitioned join materialized view\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_join as \" +\n+                \"select t2._bigint, _varchar, t1._date from test_materialized_views.base_table1 t1, test_materialized_views.base_table2 t2 where t1._date = t2._date\");\n+        // A partitioned join materialized view\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_join_part WITH (partitioning = ARRAY['_date', '_bigint']) as \" +\n+                \"select t1._bigint, _varchar, t2._date, sum(1) as my_sum from test_materialized_views.base_table1 t1, test_materialized_views.base_table2 t2 where t1._date = t2._date group by 1, 2, 3 order by 1, 2\");\n+\n+        // Base tables and materialized views for staleness check\n+        assertUpdate(\"CREATE TABLE test_materialized_views.base_table3(_bigint BIGINT, _date DATE) WITH (partitioning = ARRAY['_date'])\");\n+        assertUpdate(\"INSERT INTO test_materialized_views.base_table3 VALUES (0, CAST('2019-09-08' AS DATE)), (1, CAST('2019-09-09' AS DATE)), (2, CAST('2019-09-09' AS DATE))\", 3);\n+\n+        assertUpdate(\"CREATE TABLE test_materialized_views.base_table4 (_varchar VARCHAR, _bigint BIGINT, _date DATE) WITH (partitioning = ARRAY['_bigint', '_date'])\");\n+        assertUpdate(\"INSERT INTO test_materialized_views.base_table4 VALUES ('a', 0, CAST('2019-09-08' AS DATE)), ('a', 1, CAST('2019-09-08' AS DATE)), ('a', 0, CAST('2019-09-09' AS DATE))\", 3);\n+\n+        // A partitioned materialized view with grouping and aggregation\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_part_stale WITH (partitioning = ARRAY['_date']) as select _date, count(_date) as num_dates from test_materialized_views.base_table3 group by 1\");\n+        // A non-partitioned join materialized view\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_join_stale as \" +\n+                \"select t2._bigint, _varchar, t1._date from test_materialized_views.base_table3 t1, test_materialized_views.base_table4 t2 where t1._date = t2._date\");\n+        // A partitioned join materialized view\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_join_part_stale WITH (partitioning = ARRAY['_date', '_bigint']) as \" +\n+                \"select t1._bigint, _varchar, t2._date, sum(1) as my_sum from test_materialized_views.base_table3 t1, test_materialized_views.base_table4 t2 where t1._date = t2._date group by 1, 2, 3 order by 1, 2\");\n+\n+        // Materialized views to test SQL features\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_window WITH (partitioning = ARRAY['_date']) as select _date, \" +\n+                \"sum(_bigint) OVER (partition by _date order by _date) as sum_ints from test_materialized_views.base_table1\");\n+        assertUpdate(\"REFRESH MATERIALIZED VIEW test_materialized_views.materialized_view_window\", 6);\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_union WITH (partitioning = ARRAY['_date']) as \" +\n+                \"select _date, count(_date) as num_dates from test_materialized_views.base_table1 group by 1 union \" +\n+                \"select _date, count(_date) as num_dates from test_materialized_views.base_table2 group by 1\");\n+        assertUpdate(\"REFRESH MATERIALIZED VIEW test_materialized_views.materialized_view_union\", 5);\n+        assertUpdate(\"CREATE MATERIALIZED VIEW test_materialized_views.materialized_view_subquery WITH (partitioning = ARRAY['_date']) as \" +\n+                \"select _date, count(_date) as num_dates from test_materialized_views.base_table1 where _date = (select max(_date) from test_materialized_views.base_table2) group by 1\");\n+        assertUpdate(\"REFRESH MATERIALIZED VIEW test_materialized_views.materialized_view_subquery\", 1);\n+\n+        // Materialized view to test 'replace' feature", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1MzUyOQ==", "bodyText": "Please move the non-Iceberg changes to a separate commit. It's best to not mix engine and connector changes.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506053529", "createdAt": "2020-10-16T05:04:17Z", "author": {"login": "electrum"}, "path": "presto-main/src/main/java/io/prestosql/metadata/Metadata.java", "diffHunk": "@@ -269,7 +269,7 @@\n     /**\n      * Begin refresh materialized view query\n      */\n-    InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle);\n+    InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle, List<TableHandle> sourceTableHandles);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1MzYwNw==", "bodyText": "toImmutableList()", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506053607", "createdAt": "2020-10-16T05:04:36Z", "author": {"login": "electrum"}, "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -844,13 +846,26 @@ public boolean supportsMissingColumnsOnInsert(Session session, TableHandle table\n     }\n \n     @Override\n-    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle)\n+    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle, List<TableHandle> sourceTableHandles)\n     {\n         CatalogName catalogName = tableHandle.getCatalogName();\n         CatalogMetadata catalogMetadata = getCatalogMetadataForWrite(session, catalogName);\n         ConnectorMetadata metadata = catalogMetadata.getMetadata();\n         ConnectorTransactionHandle transactionHandle = catalogMetadata.getTransactionHandleFor(catalogName);\n-        ConnectorInsertTableHandle handle = metadata.beginRefreshMaterializedView(session.toConnectorSession(catalogName), tableHandle.getConnectorHandle());\n+\n+        List<ConnectorTableHandle> sourceConnectorHandles = sourceTableHandles.stream()\n+                .map(TableHandle::getConnectorHandle)\n+                .collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1Mzc3OQ==", "bodyText": "This can use a method reference\n.map(Object::getClass)", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506053779", "createdAt": "2020-10-16T05:05:09Z", "author": {"login": "electrum"}, "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -844,13 +846,26 @@ public boolean supportsMissingColumnsOnInsert(Session session, TableHandle table\n     }\n \n     @Override\n-    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle)\n+    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle, List<TableHandle> sourceTableHandles)\n     {\n         CatalogName catalogName = tableHandle.getCatalogName();\n         CatalogMetadata catalogMetadata = getCatalogMetadataForWrite(session, catalogName);\n         ConnectorMetadata metadata = catalogMetadata.getMetadata();\n         ConnectorTransactionHandle transactionHandle = catalogMetadata.getTransactionHandleFor(catalogName);\n-        ConnectorInsertTableHandle handle = metadata.beginRefreshMaterializedView(session.toConnectorSession(catalogName), tableHandle.getConnectorHandle());\n+\n+        List<ConnectorTableHandle> sourceConnectorHandles = sourceTableHandles.stream()\n+                .map(TableHandle::getConnectorHandle)\n+                .collect(Collectors.toList());\n+\n+        if (sourceConnectorHandles.stream()\n+                .map(handle -> handle.getClass())", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1Mzg2OQ==", "bodyText": "You can use Stream.count() here", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506053869", "createdAt": "2020-10-16T05:05:28Z", "author": {"login": "electrum"}, "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -844,13 +846,26 @@ public boolean supportsMissingColumnsOnInsert(Session session, TableHandle table\n     }\n \n     @Override\n-    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle)\n+    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle, List<TableHandle> sourceTableHandles)\n     {\n         CatalogName catalogName = tableHandle.getCatalogName();\n         CatalogMetadata catalogMetadata = getCatalogMetadataForWrite(session, catalogName);\n         ConnectorMetadata metadata = catalogMetadata.getMetadata();\n         ConnectorTransactionHandle transactionHandle = catalogMetadata.getTransactionHandleFor(catalogName);\n-        ConnectorInsertTableHandle handle = metadata.beginRefreshMaterializedView(session.toConnectorSession(catalogName), tableHandle.getConnectorHandle());\n+\n+        List<ConnectorTableHandle> sourceConnectorHandles = sourceTableHandles.stream()\n+                .map(TableHandle::getConnectorHandle)\n+                .collect(Collectors.toList());\n+\n+        if (sourceConnectorHandles.stream()\n+                .map(handle -> handle.getClass())\n+                .distinct()\n+                .collect(Collectors.toList()).size() > 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1MzkzMA==", "bodyText": "Use NOT_SUPPORTED error code", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506053930", "createdAt": "2020-10-16T05:05:41Z", "author": {"login": "electrum"}, "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -844,13 +846,26 @@ public boolean supportsMissingColumnsOnInsert(Session session, TableHandle table\n     }\n \n     @Override\n-    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle)\n+    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle, List<TableHandle> sourceTableHandles)\n     {\n         CatalogName catalogName = tableHandle.getCatalogName();\n         CatalogMetadata catalogMetadata = getCatalogMetadataForWrite(session, catalogName);\n         ConnectorMetadata metadata = catalogMetadata.getMetadata();\n         ConnectorTransactionHandle transactionHandle = catalogMetadata.getTransactionHandleFor(catalogName);\n-        ConnectorInsertTableHandle handle = metadata.beginRefreshMaterializedView(session.toConnectorSession(catalogName), tableHandle.getConnectorHandle());\n+\n+        List<ConnectorTableHandle> sourceConnectorHandles = sourceTableHandles.stream()\n+                .map(TableHandle::getConnectorHandle)\n+                .collect(Collectors.toList());\n+\n+        if (sourceConnectorHandles.stream()\n+                .map(handle -> handle.getClass())\n+                .distinct()\n+                .collect(Collectors.toList()).size() > 1) {\n+            throw new PrestoException(GENERIC_USER_ERROR, \"Cross connector materialized views are not supported\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1NDA4Ng==", "bodyText": "Should this also verify that source connector handles are for the same connector as the materialized view?", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506054086", "createdAt": "2020-10-16T05:06:24Z", "author": {"login": "electrum"}, "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -844,13 +846,26 @@ public boolean supportsMissingColumnsOnInsert(Session session, TableHandle table\n     }\n \n     @Override\n-    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle)\n+    public InsertTableHandle beginRefreshMaterializedView(Session session, TableHandle tableHandle, List<TableHandle> sourceTableHandles)\n     {\n         CatalogName catalogName = tableHandle.getCatalogName();\n         CatalogMetadata catalogMetadata = getCatalogMetadataForWrite(session, catalogName);\n         ConnectorMetadata metadata = catalogMetadata.getMetadata();\n         ConnectorTransactionHandle transactionHandle = catalogMetadata.getTransactionHandleFor(catalogName);\n-        ConnectorInsertTableHandle handle = metadata.beginRefreshMaterializedView(session.toConnectorSession(catalogName), tableHandle.getConnectorHandle());\n+\n+        List<ConnectorTableHandle> sourceConnectorHandles = sourceTableHandles.stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1NDEzMQ==", "bodyText": "toImmutableList()", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506054131", "createdAt": "2020-10-16T05:06:35Z", "author": {"login": "electrum"}, "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -864,10 +879,10 @@ public InsertTableHandle beginRefreshMaterializedView(Session session, TableHand\n     {\n         CatalogName catalogName = tableHandle.getCatalogName();\n         ConnectorMetadata metadata = getMetadata(session, catalogName);\n-        List<ConnectorTableHandle> sourceConnectorHandles = new ArrayList<>();\n-        for (TableHandle handle : sourceTableHandles) {\n-            sourceConnectorHandles.add(handle.getConnectorHandle());\n-        }\n+\n+        List<ConnectorTableHandle> sourceConnectorHandles = sourceTableHandles.stream()\n+                .map(TableHandle::getConnectorHandle)\n+                .collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1NDk5OA==", "bodyText": "How is it possible for all of them to be empty? I don't think it's valid for any of them to be empty.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506054998", "createdAt": "2020-10-16T05:09:58Z", "author": {"login": "electrum"}, "path": "presto-main/src/main/java/io/prestosql/metadata/MetadataManager.java", "diffHunk": "@@ -1145,12 +1160,23 @@ public void dropMaterializedView(Session session, QualifiedObjectName viewName)\n     }\n \n     @Override\n-    public MaterializedViewFreshness getMaterializedViewFreshness(Session session, TableHandle tableHandle)\n+    public MaterializedViewFreshness getMaterializedViewFreshness(Session session, QualifiedObjectName viewName)\n     {\n-        CatalogName catalogName = tableHandle.getCatalogName();\n-        CatalogMetadata catalogMetadata = getCatalogMetadata(session, catalogName);\n-        ConnectorMetadata metadata = catalogMetadata.getMetadataFor(catalogName);\n-        return metadata.getMaterializedViewFreshness(session.toConnectorSession(catalogName), tableHandle.getConnectorHandle());\n+        if (viewName.getCatalogName().isEmpty() || viewName.getSchemaName().isEmpty() || viewName.getObjectName().isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjA1NTY0Mw==", "bodyText": "This \"else\" is redundant since the \"if\" branch returns", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r506055643", "createdAt": "2020-10-16T05:12:19Z", "author": {"login": "electrum"}, "path": "presto-main/src/main/java/io/prestosql/sql/analyzer/StatementAnalyzer.java", "diffHunk": "@@ -1177,37 +1175,31 @@ protected Scope visitTable(Table table, Optional<Scope> scope)\n \n             QualifiedObjectName name = createQualifiedObjectName(session, table, table.getName());\n             analysis.addEmptyColumnReferencesForTable(accessControl, session.getIdentity(), name);\n-            Optional<TableHandle> tableHandle = metadata.getTableHandle(session, name);\n-\n-            // If this is a materialized view, get the name of the storage table\n-            Optional<QualifiedName> storageName = getMaterializedViewStorageTableName(name);\n-            Optional<TableHandle> storageHandle = Optional.empty();\n-            if (storageName.isPresent()) {\n-                storageHandle = metadata.getTableHandle(session, createQualifiedObjectName(session, table, storageName.get()));\n-            }\n-\n-            // If materialized view is current, answer the query using the storage table\n-            Identifier catalogName = new Identifier(name.getCatalogName());\n-            Identifier schemaName = new Identifier(name.getSchemaName());\n-            Identifier tableName = new Identifier(name.getObjectName());\n-            QualifiedName materializedViewName = QualifiedName.of(ImmutableList.of(catalogName, schemaName, tableName));\n-            Optional<TableHandle> materializedViewHandle = metadata.getTableHandle(session, createQualifiedObjectName(session, table, materializedViewName));\n-            if (storageHandle.isPresent() && metadata.getMaterializedViewFreshness(session, materializedViewHandle.get()).isMaterializedViewFresh()) {\n-                tableHandle = storageHandle;\n-            }\n-            else {\n-                // This is a stale materialized view and should be expanded like a logical view\n-                if (storageHandle.isPresent()) {\n-                    Optional<ConnectorMaterializedViewDefinition> optionalMaterializedView = metadata.getMaterializedView(session, name);\n-                    if (optionalMaterializedView.isPresent()) {\n-                        return createScopeForMaterializedView(table, name, scope, optionalMaterializedView.get());\n+            Optional<TableHandle> tableHandle = Optional.empty();\n+\n+            Optional<ConnectorMaterializedViewDefinition> optionalMaterializedView = metadata.getMaterializedView(session, name);\n+            if (optionalMaterializedView.isPresent()) {\n+                if (metadata.getMaterializedViewFreshness(session, name).isMaterializedViewFresh()) {\n+                    // If materialized view is current, answer the query using the storage table\n+                    Optional<QualifiedName> storageName = getMaterializedViewStorageTableName(name);\n+                    if (storageName.isPresent()) {\n+                        tableHandle = metadata.getTableHandle(session, createQualifiedObjectName(session, table, storageName.get()));\n                     }\n                 }\n-                // This is a reference to a logical view\n+                else {\n+                    // This is a stale materialized view and should be expanded like a logical view\n+                    return createScopeForMaterializedView(table, name, scope, optionalMaterializedView.get());\n+                }\n+            }\n+            else {\n+                // This is could be a reference to a logical view or a table\n                 Optional<ConnectorViewDefinition> optionalView = metadata.getView(session, name);\n                 if (optionalView.isPresent()) {\n                     return createScopeForView(table, name, scope, optionalView.get());\n                 }\n+                else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 72}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1OTY5MTU0", "url": "https://github.com/trinodb/trino/pull/4832#pullrequestreview-515969154", "createdAt": "2020-10-23T19:49:20Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxOTo0OToyMFrOHncYzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxOTo0OToyMFrOHncYzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyMTYxNQ==", "bodyText": "This is a good typo to fix, but can you do it in a separate commit, since it's unrelated to the materialized view change? It's fine to leave it in this PR as a separate commit.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r511121615", "createdAt": "2020-10-23T19:49:20Z", "author": {"login": "electrum"}, "path": "presto-parser/src/test/java/io/prestosql/sql/parser/TestSqlParser.java", "diffHunk": "@@ -2693,7 +2693,7 @@ private static void assertStatement(String query, Statement expected)\n         assertFormattedSql(SQL_PARSER, expected);\n     }\n \n-    private static void assertInvalidStatemennt(String statement, String expectedErrorMessageRegex)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1OTcyNDUz", "url": "https://github.com/trinodb/trino/pull/4832#pullrequestreview-515972453", "createdAt": "2020-10-23T19:55:00Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxOTo1NTowMFrOHncr9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxOTo1NTowMFrOHncr9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyNjUxNw==", "bodyText": "The addition of this class should go in the next commit. Also, should it go in the presto-iceberg module (and package) since it's only used by Iceberg?", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r511126517", "createdAt": "2020-10-23T19:55:00Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/MaterializedViewAlreadyExistsException.java", "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.SchemaTableName;\n+\n+import static io.prestosql.spi.StandardErrorCode.ALREADY_EXISTS;\n+import static java.lang.String.format;\n+\n+public class MaterializedViewAlreadyExistsException", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2MDI0ODk3", "url": "https://github.com/trinodb/trino/pull/4832#pullrequestreview-516024897", "createdAt": "2020-10-23T21:38:59Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMTozODo1OVrOHnfJmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMTozODo1OVrOHnfJmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE2Njg3NQ==", "bodyText": "We don't use final on local variables", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r511166875", "createdAt": "2020-10-23T21:38:59Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +728,277 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent()) {\n+            if (ignoreExisting) {\n+                return;\n+            }\n+            throw new MaterializedViewAlreadyExistsException(viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        storageTableProperties.putIfAbsent(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = definition.getColumns().stream()\n+                .map(column -> new ColumnMetadata(column.getName(), typeManager.getType(column.getType())))\n+                .collect(toImmutableList());\n+\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        Map<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        Column dummyColumn = new Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage -> storage.setStorageFormat(VIEW_STORAGE_FORMAT))\n+                .withStorage(storage -> storage.setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+            if (oldStorageTable != null) {\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Table view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName())\n+                .orElseThrow(() -> new MaterializedViewNotFoundException(viewName));\n+\n+        String storageTableName = view.getParameters().get(STORAGE_TABLE);\n+        if (storageTableName != null) {\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = ImmutableList.of();\n+\n+        commitTasks = fragments.stream()\n+            .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+            .collect(toImmutableList());\n+\n+        Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+            .map(field -> field.transform().getResultType(\n+                icebergTable.schema().findType(field.sourceId())))\n+            .toArray(Type[]::new);\n+\n+        AppendFiles appendFiles = transaction.newFastAppend();\n+        for (CommitTaskData task : commitTasks) {\n+            HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+            DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                    .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                    .withFormat(table.getFileFormat())\n+                    .withMetrics(task.getMetrics().metrics());\n+\n+            if (!icebergTable.spec().fields().isEmpty()) {\n+                String partitionDataJson = task.getPartitionDataJson()\n+                        .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+            }\n+\n+            appendFiles.appendFile(builder.build());\n+        }\n+\n+        String dependencies = sourceTableHandles.stream()\n+                .map(handle -> (IcebergTableHandle) handle)\n+                .filter(handle -> handle.getSnapshotId().isPresent())\n+                .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                .collect(joining(\",\"));\n+\n+        // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+        appendFiles.set(DEPENDS_ON_TABLES, dependencies);\n+        appendFiles.commit();\n+\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 299}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4MjQ3MTgy", "url": "https://github.com/trinodb/trino/pull/4832#pullrequestreview-518247182", "createdAt": "2020-10-28T00:37:59Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwMDozNzo1OVrOHpV5IA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwMDo0MToxNlrOHpV85g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExMjM1Mg==", "bodyText": "This method doesn't belong in this commit.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r513112352", "createdAt": "2020-10-28T00:37:59Z", "author": {"login": "martint"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/ViewReaderUtil.java", "diffHunk": "@@ -97,6 +102,13 @@ public static String encodeViewData(ConnectorViewDefinition definition)\n         return VIEW_PREFIX + data + VIEW_SUFFIX;\n     }\n \n+    public static String encodeMaterializedViewData(ConnectorMaterializedViewDefinition definition)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExMjM2Ng==", "bodyText": "This method doesn't belong in this commit.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r513112366", "createdAt": "2020-10-28T00:38:04Z", "author": {"login": "martint"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/ViewReaderUtil.java", "diffHunk": "@@ -113,6 +125,16 @@ public ConnectorViewDefinition decodeViewData(String viewData, Table table, Cata\n             byte[] bytes = Base64.getDecoder().decode(viewData);\n             return VIEW_CODEC.fromJson(bytes);\n         }\n+\n+        public static ConnectorMaterializedViewDefinition decodeMaterializedViewData(String data)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExMzMxOA==", "bodyText": "This class doesn't belong in this commit. It's not related to \"Connector APIs for materialized views that modeled them as TableHandles are being changed to pass in materialized view name rather than handles.\"", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r513113318", "createdAt": "2020-10-28T00:41:16Z", "author": {"login": "martint"}, "path": "presto-spi/src/main/java/io/prestosql/spi/connector/MaterializedViewNotFoundException.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.spi.connector;\n+\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class MaterializedViewNotFoundException", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE5ODY1NzEw", "url": "https://github.com/trinodb/trino/pull/4832#pullrequestreview-519865710", "createdAt": "2020-10-29T16:41:36Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNjo0MTozN1rOHqk3tw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNzowOTo0MFrOHqmFEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQwNjMyNw==", "bodyText": "This doesn't seem to be needed", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r514406327", "createdAt": "2020-10-29T16:41:37Z", "author": {"login": "electrum"}, "path": "presto-iceberg/pom.xml", "diffHunk": "@@ -212,6 +212,12 @@\n             <scope>test</scope>\n         </dependency>\n \n+        <dependency>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQwOTIwMA==", "bodyText": "This shouldn't be done here, as beginRefreshMaterializedView() is called during planning (and even for EXPLAIN), and executeDelete() commits the delete operation. We should do the delete as part of the same Iceberg transaction that inserts the data files, in finishRefreshMaterializedView().", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r514409200", "createdAt": "2020-10-29T16:45:37Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +728,277 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent()) {\n+            if (ignoreExisting) {\n+                return;\n+            }\n+            throw new PrestoException(ALREADY_EXISTS, \"Materialized view already exists: \" + viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        storageTableProperties.putIfAbsent(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = definition.getColumns().stream()\n+                .map(column -> new ColumnMetadata(column.getName(), typeManager.getType(column.getType())))\n+                .collect(toImmutableList());\n+\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        Map<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        Column dummyColumn = new Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage -> storage.setStorageFormat(VIEW_STORAGE_FORMAT))\n+                .withStorage(storage -> storage.setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+            if (oldStorageTable != null) {\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Table view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName())\n+                .orElseThrow(() -> new MaterializedViewNotFoundException(viewName));\n+\n+        String storageTableName = view.getParameters().get(STORAGE_TABLE);\n+        if (storageTableName != null) {\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 228}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQwOTMxOA==", "bodyText": "Nit: wrap all arguments if any are wrapped", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r514409318", "createdAt": "2020-10-29T16:45:48Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +728,277 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent()) {\n+            if (ignoreExisting) {\n+                return;\n+            }\n+            throw new PrestoException(ALREADY_EXISTS, \"Materialized view already exists: \" + viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        storageTableProperties.putIfAbsent(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = definition.getColumns().stream()\n+                .map(column -> new ColumnMetadata(column.getName(), typeManager.getType(column.getType())))\n+                .collect(toImmutableList());\n+\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        Map<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        Column dummyColumn = new Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage -> storage.setStorageFormat(VIEW_STORAGE_FORMAT))\n+                .withStorage(storage -> storage.setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+            if (oldStorageTable != null) {\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Table view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName())\n+                .orElseThrow(() -> new MaterializedViewNotFoundException(viewName));\n+\n+        String storageTableName = view.getParameters().get(STORAGE_TABLE);\n+        if (storageTableName != null) {\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 241}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQyNDQ1MA==", "bodyText": "We should be more explicit in the validation here and provide a good error message, since this value comes from the user. Let's move this into getMaterializedViewToken() and have it return SchemaTableName since that is where we are doing the parsing.\nif (strings.size() == 3) {\n    ...\n}\nelse if (strings.size() != 2) {\n    throw new PrestoException(ICEBERG_INVALID_METADATA, format(\"Invalid table name in '%s' property: %s'\", DEPENDS_ON_TABLES, name));\n}", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r514424450", "createdAt": "2020-10-29T17:07:09Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +728,277 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent()) {\n+            if (ignoreExisting) {\n+                return;\n+            }\n+            throw new PrestoException(ALREADY_EXISTS, \"Materialized view already exists: \" + viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        storageTableProperties.putIfAbsent(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = definition.getColumns().stream()\n+                .map(column -> new ColumnMetadata(column.getName(), typeManager.getType(column.getType())))\n+                .collect(toImmutableList());\n+\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        Map<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        Column dummyColumn = new Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage -> storage.setStorageFormat(VIEW_STORAGE_FORMAT))\n+                .withStorage(storage -> storage.setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+            if (oldStorageTable != null) {\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Table view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName())\n+                .orElseThrow(() -> new MaterializedViewNotFoundException(viewName));\n+\n+        String storageTableName = view.getParameters().get(STORAGE_TABLE);\n+        if (storageTableName != null) {\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = ImmutableList.of();\n+\n+        commitTasks = fragments.stream()\n+            .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+            .collect(toImmutableList());\n+\n+        Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+            .map(field -> field.transform().getResultType(\n+                icebergTable.schema().findType(field.sourceId())))\n+            .toArray(Type[]::new);\n+\n+        AppendFiles appendFiles = transaction.newFastAppend();\n+        for (CommitTaskData task : commitTasks) {\n+            HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+            DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                    .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                    .withFormat(table.getFileFormat())\n+                    .withMetrics(task.getMetrics().metrics());\n+\n+            if (!icebergTable.spec().fields().isEmpty()) {\n+                String partitionDataJson = task.getPartitionDataJson()\n+                        .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+            }\n+\n+            appendFiles.appendFile(builder.build());\n+        }\n+\n+        String dependencies = sourceTableHandles.stream()\n+                .map(handle -> (IcebergTableHandle) handle)\n+                .filter(handle -> handle.getSnapshotId().isPresent())\n+                .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                .collect(joining(\",\"));\n+\n+        // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+        appendFiles.set(DEPENDS_ON_TABLES, dependencies);\n+        appendFiles.commit();\n+\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        Optional<Table> materializedViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!materializedViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        if (!isMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Table materializedView = materializedViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(materializedView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = materializedView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(materializedView.getOwner()),\n+            new HashMap<>(materializedView.getParameters())));\n+    }\n+\n+    public Optional<TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        return Optional.ofNullable(icebergTable.currentSnapshot())\n+            .map(snapshot -> new TableToken(snapshot.snapshotId()));\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!tableToken.isPresent() || !currentToken.isPresent()) {\n+            return false;\n+        }\n+\n+        return tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId();\n+    }\n+\n+    @Override\n+    public MaterializedViewFreshness getMaterializedViewFreshness(ConnectorSession session, SchemaTableName materializedViewName)\n+    {\n+        Map<String, Optional<TableToken>> refreshStateMap = getMaterializedViewToken(session, materializedViewName);\n+        if (refreshStateMap.isEmpty()) {\n+            return new MaterializedViewFreshness(false);\n+        }\n+\n+        for (Map.Entry<String, Optional<TableToken>> entry : refreshStateMap.entrySet()) {\n+            List<String> strings = Splitter.on(\".\").splitToList(entry.getKey());\n+            checkState(strings.size() >= 2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 371}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQyNTA4OA==", "bodyText": "I don't think we need to check for empty, since splitter on empty will return an empty map.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r514425088", "createdAt": "2020-10-29T17:08:07Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -696,4 +728,277 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n                 .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n                 .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n     }\n+\n+    @Override\n+    public void createMaterializedView(ConnectorSession session, SchemaTableName viewName, ConnectorMaterializedViewDefinition definition, boolean replace, boolean ignoreExisting)\n+    {\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Optional<Table> existing = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName());\n+\n+        // It's a create command where the materialized view already exists and 'if not exists' clause is not specified\n+        if (!replace && existing.isPresent()) {\n+            if (ignoreExisting) {\n+                return;\n+            }\n+            throw new PrestoException(ALREADY_EXISTS, \"Materialized view already exists: \" + viewName);\n+        }\n+\n+        // Generate a storage table name and create a storage table. The properties in the definition are table properties for the\n+        // storage table as indicated in the materialized view definition.\n+        String storageTableName = \"st_\" + UUID.randomUUID().toString().replace(\"-\", \"\");\n+        Map<String, Object> storageTableProperties = new HashMap<>(definition.getProperties());\n+        storageTableProperties.putIfAbsent(FILE_FORMAT_PROPERTY, DEFAULT_FILE_FORMAT_DEFAULT);\n+\n+        SchemaTableName storageTable = new SchemaTableName(viewName.getSchemaName(), storageTableName);\n+        List<ColumnMetadata> columns = definition.getColumns().stream()\n+                .map(column -> new ColumnMetadata(column.getName(), typeManager.getType(column.getType())))\n+                .collect(toImmutableList());\n+\n+        ConnectorTableMetadata tableMetadata = new ConnectorTableMetadata(storageTable, columns, storageTableProperties, Optional.empty());\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+\n+        // Create a view indicating the storage table\n+        Map<String, String> viewProperties = ImmutableMap.<String, String>builder()\n+                .put(PRESTO_QUERY_ID_NAME, session.getQueryId())\n+                .put(STORAGE_TABLE, storageTableName)\n+                .put(PRESTO_VIEW_FLAG, \"true\")\n+                .put(TABLE_COMMENT, \"Presto Materialized View\")\n+                .build();\n+\n+        Column dummyColumn = new Column(\"dummy\", HIVE_STRING, Optional.empty());\n+\n+        String schemaName = viewName.getSchemaName();\n+        String tableName = viewName.getTableName();\n+        Table.Builder tableBuilder = Table.builder()\n+                .setDatabaseName(schemaName)\n+                .setTableName(tableName)\n+                .setOwner(session.getUser())\n+                .setTableType(VIRTUAL_VIEW.name())\n+                .setDataColumns(ImmutableList.of(dummyColumn))\n+                .setPartitionColumns(ImmutableList.of())\n+                .setParameters(viewProperties)\n+                .withStorage(storage -> storage.setStorageFormat(VIEW_STORAGE_FORMAT))\n+                .withStorage(storage -> storage.setLocation(\"\"))\n+                .setViewOriginalText(Optional.of(encodeMaterializedViewData(definition)))\n+                .setViewExpandedText(Optional.of(\"/* Presto Materialized View */\"));\n+        Table table = tableBuilder.build();\n+        PrincipalPrivileges principalPrivileges = buildInitialPrivilegeSet(session.getUser());\n+        if (existing.isPresent() && replace) {\n+            // drop the current storage table\n+            String oldStorageTable = existing.get().getParameters().get(STORAGE_TABLE);\n+            if (oldStorageTable != null) {\n+                metastore.dropTable(identity, viewName.getSchemaName(), oldStorageTable, true);\n+            }\n+            // Replace the existing view definition\n+            metastore.replaceTable(identity, viewName.getSchemaName(), viewName.getTableName(), table, principalPrivileges);\n+            return;\n+        }\n+        // create the view definition\n+        metastore.createTable(identity, table, principalPrivileges);\n+    }\n+\n+    @Override\n+    public void dropMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        Table view = metastore.getTable(identity, viewName.getSchemaName(), viewName.getTableName())\n+                .orElseThrow(() -> new MaterializedViewNotFoundException(viewName));\n+\n+        String storageTableName = view.getParameters().get(STORAGE_TABLE);\n+        if (storageTableName != null) {\n+            try {\n+                metastore.dropTable(identity, viewName.getSchemaName(), storageTableName, true);\n+            }\n+            catch (PrestoException e) {\n+                log.warn(e, \"Failed to drop storage table '%s' for materialized view '%s'\", storageTableName, viewName);\n+            }\n+        }\n+        metastore.dropTable(identity, viewName.getSchemaName(), viewName.getTableName(), true);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginRefreshMaterializedView(ConnectorSession session, ConnectorTableHandle tableHandle, List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        transaction = icebergTable.newTransaction();\n+\n+        // delete before insert .. simulating overwrite\n+        executeDelete(session, tableHandle);\n+\n+        return new IcebergWritableTableHandle(\n+            table.getSchemaName(),\n+            table.getTableName(),\n+            SchemaParser.toJson(icebergTable.schema()),\n+            PartitionSpecParser.toJson(icebergTable.spec()),\n+            getColumns(icebergTable.schema(), typeManager),\n+            getDataPath(icebergTable.location()),\n+            getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishRefreshMaterializedView(ConnectorSession session,\n+            ConnectorInsertTableHandle insertHandle,\n+            Collection<Slice> fragments,\n+            Collection<ComputedStatistics> computedStatistics,\n+            List<ConnectorTableHandle> sourceTableHandles)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+        List<CommitTaskData> commitTasks = ImmutableList.of();\n+\n+        commitTasks = fragments.stream()\n+            .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+            .collect(toImmutableList());\n+\n+        Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+            .map(field -> field.transform().getResultType(\n+                icebergTable.schema().findType(field.sourceId())))\n+            .toArray(Type[]::new);\n+\n+        AppendFiles appendFiles = transaction.newFastAppend();\n+        for (CommitTaskData task : commitTasks) {\n+            HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+            DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                    .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                    .withFormat(table.getFileFormat())\n+                    .withMetrics(task.getMetrics().metrics());\n+\n+            if (!icebergTable.spec().fields().isEmpty()) {\n+                String partitionDataJson = task.getPartitionDataJson()\n+                        .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+            }\n+\n+            appendFiles.appendFile(builder.build());\n+        }\n+\n+        String dependencies = sourceTableHandles.stream()\n+                .map(handle -> (IcebergTableHandle) handle)\n+                .filter(handle -> handle.getSnapshotId().isPresent())\n+                .map(handle -> handle.getSchemaTableName() + \"=\" + handle.getSnapshotId().get())\n+                .collect(joining(\",\"));\n+\n+        // Update the 'dependsOnTables' property that tracks tables on which the materialized view depends and the corresponding snapshot ids of the tables\n+        appendFiles.set(DEPENDS_ON_TABLES, dependencies);\n+        appendFiles.commit();\n+\n+        transaction.commitTransaction();\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+            .map(CommitTaskData::getPath)\n+            .collect(toImmutableList())));\n+    }\n+\n+    private boolean isMaterializedView(Table table)\n+    {\n+        if (table.getTableType().equals(VIRTUAL_VIEW.name()) &&\n+                \"true\".equals(table.getParameters().get(PRESTO_VIEW_FLAG)) &&\n+                table.getParameters().containsKey(STORAGE_TABLE)) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMaterializedView(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        final HiveIdentity identity = new HiveIdentity(session);\n+        if (metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).isPresent()) {\n+            Table table = metastore.getTable(identity, schemaTableName.getSchemaName(), schemaTableName.getTableName()).get();\n+            return isMaterializedView(table);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public Optional<ConnectorMaterializedViewDefinition> getMaterializedView(ConnectorSession session, SchemaTableName viewName)\n+    {\n+        Optional<Table> materializedViewOptional = metastore.getTable(new HiveIdentity(session), viewName.getSchemaName(), viewName.getTableName());\n+        if (!materializedViewOptional.isPresent()) {\n+            return Optional.empty();\n+        }\n+        if (!isMaterializedView(session, viewName)) {\n+            return Optional.empty();\n+        }\n+\n+        Table materializedView = materializedViewOptional.get();\n+\n+        ConnectorMaterializedViewDefinition definition = decodeMaterializedViewData(materializedView.getViewOriginalText()\n+                .orElseThrow(() -> new PrestoException(HIVE_INVALID_METADATA, \"No view original text: \" + viewName)));\n+\n+        String storageTable = materializedView.getParameters().getOrDefault(STORAGE_TABLE, \"\");\n+        return Optional.of(new ConnectorMaterializedViewDefinition(\n+            definition.getOriginalSql(),\n+            storageTable,\n+            definition.getCatalog(),\n+            Optional.of(viewName.getSchemaName()),\n+            definition.getColumns(),\n+            definition.getComment(),\n+            Optional.of(materializedView.getOwner()),\n+            new HashMap<>(materializedView.getParameters())));\n+    }\n+\n+    public Optional<TableToken> getTableToken(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        return Optional.ofNullable(icebergTable.currentSnapshot())\n+            .map(snapshot -> new TableToken(snapshot.snapshotId()));\n+    }\n+\n+    public boolean isTableCurrent(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<TableToken> tableToken)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        Optional<TableToken> currentToken = getTableToken(session, handle);\n+\n+        if (!tableToken.isPresent() || !currentToken.isPresent()) {\n+            return false;\n+        }\n+\n+        return tableToken.get().getSnapshotId() == currentToken.get().getSnapshotId();\n+    }\n+\n+    @Override\n+    public MaterializedViewFreshness getMaterializedViewFreshness(ConnectorSession session, SchemaTableName materializedViewName)\n+    {\n+        Map<String, Optional<TableToken>> refreshStateMap = getMaterializedViewToken(session, materializedViewName);\n+        if (refreshStateMap.isEmpty()) {\n+            return new MaterializedViewFreshness(false);\n+        }\n+\n+        for (Map.Entry<String, Optional<TableToken>> entry : refreshStateMap.entrySet()) {\n+            List<String> strings = Splitter.on(\".\").splitToList(entry.getKey());\n+            checkState(strings.size() >= 2);\n+            if (strings.size() == 3) {\n+                strings = strings.subList(1, 3);\n+            }\n+            String schema = strings.get(0);\n+            String name = strings.get(1);\n+            SchemaTableName schemaTableName = new SchemaTableName(schema, name);\n+            if (!isTableCurrent(session, getTableHandle(session, schemaTableName), entry.getValue())) {\n+                return new MaterializedViewFreshness(false);\n+            }\n+        }\n+        return new MaterializedViewFreshness(true);\n+    }\n+\n+    private Map<String, Optional<TableToken>> getMaterializedViewToken(ConnectorSession session, SchemaTableName name)\n+    {\n+        Map<String, Optional<TableToken>> viewToken = new HashMap<>();\n+        Optional<ConnectorMaterializedViewDefinition> materializedViewDefinition = getMaterializedView(session, name);\n+        if (!materializedViewDefinition.isPresent()) {\n+            return viewToken;\n+        }\n+\n+        String storageTableName = materializedViewDefinition.get().getProperties().getOrDefault(STORAGE_TABLE, \"\").toString();\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, new SchemaTableName(name.getSchemaName(), storageTableName));\n+        String dependsOnTables = icebergTable.currentSnapshot().summary().getOrDefault(DEPENDS_ON_TABLES, \"\");\n+        if (!dependsOnTables.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 396}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQyNjEzMQ==", "bodyText": "Can we make this a private class in IcebergMetadata without JSON serialization? It's confusing to have dead code. We can always move it out later when needed.", "url": "https://github.com/trinodb/trino/pull/4832#discussion_r514426131", "createdAt": "2020-10-29T17:09:40Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/TableToken.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+public class TableToken", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTkyMzkwNQ=="}, "originalCommit": null, "originalPosition": 19}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f7215ec8486747b4274fa43ce30fc397b8f9970b", "author": {"user": {"login": "anjalinorwood", "name": "Anjali Norwood"}}, "url": "https://github.com/trinodb/trino/commit/f7215ec8486747b4274fa43ce30fc397b8f9970b", "committedDate": "2020-10-31T17:54:53Z", "message": "Disallow 'CREATE OR REPLACE' and 'IF NOT EXISTS'...\n\n... clauses together in 'CREATE MATERIALIZED VIEW' statement.\n\nThe optional 'IF NOT EXISTS' clause causes the error to be suppressed\nif a materialized view already exists. This clause was silently ignored\nif 'OR REPLACE' was specified.\nThis can be confusing to users, so this commit explicitly throws an error\nwhen the two clauses are specified together."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4effcf7b3d75ca5fc0fa38ffb48b774a27e303b1", "author": {"user": {"login": "anjalinorwood", "name": "Anjali Norwood"}}, "url": "https://github.com/trinodb/trino/commit/4effcf7b3d75ca5fc0fa38ffb48b774a27e303b1", "committedDate": "2020-10-31T17:54:53Z", "message": "Materialized view connector API changes\n\nEven though materialized views are accessible through a connector and can be said to\n'belong' to a connector, they need to be treated separately from tables. Connector\nAPIs for materialized views that modeled them as TableHandles are being changed to\npass in materialized view name rather than handles."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ecbb65943dcb1f2bd2ef9bb1594d7709cdab5c63", "author": {"user": {"login": "anjalinorwood", "name": "Anjali Norwood"}}, "url": "https://github.com/trinodb/trino/commit/ecbb65943dcb1f2bd2ef9bb1594d7709cdab5c63", "committedDate": "2020-10-31T17:54:53Z", "message": "Send storage table handle to finishRefreshMaterializedView"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "2081b4d1ec51261a4722e50b92c46848da1990fa", "author": {"user": {"login": "anjalinorwood", "name": "Anjali Norwood"}}, "url": "https://github.com/trinodb/trino/commit/2081b4d1ec51261a4722e50b92c46848da1990fa", "committedDate": "2020-11-01T18:25:37Z", "message": "Iceberg connector support for materialized views\n\nIceberg connector allows creation, refresh and drop of materialized views.\nIt provides methods to determine if a given materialized view is current\nwith respect to underlying base table(s).\nFreshness of a materialized view is determined by comparing the snapshot ID\nof base table(s) at the time the materialized view was refreshed with\nsnapshot ID of base table(s) at the time of the check."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e473e3312b3fd432e0dfab25ffb803a983a51518", "author": {"user": {"login": "anjalinorwood", "name": "Anjali Norwood"}}, "url": "https://github.com/trinodb/trino/commit/e473e3312b3fd432e0dfab25ffb803a983a51518", "committedDate": "2020-11-01T18:25:53Z", "message": "Fix typo in 'assertInvalidStatemennt'"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "e473e3312b3fd432e0dfab25ffb803a983a51518", "author": {"user": {"login": "anjalinorwood", "name": "Anjali Norwood"}}, "url": "https://github.com/trinodb/trino/commit/e473e3312b3fd432e0dfab25ffb803a983a51518", "committedDate": "2020-11-01T18:25:53Z", "message": "Fix typo in 'assertInvalidStatemennt'"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4368, "cost": 1, "resetAt": "2021-10-28T20:13:43Z"}}}