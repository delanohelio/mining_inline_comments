{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzU5NjkxNDgx", "number": 2418, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQyMzoyNTo0NlrODhQl7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQyMzo0NDoxNVrODhQ1PQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2MjAxNDU0OnYy", "diffSide": "RIGHT", "path": "presto-main/src/main/java/io/prestosql/operator/BasicWorkProcessorOperatorAdapter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQyMzoyNTo0NlrOFr7Y5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQyMzoyNTo0NlrOFr7Y5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTYwNjExNw==", "bodyText": "This can be private", "url": "https://github.com/trinodb/trino/pull/2418#discussion_r381606117", "createdAt": "2020-02-19T23:25:46Z", "author": {"login": "dain"}, "path": "presto-main/src/main/java/io/prestosql/operator/BasicWorkProcessorOperatorAdapter.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.operator;\n+\n+import io.prestosql.execution.Lifespan;\n+import io.prestosql.operator.WorkProcessorOperatorAdapter.AdapterWorkProcessorOperator;\n+import io.prestosql.operator.WorkProcessorOperatorAdapter.AdapterWorkProcessorOperatorFactory;\n+import io.prestosql.spi.Page;\n+import io.prestosql.sql.planner.plan.PlanNodeId;\n+\n+import java.util.Optional;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class BasicWorkProcessorOperatorAdapter\n+        implements AdapterWorkProcessorOperator\n+{\n+    public interface BasicAdapterWorkProcessorOperatorFactory\n+            extends WorkProcessorOperatorFactory\n+    {\n+        default WorkProcessorOperator createAdapterOperator(ProcessorContext processorContext, WorkProcessor<Page> sourcePages)\n+        {\n+            return create(processorContext, sourcePages);\n+        }\n+\n+        BasicAdapterWorkProcessorOperatorFactory duplicate();\n+    }\n+\n+    public static OperatorFactory createAdapterOperatorFactory(BasicAdapterWorkProcessorOperatorFactory operatorFactory)\n+    {\n+        return WorkProcessorOperatorAdapter.createAdapterOperatorFactory(new Factory(operatorFactory));\n+    }\n+\n+    private static class Factory\n+            implements AdapterWorkProcessorOperatorFactory\n+    {\n+        final BasicAdapterWorkProcessorOperatorFactory operatorFactory;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2MjAxNTU2OnYy", "diffSide": "RIGHT", "path": "presto-main/src/main/java/io/prestosql/operator/BasicWorkProcessorOperatorAdapter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQyMzoyNjoxMVrOFr7ZeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQyMzoyNjoxMVrOFr7ZeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTYwNjI2NQ==", "bodyText": "This can be private", "url": "https://github.com/trinodb/trino/pull/2418#discussion_r381606265", "createdAt": "2020-02-19T23:26:11Z", "author": {"login": "dain"}, "path": "presto-main/src/main/java/io/prestosql/operator/BasicWorkProcessorOperatorAdapter.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.operator;\n+\n+import io.prestosql.execution.Lifespan;\n+import io.prestosql.operator.WorkProcessorOperatorAdapter.AdapterWorkProcessorOperator;\n+import io.prestosql.operator.WorkProcessorOperatorAdapter.AdapterWorkProcessorOperatorFactory;\n+import io.prestosql.spi.Page;\n+import io.prestosql.sql.planner.plan.PlanNodeId;\n+\n+import java.util.Optional;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class BasicWorkProcessorOperatorAdapter\n+        implements AdapterWorkProcessorOperator\n+{\n+    public interface BasicAdapterWorkProcessorOperatorFactory\n+            extends WorkProcessorOperatorFactory\n+    {\n+        default WorkProcessorOperator createAdapterOperator(ProcessorContext processorContext, WorkProcessor<Page> sourcePages)\n+        {\n+            return create(processorContext, sourcePages);\n+        }\n+\n+        BasicAdapterWorkProcessorOperatorFactory duplicate();\n+    }\n+\n+    public static OperatorFactory createAdapterOperatorFactory(BasicAdapterWorkProcessorOperatorFactory operatorFactory)\n+    {\n+        return WorkProcessorOperatorAdapter.createAdapterOperatorFactory(new Factory(operatorFactory));\n+    }\n+\n+    private static class Factory\n+            implements AdapterWorkProcessorOperatorFactory\n+    {\n+        final BasicAdapterWorkProcessorOperatorFactory operatorFactory;\n+\n+        Factory(BasicAdapterWorkProcessorOperatorFactory operatorFactory)\n+        {\n+            this.operatorFactory = requireNonNull(operatorFactory, \"operatorFactory is null\");\n+        }\n+\n+        @Override\n+        public AdapterWorkProcessorOperatorFactory duplicate()\n+        {\n+            return new Factory(operatorFactory.duplicate());\n+        }\n+\n+        @Override\n+        public int getOperatorId()\n+        {\n+            return operatorFactory.getOperatorId();\n+        }\n+\n+        @Override\n+        public PlanNodeId getPlanNodeId()\n+        {\n+            return operatorFactory.getPlanNodeId();\n+        }\n+\n+        @Override\n+        public String getOperatorType()\n+        {\n+            return operatorFactory.getOperatorType();\n+        }\n+\n+        @Override\n+        public WorkProcessorOperator create(ProcessorContext processorContext, WorkProcessor<Page> sourcePages)\n+        {\n+            return operatorFactory.create(processorContext, sourcePages);\n+        }\n+\n+        @Override\n+        public AdapterWorkProcessorOperator createAdapterOperator(ProcessorContext processorContext)\n+        {\n+            return new BasicWorkProcessorOperatorAdapter(processorContext, operatorFactory);\n+        }\n+\n+        @Override\n+        public void lifespanFinished(Lifespan lifespan)\n+        {\n+            operatorFactory.lifespanFinished(lifespan);\n+        }\n+\n+        @Override\n+        public void close()\n+        {\n+            operatorFactory.close();\n+        }\n+    }\n+\n+    private final PageBuffer pageBuffer;\n+    private final WorkProcessorOperator operator;\n+\n+    public BasicWorkProcessorOperatorAdapter(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2MjAzMjY5OnYy", "diffSide": "RIGHT", "path": "presto-main/src/main/java/io/prestosql/operator/BasicWorkProcessorOperatorAdapter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQyMzozNDowNFrOFr7j6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxMjowMjoyNFrOFsQzHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTYwODkzOA==", "bodyText": "It isn't clear when I would use this instead of AdapterWorkProcessorOperator.  The other direct implementations of AdapterWorkProcessorOperator use a buffer, but seem to have some slightly different code for things like needs input... maybe add a comment about the requirement (assumptions?) for using this class.", "url": "https://github.com/trinodb/trino/pull/2418#discussion_r381608938", "createdAt": "2020-02-19T23:34:04Z", "author": {"login": "dain"}, "path": "presto-main/src/main/java/io/prestosql/operator/BasicWorkProcessorOperatorAdapter.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.operator;\n+\n+import io.prestosql.execution.Lifespan;\n+import io.prestosql.operator.WorkProcessorOperatorAdapter.AdapterWorkProcessorOperator;\n+import io.prestosql.operator.WorkProcessorOperatorAdapter.AdapterWorkProcessorOperatorFactory;\n+import io.prestosql.spi.Page;\n+import io.prestosql.sql.planner.plan.PlanNodeId;\n+\n+import java.util.Optional;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class BasicWorkProcessorOperatorAdapter", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk1Njg5Mw==", "bodyText": "added comments", "url": "https://github.com/trinodb/trino/pull/2418#discussion_r381956893", "createdAt": "2020-02-20T12:02:24Z", "author": {"login": "sopel39"}, "path": "presto-main/src/main/java/io/prestosql/operator/BasicWorkProcessorOperatorAdapter.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.operator;\n+\n+import io.prestosql.execution.Lifespan;\n+import io.prestosql.operator.WorkProcessorOperatorAdapter.AdapterWorkProcessorOperator;\n+import io.prestosql.operator.WorkProcessorOperatorAdapter.AdapterWorkProcessorOperatorFactory;\n+import io.prestosql.spi.Page;\n+import io.prestosql.sql.planner.plan.PlanNodeId;\n+\n+import java.util.Optional;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class BasicWorkProcessorOperatorAdapter", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTYwODkzOA=="}, "originalCommit": null, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2MjA0NzA2OnYy", "diffSide": "RIGHT", "path": "presto-main/src/main/java/io/prestosql/operator/StreamingAggregationOperator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQyMzo0MToxMFrOFr7sjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQyMzo0MToxMFrOFr7sjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTYxMTE0OA==", "bodyText": "I think the invert of this condition might be more readable:\nif (outputPages.isEmpty()) {\n    return needsMoreData();\n}\nreturn ofResult(outputPages.removeFirst(), !finishing);", "url": "https://github.com/trinodb/trino/pull/2418#discussion_r381611148", "createdAt": "2020-02-19T23:41:10Z", "author": {"login": "dain"}, "path": "presto-main/src/main/java/io/prestosql/operator/StreamingAggregationOperator.java", "diffHunk": "@@ -64,234 +102,267 @@ public StreamingAggregationOperatorFactory(int operatorId, PlanNodeId planNodeId\n         }\n \n         @Override\n-        public Operator createOperator(DriverContext driverContext)\n+        public int getOperatorId()\n         {\n-            checkState(!closed, \"Factory is already closed\");\n-            OperatorContext operatorContext = driverContext.addOperatorContext(operatorId, planNodeId, StreamingAggregationOperator.class.getSimpleName());\n-            return new StreamingAggregationOperator(operatorContext, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+            return operatorId;\n         }\n \n         @Override\n-        public void noMoreOperators()\n+        public PlanNodeId getPlanNodeId()\n         {\n-            closed = true;\n+            return planNodeId;\n         }\n \n         @Override\n-        public OperatorFactory duplicate()\n+        public String getOperatorType()\n         {\n-            return new StreamingAggregationOperatorFactory(operatorId, planNodeId, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+            return StreamingAggregationOperator.class.getSimpleName();\n         }\n-    }\n \n-    private final OperatorContext operatorContext;\n-    private final LocalMemoryContext systemMemoryContext;\n-    private final LocalMemoryContext userMemoryContext;\n-    private final List<Type> groupByTypes;\n-    private final int[] groupByChannels;\n-    private final List<AccumulatorFactory> accumulatorFactories;\n-    private final Step step;\n-    private final PagesHashStrategy pagesHashStrategy;\n-\n-    private List<Aggregator> aggregates;\n-    private final PageBuilder pageBuilder;\n-    private final Deque<Page> outputPages = new LinkedList<>();\n-    private Page currentGroup;\n-    private boolean finishing;\n-\n-    public StreamingAggregationOperator(OperatorContext operatorContext, List<Type> sourceTypes, List<Type> groupByTypes, List<Integer> groupByChannels, Step step, List<AccumulatorFactory> accumulatorFactories, JoinCompiler joinCompiler)\n-    {\n-        this.operatorContext = requireNonNull(operatorContext, \"operatorContext is null\");\n-        this.systemMemoryContext = operatorContext.newLocalSystemMemoryContext(StreamingAggregationOperator.class.getSimpleName());\n-        this.userMemoryContext = operatorContext.localUserMemoryContext();\n-        this.groupByTypes = ImmutableList.copyOf(requireNonNull(groupByTypes, \"groupByTypes is null\"));\n-        this.groupByChannels = Ints.toArray(requireNonNull(groupByChannels, \"groupByChannels is null\"));\n-        this.accumulatorFactories = requireNonNull(accumulatorFactories, \"accumulatorFactories is null\");\n-        this.step = requireNonNull(step, \"step is null\");\n-\n-        this.aggregates = setupAggregates(step, accumulatorFactories);\n-        this.pageBuilder = new PageBuilder(toTypes(groupByTypes, aggregates));\n-        requireNonNull(joinCompiler, \"joinCompiler is null\");\n-\n-        requireNonNull(sourceTypes, \"sourceTypes is null\");\n-        pagesHashStrategy = joinCompiler.compilePagesHashStrategyFactory(sourceTypes, groupByChannels, Optional.empty())\n-                .createPagesHashStrategy(\n-                        sourceTypes.stream()\n-                                .map(type -> ImmutableList.<Block>of())\n-                                .collect(toImmutableList()), OptionalInt.empty());\n-    }\n-\n-    private List<Aggregator> setupAggregates(Step step, List<AccumulatorFactory> accumulatorFactories)\n-    {\n-        ImmutableList.Builder<Aggregator> builder = ImmutableList.builder();\n-        for (AccumulatorFactory factory : accumulatorFactories) {\n-            builder.add(new Aggregator(factory, step));\n+        @Override\n+        public WorkProcessorOperator create(ProcessorContext processorContext, WorkProcessor<Page> sourcePages)\n+        {\n+            checkState(!closed, \"Factory is already closed\");\n+            return new StreamingAggregationOperator(processorContext, sourcePages, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n         }\n-        return builder.build();\n-    }\n \n-    private static List<Type> toTypes(List<Type> groupByTypes, List<Aggregator> aggregates)\n-    {\n-        ImmutableList.Builder<Type> builder = ImmutableList.builder();\n-        builder.addAll(groupByTypes);\n-        aggregates.stream()\n-                .map(Aggregator::getType)\n-                .forEach(builder::add);\n-        return builder.build();\n-    }\n+        @Override\n+        public void close()\n+        {\n+            closed = true;\n+        }\n \n-    @Override\n-    public OperatorContext getOperatorContext()\n-    {\n-        return operatorContext;\n+        @Override\n+        public Factory duplicate()\n+        {\n+            return new Factory(operatorId, planNodeId, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+        }\n     }\n \n-    @Override\n-    public boolean needsInput()\n+    private final WorkProcessor<Page> pages;\n+\n+    private StreamingAggregationOperator(\n+            ProcessorContext processorContext,\n+            WorkProcessor<Page> sourcePages,\n+            List<Type> sourceTypes,\n+            List<Type> groupByTypes,\n+            List<Integer> groupByChannels,\n+            Step step,\n+            List<AccumulatorFactory> accumulatorFactories,\n+            JoinCompiler joinCompiler)\n     {\n-        return !finishing && outputPages.isEmpty();\n+        pages = sourcePages\n+                .transform(new StreamingAggregation(\n+                        processorContext,\n+                        sourceTypes,\n+                        groupByTypes,\n+                        groupByChannels,\n+                        step,\n+                        accumulatorFactories,\n+                        joinCompiler));\n     }\n \n     @Override\n-    public void addInput(Page page)\n+    public WorkProcessor<Page> getOutputPages()\n     {\n-        checkState(!finishing, \"Operator is already finishing\");\n-        requireNonNull(page, \"page is null\");\n-\n-        processInput(page);\n-        updateMemoryUsage();\n+        return pages;\n     }\n \n-    private void updateMemoryUsage()\n+    private static class StreamingAggregation\n+            implements Transformation<Page, Page>\n     {\n-        long memorySize = pageBuilder.getRetainedSizeInBytes();\n-        for (Page output : outputPages) {\n-            memorySize += output.getRetainedSizeInBytes();\n-        }\n-        for (Aggregator aggregator : aggregates) {\n-            memorySize += aggregator.getEstimatedSize();\n+        private final LocalMemoryContext systemMemoryContext;\n+        private final LocalMemoryContext userMemoryContext;\n+        private final List<Type> groupByTypes;\n+        private final int[] groupByChannels;\n+        private final List<AccumulatorFactory> accumulatorFactories;\n+        private final Step step;\n+        private final PagesHashStrategy pagesHashStrategy;\n+\n+        private List<Aggregator> aggregates;\n+        private final PageBuilder pageBuilder;\n+        private final Deque<Page> outputPages = new LinkedList<>();\n+        private Page currentGroup;\n+\n+        private StreamingAggregation(\n+                ProcessorContext processorContext,\n+                List<Type> sourceTypes,\n+                List<Type> groupByTypes,\n+                List<Integer> groupByChannels,\n+                Step step,\n+                List<AccumulatorFactory> accumulatorFactories,\n+                JoinCompiler joinCompiler)\n+        {\n+            requireNonNull(processorContext, \"processorContext is null\");\n+            this.systemMemoryContext = processorContext.getMemoryTrackingContext().localSystemMemoryContext();\n+            this.userMemoryContext = processorContext.getMemoryTrackingContext().localUserMemoryContext();\n+            this.groupByTypes = ImmutableList.copyOf(requireNonNull(groupByTypes, \"groupByTypes is null\"));\n+            this.groupByChannels = Ints.toArray(requireNonNull(groupByChannels, \"groupByChannels is null\"));\n+            this.accumulatorFactories = requireNonNull(accumulatorFactories, \"accumulatorFactories is null\");\n+            this.step = requireNonNull(step, \"step is null\");\n+\n+            this.aggregates = setupAggregates(step, accumulatorFactories);\n+            this.pageBuilder = new PageBuilder(toTypes(groupByTypes, aggregates));\n+            requireNonNull(joinCompiler, \"joinCompiler is null\");\n+\n+            requireNonNull(sourceTypes, \"sourceTypes is null\");\n+            pagesHashStrategy = joinCompiler.compilePagesHashStrategyFactory(sourceTypes, groupByChannels, Optional.empty())\n+                    .createPagesHashStrategy(\n+                            sourceTypes.stream()\n+                                    .map(type -> ImmutableList.<Block>of())\n+                                    .collect(toImmutableList()), OptionalInt.empty());\n         }\n \n-        if (currentGroup != null) {\n-            memorySize += currentGroup.getRetainedSizeInBytes();\n-        }\n+        @Override\n+        public TransformationState<Page> process(@Nullable Page inputPage)\n+        {\n+            boolean finishing = inputPage == null;\n+\n+            if (finishing) {\n+                if (currentGroup != null) {\n+                    evaluateAndFlushGroup(currentGroup, 0);\n+                    currentGroup = null;\n+                }\n+\n+                if (!pageBuilder.isEmpty()) {\n+                    outputPages.add(pageBuilder.build());\n+                    pageBuilder.reset();\n+                }\n+\n+                if (outputPages.isEmpty()) {\n+                    return finished();\n+                }\n+            }\n+            else {\n+                processInput(inputPage);\n+                updateMemoryUsage();\n+            }\n \n-        if (step.isOutputPartial()) {\n-            systemMemoryContext.setBytes(memorySize);\n-        }\n-        else {\n-            userMemoryContext.setBytes(memorySize);\n-        }\n-    }\n+            if (!outputPages.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 310}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2MjA1MzczOnYy", "diffSide": "RIGHT", "path": "presto-main/src/main/java/io/prestosql/operator/StreamingAggregationOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQyMzo0NDoxNVrOFr7wdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxMTo0NzoxOVrOFsQYSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTYxMjE0OQ==", "bodyText": "It is easier to review if you reorganize methods in a separate commit", "url": "https://github.com/trinodb/trino/pull/2418#discussion_r381612149", "createdAt": "2020-02-19T23:44:15Z", "author": {"login": "dain"}, "path": "presto-main/src/main/java/io/prestosql/operator/StreamingAggregationOperator.java", "diffHunk": "@@ -64,234 +102,267 @@ public StreamingAggregationOperatorFactory(int operatorId, PlanNodeId planNodeId\n         }\n \n         @Override\n-        public Operator createOperator(DriverContext driverContext)\n+        public int getOperatorId()\n         {\n-            checkState(!closed, \"Factory is already closed\");\n-            OperatorContext operatorContext = driverContext.addOperatorContext(operatorId, planNodeId, StreamingAggregationOperator.class.getSimpleName());\n-            return new StreamingAggregationOperator(operatorContext, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+            return operatorId;\n         }\n \n         @Override\n-        public void noMoreOperators()\n+        public PlanNodeId getPlanNodeId()\n         {\n-            closed = true;\n+            return planNodeId;\n         }\n \n         @Override\n-        public OperatorFactory duplicate()\n+        public String getOperatorType()\n         {\n-            return new StreamingAggregationOperatorFactory(operatorId, planNodeId, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+            return StreamingAggregationOperator.class.getSimpleName();\n         }\n-    }\n \n-    private final OperatorContext operatorContext;\n-    private final LocalMemoryContext systemMemoryContext;\n-    private final LocalMemoryContext userMemoryContext;\n-    private final List<Type> groupByTypes;\n-    private final int[] groupByChannels;\n-    private final List<AccumulatorFactory> accumulatorFactories;\n-    private final Step step;\n-    private final PagesHashStrategy pagesHashStrategy;\n-\n-    private List<Aggregator> aggregates;\n-    private final PageBuilder pageBuilder;\n-    private final Deque<Page> outputPages = new LinkedList<>();\n-    private Page currentGroup;\n-    private boolean finishing;\n-\n-    public StreamingAggregationOperator(OperatorContext operatorContext, List<Type> sourceTypes, List<Type> groupByTypes, List<Integer> groupByChannels, Step step, List<AccumulatorFactory> accumulatorFactories, JoinCompiler joinCompiler)\n-    {\n-        this.operatorContext = requireNonNull(operatorContext, \"operatorContext is null\");\n-        this.systemMemoryContext = operatorContext.newLocalSystemMemoryContext(StreamingAggregationOperator.class.getSimpleName());\n-        this.userMemoryContext = operatorContext.localUserMemoryContext();\n-        this.groupByTypes = ImmutableList.copyOf(requireNonNull(groupByTypes, \"groupByTypes is null\"));\n-        this.groupByChannels = Ints.toArray(requireNonNull(groupByChannels, \"groupByChannels is null\"));\n-        this.accumulatorFactories = requireNonNull(accumulatorFactories, \"accumulatorFactories is null\");\n-        this.step = requireNonNull(step, \"step is null\");\n-\n-        this.aggregates = setupAggregates(step, accumulatorFactories);\n-        this.pageBuilder = new PageBuilder(toTypes(groupByTypes, aggregates));\n-        requireNonNull(joinCompiler, \"joinCompiler is null\");\n-\n-        requireNonNull(sourceTypes, \"sourceTypes is null\");\n-        pagesHashStrategy = joinCompiler.compilePagesHashStrategyFactory(sourceTypes, groupByChannels, Optional.empty())\n-                .createPagesHashStrategy(\n-                        sourceTypes.stream()\n-                                .map(type -> ImmutableList.<Block>of())\n-                                .collect(toImmutableList()), OptionalInt.empty());\n-    }\n-\n-    private List<Aggregator> setupAggregates(Step step, List<AccumulatorFactory> accumulatorFactories)\n-    {\n-        ImmutableList.Builder<Aggregator> builder = ImmutableList.builder();\n-        for (AccumulatorFactory factory : accumulatorFactories) {\n-            builder.add(new Aggregator(factory, step));\n+        @Override\n+        public WorkProcessorOperator create(ProcessorContext processorContext, WorkProcessor<Page> sourcePages)\n+        {\n+            checkState(!closed, \"Factory is already closed\");\n+            return new StreamingAggregationOperator(processorContext, sourcePages, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n         }\n-        return builder.build();\n-    }\n \n-    private static List<Type> toTypes(List<Type> groupByTypes, List<Aggregator> aggregates)\n-    {\n-        ImmutableList.Builder<Type> builder = ImmutableList.builder();\n-        builder.addAll(groupByTypes);\n-        aggregates.stream()\n-                .map(Aggregator::getType)\n-                .forEach(builder::add);\n-        return builder.build();\n-    }\n+        @Override\n+        public void close()\n+        {\n+            closed = true;\n+        }\n \n-    @Override\n-    public OperatorContext getOperatorContext()\n-    {\n-        return operatorContext;\n+        @Override\n+        public Factory duplicate()\n+        {\n+            return new Factory(operatorId, planNodeId, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+        }\n     }\n \n-    @Override\n-    public boolean needsInput()\n+    private final WorkProcessor<Page> pages;\n+\n+    private StreamingAggregationOperator(\n+            ProcessorContext processorContext,\n+            WorkProcessor<Page> sourcePages,\n+            List<Type> sourceTypes,\n+            List<Type> groupByTypes,\n+            List<Integer> groupByChannels,\n+            Step step,\n+            List<AccumulatorFactory> accumulatorFactories,\n+            JoinCompiler joinCompiler)\n     {\n-        return !finishing && outputPages.isEmpty();\n+        pages = sourcePages\n+                .transform(new StreamingAggregation(\n+                        processorContext,\n+                        sourceTypes,\n+                        groupByTypes,\n+                        groupByChannels,\n+                        step,\n+                        accumulatorFactories,\n+                        joinCompiler));\n     }\n \n     @Override\n-    public void addInput(Page page)\n+    public WorkProcessor<Page> getOutputPages()\n     {\n-        checkState(!finishing, \"Operator is already finishing\");\n-        requireNonNull(page, \"page is null\");\n-\n-        processInput(page);\n-        updateMemoryUsage();\n+        return pages;\n     }\n \n-    private void updateMemoryUsage()\n+    private static class StreamingAggregation\n+            implements Transformation<Page, Page>\n     {\n-        long memorySize = pageBuilder.getRetainedSizeInBytes();\n-        for (Page output : outputPages) {\n-            memorySize += output.getRetainedSizeInBytes();\n-        }\n-        for (Aggregator aggregator : aggregates) {\n-            memorySize += aggregator.getEstimatedSize();\n+        private final LocalMemoryContext systemMemoryContext;\n+        private final LocalMemoryContext userMemoryContext;\n+        private final List<Type> groupByTypes;\n+        private final int[] groupByChannels;\n+        private final List<AccumulatorFactory> accumulatorFactories;\n+        private final Step step;\n+        private final PagesHashStrategy pagesHashStrategy;\n+\n+        private List<Aggregator> aggregates;\n+        private final PageBuilder pageBuilder;\n+        private final Deque<Page> outputPages = new LinkedList<>();\n+        private Page currentGroup;\n+\n+        private StreamingAggregation(\n+                ProcessorContext processorContext,\n+                List<Type> sourceTypes,\n+                List<Type> groupByTypes,\n+                List<Integer> groupByChannels,\n+                Step step,\n+                List<AccumulatorFactory> accumulatorFactories,\n+                JoinCompiler joinCompiler)\n+        {\n+            requireNonNull(processorContext, \"processorContext is null\");\n+            this.systemMemoryContext = processorContext.getMemoryTrackingContext().localSystemMemoryContext();\n+            this.userMemoryContext = processorContext.getMemoryTrackingContext().localUserMemoryContext();\n+            this.groupByTypes = ImmutableList.copyOf(requireNonNull(groupByTypes, \"groupByTypes is null\"));\n+            this.groupByChannels = Ints.toArray(requireNonNull(groupByChannels, \"groupByChannels is null\"));\n+            this.accumulatorFactories = requireNonNull(accumulatorFactories, \"accumulatorFactories is null\");\n+            this.step = requireNonNull(step, \"step is null\");\n+\n+            this.aggregates = setupAggregates(step, accumulatorFactories);\n+            this.pageBuilder = new PageBuilder(toTypes(groupByTypes, aggregates));\n+            requireNonNull(joinCompiler, \"joinCompiler is null\");\n+\n+            requireNonNull(sourceTypes, \"sourceTypes is null\");\n+            pagesHashStrategy = joinCompiler.compilePagesHashStrategyFactory(sourceTypes, groupByChannels, Optional.empty())\n+                    .createPagesHashStrategy(\n+                            sourceTypes.stream()\n+                                    .map(type -> ImmutableList.<Block>of())\n+                                    .collect(toImmutableList()), OptionalInt.empty());\n         }\n \n-        if (currentGroup != null) {\n-            memorySize += currentGroup.getRetainedSizeInBytes();\n-        }\n+        @Override\n+        public TransformationState<Page> process(@Nullable Page inputPage)\n+        {\n+            boolean finishing = inputPage == null;\n+\n+            if (finishing) {\n+                if (currentGroup != null) {\n+                    evaluateAndFlushGroup(currentGroup, 0);\n+                    currentGroup = null;\n+                }\n+\n+                if (!pageBuilder.isEmpty()) {\n+                    outputPages.add(pageBuilder.build());\n+                    pageBuilder.reset();\n+                }\n+\n+                if (outputPages.isEmpty()) {\n+                    return finished();\n+                }\n+            }\n+            else {\n+                processInput(inputPage);\n+                updateMemoryUsage();\n+            }\n \n-        if (step.isOutputPartial()) {\n-            systemMemoryContext.setBytes(memorySize);\n-        }\n-        else {\n-            userMemoryContext.setBytes(memorySize);\n-        }\n-    }\n+            if (!outputPages.isEmpty()) {\n+                return ofResult(outputPages.removeFirst(), !finishing);\n+            }\n \n-    private void processInput(Page page)\n-    {\n-        requireNonNull(page, \"page is null\");\n+            return needsMoreData();\n+        }\n \n-        Page groupByPage = extractColumns(page, groupByChannels);\n-        if (currentGroup != null) {\n-            if (!pagesHashStrategy.rowEqualsRow(0, extractColumns(currentGroup, groupByChannels), 0, groupByPage)) {\n-                // page starts with new group, so flush it\n-                evaluateAndFlushGroup(currentGroup, 0);\n+        private void updateMemoryUsage()\n+        {\n+            long memorySize = pageBuilder.getRetainedSizeInBytes();\n+            for (Page output : outputPages) {\n+                memorySize += output.getRetainedSizeInBytes();\n+            }\n+            for (Aggregator aggregator : aggregates) {\n+                memorySize += aggregator.getEstimatedSize();\n             }\n-            currentGroup = null;\n-        }\n \n-        int startPosition = 0;\n-        while (true) {\n-            // may be equal to page.getPositionCount() if the end is not found in this page\n-            int nextGroupStart = findNextGroupStart(startPosition, groupByPage);\n-            addRowsToAggregates(page, startPosition, nextGroupStart - 1);\n+            if (currentGroup != null) {\n+                memorySize += currentGroup.getRetainedSizeInBytes();\n+            }\n \n-            if (nextGroupStart < page.getPositionCount()) {\n-                // current group stops somewhere in the middle of the page, so flush it\n-                evaluateAndFlushGroup(page, startPosition);\n-                startPosition = nextGroupStart;\n+            if (step.isOutputPartial()) {\n+                systemMemoryContext.setBytes(memorySize);\n             }\n             else {\n-                currentGroup = page.getRegion(page.getPositionCount() - 1, 1);\n-                return;\n+                userMemoryContext.setBytes(memorySize);\n             }\n         }\n-    }\n \n-    private static Page extractColumns(Page page, int[] channels)\n-    {\n-        Block[] newBlocks = new Block[channels.length];\n-        for (int i = 0; i < channels.length; i++) {\n-            newBlocks[i] = page.getBlock(channels[i]);\n-        }\n-        return new Page(page.getPositionCount(), newBlocks);\n-    }\n+        private void processInput(Page page)\n+        {\n+            requireNonNull(page, \"page is null\");\n+\n+            Page groupByPage = extractColumns(page, groupByChannels);\n+            if (currentGroup != null) {\n+                if (!pagesHashStrategy.rowEqualsRow(0, extractColumns(currentGroup, groupByChannels), 0, groupByPage)) {\n+                    // page starts with new group, so flush it\n+                    evaluateAndFlushGroup(currentGroup, 0);\n+                }\n+                currentGroup = null;\n+            }\n \n-    private void addRowsToAggregates(Page page, int startPosition, int endPosition)\n-    {\n-        for (Aggregator aggregator : aggregates) {\n-            aggregator.processPage(page.getRegion(startPosition, endPosition - startPosition + 1));\n+            int startPosition = 0;\n+            while (true) {\n+                // may be equal to page.getPositionCount() if the end is not found in this page\n+                int nextGroupStart = findNextGroupStart(startPosition, groupByPage);\n+                addRowsToAggregates(page, startPosition, nextGroupStart - 1);\n+\n+                if (nextGroupStart < page.getPositionCount()) {\n+                    // current group stops somewhere in the middle of the page, so flush it\n+                    evaluateAndFlushGroup(page, startPosition);\n+                    startPosition = nextGroupStart;\n+                }\n+                else {\n+                    currentGroup = page.getRegion(page.getPositionCount() - 1, 1);\n+                    return;\n+                }\n+            }\n         }\n-    }\n \n-    private void evaluateAndFlushGroup(Page page, int position)\n-    {\n-        pageBuilder.declarePosition();\n-        for (int i = 0; i < groupByTypes.size(); i++) {\n-            Block block = page.getBlock(groupByChannels[i]);\n-            Type type = groupByTypes.get(i);\n-            type.appendTo(block, position, pageBuilder.getBlockBuilder(i));\n-        }\n-        int offset = groupByTypes.size();\n-        for (int i = 0; i < aggregates.size(); i++) {\n-            aggregates.get(i).evaluate(pageBuilder.getBlockBuilder(offset + i));\n+        private static Page extractColumns(Page page, int[] channels)\n+        {\n+            Block[] newBlocks = new Block[channels.length];\n+            for (int i = 0; i < channels.length; i++) {\n+                newBlocks[i] = page.getBlock(channels[i]);\n+            }\n+            return new Page(page.getPositionCount(), newBlocks);\n         }\n \n-        if (pageBuilder.isFull()) {\n-            outputPages.add(pageBuilder.build());\n-            pageBuilder.reset();\n+        private void addRowsToAggregates(Page page, int startPosition, int endPosition)\n+        {\n+            for (Aggregator aggregator : aggregates) {\n+                aggregator.processPage(page.getRegion(startPosition, endPosition - startPosition + 1));\n+            }\n         }\n \n-        aggregates = setupAggregates(step, accumulatorFactories);\n-    }\n-\n-    private int findNextGroupStart(int startPosition, Page page)\n-    {\n-        for (int i = startPosition + 1; i < page.getPositionCount(); i++) {\n-            if (!pagesHashStrategy.rowEqualsRow(startPosition, page, i, page)) {\n-                return i;\n+        private void evaluateAndFlushGroup(Page page, int position)\n+        {\n+            pageBuilder.declarePosition();\n+            for (int i = 0; i < groupByTypes.size(); i++) {\n+                Block block = page.getBlock(groupByChannels[i]);\n+                Type type = groupByTypes.get(i);\n+                type.appendTo(block, position, pageBuilder.getBlockBuilder(i));\n+            }\n+            int offset = groupByTypes.size();\n+            for (int i = 0; i < aggregates.size(); i++) {\n+                aggregates.get(i).evaluate(pageBuilder.getBlockBuilder(offset + i));\n             }\n-        }\n \n-        return page.getPositionCount();\n-    }\n+            if (pageBuilder.isFull()) {\n+                outputPages.add(pageBuilder.build());\n+                pageBuilder.reset();\n+            }\n \n-    @Override\n-    public Page getOutput()\n-    {\n-        if (!outputPages.isEmpty()) {\n-            return outputPages.removeFirst();\n+            aggregates = setupAggregates(step, accumulatorFactories);\n         }\n \n-        return null;\n-    }\n-\n-    @Override\n-    public void finish()\n-    {\n-        finishing = true;\n+        private int findNextGroupStart(int startPosition, Page page)\n+        {\n+            for (int i = startPosition + 1; i < page.getPositionCount(); i++) {\n+                if (!pagesHashStrategy.rowEqualsRow(startPosition, page, i, page)) {\n+                    return i;\n+                }\n+            }\n \n-        if (currentGroup != null) {\n-            evaluateAndFlushGroup(currentGroup, 0);\n-            currentGroup = null;\n+            return page.getPositionCount();\n         }\n \n-        if (!pageBuilder.isEmpty()) {\n-            outputPages.add(pageBuilder.build());\n-            pageBuilder.reset();\n+        private static List<Aggregator> setupAggregates(Step step, List<AccumulatorFactory> accumulatorFactories)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 496}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk1MDAyNg==", "bodyText": "Github has an option to ignore indents. Then reviewing changes like this is easier", "url": "https://github.com/trinodb/trino/pull/2418#discussion_r381950026", "createdAt": "2020-02-20T11:47:19Z", "author": {"login": "sopel39"}, "path": "presto-main/src/main/java/io/prestosql/operator/StreamingAggregationOperator.java", "diffHunk": "@@ -64,234 +102,267 @@ public StreamingAggregationOperatorFactory(int operatorId, PlanNodeId planNodeId\n         }\n \n         @Override\n-        public Operator createOperator(DriverContext driverContext)\n+        public int getOperatorId()\n         {\n-            checkState(!closed, \"Factory is already closed\");\n-            OperatorContext operatorContext = driverContext.addOperatorContext(operatorId, planNodeId, StreamingAggregationOperator.class.getSimpleName());\n-            return new StreamingAggregationOperator(operatorContext, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+            return operatorId;\n         }\n \n         @Override\n-        public void noMoreOperators()\n+        public PlanNodeId getPlanNodeId()\n         {\n-            closed = true;\n+            return planNodeId;\n         }\n \n         @Override\n-        public OperatorFactory duplicate()\n+        public String getOperatorType()\n         {\n-            return new StreamingAggregationOperatorFactory(operatorId, planNodeId, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+            return StreamingAggregationOperator.class.getSimpleName();\n         }\n-    }\n \n-    private final OperatorContext operatorContext;\n-    private final LocalMemoryContext systemMemoryContext;\n-    private final LocalMemoryContext userMemoryContext;\n-    private final List<Type> groupByTypes;\n-    private final int[] groupByChannels;\n-    private final List<AccumulatorFactory> accumulatorFactories;\n-    private final Step step;\n-    private final PagesHashStrategy pagesHashStrategy;\n-\n-    private List<Aggregator> aggregates;\n-    private final PageBuilder pageBuilder;\n-    private final Deque<Page> outputPages = new LinkedList<>();\n-    private Page currentGroup;\n-    private boolean finishing;\n-\n-    public StreamingAggregationOperator(OperatorContext operatorContext, List<Type> sourceTypes, List<Type> groupByTypes, List<Integer> groupByChannels, Step step, List<AccumulatorFactory> accumulatorFactories, JoinCompiler joinCompiler)\n-    {\n-        this.operatorContext = requireNonNull(operatorContext, \"operatorContext is null\");\n-        this.systemMemoryContext = operatorContext.newLocalSystemMemoryContext(StreamingAggregationOperator.class.getSimpleName());\n-        this.userMemoryContext = operatorContext.localUserMemoryContext();\n-        this.groupByTypes = ImmutableList.copyOf(requireNonNull(groupByTypes, \"groupByTypes is null\"));\n-        this.groupByChannels = Ints.toArray(requireNonNull(groupByChannels, \"groupByChannels is null\"));\n-        this.accumulatorFactories = requireNonNull(accumulatorFactories, \"accumulatorFactories is null\");\n-        this.step = requireNonNull(step, \"step is null\");\n-\n-        this.aggregates = setupAggregates(step, accumulatorFactories);\n-        this.pageBuilder = new PageBuilder(toTypes(groupByTypes, aggregates));\n-        requireNonNull(joinCompiler, \"joinCompiler is null\");\n-\n-        requireNonNull(sourceTypes, \"sourceTypes is null\");\n-        pagesHashStrategy = joinCompiler.compilePagesHashStrategyFactory(sourceTypes, groupByChannels, Optional.empty())\n-                .createPagesHashStrategy(\n-                        sourceTypes.stream()\n-                                .map(type -> ImmutableList.<Block>of())\n-                                .collect(toImmutableList()), OptionalInt.empty());\n-    }\n-\n-    private List<Aggregator> setupAggregates(Step step, List<AccumulatorFactory> accumulatorFactories)\n-    {\n-        ImmutableList.Builder<Aggregator> builder = ImmutableList.builder();\n-        for (AccumulatorFactory factory : accumulatorFactories) {\n-            builder.add(new Aggregator(factory, step));\n+        @Override\n+        public WorkProcessorOperator create(ProcessorContext processorContext, WorkProcessor<Page> sourcePages)\n+        {\n+            checkState(!closed, \"Factory is already closed\");\n+            return new StreamingAggregationOperator(processorContext, sourcePages, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n         }\n-        return builder.build();\n-    }\n \n-    private static List<Type> toTypes(List<Type> groupByTypes, List<Aggregator> aggregates)\n-    {\n-        ImmutableList.Builder<Type> builder = ImmutableList.builder();\n-        builder.addAll(groupByTypes);\n-        aggregates.stream()\n-                .map(Aggregator::getType)\n-                .forEach(builder::add);\n-        return builder.build();\n-    }\n+        @Override\n+        public void close()\n+        {\n+            closed = true;\n+        }\n \n-    @Override\n-    public OperatorContext getOperatorContext()\n-    {\n-        return operatorContext;\n+        @Override\n+        public Factory duplicate()\n+        {\n+            return new Factory(operatorId, planNodeId, sourceTypes, groupByTypes, groupByChannels, step, accumulatorFactories, joinCompiler);\n+        }\n     }\n \n-    @Override\n-    public boolean needsInput()\n+    private final WorkProcessor<Page> pages;\n+\n+    private StreamingAggregationOperator(\n+            ProcessorContext processorContext,\n+            WorkProcessor<Page> sourcePages,\n+            List<Type> sourceTypes,\n+            List<Type> groupByTypes,\n+            List<Integer> groupByChannels,\n+            Step step,\n+            List<AccumulatorFactory> accumulatorFactories,\n+            JoinCompiler joinCompiler)\n     {\n-        return !finishing && outputPages.isEmpty();\n+        pages = sourcePages\n+                .transform(new StreamingAggregation(\n+                        processorContext,\n+                        sourceTypes,\n+                        groupByTypes,\n+                        groupByChannels,\n+                        step,\n+                        accumulatorFactories,\n+                        joinCompiler));\n     }\n \n     @Override\n-    public void addInput(Page page)\n+    public WorkProcessor<Page> getOutputPages()\n     {\n-        checkState(!finishing, \"Operator is already finishing\");\n-        requireNonNull(page, \"page is null\");\n-\n-        processInput(page);\n-        updateMemoryUsage();\n+        return pages;\n     }\n \n-    private void updateMemoryUsage()\n+    private static class StreamingAggregation\n+            implements Transformation<Page, Page>\n     {\n-        long memorySize = pageBuilder.getRetainedSizeInBytes();\n-        for (Page output : outputPages) {\n-            memorySize += output.getRetainedSizeInBytes();\n-        }\n-        for (Aggregator aggregator : aggregates) {\n-            memorySize += aggregator.getEstimatedSize();\n+        private final LocalMemoryContext systemMemoryContext;\n+        private final LocalMemoryContext userMemoryContext;\n+        private final List<Type> groupByTypes;\n+        private final int[] groupByChannels;\n+        private final List<AccumulatorFactory> accumulatorFactories;\n+        private final Step step;\n+        private final PagesHashStrategy pagesHashStrategy;\n+\n+        private List<Aggregator> aggregates;\n+        private final PageBuilder pageBuilder;\n+        private final Deque<Page> outputPages = new LinkedList<>();\n+        private Page currentGroup;\n+\n+        private StreamingAggregation(\n+                ProcessorContext processorContext,\n+                List<Type> sourceTypes,\n+                List<Type> groupByTypes,\n+                List<Integer> groupByChannels,\n+                Step step,\n+                List<AccumulatorFactory> accumulatorFactories,\n+                JoinCompiler joinCompiler)\n+        {\n+            requireNonNull(processorContext, \"processorContext is null\");\n+            this.systemMemoryContext = processorContext.getMemoryTrackingContext().localSystemMemoryContext();\n+            this.userMemoryContext = processorContext.getMemoryTrackingContext().localUserMemoryContext();\n+            this.groupByTypes = ImmutableList.copyOf(requireNonNull(groupByTypes, \"groupByTypes is null\"));\n+            this.groupByChannels = Ints.toArray(requireNonNull(groupByChannels, \"groupByChannels is null\"));\n+            this.accumulatorFactories = requireNonNull(accumulatorFactories, \"accumulatorFactories is null\");\n+            this.step = requireNonNull(step, \"step is null\");\n+\n+            this.aggregates = setupAggregates(step, accumulatorFactories);\n+            this.pageBuilder = new PageBuilder(toTypes(groupByTypes, aggregates));\n+            requireNonNull(joinCompiler, \"joinCompiler is null\");\n+\n+            requireNonNull(sourceTypes, \"sourceTypes is null\");\n+            pagesHashStrategy = joinCompiler.compilePagesHashStrategyFactory(sourceTypes, groupByChannels, Optional.empty())\n+                    .createPagesHashStrategy(\n+                            sourceTypes.stream()\n+                                    .map(type -> ImmutableList.<Block>of())\n+                                    .collect(toImmutableList()), OptionalInt.empty());\n         }\n \n-        if (currentGroup != null) {\n-            memorySize += currentGroup.getRetainedSizeInBytes();\n-        }\n+        @Override\n+        public TransformationState<Page> process(@Nullable Page inputPage)\n+        {\n+            boolean finishing = inputPage == null;\n+\n+            if (finishing) {\n+                if (currentGroup != null) {\n+                    evaluateAndFlushGroup(currentGroup, 0);\n+                    currentGroup = null;\n+                }\n+\n+                if (!pageBuilder.isEmpty()) {\n+                    outputPages.add(pageBuilder.build());\n+                    pageBuilder.reset();\n+                }\n+\n+                if (outputPages.isEmpty()) {\n+                    return finished();\n+                }\n+            }\n+            else {\n+                processInput(inputPage);\n+                updateMemoryUsage();\n+            }\n \n-        if (step.isOutputPartial()) {\n-            systemMemoryContext.setBytes(memorySize);\n-        }\n-        else {\n-            userMemoryContext.setBytes(memorySize);\n-        }\n-    }\n+            if (!outputPages.isEmpty()) {\n+                return ofResult(outputPages.removeFirst(), !finishing);\n+            }\n \n-    private void processInput(Page page)\n-    {\n-        requireNonNull(page, \"page is null\");\n+            return needsMoreData();\n+        }\n \n-        Page groupByPage = extractColumns(page, groupByChannels);\n-        if (currentGroup != null) {\n-            if (!pagesHashStrategy.rowEqualsRow(0, extractColumns(currentGroup, groupByChannels), 0, groupByPage)) {\n-                // page starts with new group, so flush it\n-                evaluateAndFlushGroup(currentGroup, 0);\n+        private void updateMemoryUsage()\n+        {\n+            long memorySize = pageBuilder.getRetainedSizeInBytes();\n+            for (Page output : outputPages) {\n+                memorySize += output.getRetainedSizeInBytes();\n+            }\n+            for (Aggregator aggregator : aggregates) {\n+                memorySize += aggregator.getEstimatedSize();\n             }\n-            currentGroup = null;\n-        }\n \n-        int startPosition = 0;\n-        while (true) {\n-            // may be equal to page.getPositionCount() if the end is not found in this page\n-            int nextGroupStart = findNextGroupStart(startPosition, groupByPage);\n-            addRowsToAggregates(page, startPosition, nextGroupStart - 1);\n+            if (currentGroup != null) {\n+                memorySize += currentGroup.getRetainedSizeInBytes();\n+            }\n \n-            if (nextGroupStart < page.getPositionCount()) {\n-                // current group stops somewhere in the middle of the page, so flush it\n-                evaluateAndFlushGroup(page, startPosition);\n-                startPosition = nextGroupStart;\n+            if (step.isOutputPartial()) {\n+                systemMemoryContext.setBytes(memorySize);\n             }\n             else {\n-                currentGroup = page.getRegion(page.getPositionCount() - 1, 1);\n-                return;\n+                userMemoryContext.setBytes(memorySize);\n             }\n         }\n-    }\n \n-    private static Page extractColumns(Page page, int[] channels)\n-    {\n-        Block[] newBlocks = new Block[channels.length];\n-        for (int i = 0; i < channels.length; i++) {\n-            newBlocks[i] = page.getBlock(channels[i]);\n-        }\n-        return new Page(page.getPositionCount(), newBlocks);\n-    }\n+        private void processInput(Page page)\n+        {\n+            requireNonNull(page, \"page is null\");\n+\n+            Page groupByPage = extractColumns(page, groupByChannels);\n+            if (currentGroup != null) {\n+                if (!pagesHashStrategy.rowEqualsRow(0, extractColumns(currentGroup, groupByChannels), 0, groupByPage)) {\n+                    // page starts with new group, so flush it\n+                    evaluateAndFlushGroup(currentGroup, 0);\n+                }\n+                currentGroup = null;\n+            }\n \n-    private void addRowsToAggregates(Page page, int startPosition, int endPosition)\n-    {\n-        for (Aggregator aggregator : aggregates) {\n-            aggregator.processPage(page.getRegion(startPosition, endPosition - startPosition + 1));\n+            int startPosition = 0;\n+            while (true) {\n+                // may be equal to page.getPositionCount() if the end is not found in this page\n+                int nextGroupStart = findNextGroupStart(startPosition, groupByPage);\n+                addRowsToAggregates(page, startPosition, nextGroupStart - 1);\n+\n+                if (nextGroupStart < page.getPositionCount()) {\n+                    // current group stops somewhere in the middle of the page, so flush it\n+                    evaluateAndFlushGroup(page, startPosition);\n+                    startPosition = nextGroupStart;\n+                }\n+                else {\n+                    currentGroup = page.getRegion(page.getPositionCount() - 1, 1);\n+                    return;\n+                }\n+            }\n         }\n-    }\n \n-    private void evaluateAndFlushGroup(Page page, int position)\n-    {\n-        pageBuilder.declarePosition();\n-        for (int i = 0; i < groupByTypes.size(); i++) {\n-            Block block = page.getBlock(groupByChannels[i]);\n-            Type type = groupByTypes.get(i);\n-            type.appendTo(block, position, pageBuilder.getBlockBuilder(i));\n-        }\n-        int offset = groupByTypes.size();\n-        for (int i = 0; i < aggregates.size(); i++) {\n-            aggregates.get(i).evaluate(pageBuilder.getBlockBuilder(offset + i));\n+        private static Page extractColumns(Page page, int[] channels)\n+        {\n+            Block[] newBlocks = new Block[channels.length];\n+            for (int i = 0; i < channels.length; i++) {\n+                newBlocks[i] = page.getBlock(channels[i]);\n+            }\n+            return new Page(page.getPositionCount(), newBlocks);\n         }\n \n-        if (pageBuilder.isFull()) {\n-            outputPages.add(pageBuilder.build());\n-            pageBuilder.reset();\n+        private void addRowsToAggregates(Page page, int startPosition, int endPosition)\n+        {\n+            for (Aggregator aggregator : aggregates) {\n+                aggregator.processPage(page.getRegion(startPosition, endPosition - startPosition + 1));\n+            }\n         }\n \n-        aggregates = setupAggregates(step, accumulatorFactories);\n-    }\n-\n-    private int findNextGroupStart(int startPosition, Page page)\n-    {\n-        for (int i = startPosition + 1; i < page.getPositionCount(); i++) {\n-            if (!pagesHashStrategy.rowEqualsRow(startPosition, page, i, page)) {\n-                return i;\n+        private void evaluateAndFlushGroup(Page page, int position)\n+        {\n+            pageBuilder.declarePosition();\n+            for (int i = 0; i < groupByTypes.size(); i++) {\n+                Block block = page.getBlock(groupByChannels[i]);\n+                Type type = groupByTypes.get(i);\n+                type.appendTo(block, position, pageBuilder.getBlockBuilder(i));\n+            }\n+            int offset = groupByTypes.size();\n+            for (int i = 0; i < aggregates.size(); i++) {\n+                aggregates.get(i).evaluate(pageBuilder.getBlockBuilder(offset + i));\n             }\n-        }\n \n-        return page.getPositionCount();\n-    }\n+            if (pageBuilder.isFull()) {\n+                outputPages.add(pageBuilder.build());\n+                pageBuilder.reset();\n+            }\n \n-    @Override\n-    public Page getOutput()\n-    {\n-        if (!outputPages.isEmpty()) {\n-            return outputPages.removeFirst();\n+            aggregates = setupAggregates(step, accumulatorFactories);\n         }\n \n-        return null;\n-    }\n-\n-    @Override\n-    public void finish()\n-    {\n-        finishing = true;\n+        private int findNextGroupStart(int startPosition, Page page)\n+        {\n+            for (int i = startPosition + 1; i < page.getPositionCount(); i++) {\n+                if (!pagesHashStrategy.rowEqualsRow(startPosition, page, i, page)) {\n+                    return i;\n+                }\n+            }\n \n-        if (currentGroup != null) {\n-            evaluateAndFlushGroup(currentGroup, 0);\n-            currentGroup = null;\n+            return page.getPositionCount();\n         }\n \n-        if (!pageBuilder.isEmpty()) {\n-            outputPages.add(pageBuilder.build());\n-            pageBuilder.reset();\n+        private static List<Aggregator> setupAggregates(Step step, List<AccumulatorFactory> accumulatorFactories)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTYxMjE0OQ=="}, "originalCommit": null, "originalPosition": 496}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1100, "cost": 1, "resetAt": "2021-11-13T14:23:39Z"}}}