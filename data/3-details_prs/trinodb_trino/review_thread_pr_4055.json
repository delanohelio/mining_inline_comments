{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM1NTMwMDI3", "number": 4055, "reviewThreads": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxOToxNTo0MlrOEGe9dQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOTo0MDoyOVrOEG42ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MjM0MTY1OnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/FilesTable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxOToxNTo0MlrOGlT5kg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxOToxNTo0MlrOGlT5kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc3NjUzMA==", "bodyText": "For simple iteration, it's preferable to use a normal for-each loop. Use forEach() where it's the terminal operation of a stream, or for Map.forEach() where it has the advantage of naming the key/value pair variables.", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r441776530", "createdAt": "2020-06-17T19:15:42Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/FilesTable.java", "diffHunk": "@@ -154,4 +155,20 @@ private static boolean checkNonNull(Object object, PageListBuilder pagesBuilder)\n         }\n         return true;\n     }\n+\n+    private static Map<Integer, Type> getIcebergIdToTypeMapping(Schema schema)\n+    {\n+        ImmutableMap.Builder<Integer, Type> icebergIdToTypeMapping = ImmutableMap.builder();\n+        schema.columns().forEach(field -> populateIcebergIdToTypeMapping(field, icebergIdToTypeMapping));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MjQyMDI3OnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergPageSink.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxOTo0MDo0MVrOGlUrxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxOTo0MDo0MVrOGlUrxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc4OTM4MQ==", "bodyText": "We can remove this now (since there are two branches in the switch)", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r441789381", "createdAt": "2020-06-17T19:40:41Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergPageSink.java", "diffHunk": "@@ -324,14 +323,15 @@ private WriteContext createWriter(Optional<String> partitionPath, Optional<Parti\n     }\n \n     @SuppressWarnings(\"SwitchStatementWithTooFewBranches\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MjQyNjMwOnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergPageSink.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxOTo0MjoyOVrOGlUvlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxOTo0MjoyOVrOGlUvlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc5MDM1OQ==", "bodyText": "return writeContext.getWriter().getMetrics()\n        .orElseThrow(() -> new VerifyException(\"Iceberg ORC file writers should return Iceberg metrics\"));", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r441790359", "createdAt": "2020-06-17T19:42:29Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergPageSink.java", "diffHunk": "@@ -324,14 +323,15 @@ private WriteContext createWriter(Optional<String> partitionPath, Optional<Parti\n     }\n \n     @SuppressWarnings(\"SwitchStatementWithTooFewBranches\")\n-    private Metrics readMetrics(Path path)\n+    private Metrics getMetrics(WriteContext writeContext)\n     {\n         switch (fileFormat) {\n             case PARQUET:\n-                return ParquetUtil.fileMetrics(HadoopInputFile.fromPath(path, jobConf), MetricsConfig.getDefault());\n+                return ParquetUtil.fileMetrics(HadoopInputFile.fromPath(writeContext.getPath(), jobConf), MetricsConfig.getDefault());\n             case ORC:\n-                // TODO: update Iceberg version after OrcMetrics is completed\n-                return OrcMetrics.fromInputFile(HadoopInputFile.fromPath(path, jobConf), jobConf);\n+                IcebergFileWriter fileWriter = writeContext.getWriter();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjQ2NzY5OnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOTowNTo1M1rOGl8gGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOTowNTo1M1rOGl8gGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0MTc1Mg==", "bodyText": "Make this the first or last argument so that the argument ordering matches the parent class (keeps things simpler when reading the code).", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442441752", "createdAt": "2020-06-18T19:05:53Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjQ3MjgzOnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOTowNzozNFrOGl8jZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOTowNzozNFrOGl8jZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0MjU5Ng==", "bodyText": "Maybe invert this\nif (excludedColumns.contains(orcColumnId)) {\n    continue;\n}", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442442596", "createdAt": "2020-06-18T19:07:34Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjQ3MzY4OnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOTowNzo1MlrOGl8j8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOTowNzo1MlrOGl8j8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0MjczNg==", "bodyText": "The local variable could be inlined", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442442736", "createdAt": "2020-06-18T19:07:52Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjQ3NjQ4OnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOTowODo1N1rOGl8lyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOTowODo1N1rOGl8lyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0MzIxMQ==", "bodyText": "This shouldn't throw NPE. Make this\nverify(icebergSchema.findField(icebergId) != null, \"Cannot find Iceberg column with ID %s in schema %s\", icebergId, icebergSchema);", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442443211", "createdAt": "2020-06-18T19:08:57Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjQ4MzgzOnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToxMToxN1rOGl8qaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToxMToxN1rOGl8qaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0NDM5Mg==", "bodyText": "Nit: also put first argument on separate line", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442444392", "createdAt": "2020-06-18T19:11:17Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjQ4NjkxOnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToxMjoxOVrOGl8sWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToxMjoxOVrOGl8sWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0NDg4OA==", "bodyText": "Move this to the null below\nnew Metrics(\n        fileRowCount,\n        null, // TODO: Add column size accounting to ORC column writers\nOr maybe add above like\nMap<Integer, Long> columnSizes = null; // TODO: Add column size accounting to ORC column writers", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442444888", "createdAt": "2020-06-18T19:12:19Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjQ5MjkyOnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToxNDoxNFrOGl8wFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToxNDoxNFrOGl8wFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0NTg0NA==", "bodyText": "Can shorten shouldBeExcluded to exclude\nif (exclude) {", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442445844", "createdAt": "2020-06-18T19:14:14Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjQ5ODEwOnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToxNjowMVrOGl8zZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToxNjowMVrOGl8zZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0NjY5NQ==", "bodyText": "No need for empty default block", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442446695", "createdAt": "2020-06-18T19:16:01Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {\n+            excludedColumns.add(orcColumnId);\n+        }\n+        OrcType orcColumn = orcColumns.get(orcColumnId);\n+        switch (orcColumn.getOrcTypeKind()) {\n+            case LIST:\n+            case MAP:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, true, excludedColumns);\n+                }\n+                return;\n+            case STRUCT:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, shouldBeExcluded, excludedColumns);\n+                }\n+                return;\n+            default:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 166}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjUwNTI0OnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToxODoyMVrOGl832A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToxODoyMVrOGl832A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0NzgzMg==", "bodyText": "String icebergId = orcColumn.getAttributes().get(ORC_ICEBERG_ID_KEY);\nverify(icebergId != null, \"ORC column %s doesn't have an associated Iceberg ID\", orcColumn);", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442447832", "createdAt": "2020-06-18T19:18:21Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {\n+            excludedColumns.add(orcColumnId);\n+        }\n+        OrcType orcColumn = orcColumns.get(orcColumnId);\n+        switch (orcColumn.getOrcTypeKind()) {\n+            case LIST:\n+            case MAP:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, true, excludedColumns);\n+                }\n+                return;\n+            case STRUCT:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, shouldBeExcluded, excludedColumns);\n+                }\n+                return;\n+            default:\n+        }\n+    }\n+\n+    private static int getIcebergId(OrcType orcColumn)\n+    {\n+        String icebergId = requireNonNull(orcColumn.getAttributes().get(ORC_ICEBERG_ID_KEY), format(\"ORC column %s doesn't have an associated Iceberg ID\", orcColumn.toString()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 172}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjUxODcyOnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToyMjo1MVrOGl9Afw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNjoyMDoyOVrOGpm5-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ1MDA0Nw==", "bodyText": "These can be declared as Long", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442450047", "createdAt": "2020-06-18T19:22:51Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {\n+            excludedColumns.add(orcColumnId);\n+        }\n+        OrcType orcColumn = orcColumns.get(orcColumnId);\n+        switch (orcColumn.getOrcTypeKind()) {\n+            case LIST:\n+            case MAP:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, true, excludedColumns);\n+                }\n+                return;\n+            case STRUCT:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, shouldBeExcluded, excludedColumns);\n+                }\n+                return;\n+            default:\n+        }\n+    }\n+\n+    private static int getIcebergId(OrcType orcColumn)\n+    {\n+        String icebergId = requireNonNull(orcColumn.getAttributes().get(ORC_ICEBERG_ID_KEY), format(\"ORC column %s doesn't have an associated Iceberg ID\", orcColumn.toString()));\n+        return Integer.parseInt(icebergId);\n+    }\n+\n+    private static Optional<IcebergMinMax> toIcebergMinMax(ColumnStatistics orcColumnStats, org.apache.iceberg.types.Type icebergType)\n+    {\n+        IntegerStatistics integerStatistics = orcColumnStats.getIntegerStatistics();\n+        if (integerStatistics != null) {\n+            Object min = integerStatistics.getMin();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwMzEzMg==", "bodyText": "We can use Object so that it can also reference an Integer?", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r443903132", "createdAt": "2020-06-23T00:58:34Z", "author": {"login": "lxynov"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {\n+            excludedColumns.add(orcColumnId);\n+        }\n+        OrcType orcColumn = orcColumns.get(orcColumnId);\n+        switch (orcColumn.getOrcTypeKind()) {\n+            case LIST:\n+            case MAP:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, true, excludedColumns);\n+                }\n+                return;\n+            case STRUCT:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, shouldBeExcluded, excludedColumns);\n+                }\n+                return;\n+            default:\n+        }\n+    }\n+\n+    private static int getIcebergId(OrcType orcColumn)\n+    {\n+        String icebergId = requireNonNull(orcColumn.getAttributes().get(ORC_ICEBERG_ID_KEY), format(\"ORC column %s doesn't have an associated Iceberg ID\", orcColumn.toString()));\n+        return Integer.parseInt(icebergId);\n+    }\n+\n+    private static Optional<IcebergMinMax> toIcebergMinMax(ColumnStatistics orcColumnStats, org.apache.iceberg.types.Type icebergType)\n+    {\n+        IntegerStatistics integerStatistics = orcColumnStats.getIntegerStatistics();\n+        if (integerStatistics != null) {\n+            Object min = integerStatistics.getMin();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ1MDA0Nw=="}, "originalCommit": null, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MjIzMw==", "bodyText": "Ah, I missed that the toIntExact() below is changing to Integer.", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r446282233", "createdAt": "2020-06-26T16:20:29Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {\n+            excludedColumns.add(orcColumnId);\n+        }\n+        OrcType orcColumn = orcColumns.get(orcColumnId);\n+        switch (orcColumn.getOrcTypeKind()) {\n+            case LIST:\n+            case MAP:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, true, excludedColumns);\n+                }\n+                return;\n+            case STRUCT:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, shouldBeExcluded, excludedColumns);\n+                }\n+                return;\n+            default:\n+        }\n+    }\n+\n+    private static int getIcebergId(OrcType orcColumn)\n+    {\n+        String icebergId = requireNonNull(orcColumn.getAttributes().get(ORC_ICEBERG_ID_KEY), format(\"ORC column %s doesn't have an associated Iceberg ID\", orcColumn.toString()));\n+        return Integer.parseInt(icebergId);\n+    }\n+\n+    private static Optional<IcebergMinMax> toIcebergMinMax(ColumnStatistics orcColumnStats, org.apache.iceberg.types.Type icebergType)\n+    {\n+        IntegerStatistics integerStatistics = orcColumnStats.getIntegerStatistics();\n+        if (integerStatistics != null) {\n+            Object min = integerStatistics.getMin();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ1MDA0Nw=="}, "originalCommit": null, "originalPosition": 180}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjUxOTE5OnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToyMzowMFrOGl9A0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToyMzowMFrOGl9A0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ1MDEzMQ==", "bodyText": "Cast isn't needed if you change the type", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442450131", "createdAt": "2020-06-18T19:23:00Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {\n+            excludedColumns.add(orcColumnId);\n+        }\n+        OrcType orcColumn = orcColumns.get(orcColumnId);\n+        switch (orcColumn.getOrcTypeKind()) {\n+            case LIST:\n+            case MAP:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, true, excludedColumns);\n+                }\n+                return;\n+            case STRUCT:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, shouldBeExcluded, excludedColumns);\n+                }\n+                return;\n+            default:\n+        }\n+    }\n+\n+    private static int getIcebergId(OrcType orcColumn)\n+    {\n+        String icebergId = requireNonNull(orcColumn.getAttributes().get(ORC_ICEBERG_ID_KEY), format(\"ORC column %s doesn't have an associated Iceberg ID\", orcColumn.toString()));\n+        return Integer.parseInt(icebergId);\n+    }\n+\n+    private static Optional<IcebergMinMax> toIcebergMinMax(ColumnStatistics orcColumnStats, org.apache.iceberg.types.Type icebergType)\n+    {\n+        IntegerStatistics integerStatistics = orcColumnStats.getIntegerStatistics();\n+        if (integerStatistics != null) {\n+            Object min = integerStatistics.getMin();\n+            Object max = integerStatistics.getMax();\n+            if (min == null || max == null) {\n+                return Optional.empty();\n+            }\n+            if (icebergType.typeId() == org.apache.iceberg.types.Type.TypeID.INTEGER) {\n+                min = toIntExact((Long) min);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 186}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjUyMDA0OnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToyMzoxN1rOGl9BXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToyMzoxN1rOGl9BXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ1MDI3MQ==", "bodyText": "Double", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442450271", "createdAt": "2020-06-18T19:23:17Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {\n+            excludedColumns.add(orcColumnId);\n+        }\n+        OrcType orcColumn = orcColumns.get(orcColumnId);\n+        switch (orcColumn.getOrcTypeKind()) {\n+            case LIST:\n+            case MAP:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, true, excludedColumns);\n+                }\n+                return;\n+            case STRUCT:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, shouldBeExcluded, excludedColumns);\n+                }\n+                return;\n+            default:\n+        }\n+    }\n+\n+    private static int getIcebergId(OrcType orcColumn)\n+    {\n+        String icebergId = requireNonNull(orcColumn.getAttributes().get(ORC_ICEBERG_ID_KEY), format(\"ORC column %s doesn't have an associated Iceberg ID\", orcColumn.toString()));\n+        return Integer.parseInt(icebergId);\n+    }\n+\n+    private static Optional<IcebergMinMax> toIcebergMinMax(ColumnStatistics orcColumnStats, org.apache.iceberg.types.Type icebergType)\n+    {\n+        IntegerStatistics integerStatistics = orcColumnStats.getIntegerStatistics();\n+        if (integerStatistics != null) {\n+            Object min = integerStatistics.getMin();\n+            Object max = integerStatistics.getMax();\n+            if (min == null || max == null) {\n+                return Optional.empty();\n+            }\n+            if (icebergType.typeId() == org.apache.iceberg.types.Type.TypeID.INTEGER) {\n+                min = toIntExact((Long) min);\n+                max = toIntExact((Long) max);\n+            }\n+            return Optional.of(IcebergMinMax.builder(icebergType).setMin(min).setMax(max).build());\n+        }\n+        DoubleStatistics doubleStatistics = orcColumnStats.getDoubleStatistics();\n+        if (doubleStatistics != null) {\n+            Object min = doubleStatistics.getMin();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 193}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjUyNjA1OnYy", "diffSide": "RIGHT", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToyNToyMFrOGl9FVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOToyNToyMFrOGl9FVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ1MTI4NQ==", "bodyText": "I don't think we need a builder for this. All the usages are for all arguments. So this be\nreturn Optional.of(new IcebergMinMax(icebergType, min, max));\nWhich I find more readable here.", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442451285", "createdAt": "2020-06-18T19:25:20Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {\n+            excludedColumns.add(orcColumnId);\n+        }\n+        OrcType orcColumn = orcColumns.get(orcColumnId);\n+        switch (orcColumn.getOrcTypeKind()) {\n+            case LIST:\n+            case MAP:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, true, excludedColumns);\n+                }\n+                return;\n+            case STRUCT:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, shouldBeExcluded, excludedColumns);\n+                }\n+                return;\n+            default:\n+        }\n+    }\n+\n+    private static int getIcebergId(OrcType orcColumn)\n+    {\n+        String icebergId = requireNonNull(orcColumn.getAttributes().get(ORC_ICEBERG_ID_KEY), format(\"ORC column %s doesn't have an associated Iceberg ID\", orcColumn.toString()));\n+        return Integer.parseInt(icebergId);\n+    }\n+\n+    private static Optional<IcebergMinMax> toIcebergMinMax(ColumnStatistics orcColumnStats, org.apache.iceberg.types.Type icebergType)\n+    {\n+        IntegerStatistics integerStatistics = orcColumnStats.getIntegerStatistics();\n+        if (integerStatistics != null) {\n+            Object min = integerStatistics.getMin();\n+            Object max = integerStatistics.getMax();\n+            if (min == null || max == null) {\n+                return Optional.empty();\n+            }\n+            if (icebergType.typeId() == org.apache.iceberg.types.Type.TypeID.INTEGER) {\n+                min = toIntExact((Long) min);\n+                max = toIntExact((Long) max);\n+            }\n+            return Optional.of(IcebergMinMax.builder(icebergType).setMin(min).setMax(max).build());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 189}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjU1NTA0OnYy", "diffSide": "RIGHT", "path": "presto-orc/src/main/java/io/prestosql/orc/OrcWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOTozMzo0OVrOGl9XNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOTozMzo0OVrOGl9XNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ1NTg2Mw==", "bodyText": "Replace numberOfRows in bufferFileFooter() with fileRowCount", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442455863", "createdAt": "2020-06-18T19:33:49Z", "author": {"login": "electrum"}, "path": "presto-orc/src/main/java/io/prestosql/orc/OrcWriter.java", "diffHunk": "@@ -264,6 +269,7 @@ public void write(Page page)\n             }\n \n             writeChunk(chunk);\n+            fileRowCount += chunkRows;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjU4Mzk4OnYy", "diffSide": "RIGHT", "path": "presto-orc/src/main/java/io/prestosql/orc/OrcWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOTo0MDoyOVrOGl9pfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOTo0MDoyOVrOGl9pfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ2MDU0MA==", "bodyText": "Assigning a field inside a lambda is not typical and easy to miss, so it'd be better to write like\nfileStatsRetainedBytes = fileStats.map(stats -> stats.stream()\n        .mapToLong(ColumnStatistics::getRetainedSizeInBytes)\n        .sum()).orElse(0L);", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442460540", "createdAt": "2020-06-18T19:40:29Z", "author": {"login": "electrum"}, "path": "presto-orc/src/main/java/io/prestosql/orc/OrcWriter.java", "diffHunk": "@@ -471,10 +477,11 @@ public void close()\n                 .mapToLong(stripe -> stripe.getStripeInformation().getNumberOfRows())\n                 .sum();\n \n-        Optional<ColumnMetadata<ColumnStatistics>> fileStats = toFileStats(closedStripes.stream()\n+        fileStats = toFileStats(closedStripes.stream()\n                 .map(ClosedStripe::getStatistics)\n                 .map(StripeStatistics::getColumnStatistics)\n                 .collect(toList()));\n+        fileStats.ifPresent(stats -> fileStatsRetainedBytes = stats.stream().mapToLong(ColumnStatistics::getRetainedSizeInBytes).sum());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4093, "cost": 1, "resetAt": "2021-11-12T12:57:47Z"}}}