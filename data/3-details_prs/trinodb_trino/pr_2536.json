{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYzOTk2NDYw", "number": 2536, "title": "Support Reading Encrypted S3 Objects of Unknown Size", "bodyText": "Slightly different solution to #2434", "createdAt": "2020-01-17T06:56:38Z", "url": "https://github.com/trinodb/trino/pull/2536", "merged": true, "mergeCommit": {"oid": "75f426e9c59c3c2f2126e6dd85ad58e397ed886a"}, "closed": true, "closedAt": "2020-09-12T03:58:18Z", "author": {"login": "dain"}, "timelineItems": {"totalCount": 38, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb7Waj3gBqjI5NTk2Njc4MjA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdH5VcbgBqjM3NTc0MTEwNTI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0Njg5OTc2", "url": "https://github.com/trinodb/trino/pull/2536#pullrequestreview-344689976", "createdAt": "2020-01-17T16:20:53Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QxNjoyMDo1NFrOFe-LAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMFQxNTo1MTowMFrOFfik9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODAyMDIyNg==", "bodyText": "This seems incoherent with the handling of empty estimatedFileSize", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r368020226", "createdAt": "2020-01-17T16:20:54Z", "author": {"login": "pettyjamesm"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/rcfile/RcFilePageSourceFactory.java", "diffHunk": "@@ -132,26 +155,32 @@ else if (deserializerClassName.equals(ColumnarSerDe.class.getName())) {\n             throw new PrestoException(HIVE_CANNOT_OPEN_SPLIT, splitError(e, path, start, length), e);\n         }\n \n+        length = min(dataSource.getSize() - start, length);\n+        // Split may be empty now that the correct file size is known\n+        if (length <= 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYxMzMyNA==", "bodyText": "Looks like this would also have to handle handle the case where split start + length exceeds the discovered file size as well as the rarer case where start >= discovered file size.", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r368613324", "createdAt": "2020-01-20T15:44:05Z", "author": {"login": "pettyjamesm"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcPageSourceFactory.java", "diffHunk": "@@ -191,7 +191,9 @@ private static OrcPageSource createOrcPageSource(\n         AggregatedMemoryContext systemMemoryUsage = newSimpleAggregatedMemoryContext();\n         try {\n             OrcReader reader = new OrcReader(orcDataSource, options);\n-\n+            if (reader.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYxNjA5OA==", "bodyText": "This needs to handle start + length > discovered fileSize and start >= discovered file size as well.", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r368616098", "createdAt": "2020-01-20T15:49:49Z", "author": {"login": "pettyjamesm"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -167,10 +167,11 @@ private static ParquetPageSource createParquetPageSource(\n         try {\n             FileSystem fileSystem = hdfsEnvironment.getFileSystem(user, path, configuration);\n             FSDataInputStream inputStream = hdfsEnvironment.doAs(user, () -> fileSystem.open(path));\n-            ParquetMetadata parquetMetadata = MetadataReader.readFooter(inputStream, path, fileSize);\n+            dataSource = new HdfsParquetDataSource(new ParquetDataSourceId(path.toString()), estimatedFileSize, inputStream, stats, options);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYxNjY5NA==", "bodyText": "This is an exception because actually the getFileStatus call here will actually be the exact file size now. If this code changes to avoid the status call, then it'll need to handle start >= actual file size and start + length > actual file size just like the other page sources.", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r368616694", "createdAt": "2020-01-20T15:51:00Z", "author": {"login": "pettyjamesm"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergPageSourceProvider.java", "diffHunk": "@@ -158,10 +159,10 @@ private static ConnectorPageSource createParquetPageSource(\n         try {\n             FileSystem fileSystem = hdfsEnvironment.getFileSystem(user, path, configuration);\n             FileStatus fileStatus = fileSystem.getFileStatus(path);\n-            long fileSize = fileStatus.getLen();\n+            long estimatedFileSize = fileStatus.getLen();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ1OTU1Nzkx", "url": "https://github.com/trinodb/trino/pull/2536#pullrequestreview-345955791", "createdAt": "2020-01-21T15:16:22Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNToxNjoyM1rOFf9tQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNToxNjoyM1rOFf9tQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA2MTE4NQ==", "bodyText": "Should probably record bytes read and timing like readInternal does.", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r369061185", "createdAt": "2020-01-21T15:16:23Z", "author": {"login": "pettyjamesm"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/HdfsOrcDataSource.java", "diffHunk": "@@ -54,6 +56,15 @@ public void close()\n         inputStream.close();\n     }\n \n+    @Override\n+    public Slice readTail(int length)\n+            throws IOException\n+    {\n+        //  Handle potentially imprecise file lengths by reading the footer", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUzODM1MTQ0", "url": "https://github.com/trinodb/trino/pull/2536#pullrequestreview-353835144", "createdAt": "2020-02-05T16:01:07Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwMDgwMDIy", "url": "https://github.com/trinodb/trino/pull/2536#pullrequestreview-480080022", "createdAt": "2020-09-01T20:11:39Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMDoxMTozOVrOHLGgsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMTo0NzowNFrOHLJabQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQwMzA1Nw==", "bodyText": "There is a new method\n.orElseThrow();", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481403057", "createdAt": "2020-09-01T20:11:39Z", "author": {"login": "electrum"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/TestOrcPageSourceMemoryTracking.java", "diffHunk": "@@ -497,14 +496,14 @@ public ConnectorPageSource newPageSource(FileFormatDataSourceStats stats, Connec\n                     schema,\n                     TupleDomain.all(),\n                     columns,\n-                    partitonName,\n+                    partitionName,\n                     partitionKeys,\n                     TYPE_MANAGER,\n                     TableToPartitionMapping.empty(),\n                     Optional.empty(),\n                     false,\n                     Optional.empty())\n-                    .get();\n+                    .orElseThrow(RuntimeException::new);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQwNDY4Nw==", "bodyText": "\"data source\"", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481404687", "createdAt": "2020-09-01T20:14:57Z", "author": {"login": "electrum"}, "path": "presto-orc/src/main/java/io/prestosql/orc/OrcDataSource.java", "diffHunk": "@@ -31,6 +31,12 @@\n \n     long getSize();\n \n+    /**\n+     * Gets the memory size of this datasource.  This only includes memory", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQwNTM3Mw==", "bodyText": "Should this be OrcDataSource.class.getSimpleName() to match other usages?", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481405373", "createdAt": "2020-09-01T20:16:03Z", "author": {"login": "electrum"}, "path": "presto-orc/src/main/java/io/prestosql/orc/OrcRecordReader.java", "diffHunk": "@@ -205,6 +207,8 @@ public OrcRecordReader(\n \n         orcDataSource = wrapWithCacheIfTinyStripes(orcDataSource, this.stripes, options.getMaxMergeDistance(), options.getTinyStripeThreshold());\n         this.orcDataSource = orcDataSource;\n+        this.orcDataSourceMemoryUsage = systemMemoryUsage.newLocalMemoryContext(\"orcDataSource\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQzODEwNg==", "bodyText": "Should these be in the previous commit Add memory tests for both cached and uncached ORC?", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481438106", "createdAt": "2020-09-01T21:19:42Z", "author": {"login": "electrum"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/TestOrcPageSourceMemoryTracking.java", "diffHunk": "@@ -194,7 +194,13 @@ private void testPageSource(boolean useCache)\n         FileFormatDataSourceStats stats = new FileFormatDataSourceStats();\n         ConnectorPageSource pageSource = testPreparer.newPageSource(stats, useCache ? CACHED_SESSION : UNCACHED_SESSION);\n \n-        assertEquals(pageSource.getSystemMemoryUsage(), 0);\n+        if (useCache) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQzOTIyMQ==", "bodyText": "toIntExact should not be needed here", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481439221", "createdAt": "2020-09-01T21:22:03Z", "author": {"login": "electrum"}, "path": "presto-orc/src/main/java/io/prestosql/orc/MemoryOrcDataSource.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.orc;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.stream.MemoryOrcDataReader;\n+import io.prestosql.orc.stream.OrcDataReader;\n+\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+\n+public class MemoryOrcDataSource\n+        implements OrcDataSource\n+{\n+    private final OrcDataSourceId id;\n+    private final Slice data;\n+    private long readBytes;\n+\n+    public MemoryOrcDataSource(OrcDataSourceId id, Slice data)\n+    {\n+        this.id = requireNonNull(id, \"id is null\");\n+        this.data = requireNonNull(data, \"data is null\");\n+    }\n+\n+    @Override\n+    public OrcDataSourceId getId()\n+    {\n+        return id;\n+    }\n+\n+    @Override\n+    public long getReadBytes()\n+    {\n+        return readBytes;\n+    }\n+\n+    @Override\n+    public long getReadTimeNanos()\n+    {\n+        return 0;\n+    }\n+\n+    @Override\n+    public final long getSize()\n+    {\n+        return data.length();\n+    }\n+\n+    @Override\n+    public long getRetainedSize()\n+    {\n+        return data.getRetainedSize();\n+    }\n+\n+    @Override\n+    public final Slice readFully(long position, int length)\n+    {\n+        readBytes += length;\n+        return data.slice(toIntExact(position), length);\n+    }\n+\n+    @Override\n+    public final <K> Map<K, OrcDataReader> readFully(Map<K, DiskRange> diskRanges)\n+    {\n+        requireNonNull(diskRanges, \"diskRanges is null\");\n+\n+        if (diskRanges.isEmpty()) {\n+            return ImmutableMap.of();\n+        }\n+\n+        ImmutableMap.Builder<K, OrcDataReader> slices = ImmutableMap.builder();\n+        for (Entry<K, DiskRange> entry : diskRanges.entrySet()) {\n+            DiskRange diskRange = entry.getValue();\n+            Slice slice = readFully(toIntExact(diskRange.getOffset()), diskRange.getLength());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0MTc4Ng==", "bodyText": "Static import", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481441786", "createdAt": "2020-09-01T21:27:08Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcDeleteDeltaPageSourceFactory.java", "diffHunk": "@@ -43,9 +46,9 @@ public OrcDeleteDeltaPageSourceFactory(\n         this.stats = requireNonNull(stats, \"stats is null\");\n     }\n \n-    public OrcDeleteDeltaPageSource createPageSource(Path path, long fileSize)\n+    public Optional<ConnectorPageSource> createPageSource(Path path, long fileSize)\n     {\n-        return new OrcDeleteDeltaPageSource(\n+        return OrcDeleteDeltaPageSource.createOrcDeleteDeltaPageSource(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0MjYyNg==", "bodyText": "Static import", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481442626", "createdAt": "2020-09-01T21:28:51Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcDeleteDeltaPageSource.java", "diffHunk": "@@ -96,26 +97,11 @@ public OrcDeleteDeltaPageSource(\n         }\n \n         try {\n-            OrcReader reader = new OrcReader(orcDataSource, options);\n-\n-            verifyAcidSchema(reader, path);\n-            Map<String, OrcColumn> acidColumns = uniqueIndex(\n-                    reader.getRootColumn().getNestedColumns(),\n-                    orcColumn -> orcColumn.getColumnName().toLowerCase(ENGLISH));\n-            List<OrcColumn> rowIdColumns = ImmutableList.of(\n-                    acidColumns.get(ACID_COLUMN_ORIGINAL_TRANSACTION.toLowerCase(ENGLISH)),\n-                    acidColumns.get(ACID_COLUMN_ROW_ID.toLowerCase(ENGLISH)));\n-\n-            recordReader = reader.createRecordReader(\n-                    rowIdColumns,\n-                    ImmutableList.of(BIGINT, BIGINT),\n-                    OrcPredicate.TRUE,\n-                    0,\n-                    fileSize,\n-                    UTC,\n-                    systemMemoryContext,\n-                    MAX_BATCH_SIZE,\n-                    exception -> handleException(orcDataSource.getId(), exception));\n+            Optional<OrcReader> orcReader = OrcReader.createOrcReader(orcDataSource, options);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0NDAyOQ==", "bodyText": "MISSING_DATA is used for HDFS BlockMissingException. This is like a corrupt file or something. How about\n.orElseThrow(() -> new PrestoException(ICEBERG_BAD_DATA, \"ORC file is zero length\"));", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481444029", "createdAt": "2020-09-01T21:32:00Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergPageSourceProvider.java", "diffHunk": "@@ -254,7 +254,8 @@ private static ConnectorPageSource createOrcPageSource(\n                     inputStream,\n                     stats);\n \n-            OrcReader reader = new OrcReader(orcDataSource, options);\n+            OrcReader reader = OrcReader.createOrcReader(orcDataSource, options)\n+                    .orElseThrow(() -> new PrestoException(ICEBERG_MISSING_DATA, \"File is empty\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0NDQ2OA==", "bodyText": "Move this next to the other read method", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481444468", "createdAt": "2020-09-01T21:32:59Z", "author": {"login": "electrum"}, "path": "presto-orc/src/main/java/io/prestosql/orc/OrcDataSource.java", "diffHunk": "@@ -29,7 +29,10 @@\n \n     long getReadTimeNanos();\n \n-    long getSize();\n+    long getEstimatedSize();\n+\n+    Slice readTail(int length)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0NTkyMQ==", "bodyText": "Why the change? Is seems the caller still handles IOException", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481445921", "createdAt": "2020-09-01T21:36:19Z", "author": {"login": "electrum"}, "path": "presto-orc/src/main/java/io/prestosql/orc/OrcReader.java", "diffHunk": "@@ -374,10 +387,15 @@ else if (orcType.getOrcTypeKind() == OrcTypeKind.UNION) {\n      * Does the file start with the ORC magic bytes?\n      */\n     private static boolean isValidHeaderMagic(OrcDataSource source)\n-            throws IOException\n     {\n-        Slice headerMagic = source.readFully(0, MAGIC.length());\n-        return MAGIC.equals(headerMagic);\n+        try {\n+            Slice headerMagic = source.readFully(0, MAGIC.length());\n+            return MAGIC.equals(headerMagic);\n+        }\n+        catch (IOException e) {\n+            // assume file is an ORC file", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0NzcyMg==", "bodyText": "This isn't called if the read throws. I think we can wrap this in try-with-resources", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481447722", "createdAt": "2020-09-01T21:40:27Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/rcfile/RcFilePageSourceFactory.java", "diffHunk": "@@ -134,10 +138,17 @@ else if (deserializerClassName.equals(ColumnarSerDe.class.getName())) {\n                 .map(ReaderProjections::getReaderColumns)\n                 .orElse(columns);\n \n-        FSDataInputStream inputStream;\n+        RcFileDataSource dataSource;\n         try {\n             FileSystem fileSystem = hdfsEnvironment.getFileSystem(session.getUser(), path, configuration);\n-            inputStream = hdfsEnvironment.doAs(session.getUser(), () -> fileSystem.open(path));\n+            FSDataInputStream inputStream = hdfsEnvironment.doAs(session.getUser(), () -> fileSystem.open(path));\n+            dataSource = new HdfsRcFileDataSource(path.toString(), inputStream, estimatedFileSize, stats);\n+            if (estimatedFileSize < BUFFER_SIZE.toBytes()) {\n+                byte[] buffer = new byte[toIntExact(estimatedFileSize)];\n+                dataSource.readFully(0, buffer, 0, buffer.length);\n+                dataSource.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0OTUwNw==", "bodyText": "checkArgument(\"fileSize is negative: %s\", fileSize);\ncheckArgument(tailSlice.length() <= fileSize, \"length (%s) is greater than fileSize (%s)\", tailSlice.length(), fileSize);", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481449507", "createdAt": "2020-09-01T21:44:42Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/FSDataInputStreamTail.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.util;\n+\n+import io.airlift.slice.Slice;\n+import io.airlift.slice.Slices;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+import java.io.IOException;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveErrorCode.HIVE_FILESYSTEM_ERROR;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public final class FSDataInputStreamTail\n+{\n+    public static final int MAX_SUPPORTED_PADDING_BYTES = 64;\n+    private static final int MAXIMUM_READ_LENGTH = Integer.MAX_VALUE - (MAX_SUPPORTED_PADDING_BYTES + 1);\n+\n+    private final Slice tailSlice;\n+    private final long fileSize;\n+\n+    private FSDataInputStreamTail(long fileSize, Slice tailSlice)\n+    {\n+        this.tailSlice = requireNonNull(tailSlice, \"tailBuffer is null\");\n+        this.fileSize = fileSize;\n+        checkArgument(fileSize >= 0, \"fileSize must be >= 0, found: %s\", fileSize);\n+        checkArgument(tailSlice.length() <= fileSize, \"length must be <= fileSize, found: %s > %s\", tailSlice.length(), fileSize);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MDA3MQ==", "bodyText": "checkArgument(length >= 0, \"length is negative: %s\", length);\ncheckArgument(length <= MAXIMUM_READ_LENGTH, \"length (%s) exceeds maximum (%s)\", length, MAXIMUM_READ_LENGTH);", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481450071", "createdAt": "2020-09-01T21:45:52Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/FSDataInputStreamTail.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.util;\n+\n+import io.airlift.slice.Slice;\n+import io.airlift.slice.Slices;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+import java.io.IOException;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveErrorCode.HIVE_FILESYSTEM_ERROR;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public final class FSDataInputStreamTail\n+{\n+    public static final int MAX_SUPPORTED_PADDING_BYTES = 64;\n+    private static final int MAXIMUM_READ_LENGTH = Integer.MAX_VALUE - (MAX_SUPPORTED_PADDING_BYTES + 1);\n+\n+    private final Slice tailSlice;\n+    private final long fileSize;\n+\n+    private FSDataInputStreamTail(long fileSize, Slice tailSlice)\n+    {\n+        this.tailSlice = requireNonNull(tailSlice, \"tailBuffer is null\");\n+        this.fileSize = fileSize;\n+        checkArgument(fileSize >= 0, \"fileSize must be >= 0, found: %s\", fileSize);\n+        checkArgument(tailSlice.length() <= fileSize, \"length must be <= fileSize, found: %s > %s\", tailSlice.length(), fileSize);\n+    }\n+\n+    public static FSDataInputStreamTail readTail(String path, long paddedFileSize, FSDataInputStream inputStream, int length)\n+            throws IOException\n+    {\n+        checkArgument(length >= 0 && length <= MAXIMUM_READ_LENGTH, \"length too large, found: %s\", length);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MDYwNQ==", "bodyText": "Should this be < 0 to be consistent with the above?", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481450605", "createdAt": "2020-09-01T21:47:04Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/FSDataInputStreamTail.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.util;\n+\n+import io.airlift.slice.Slice;\n+import io.airlift.slice.Slices;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+import java.io.IOException;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveErrorCode.HIVE_FILESYSTEM_ERROR;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public final class FSDataInputStreamTail\n+{\n+    public static final int MAX_SUPPORTED_PADDING_BYTES = 64;\n+    private static final int MAXIMUM_READ_LENGTH = Integer.MAX_VALUE - (MAX_SUPPORTED_PADDING_BYTES + 1);\n+\n+    private final Slice tailSlice;\n+    private final long fileSize;\n+\n+    private FSDataInputStreamTail(long fileSize, Slice tailSlice)\n+    {\n+        this.tailSlice = requireNonNull(tailSlice, \"tailBuffer is null\");\n+        this.fileSize = fileSize;\n+        checkArgument(fileSize >= 0, \"fileSize must be >= 0, found: %s\", fileSize);\n+        checkArgument(tailSlice.length() <= fileSize, \"length must be <= fileSize, found: %s > %s\", tailSlice.length(), fileSize);\n+    }\n+\n+    public static FSDataInputStreamTail readTail(String path, long paddedFileSize, FSDataInputStream inputStream, int length)\n+            throws IOException\n+    {\n+        checkArgument(length >= 0 && length <= MAXIMUM_READ_LENGTH, \"length too large, found: %s\", length);\n+        long readSize = min(paddedFileSize, (length + MAX_SUPPORTED_PADDING_BYTES));\n+        long position = paddedFileSize - readSize;\n+        // Actual read will be 1 byte larger to ensure we encounter an EOF where expected\n+        byte[] buffer = new byte[toIntExact(readSize + 1)];\n+        int bytesRead = 0;\n+        long startPos = inputStream.getPos();\n+        try {\n+            inputStream.seek(position);\n+            while (bytesRead < buffer.length) {\n+                int n = inputStream.read(buffer, bytesRead, buffer.length - bytesRead);\n+                if (n < 0) {\n+                    break;\n+                }\n+                bytesRead += n;\n+            }\n+        }\n+        finally {\n+            inputStream.seek(startPos);\n+        }\n+        if (bytesRead > readSize) {\n+            throw rejectInvalidFileSize(path, paddedFileSize);\n+        }\n+        return new FSDataInputStreamTail(position + bytesRead, Slices.wrappedBuffer(buffer, max(0, bytesRead - length), min(bytesRead, length)));\n+    }\n+\n+    public static long readTailForFileSize(String path, long paddedFileSize, FSDataInputStream inputStream)\n+            throws IOException\n+    {\n+        long position = max(paddedFileSize - MAX_SUPPORTED_PADDING_BYTES, 0);\n+        long maxEOFAt = paddedFileSize + 1;\n+        long startPos = inputStream.getPos();\n+        try {\n+            inputStream.seek(position);\n+            int c;\n+            while (position < maxEOFAt) {\n+                c = inputStream.read();\n+                if (c == -1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 87}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwMTQyOTEw", "url": "https://github.com/trinodb/trino/pull/2536#pullrequestreview-480142910", "createdAt": "2020-09-01T21:51:35Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMTo1MTozNVrOHLJh5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMTo1MTozNVrOHLJh5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MjUxNw==", "bodyText": "This should throw IOException since it is used by PrestoS3FileSystem:\nthrow new IOException(format(\"Incorrect file size (%s) for file (end of stream not reached): %s\", reportedSize, path));", "url": "https://github.com/trinodb/trino/pull/2536#discussion_r481452517", "createdAt": "2020-09-01T21:51:35Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/FSDataInputStreamTail.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.util;\n+\n+import io.airlift.slice.Slice;\n+import io.airlift.slice.Slices;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+import java.io.IOException;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveErrorCode.HIVE_FILESYSTEM_ERROR;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public final class FSDataInputStreamTail\n+{\n+    public static final int MAX_SUPPORTED_PADDING_BYTES = 64;\n+    private static final int MAXIMUM_READ_LENGTH = Integer.MAX_VALUE - (MAX_SUPPORTED_PADDING_BYTES + 1);\n+\n+    private final Slice tailSlice;\n+    private final long fileSize;\n+\n+    private FSDataInputStreamTail(long fileSize, Slice tailSlice)\n+    {\n+        this.tailSlice = requireNonNull(tailSlice, \"tailBuffer is null\");\n+        this.fileSize = fileSize;\n+        checkArgument(fileSize >= 0, \"fileSize must be >= 0, found: %s\", fileSize);\n+        checkArgument(tailSlice.length() <= fileSize, \"length must be <= fileSize, found: %s > %s\", tailSlice.length(), fileSize);\n+    }\n+\n+    public static FSDataInputStreamTail readTail(String path, long paddedFileSize, FSDataInputStream inputStream, int length)\n+            throws IOException\n+    {\n+        checkArgument(length >= 0 && length <= MAXIMUM_READ_LENGTH, \"length too large, found: %s\", length);\n+        long readSize = min(paddedFileSize, (length + MAX_SUPPORTED_PADDING_BYTES));\n+        long position = paddedFileSize - readSize;\n+        // Actual read will be 1 byte larger to ensure we encounter an EOF where expected\n+        byte[] buffer = new byte[toIntExact(readSize + 1)];\n+        int bytesRead = 0;\n+        long startPos = inputStream.getPos();\n+        try {\n+            inputStream.seek(position);\n+            while (bytesRead < buffer.length) {\n+                int n = inputStream.read(buffer, bytesRead, buffer.length - bytesRead);\n+                if (n < 0) {\n+                    break;\n+                }\n+                bytesRead += n;\n+            }\n+        }\n+        finally {\n+            inputStream.seek(startPos);\n+        }\n+        if (bytesRead > readSize) {\n+            throw rejectInvalidFileSize(path, paddedFileSize);\n+        }\n+        return new FSDataInputStreamTail(position + bytesRead, Slices.wrappedBuffer(buffer, max(0, bytesRead - length), min(bytesRead, length)));\n+    }\n+\n+    public static long readTailForFileSize(String path, long paddedFileSize, FSDataInputStream inputStream)\n+            throws IOException\n+    {\n+        long position = max(paddedFileSize - MAX_SUPPORTED_PADDING_BYTES, 0);\n+        long maxEOFAt = paddedFileSize + 1;\n+        long startPos = inputStream.getPos();\n+        try {\n+            inputStream.seek(position);\n+            int c;\n+            while (position < maxEOFAt) {\n+                c = inputStream.read();\n+                if (c == -1) {\n+                    return position;\n+                }\n+                position++;\n+            }\n+            throw rejectInvalidFileSize(path, paddedFileSize);\n+        }\n+        finally {\n+            inputStream.seek(startPos);\n+        }\n+    }\n+\n+    private static PrestoException rejectInvalidFileSize(String path, long reportedSize)\n+    {\n+        throw new PrestoException(HIVE_FILESYSTEM_ERROR, format(\"Incorrect fileSize %s for file %s, end of stream not reached\", reportedSize, path));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 101}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "19bfa154cf450f295071f03e5557ed00883f47e2", "author": {"user": {"login": "dain", "name": "Dain Sundstrom"}}, "url": "https://github.com/trinodb/trino/commit/19bfa154cf450f295071f03e5557ed00883f47e2", "committedDate": "2020-09-11T18:02:00Z", "message": "Rename Hive fileSize to estimatedFileSize\n\nEncrypted S3 files may be padded, so reported size may not refect actual size."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f5985243582a78c7f19f6e876962c6387627d7f3", "author": {"user": {"login": "dain", "name": "Dain Sundstrom"}}, "url": "https://github.com/trinodb/trino/commit/f5985243582a78c7f19f6e876962c6387627d7f3", "committedDate": "2020-09-11T18:02:02Z", "message": "Cleanup warnings in TestOrcPageSourceMemoryTracking"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b0c04f157ebae5940c35a7e4185b5b8e25454777", "author": {"user": {"login": "dain", "name": "Dain Sundstrom"}}, "url": "https://github.com/trinodb/trino/commit/b0c04f157ebae5940c35a7e4185b5b8e25454777", "committedDate": "2020-09-11T18:02:03Z", "message": "Add memory tests for both cached and uncached ORC"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "377014336f4241832fe964946651115f12f492c7", "author": {"user": {"login": "dain", "name": "Dain Sundstrom"}}, "url": "https://github.com/trinodb/trino/commit/377014336f4241832fe964946651115f12f492c7", "committedDate": "2020-09-11T18:02:04Z", "message": "Add getRetainedSize to OrcDataSource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf94d20a5d5940e1aaaf2493334af3be18894975", "author": {"user": {"login": "dain", "name": "Dain Sundstrom"}}, "url": "https://github.com/trinodb/trino/commit/cf94d20a5d5940e1aaaf2493334af3be18894975", "committedDate": "2020-09-11T18:02:05Z", "message": "Add MemoryOrcDataSource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "288de65d048387a1498d11bcd063a618e1502c0a", "author": {"user": {"login": "dain", "name": "Dain Sundstrom"}}, "url": "https://github.com/trinodb/trino/commit/288de65d048387a1498d11bcd063a618e1502c0a", "committedDate": "2020-09-11T18:02:06Z", "message": "Add readTail to OrcDataSource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cdb3c9c25bcd5e7dbd600f5291086420f87591d5", "author": {"user": {"login": "dain", "name": "Dain Sundstrom"}}, "url": "https://github.com/trinodb/trino/commit/cdb3c9c25bcd5e7dbd600f5291086420f87591d5", "committedDate": "2020-09-11T18:02:07Z", "message": "Cleanup ParquetPageSourceFactory"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c2bb601599344477dbbd47cb159db895ef6ef5cf", "author": {"user": {"login": "dain", "name": "Dain Sundstrom"}}, "url": "https://github.com/trinodb/trino/commit/c2bb601599344477dbbd47cb159db895ef6ef5cf", "committedDate": "2020-09-11T18:02:08Z", "message": "Remove unused method from ParquetDataSource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3e4eb185a86f26b22de3ef971f19faacf0511f49", "author": {"user": {"login": "dain", "name": "Dain Sundstrom"}}, "url": "https://github.com/trinodb/trino/commit/3e4eb185a86f26b22de3ef971f19faacf0511f49", "committedDate": "2020-09-11T18:02:10Z", "message": "Change ParquetDataSource readFully to return Slice"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "26a3035b41a5406a9351a0c265b147ea872ee7a7", "author": {"user": {"login": "dain", "name": "Dain Sundstrom"}}, "url": "https://github.com/trinodb/trino/commit/26a3035b41a5406a9351a0c265b147ea872ee7a7", "committedDate": "2020-09-11T18:02:11Z", "message": "Change Parquet MetadataReader to use ParquetDataSource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6031c52166c42ece7023be73ac84620f9e98bb94", "author": {"user": {"login": "dain", "name": "Dain Sundstrom"}}, "url": "https://github.com/trinodb/trino/commit/6031c52166c42ece7023be73ac84620f9e98bb94", "committedDate": "2020-09-11T18:02:12Z", "message": "Add readTail to ParquetDataSource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f2ef167bba4dc1757eea909a2e1d648f00d17094", "author": {"user": {"login": "dain", "name": "Dain Sundstrom"}}, "url": "https://github.com/trinodb/trino/commit/f2ef167bba4dc1757eea909a2e1d648f00d17094", "committedDate": "2020-09-11T18:02:13Z", "message": "Fully buffer small RC files"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a5bed0427f26764044bfea94409f9a74f179e330", "author": {"user": {"login": "pettyjamesm", "name": "James Petty"}}, "url": "https://github.com/trinodb/trino/commit/a5bed0427f26764044bfea94409f9a74f179e330", "committedDate": "2020-09-11T18:02:14Z", "message": "Introduce FSDataInputStreamTail reads\n\nAdds support for pre-reading the tail section of FSDataInputStream to\ndetect and handle file sizes that are actually smaller than initially\nreported by some small padding factor (eg: client side encrypted S3\nobjects where the last block must be read to determine the decrypted\nlength).\n\nThese pre-reads are then used to optimize Parquet and ORC metadata\nsection reads which can re-use the result. RCFile PageSources don't\nbenefit from pre-reading the tail of the file, and therefore must\nenable file size is calculated directly."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6796dad70168ec79994eb2d00fd3e03c59975ba8", "author": {"user": {"login": "pettyjamesm", "name": "James Petty"}}, "url": "https://github.com/trinodb/trino/commit/6796dad70168ec79994eb2d00fd3e03c59975ba8", "committedDate": "2020-09-11T18:02:15Z", "message": "Support CSE-KMS S3 object file size detection via tail read"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "6796dad70168ec79994eb2d00fd3e03c59975ba8", "author": {"user": {"login": "pettyjamesm", "name": "James Petty"}}, "url": "https://github.com/trinodb/trino/commit/6796dad70168ec79994eb2d00fd3e03c59975ba8", "committedDate": "2020-09-11T18:02:15Z", "message": "Support CSE-KMS S3 object file size detection via tail read"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 904, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}