{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY3MjEyNjk2", "number": 4805, "title": "kafka connector: add internal columns filter pushdown", "bodyText": "add _timestamp as internal columns.\nadd _partion_offset/_partition_id/_timestamp filter pushdown.\nSigned-off-by: Li Wang wangli@thinkingdata.cn", "createdAt": "2020-08-13T07:50:12Z", "url": "https://github.com/trinodb/trino/pull/4805", "merged": true, "mergeCommit": {"oid": "a2ec0e357c0c7301e38fa595233248ff1d9dc391"}, "closed": true, "closedAt": "2020-09-29T10:45:29Z", "author": {"login": "wangli-td"}, "timelineItems": {"totalCount": 57, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc-cyJRABqjM2NTEyNjgxMDk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdNl1ZsAFqTQ5ODMzODU3Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc0MDM3ODM2", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-474037836", "createdAt": "2020-08-25T01:48:14Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMTo0ODoxNFrOHF_53A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMzoyMjoxMFrOHGDfEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA1MTkzMg==", "bodyText": "This should be Optional.empty().", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476051932", "createdAt": "2020-08-25T01:48:14Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaMetadata.java", "diffHunk": "@@ -265,7 +293,8 @@ public ConnectorInsertTableHandle beginInsert(ConnectorSession session, Connecto\n                 table.getMessageDataFormat(),\n                 table.getKeyDataSchemaLocation(),\n                 table.getMessageDataSchemaLocation(),\n-                actualColumns);\n+                actualColumns,\n+                table.getConstraint());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA5MTAxNg==", "bodyText": "KafkaConsumer#offsetsForTimes() will give you the earliest offset whose timestamp is greater than or equal to the argument you're passing.  That's what you want for the lower bound of a range, but using it for the upper bound leads to the issue that @gschmutz pointed out.  I think the right thing to do is to not try to limit the end offset using a _timestamp filter.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476091016", "createdAt": "2020-08-25T02:46:56Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSplitManager.java", "diffHunk": "@@ -74,12 +79,25 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, Co\n \n             Map<TopicPartition, Long> partitionBeginOffsets = kafkaConsumer.beginningOffsets(topicPartitions);\n             Map<TopicPartition, Long> partitionEndOffsets = kafkaConsumer.endOffsets(topicPartitions);\n+            KafkaFilter kafkaFilter = getKafkaFilter(kafkaTableHandle, new KafkaFilter(partitionInfos, partitionBeginOffsets, partitionEndOffsets, null, null));\n+            partitionInfos = kafkaFilter.getPartitionInfos();\n+            partitionBeginOffsets = kafkaFilter.getPartitionBeginOffsets();\n+            partitionEndOffsets = kafkaFilter.getPartitionEndOffsets();\n+            if (kafkaFilter.getBeginOffsetTs() != null) {\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> findOffsetsByTimestamp(kafkaConsumer, p, kafkaFilter.getBeginOffsetTs()));\n+            }\n+            if (kafkaFilter.getEndOffsetTs() != null) {\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> findOffsetsByTimestamp(kafkaConsumer, p, kafkaFilter.getEndOffsetTs()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA5MjA2MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                .collect(Collectors.toList());\n          \n          \n            \n                                .collect(toImmutableList());", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476092060", "createdAt": "2020-08-25T02:48:32Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Verify.verify;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle, KafkaFilter defKafkaFilter)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(Collectors.toSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA5NDE2OA==", "bodyText": "Please spell out the argument name defKafkaFilter -> defaultKafkaFilter (I assume).", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476094168", "createdAt": "2020-08-25T02:51:43Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Verify.verify;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle, KafkaFilter defKafkaFilter)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA5NjA2OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(Collectors.toSet());\n          \n          \n            \n                        Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n          \n      \n    \n    \n  \n\nSame suggestion in several other places.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476096068", "createdAt": "2020-08-25T02:54:37Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Verify.verify;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle, KafkaFilter defKafkaFilter)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(Collectors.toSet());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA5Njk5Mg==", "bodyText": "No need for the else, just return defKafkaFilter.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476096992", "createdAt": "2020-08-25T02:57:43Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Verify.verify;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle, KafkaFilter defKafkaFilter)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(Collectors.toSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(Collectors.toList());\n+            return new KafkaFilter(partitionFilteredInfos,\n+                    partitionBeginOffsets,\n+                    partitionEndOffsets,\n+                    offsetTimestampRanged.isPresent() && offsetTimestampRanged.get().getBegin() >= 0 ? offsetTimestampRanged.get().getBegin() : null,\n+                    offsetTimestampRanged.isPresent() && offsetTimestampRanged.get().getEnd() >= 0 ? offsetTimestampRanged.get().getEnd() : null);\n+        }\n+        else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA5ODY2MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                offsetTimestampRanged.isPresent() && offsetTimestampRanged.get().getBegin() >= 0 ? offsetTimestampRanged.get().getBegin() : null,\n          \n          \n            \n                                offsetTimestampRanged.map(Range::getBegin).filter(begin -> begin >= 0).orElse(null),", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476098661", "createdAt": "2020-08-25T03:04:06Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Verify.verify;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle, KafkaFilter defKafkaFilter)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(Collectors.toSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(Collectors.toList());\n+            return new KafkaFilter(partitionFilteredInfos,\n+                    partitionBeginOffsets,\n+                    partitionEndOffsets,\n+                    offsetTimestampRanged.isPresent() && offsetTimestampRanged.get().getBegin() >= 0 ? offsetTimestampRanged.get().getBegin() : null,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEwMDAyMA==", "bodyText": "Perhaps you can find a more descriptive name for genBeginOffset, something like computeOffset.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476100020", "createdAt": "2020-08-25T03:06:17Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Verify.verify;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle, KafkaFilter defKafkaFilter)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(Collectors.toSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(Collectors.toList());\n+            return new KafkaFilter(partitionFilteredInfos,\n+                    partitionBeginOffsets,\n+                    partitionEndOffsets,\n+                    offsetTimestampRanged.isPresent() && offsetTimestampRanged.get().getBegin() >= 0 ? offsetTimestampRanged.get().getBegin() : null,\n+                    offsetTimestampRanged.isPresent() && offsetTimestampRanged.get().getEnd() >= 0 ? offsetTimestampRanged.get().getEnd() : null);\n+        }\n+        else {\n+            return defKafkaFilter;\n+        }\n+    }\n+\n+    public static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> genBeginOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEwNzg5NQ==", "bodyText": "Use TimestampType createTimestampType(3) instead of the deprecated TimestampType.TIMESTAMP.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476107895", "createdAt": "2020-08-25T03:18:04Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldDescription.java", "diffHunk": "@@ -75,7 +76,12 @@\n     /**\n      * <tt>_key_length</tt> - length in bytes of the key.\n      */\n-    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\");\n+    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\"),\n+\n+    /**\n+     * <tt>_timestamp</tt> - offset timestamp\n+     */\n+    OFFSET_TIMESTAMP_FIELD(\"_timestamp\", TimestampType.TIMESTAMP, \"Offset Timestamp\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjExMDYxMQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    KafkaInternalFieldDescription description = BY_COLUMN_NAME.get(columnName);\n          \n          \n            \n                    return description != null;\n          \n          \n            \n                    return BY_COLUMN_NAME.containsKey(columnName);", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r476110611", "createdAt": "2020-08-25T03:22:10Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldDescription.java", "diffHunk": "@@ -88,6 +94,12 @@ public static KafkaInternalFieldDescription forColumnName(String columnName)\n         return description;\n     }\n \n+    public static boolean isInternalColumn(String columnName)\n+    {\n+        KafkaInternalFieldDescription description = BY_COLUMN_NAME.get(columnName);\n+        return description != null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc2NzI2MTYw", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-476726160", "createdAt": "2020-08-27T13:35:15Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMzozNToxNVrOHIQp0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxNTowNjo1NFrOHIUwiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQyMzUwNw==", "bodyText": "I think what we concluded was that we can only push down upper bounds when the topic has message.timestamp.type=LogAppendTime.  So we should name the property to reflect that, maybe something like kafka.timestamp-type with allowable values LogAppendTime and CreateTime (this should be the default, equivalent to false as it is written now, since it is the 'safe' setting - queries will return correct results, even though performance might suffer).\nIdeally, Presto would obtain this value from Kafka, but I am not sure it is exposed.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r478423507", "createdAt": "2020-08-27T13:35:15Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaConfig.java", "diffHunk": "@@ -153,4 +154,17 @@ public KafkaConfig setMessagesPerSplit(int messagesPerSplit)\n         this.messagesPerSplit = messagesPerSplit;\n         return this;\n     }\n+\n+    public boolean isTimestampUpperBoundPushDownEnabled()\n+    {\n+        return timestampUpperBoundPushDownEnabled;\n+    }\n+\n+    @Config(\"kafka.timestamp-upper-bound-push-down-enabled\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODQ5MDc2MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if (topicPartitionOffsets == null || topicPartitionOffsets.values().size() == 0) {\n          \n          \n            \n                        if (topicPartitionOffsets.isEmpty()) {\n          \n      \n    \n    \n  \n\nI don't think kafkaConsumer.offsetsForTimes() returns null.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r478490760", "createdAt": "2020-08-27T15:06:54Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSplitManager.java", "diffHunk": "@@ -106,6 +118,25 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, Co\n         }\n     }\n \n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            if (topicPartitionOffsets == null || topicPartitionOffsets.values().size() == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5MjgxNjcz", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-479281673", "createdAt": "2020-09-01T01:29:51Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMToyOTo1MVrOHKSrsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMjozOTozM1rOHKXkfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU1MzkwNw==", "bodyText": "I don't think you need parentheses; if you feel they improve readability, you could have something like (p -> range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty()", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480553907", "createdAt": "2020-09-01T01:29:51Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU2OTE5NA==", "bodyText": "You can have this on a single line.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480569194", "createdAt": "2020-09-01T01:44:00Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (computeOffsetByTimestamp != null && offsetTimestampRanged.isPresent()) {\n+                Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getBegin()));\n+                if (needPushTimeStampUpperBound) {\n+                    partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                            (p) -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getEnd()));\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilter(partitionFilteredInfos,\n+                    partitionBeginOffsets,\n+                    partitionEndOffsets);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU3NzU3Nw==", "bodyText": "getColumnDomains() has a comment that says: // Available for Jackson serialization only!. Please use getDomains() instead.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480577577", "createdAt": "2020-09-01T01:51:51Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU3ODQyMQ==", "bodyText": "There is no need to check there are domains present - you already did that earlier with verify(!constraint.isNone(), \"constraint is none\").", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480578421", "createdAt": "2020-09-01T01:52:43Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU4NDAwNQ==", "bodyText": "Can you please use descriptive names fo k and v?", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480584005", "createdAt": "2020-09-01T01:57:47Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (computeOffsetByTimestamp != null && offsetTimestampRanged.isPresent()) {\n+                Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getBegin()));\n+                if (needPushTimeStampUpperBound) {\n+                    partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                            (p) -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getEnd()));\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilter(partitionFilteredInfos,\n+                    partitionBeginOffsets,\n+                    partitionEndOffsets);\n+        }\n+        return defaultKafkaFilter;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> genBeginOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((k, v) -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDYxODI5Mg==", "bodyText": "No need to split declaration and assignment.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480618292", "createdAt": "2020-09-01T02:28:10Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSplitManager.java", "diffHunk": "@@ -106,6 +127,49 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, Co\n         }\n     }\n \n+    private boolean isTimestampUpperBoundForcePushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try {\n+            AdminClient adminClient = adminFactory.create();\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> topicConfig;\n+            topicConfig = describeResult.all().get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDYzMjY4Mw==", "bodyText": "requireNonNull() for non-primitive arguments.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480632683", "createdAt": "2020-09-01T02:38:36Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDYzMzk4MA==", "bodyText": "computeOffsetByTimestamp should never be null.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r480633980", "createdAt": "2020-09-01T02:39:33Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<List<TupleDomain.ColumnDomain<ColumnHandle>>> columnDomains = constraint.getColumnDomains();\n+            if (columnDomains.isPresent()) {\n+                for (TupleDomain.ColumnDomain<ColumnHandle> columnDomain : columnDomains.get()) {\n+                    KafkaColumnHandle columnHandle = (KafkaColumnHandle) columnDomain.getColumn();\n+                    if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                        break;\n+                    }\n+                    KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                    switch (fieldDescription) {\n+                        case PARTITION_OFFSET_FIELD:\n+                            offsetRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        case PARTITION_ID_FIELD:\n+                            partitionIdsFiltered = getSingleSet(columnDomain.getDomain(), partitionIds);\n+                            break;\n+                        case OFFSET_TIMESTAMP_FIELD:\n+                            offsetTimestampRanged = getRange(columnDomain.getDomain());\n+                            break;\n+                        default:\n+                            break;\n+                    }\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        (p) -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        (p) -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (computeOffsetByTimestamp != null && offsetTimestampRanged.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 98}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1NTIyMzg1", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-485522385", "createdAt": "2020-09-10T02:26:26Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwMjoyNjoyNlrOHPgm7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwMzowMzoyMVrOHPhNFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAyNDk0MA==", "bodyText": "Typo: hasPartitonOffset -> hasPartitionOffset", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486024940", "createdAt": "2020-09-10T02:26:26Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.testng.annotations.Test;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static io.prestosql.plugin.kafka.KafkaFilterManager.getKafkaFilter;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldDescription.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldDescription.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldDescription.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestKafkaFilterManager\n+{\n+    private static final int PARTITION_0 = 0;\n+    private static final int PARTITION_1 = 1;\n+    private static final int PARTITION_2 = 2;\n+    private static final long START_TIMESTAMP = 1599035000;\n+    private static final long END_TIMESTAMP = 1599035650;\n+    private static final String TOPIC_NAME = \"topic1\";\n+    private static final String SCHEMA_NAME = \"kafka\";\n+\n+    private final Map<Integer, Range> defaultPartitionValues = ImmutableMap.of(\n+            PARTITION_0, new Range(10, 100),\n+            PARTITION_1, new Range(11, 200),\n+            PARTITION_2, new Range(12, 300));\n+    private final Range offsetFilterPartitionValues = new Range(20, 80);\n+    private final Map<Integer, Range> timestampFilterPartitionValues = ImmutableMap.of(\n+            PARTITION_0, new Range(30, 70),\n+            PARTITION_1, new Range(31, 170),\n+            PARTITION_2, new Range(32, 270));\n+\n+    @Test\n+    public void testGetKafkaFilter()\n+    {\n+        // test partition only\n+        testFilter(true, false, false, false,\n+                filter -> assertEquals(filter.getPartitionInfos().size(), defaultPartitionValues.size()),\n+                filter -> {\n+                    assertEquals(filter.getPartitionInfos().size(), 1);\n+                    assertEquals(filter.getPartitionInfos().get(0).partition(), 1);\n+                });\n+\n+        // test offset only\n+        testFilter(false, true, false, false,\n+                filter -> {\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getEnd());\n+                },\n+                filter -> {\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), offsetFilterPartitionValues.getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), offsetFilterPartitionValues.getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), offsetFilterPartitionValues.getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), offsetFilterPartitionValues.getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), offsetFilterPartitionValues.getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), offsetFilterPartitionValues.getEnd());\n+                });\n+\n+        // test timestamp only without pushing down upper bound\n+        testFilter(false, false, true, false,\n+                filter -> {\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getEnd());\n+                },\n+                filter -> {\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), timestampFilterPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), timestampFilterPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), timestampFilterPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getEnd());\n+                });\n+\n+        // test timestamp only with pushing down upper bound\n+        testFilter(false, false, true, true,\n+                filter -> {\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getEnd());\n+                },\n+                filter -> {\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), timestampFilterPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), timestampFilterPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), timestampFilterPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), timestampFilterPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), timestampFilterPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), timestampFilterPartitionValues.get(PARTITION_2).getEnd());\n+                });\n+\n+        // test both without pushing down upper bound\n+        testFilter(true, true, true, false,\n+                filter -> {\n+                    assertEquals(filter.getPartitionInfos().size(), defaultPartitionValues.size());\n+\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getEnd());\n+                },\n+                filter -> {\n+                    assertEquals(filter.getPartitionInfos().size(), 1);\n+                    assertEquals(filter.getPartitionInfos().get(0).partition(), 1);\n+\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), timestampFilterPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), offsetFilterPartitionValues.getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), timestampFilterPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), offsetFilterPartitionValues.getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), timestampFilterPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), offsetFilterPartitionValues.getEnd());\n+                });\n+\n+        // test both with pushing down upper bound\n+        testFilter(true, true, true, true,\n+                filter -> {\n+                    assertEquals(filter.getPartitionInfos().size(), defaultPartitionValues.size());\n+\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getEnd());\n+                },\n+                filter -> {\n+                    assertEquals(filter.getPartitionInfos().size(), 1);\n+                    assertEquals(filter.getPartitionInfos().get(0).partition(), 1);\n+\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), timestampFilterPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), timestampFilterPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), timestampFilterPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), offsetFilterPartitionValues.getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), timestampFilterPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), offsetFilterPartitionValues.getEnd());\n+                });\n+    }\n+\n+    private void testFilter(boolean hasPartition,\n+                                     boolean hasPartitonOffset,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjAzNDcxMQ==", "bodyText": "We're making this set of assertions multiple times.  Since the filter is the same every time (result of buildDefaultKafkaFilter()), it would seem sufficient to assert just once (if we need them at all).", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486034711", "createdAt": "2020-09-10T03:03:21Z", "author": {"login": "aalbu"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.testng.annotations.Test;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static io.prestosql.plugin.kafka.KafkaFilterManager.getKafkaFilter;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldDescription.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldDescription.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldDescription.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestKafkaFilterManager\n+{\n+    private static final int PARTITION_0 = 0;\n+    private static final int PARTITION_1 = 1;\n+    private static final int PARTITION_2 = 2;\n+    private static final long START_TIMESTAMP = 1599035000;\n+    private static final long END_TIMESTAMP = 1599035650;\n+    private static final String TOPIC_NAME = \"topic1\";\n+    private static final String SCHEMA_NAME = \"kafka\";\n+\n+    private final Map<Integer, Range> defaultPartitionValues = ImmutableMap.of(\n+            PARTITION_0, new Range(10, 100),\n+            PARTITION_1, new Range(11, 200),\n+            PARTITION_2, new Range(12, 300));\n+    private final Range offsetFilterPartitionValues = new Range(20, 80);\n+    private final Map<Integer, Range> timestampFilterPartitionValues = ImmutableMap.of(\n+            PARTITION_0, new Range(30, 70),\n+            PARTITION_1, new Range(31, 170),\n+            PARTITION_2, new Range(32, 270));\n+\n+    @Test\n+    public void testGetKafkaFilter()\n+    {\n+        // test partition only\n+        testFilter(true, false, false, false,\n+                filter -> assertEquals(filter.getPartitionInfos().size(), defaultPartitionValues.size()),\n+                filter -> {\n+                    assertEquals(filter.getPartitionInfos().size(), 1);\n+                    assertEquals(filter.getPartitionInfos().get(0).partition(), 1);\n+                });\n+\n+        // test offset only\n+        testFilter(false, true, false, false,\n+                filter -> {\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_0)), defaultPartitionValues.get(PARTITION_0).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_1)), defaultPartitionValues.get(PARTITION_1).getEnd());\n+                    assertEquals((long) filter.getPartitionBeginOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getBegin());\n+                    assertEquals((long) filter.getPartitionEndOffsets().get(new TopicPartition(TOPIC_NAME, PARTITION_2)), defaultPartitionValues.get(PARTITION_2).getEnd());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 80}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg2MTIzNDI1", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-486123425", "createdAt": "2020-09-10T16:44:30Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo0NDozMVrOHP80Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxODozNTozM1rOHQBBEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4NzE0Mw==", "bodyText": "This is a utility class. If it was to stay such please rename to KafkaFilters.\nBut I think it should be converted class which we instantiate.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486487143", "createdAt": "2020-09-10T16:44:31Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NDA0Nw==", "bodyText": "I suggest that you rename KafkaFilter to KafkaFilteringResult and use it only as a return value of getKafkaFilter.\nThen change signature of getKafkaFilter to\n    public  KafkaFilteringResult getKafkaFilter(\n            ConnectorSession session,\n            KafkaTableHandle kafkaTableHandle,\n            List<PartitionInfo> partitionInfos,\n            Map<TopicPartition, Long> partitionBeginOffsets,\n            Map<TopicPartition, Long> partitionEndOffsets) {\nDerive needPushTimeStampUpperBound internally based on injected configuration object and session passed as an argument.\nThe computeOffsetByTimestamp can be injected to KafkaFilterManager at construction time if we need that paremetrization at all.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486494047", "createdAt": "2020-09-10T16:55:54Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NTEzMg==", "bodyText": "inline s1", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486495132", "createdAt": "2020-09-10T16:57:40Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSessionProperties.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.session.PropertyMetadata;\n+\n+import javax.inject.Inject;\n+\n+import java.util.List;\n+\n+public final class KafkaSessionProperties\n+{\n+    private static final String TIMESTAMP_UPPER_BOUND_FORCE_PUSH_DOWN_ENABLED = \"timestamp_upper_bound_force_push_down_enabled\";\n+    private final List<PropertyMetadata<?>> sessionProperties;\n+\n+    @Inject\n+    public KafkaSessionProperties(KafkaConfig kafkaConfig)\n+    {\n+        PropertyMetadata<Boolean> s1 = PropertyMetadata.booleanProperty(\n+                TIMESTAMP_UPPER_BOUND_FORCE_PUSH_DOWN_ENABLED,\n+                \"Enable or disable timestamp upper bound push down for topic createTime mode\",\n+                kafkaConfig.isTimestampUpperBoundPushDownEnabled(), false);\n+\n+        sessionProperties = ImmutableList.of(s1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NjkwMw==", "bodyText": "Just inline contents into KafkaConnectorModule", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486496903", "createdAt": "2020-09-10T17:00:25Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaAdminModule.java", "diffHunk": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.inject.Binder;\n+import com.google.inject.Module;\n+import com.google.inject.Scopes;\n+\n+public class KafkaAdminModule", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NzYyMg==", "bodyText": "do defensive copies using ImmutableList.copyOf()/ImmutableMap.copyOf()", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486497622", "createdAt": "2020-09-10T17:01:37Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilter.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class KafkaFilter\n+{\n+    private final List<PartitionInfo> partitionInfos;\n+    private final Map<TopicPartition, Long> partitionBeginOffsets;\n+    private final Map<TopicPartition, Long> partitionEndOffsets;\n+\n+    public KafkaFilter(List<PartitionInfo> partitionInfos,\n+                       Map<TopicPartition, Long> partitionBeginOffsets,\n+                       Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        this.partitionInfos = partitionInfos;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUwNDU4MQ==", "bodyText": "rename to config", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486504581", "createdAt": "2020-09-10T17:13:34Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSplitManager.java", "diffHunk": "@@ -106,6 +127,48 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, Co\n         }\n     }\n \n+    private boolean isTimestampUpperBoundForcePushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try {\n+            AdminClient adminClient = adminFactory.create();\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> topicConfig = describeResult.all().get();\n+\n+            if (topicConfig != null) {\n+                Config c = topicConfig.get(topicResource);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUyNTM1OA==", "bodyText": "This should be isTimestampUpperBoundPushdownEnabled (drop force).\nPlease also confirm that we are certain that if topic is in LOG_APPEND mode, we are sure that timestamps are alway recored in non-decreasing order when looking at messages by offset.\nI would expect so, but I do not know details of Kafka, so maybe if there is time skew between machines participating in the communication this does not need to always be true.\nIf above does not hold we cannot filter agains upper bound at all.\nAs for the CREATE_TIME mode. I am not sure that we want to expose the config property which enables upper bound filtering in that mode.\nIt is easy to shoot oneself in the foot with the property, if user does not understand it's data, and filtering logic very well.\nGiven fact that upper bound filtering is not that useful IMO (I would expect that you mostly need filtering to incrementally process new batches of data which are added to the topic; and for that lower bound filtering is enough), I would disable upper bound filter for CREATE_TIME mode.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486525358", "createdAt": "2020-09-10T17:49:06Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSplitManager.java", "diffHunk": "@@ -106,6 +127,48 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, Co\n         }\n     }\n \n+    private boolean isTimestampUpperBoundForcePushdownEnabled(ConnectorSession session, String topic)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUyODI4NA==", "bodyText": "This should be moved to KafkaFilteringManager.class.\nIt should be used by default for standard setup of KafkaFilterManager.class. You can add another package visible constructor for KafkaFilterManager where you can pass BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp strictly for sake of unit tests. Annotate the costructor with @VisibleForTesting", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486528284", "createdAt": "2020-09-10T17:53:56Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSplitManager.java", "diffHunk": "@@ -106,6 +127,48 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, Co\n         }\n     }\n \n+    private boolean isTimestampUpperBoundForcePushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try {\n+            AdminClient adminClient = adminFactory.create();\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> topicConfig = describeResult.all().get();\n+\n+            if (topicConfig != null) {\n+                Config c = topicConfig.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUzMTg4MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n          \n          \n            \n                        if (topicPartitionOffsets.isEmpty()) {\n          \n          \n            \n                            return Optional.empty();\n          \n          \n            \n                        }\n          \n          \n            \n                        OffsetAndTimestamp offsetAndTimestamp = topicPartitionOffsets.values().iterator().next();\n          \n          \n            \n                        if (offsetAndTimestamp == null) {\n          \n          \n            \n                            return Optional.empty();\n          \n          \n            \n                        }\n          \n          \n            \n            \n          \n          \n            \n                        return Optional.of(offsetAndTimestamp.offset());\n          \n          \n            \n                        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n          \n          \n            \n                        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486531880", "createdAt": "2020-09-10T18:00:07Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSplitManager.java", "diffHunk": "@@ -106,6 +127,48 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, Co\n         }\n     }\n \n+    private boolean isTimestampUpperBoundForcePushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try {\n+            AdminClient adminClient = adminFactory.create();\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> topicConfig = describeResult.all().get();\n+\n+            if (topicConfig != null) {\n+                Config c = topicConfig.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            if (topicPartitionOffsets.isEmpty()) {\n+                return Optional.empty();\n+            }\n+            OffsetAndTimestamp offsetAndTimestamp = topicPartitionOffsets.values().iterator().next();\n+            if (offsetAndTimestamp == null) {\n+                return Optional.empty();\n+            }\n+\n+            return Optional.of(offsetAndTimestamp.offset());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0OTU5Mw==", "bodyText": "add a comment that it also covers empty range list", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486549593", "createdAt": "2020-09-10T18:27:02Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(defaultKafkaFilter, \"defaultKafkaFilter is null\");\n+        requireNonNull(computeOffsetByTimestamp, \"computeOffsetByTimestamp is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                    break;\n+                }\n+                KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                switch (fieldDescription) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getBegin()));\n+                if (needPushTimeStampUpperBound) {\n+                    partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                            p -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getEnd()));\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilter(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return defaultKafkaFilter;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredEndOffsets = new HashMap<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredEndOffsets.put(partition, filterIndex.map(index -> Long.min(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredEndOffsets;\n+    }\n+\n+    private static Optional<Range> getRange(Domain domain)\n+    {\n+        Long low = INVALID_KAFKA_RANGE_INDEX;\n+        Long high = INVALID_KAFKA_RANGE_INDEX;\n+        if (domain.isSingleValue()) {\n+            low = (long) domain.getSingleValue();\n+            high = (long) domain.getSingleValue();\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    List<Long> values = rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .collect(toImmutableList());\n+                    low = Collections.min(values);\n+                    high = Collections.max(values);\n+                }\n+                else {\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = getLowByLowMark(lowMark).orElse(low);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = getHighByHighMark(highMark).orElse(high);\n+                }\n+            }\n+        }\n+        if (high != INVALID_KAFKA_RANGE_INDEX) {\n+            high = high + 1;\n+        }\n+        return Optional.of(new Range(low, high));\n+    }\n+\n+    private static Set<Long> getSingleSet(Domain domain, Set<Long> sourceValues)\n+    {\n+        requireNonNull(sourceValues, \"sourceValues is none\");\n+        if (domain.isSingleValue()) {\n+            long singleValue = (long) domain.getSingleValue();\n+            return sourceValues.stream().filter(sourceValue -> sourceValue == singleValue).collect(toImmutableSet());\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    return rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU1MTk1OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                long low = sourceValues.stream().min(Long::compareTo).orElse(0L);\n          \n          \n            \n                                long high = sourceValues.stream().max(Long::compareTo).orElse(0L);\n          \n          \n            \n                                long low = 0;\n          \n          \n            \n                                long high = Long.MAX_VALUE;", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486551958", "createdAt": "2020-09-10T18:30:08Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(defaultKafkaFilter, \"defaultKafkaFilter is null\");\n+        requireNonNull(computeOffsetByTimestamp, \"computeOffsetByTimestamp is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                    break;\n+                }\n+                KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                switch (fieldDescription) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getBegin()));\n+                if (needPushTimeStampUpperBound) {\n+                    partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                            p -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getEnd()));\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilter(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return defaultKafkaFilter;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredEndOffsets = new HashMap<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredEndOffsets.put(partition, filterIndex.map(index -> Long.min(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredEndOffsets;\n+    }\n+\n+    private static Optional<Range> getRange(Domain domain)\n+    {\n+        Long low = INVALID_KAFKA_RANGE_INDEX;\n+        Long high = INVALID_KAFKA_RANGE_INDEX;\n+        if (domain.isSingleValue()) {\n+            low = (long) domain.getSingleValue();\n+            high = (long) domain.getSingleValue();\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    List<Long> values = rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .collect(toImmutableList());\n+                    low = Collections.min(values);\n+                    high = Collections.max(values);\n+                }\n+                else {\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = getLowByLowMark(lowMark).orElse(low);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = getHighByHighMark(highMark).orElse(high);\n+                }\n+            }\n+        }\n+        if (high != INVALID_KAFKA_RANGE_INDEX) {\n+            high = high + 1;\n+        }\n+        return Optional.of(new Range(low, high));\n+    }\n+\n+    private static Set<Long> getSingleSet(Domain domain, Set<Long> sourceValues)\n+    {\n+        requireNonNull(sourceValues, \"sourceValues is none\");\n+        if (domain.isSingleValue()) {\n+            long singleValue = (long) domain.getSingleValue();\n+            return sourceValues.stream().filter(sourceValue -> sourceValue == singleValue).collect(toImmutableSet());\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    return rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .filter(sourceValues::contains)\n+                            .collect(toImmutableSet());\n+                }\n+                else {\n+                    long low = sourceValues.stream().min(Long::compareTo).orElse(0L);\n+                    long high = sourceValues.stream().max(Long::compareTo).orElse(0L);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU1MzM2OQ==", "bodyText": "Maybe define helper method long min(long a, Optional<Marker> b) and max .... and use it here and in getRange above", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486553369", "createdAt": "2020-09-10T18:32:11Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+\n+    private KafkaFilterManager() {}\n+\n+    public static KafkaFilter getKafkaFilter(KafkaTableHandle kafkaTableHandle,\n+                                             KafkaFilter defaultKafkaFilter,\n+                                             boolean needPushTimeStampUpperBound,\n+                                             BiFunction<TopicPartition, Long, Optional<Long>> computeOffsetByTimestamp)\n+    {\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(defaultKafkaFilter, \"defaultKafkaFilter is null\");\n+        requireNonNull(computeOffsetByTimestamp, \"computeOffsetByTimestamp is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            List<PartitionInfo> partitionInfos = defaultKafkaFilter.getPartitionInfos();\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Map<TopicPartition, Long> partitionBeginOffsets = defaultKafkaFilter.getPartitionBeginOffsets();\n+            Map<TopicPartition, Long> partitionEndOffsets = defaultKafkaFilter.getPartitionEndOffsets();\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!KafkaInternalFieldDescription.isInternalColumn(columnHandle.getName())) {\n+                    break;\n+                }\n+                KafkaInternalFieldDescription fieldDescription = KafkaInternalFieldDescription.forColumnName(columnHandle.getName());\n+                switch (fieldDescription) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getBegin()));\n+                if (needPushTimeStampUpperBound) {\n+                    partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                            p -> computeOffsetByTimestamp.apply(p, finalOffsetTimestampRanged.get().getEnd()));\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilter(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return defaultKafkaFilter;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredEndOffsets = new HashMap<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredEndOffsets.put(partition, filterIndex.map(index -> Long.min(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredEndOffsets;\n+    }\n+\n+    private static Optional<Range> getRange(Domain domain)\n+    {\n+        Long low = INVALID_KAFKA_RANGE_INDEX;\n+        Long high = INVALID_KAFKA_RANGE_INDEX;\n+        if (domain.isSingleValue()) {\n+            low = (long) domain.getSingleValue();\n+            high = (long) domain.getSingleValue();\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    List<Long> values = rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .collect(toImmutableList());\n+                    low = Collections.min(values);\n+                    high = Collections.max(values);\n+                }\n+                else {\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = getLowByLowMark(lowMark).orElse(low);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = getHighByHighMark(highMark).orElse(high);\n+                }\n+            }\n+        }\n+        if (high != INVALID_KAFKA_RANGE_INDEX) {\n+            high = high + 1;\n+        }\n+        return Optional.of(new Range(low, high));\n+    }\n+\n+    private static Set<Long> getSingleSet(Domain domain, Set<Long> sourceValues)\n+    {\n+        requireNonNull(sourceValues, \"sourceValues is none\");\n+        if (domain.isSingleValue()) {\n+            long singleValue = (long) domain.getSingleValue();\n+            return sourceValues.stream().filter(sourceValue -> sourceValue == singleValue).collect(toImmutableSet());\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    return rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .filter(sourceValues::contains)\n+                            .collect(toImmutableSet());\n+                }\n+                else {\n+                    long low = sourceValues.stream().min(Long::compareTo).orElse(0L);\n+                    long high = sourceValues.stream().max(Long::compareTo).orElse(0L);\n+\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    Optional<Long> lowByLowMark = getLowByLowMark(lowMark);\n+                    if (lowByLowMark.isPresent()) {\n+                        low = Long.max(lowByLowMark.get(), low);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU1NTkyMQ==", "bodyText": "adding OFFSET_TIMESTAMP_FIELD could be a separate commit.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r486555921", "createdAt": "2020-09-10T18:35:33Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldDescription.java", "diffHunk": "@@ -75,7 +77,12 @@\n     /**\n      * <tt>_key_length</tt> - length in bytes of the key.\n      */\n-    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\");\n+    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\"),\n+\n+    /**\n+     * <tt>_timestamp</tt> - offset timestamp\n+     */\n+    OFFSET_TIMESTAMP_FIELD(\"_timestamp\", TimestampType.createTimestampType(DEFAULT_PRECISION), \"Offset Timestamp\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMTkyNTc4", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493192578", "createdAt": "2020-09-22T07:34:20Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwNzozNDoyMFrOHVtimQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwNzozNDoyMFrOHVtimQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjUyODI4MQ==", "bodyText": "Rename commit message to Add internal Kafka column for OFFSET_TIMESTAMP_FIELD", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492528281", "createdAt": "2020-09-22T07:34:20Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldManager.java", "diffHunk": "@@ -18,12 +18,14 @@\n import io.prestosql.spi.connector.ColumnMetadata;\n import io.prestosql.spi.type.BigintType;\n import io.prestosql.spi.type.BooleanType;\n+import io.prestosql.spi.type.TimestampType;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMTk4ODU5", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493198859", "createdAt": "2020-09-22T07:43:26Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwNzo0MzoyNlrOHVt1MQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwNzo0MzoyNlrOHVt1MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjUzMzA0MQ==", "bodyText": "Rename commit message to Add predicate push down support for internal Kafka columns. The reminder can be phrased as:\nAdd support for predicate pushdown for following Kafka internal columns\n * _timestamp \n * _partition_offset\n * _partition_id\n\nIf predicate specifies lower bound on _timestamp column (_timestamp > XXXX), it is always pushed down. \nThe upper bound predicate is pushed down only for topics using ``LogAppendTime`` mode. \nFor topics using ``CreateTime`` mode, upper bound pushdown must be explicitly \nallowed via ``kafka.timestamp-upper-bound-force-push-down-enabled`` config property\nor ``timestamp_upper_bound_force_push_down_enabled`` session property.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492533041", "createdAt": "2020-09-22T07:43:26Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaAdminFactory.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 1}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjA2NTI5", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493206529", "createdAt": "2020-09-22T07:54:23Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwNzo1NDoyNFrOHVuM1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwNzo1NDoyNFrOHVuM1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjUzOTA5NQ==", "bodyText": "Add comment describing logic and motivation here. The phrasing I suggested for commit message will do.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492539095", "createdAt": "2020-09-22T07:54:24Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 149}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjQzMDI5", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493243029", "createdAt": "2020-09-22T08:42:28Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo0MjoyOVrOHVv9QQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo0MjoyOVrOHVv9QQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU2Nzg3Mw==", "bodyText": "This is super cryptic. Can you explain in a comment what is happening here?", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492567873", "createdAt": "2020-09-22T08:42:29Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 174}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjQzNTg0", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493243584", "createdAt": "2020-09-22T08:43:09Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo0MzowOVrOHVv-9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo0MzowOVrOHVv-9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU2ODMwOQ==", "bodyText": "What exceptions are we expecting here. And why at all?\nAs a rule of thumb we should propagate exception. Not mask it with Optional.empty()", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492568309", "createdAt": "2020-09-22T08:43:09Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 181}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjQ3ODY3", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493247867", "createdAt": "2020-09-22T08:48:21Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo0ODoyMlrOHVwMVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo0ODoyMlrOHVwMVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MTczNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n          \n          \n            \n                                                                                         Function<TopicPartition, Optional<Long>> computeOffset)\n          \n          \n            \n                private static Map<TopicPartition, Long> overridePartitionBeginOffsets(Map<TopicPartition, Long> partitionBeginOffsets,\n          \n          \n            \n                                                                                         Function<TopicPartition, Optional<Long>> overrideFunction)", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492571734", "createdAt": "2020-09-22T08:48:22Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 186}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjQ5MjA3", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493249207", "createdAt": "2020-09-22T08:50:00Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo1MDowMFrOHVwQeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo1MDowMFrOHVwQeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3Mjc5Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n          \n          \n            \n                                                                                       Function<TopicPartition, Optional<Long>> computeOffset)\n          \n          \n            \n                private static Map<TopicPartition, Long> overridePartitionEndOffsets(Map<TopicPartition, Long> partitionEndOffsets,\n          \n          \n            \n                                                                                       Function<TopicPartition, Optional<Long>> overrideFunction)", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492572793", "createdAt": "2020-09-22T08:50:00Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 197}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjQ5NzQ2", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493249746", "createdAt": "2020-09-22T08:50:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo1MDozOVrOHVwR-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo1MDozOVrOHVwR-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MzE3OQ==", "bodyText": "Use ImmutableMap.builder instead", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492573179", "createdAt": "2020-09-22T08:50:39Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 188}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjQ5OTY0", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493249964", "createdAt": "2020-09-22T08:50:53Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo1MDo1NFrOHVwSmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo1MDo1NFrOHVwSmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MzMzOQ==", "bodyText": "use ImmutableMap.builder", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492573339", "createdAt": "2020-09-22T08:50:54Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredEndOffsets = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 199}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjUwMzc0", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493250374", "createdAt": "2020-09-22T08:51:25Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo1MToyNVrOHVwT0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo1MToyNVrOHVwT0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MzY0OA==", "bodyText": "rename filterIndex to newOffset", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492573648", "createdAt": "2020-09-22T08:51:25Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 190}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjUwNTU3", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493250557", "createdAt": "2020-09-22T08:51:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo1MTozOVrOHVwUQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo1MTozOVrOHVwUQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3Mzc2MQ==", "bodyText": "rename filterIndex to newOffset", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492573761", "createdAt": "2020-09-22T08:51:39Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredEndOffsets = new HashMap<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 201}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjYwOTIx", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493260921", "createdAt": "2020-09-22T09:04:50Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwOTowNDo1MVrOHVwz0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwOTowNDo1MVrOHVwz0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU4MTg0MA==", "bodyText": "This loop feels wrong we are breaking whole process on the first non-internal column.\nAnd even if all constraints are on internal columns we are only processing arbitrary first one.\nInstead we should skip unsupported columns. And for internal once, gradually build up the predicate to be pushed down to kafka.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492581840", "createdAt": "2020-09-22T09:04:51Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 98}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjY3OTE4", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493267918", "createdAt": "2020-09-22T09:14:00Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwOToxNDowMFrOHVxJ7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwOToxNDowMFrOHVxJ7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU4NzUwMw==", "bodyText": "It feels to me that if !highMark.isUpperUnbounded() then highMark.getValueBlock().isPresent() should always be true. Can we make it checkArgument instead?", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492587503", "createdAt": "2020-09-22T09:14:00Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredEndOffsets = new HashMap<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredEndOffsets.put(partition, filterIndex.map(index -> Long.min(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredEndOffsets;\n+    }\n+\n+    private static Optional<Range> getRange(Domain domain)\n+    {\n+        Long low = INVALID_KAFKA_RANGE_INDEX;\n+        Long high = INVALID_KAFKA_RANGE_INDEX;\n+        if (domain.isSingleValue()) {\n+            low = (long) domain.getSingleValue();\n+            high = (long) domain.getSingleValue();\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    List<Long> values = rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .collect(toImmutableList());\n+                    low = Collections.min(values);\n+                    high = Collections.max(values);\n+                }\n+                else {\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = getLowByLowMark(lowMark).orElse(low);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = getHighByHighMark(highMark).orElse(high);\n+                }\n+            }\n+        }\n+        if (high != INVALID_KAFKA_RANGE_INDEX) {\n+            high = high + 1;\n+        }\n+        return Optional.of(new Range(low, high));\n+    }\n+\n+    private static Set<Long> getSingleSet(Domain domain, Set<Long> sourceValues)\n+    {\n+        requireNonNull(sourceValues, \"sourceValues is none\");\n+        if (domain.isSingleValue()) {\n+            long singleValue = (long) domain.getSingleValue();\n+            return sourceValues.stream().filter(sourceValue -> sourceValue == singleValue).collect(toImmutableSet());\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    return rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .filter(sourceValues::contains)\n+                            .collect(toImmutableSet());\n+                }\n+                else {\n+                    long low = 0;\n+                    long high = Long.MAX_VALUE;\n+\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = maxLow(low, lowMark);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = minHigh(high, highMark);\n+                    final long finalLow = low;\n+                    final long finalHigh = high;\n+                    return sourceValues.stream()\n+                            .filter(item -> item >= finalLow && item <= finalHigh)\n+                            .collect(toImmutableSet());\n+                }\n+            }\n+        }\n+        return sourceValues;\n+    }\n+\n+    private static long minHigh(long high, Marker highMark)\n+    {\n+        Optional<Long> highByHighMark = getHighByHighMark(highMark);\n+        if (highByHighMark.isPresent()) {\n+            high = Long.min(highByHighMark.get(), high);\n+        }\n+        return high;\n+    }\n+\n+    private static long maxLow(long low, Marker lowMark)\n+    {\n+        Optional<Long> lowByLowMark = getLowByLowMark(lowMark);\n+        if (lowByLowMark.isPresent()) {\n+            low = Long.max(lowByLowMark.get(), low);\n+        }\n+        return low;\n+    }\n+\n+    private static Optional<Long> getHighByHighMark(Marker highMark)\n+    {\n+        if (!highMark.isUpperUnbounded() && highMark.getValueBlock().isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 298}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjY5NDYw", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493269460", "createdAt": "2020-09-22T09:16:04Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwOToxNjowNVrOHVxO5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwOToxNjowNVrOHVxO5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU4ODc3NQ==", "bodyText": "Why do we need && high - 1 >= 0? Is that legal state?", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492588775", "createdAt": "2020-09-22T09:16:05Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredEndOffsets = new HashMap<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredEndOffsets.put(partition, filterIndex.map(index -> Long.min(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredEndOffsets;\n+    }\n+\n+    private static Optional<Range> getRange(Domain domain)\n+    {\n+        Long low = INVALID_KAFKA_RANGE_INDEX;\n+        Long high = INVALID_KAFKA_RANGE_INDEX;\n+        if (domain.isSingleValue()) {\n+            low = (long) domain.getSingleValue();\n+            high = (long) domain.getSingleValue();\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    List<Long> values = rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .collect(toImmutableList());\n+                    low = Collections.min(values);\n+                    high = Collections.max(values);\n+                }\n+                else {\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = getLowByLowMark(lowMark).orElse(low);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = getHighByHighMark(highMark).orElse(high);\n+                }\n+            }\n+        }\n+        if (high != INVALID_KAFKA_RANGE_INDEX) {\n+            high = high + 1;\n+        }\n+        return Optional.of(new Range(low, high));\n+    }\n+\n+    private static Set<Long> getSingleSet(Domain domain, Set<Long> sourceValues)\n+    {\n+        requireNonNull(sourceValues, \"sourceValues is none\");\n+        if (domain.isSingleValue()) {\n+            long singleValue = (long) domain.getSingleValue();\n+            return sourceValues.stream().filter(sourceValue -> sourceValue == singleValue).collect(toImmutableSet());\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    return rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .filter(sourceValues::contains)\n+                            .collect(toImmutableSet());\n+                }\n+                else {\n+                    long low = 0;\n+                    long high = Long.MAX_VALUE;\n+\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = maxLow(low, lowMark);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = minHigh(high, highMark);\n+                    final long finalLow = low;\n+                    final long finalHigh = high;\n+                    return sourceValues.stream()\n+                            .filter(item -> item >= finalLow && item <= finalHigh)\n+                            .collect(toImmutableSet());\n+                }\n+            }\n+        }\n+        return sourceValues;\n+    }\n+\n+    private static long minHigh(long high, Marker highMark)\n+    {\n+        Optional<Long> highByHighMark = getHighByHighMark(highMark);\n+        if (highByHighMark.isPresent()) {\n+            high = Long.min(highByHighMark.get(), high);\n+        }\n+        return high;\n+    }\n+\n+    private static long maxLow(long low, Marker lowMark)\n+    {\n+        Optional<Long> lowByLowMark = getLowByLowMark(lowMark);\n+        if (lowByLowMark.isPresent()) {\n+            low = Long.max(lowByLowMark.get(), low);\n+        }\n+        return low;\n+    }\n+\n+    private static Optional<Long> getHighByHighMark(Marker highMark)\n+    {\n+        if (!highMark.isUpperUnbounded() && highMark.getValueBlock().isPresent()) {\n+            long high = (Long) highMark.getValue();\n+            if (BELOW == highMark.getBound() && high - 1 >= 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 300}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjc1NDYw", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493275460", "createdAt": "2020-09-22T09:23:52Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwOToyMzo1MlrOHVxhjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwOToyMzo1MlrOHVxhjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU5MzU1MQ==", "bodyText": "Are EndOffsets inclusive or exclusive?\nYou are calling calcTopicPartitionEndOffsetMap as if they were exclusive. Is that the case?\nedit:\nActually it looks like you are calling calcTopicPartitionEndOffsetMap differently in different contexts.\nFor PARTITION_OFFSET_FIELD the computeOffset will return a value for partition which we do not care about.\nBut for OFFSET_TIMESTAMP_FIELD it will return id which we should still scan.\nPlease clean it up.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r492593551", "createdAt": "2020-09-22T09:23:52Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.predicate.Marker.Bound.ABOVE;\n+import static io.prestosql.spi.predicate.Marker.Bound.BELOW;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    break;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = calcTopicPartitionBeginOffsetMap(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = calcTopicPartitionEndOffsetMap(partitionEndOffsets,\n+                                p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        try {\n+            if ((timestamp + \"\").length() == 16) {\n+                timestamp /= 1000;\n+            }\n+            Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, timestamp));\n+            return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+        }\n+        catch (Exception e) {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionBeginOffsetMap(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                             Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        final Map<TopicPartition, Long> partitionFilteredBeginOffsets = new HashMap<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> filterIndex = computeOffset.apply(partition);\n+            partitionFilteredBeginOffsets.put(partition, filterIndex.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsets;\n+    }\n+\n+    private static Map<TopicPartition, Long> calcTopicPartitionEndOffsetMap(Map<TopicPartition, Long> partitionEndOffsets,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 196}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjgxODYy", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-493281862", "createdAt": "2020-09-22T09:32:10Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk2NTU3NTg0", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-496557584", "createdAt": "2020-09-25T16:07:41Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjowNzo0MVrOHYJ2mA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNzoxMzowOVrOHYL_Iw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA4OTMwNA==", "bodyText": "Use TimestampType.TIMESTAMP_MILLIS here explicitly. The DEFAULT_PRECISION may change in the future and type of this column should not.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495089304", "createdAt": "2020-09-25T16:07:41Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldManager.java", "diffHunk": "@@ -169,6 +176,10 @@ public KafkaInternalFieldManager(TypeManager typeManager)\n                         KEY_LENGTH_FIELD,\n                         \"Total number of key bytes\",\n                         BigintType.BIGINT))\n+                .put(OFFSET_TIMESTAMP_FIELD, new InternalField(\n+                        OFFSET_TIMESTAMP_FIELD,\n+                        \"Message timestamp\",\n+                        TimestampType.createTimestampType(DEFAULT_PRECISION)))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA5MTI4NA==", "bodyText": "Would you be fine with adding documentation for property to kafka.rst?", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495091284", "createdAt": "2020-09-25T16:11:08Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaConfig.java", "diffHunk": "@@ -153,4 +154,17 @@ public KafkaConfig setMessagesPerSplit(int messagesPerSplit)\n         this.messagesPerSplit = messagesPerSplit;\n         return this;\n     }\n+\n+    public boolean isTimestampUpperBoundPushDownEnabled()\n+    {\n+        return timestampUpperBoundPushDownEnabled;\n+    }\n+\n+    @Config(\"kafka.timestamp-upper-bound-force-push-down-enabled\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA5MzQwNA==", "bodyText": "Use partition instead of p", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495093404", "createdAt": "2020-09-25T16:15:03Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA5NjMwNQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    try (AdminClient adminClient = adminFactory.create()) {\n          \n          \n            \n                        ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n          \n          \n            \n            \n          \n          \n            \n                        DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n          \n          \n            \n                        Map<ConfigResource, Config> config = describeResult.all().get();\n          \n          \n            \n            \n          \n          \n            \n                        if (config != null) {\n          \n          \n            \n                            Config c = config.get(topicResource);\n          \n          \n            \n                            String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n          \n          \n            \n                            if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n          \n          \n            \n                                return true;\n          \n          \n            \n                            }\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n          \n            \n                    catch (Exception e) {\n          \n          \n            \n                        throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n          \n          \n            \n                    }\n          \n          \n            \n                    try (AdminClient adminClient = adminFactory.create()) {\n          \n          \n            \n                        ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n          \n          \n            \n            \n          \n          \n            \n                        DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n          \n          \n            \n                        Map<ConfigResource, Config> configMap = describeResult.all().get();\n          \n          \n            \n            \n          \n          \n            \n                        if (configMap != null) {\n          \n          \n            \n                            Config config = configMap.get(topicResource);\n          \n          \n            \n                            String timestampType = config.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n          \n          \n            \n                            if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n          \n          \n            \n                                return true;\n          \n          \n            \n                            }\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n          \n            \n                    catch (Exception e) {\n          \n          \n            \n                        throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get configuration for topic '%s'\", topic), e);\n          \n          \n            \n                    }", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495096305", "createdAt": "2020-09-25T16:20:26Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEwNjAxNw==", "bodyText": "I think it is wrong. Take a look at this example\ntimestamps:     10  20  20  20  21 \noffsets:        1   2   3   4   5    \n\nIf I search for 21 you would like this code to return 4. But it will return 3. findOffsetsByTimestamp(...21-1) will return 3. And then we will increment it.\nI think the correct code would search for first offset for upperbound. And then move back one offset.\n        Optional<Long> offsetsByTimestamp = findOffsetsByTimestamp(kafkaConsumer, topicPartition, timestamp);\n        return offsetsByTimestamp.map(aLong -> aLong - 1);", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495106017", "createdAt": "2020-09-25T16:38:22Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        final long transferTimestamp = floorDiv(timestamp, MICROSECONDS_PER_MILLISECOND);\n+        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, transferTimestamp));\n+        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestampInclusive(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        Optional<Long> offsetsByTimestamp = findOffsetsByTimestamp(kafkaConsumer, topicPartition, timestamp - 1);\n+        return offsetsByTimestamp.map(aLong -> aLong + 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExMDY2Mw==", "bodyText": "Rename to findOffsetsForTimestampGreaterOrEqual", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495110663", "createdAt": "2020-09-25T16:47:14Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExMTAzMA==", "bodyText": "rename to findOffsetsForTimestampLessThan. Unless I get the expected semantics wrong.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495111030", "createdAt": "2020-09-25T16:47:58Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        final long transferTimestamp = floorDiv(timestamp, MICROSECONDS_PER_MILLISECOND);\n+        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, transferTimestamp));\n+        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestampInclusive(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExMTI3MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    ImmutableMap.Builder<TopicPartition, Long> partitionFilteredBeginOffsetsBuilder = new ImmutableMap.Builder<>();\n          \n          \n            \n                    ImmutableMap.Builder<TopicPartition, Long> partitionFilteredBeginOffsetsBuilder = ImmutableMap.builder();", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495111270", "createdAt": "2020-09-25T16:48:31Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        final long transferTimestamp = floorDiv(timestamp, MICROSECONDS_PER_MILLISECOND);\n+        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, transferTimestamp));\n+        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestampInclusive(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        Optional<Long> offsetsByTimestamp = findOffsetsByTimestamp(kafkaConsumer, topicPartition, timestamp - 1);\n+        return offsetsByTimestamp.map(aLong -> aLong + 1);\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionBeginOffsets(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> overrideFunction)\n+    {\n+        ImmutableMap.Builder<TopicPartition, Long> partitionFilteredBeginOffsetsBuilder = new ImmutableMap.Builder<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExMTQ3NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    ImmutableMap.Builder<TopicPartition, Long> partitionFilteredEndOffsetsBuilder = new ImmutableMap.Builder<>();\n          \n          \n            \n                    ImmutableMap.Builder<TopicPartition, Long> partitionFilteredEndOffsetsBuilder = ImmutableMap.builder();", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495111475", "createdAt": "2020-09-25T16:48:58Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        final long transferTimestamp = floorDiv(timestamp, MICROSECONDS_PER_MILLISECOND);\n+        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, transferTimestamp));\n+        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestampInclusive(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        Optional<Long> offsetsByTimestamp = findOffsetsByTimestamp(kafkaConsumer, topicPartition, timestamp - 1);\n+        return offsetsByTimestamp.map(aLong -> aLong + 1);\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionBeginOffsets(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> overrideFunction)\n+    {\n+        ImmutableMap.Builder<TopicPartition, Long> partitionFilteredBeginOffsetsBuilder = new ImmutableMap.Builder<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> newOffset = overrideFunction.apply(partition);\n+            partitionFilteredBeginOffsetsBuilder.put(partition, newOffset.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsetsBuilder.build();\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionEndOffsets(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                         Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        ImmutableMap.Builder<TopicPartition, Long> partitionFilteredEndOffsetsBuilder = new ImmutableMap.Builder<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExMzc3Nw==", "bodyText": "same here", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495113777", "createdAt": "2020-09-25T16:53:22Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExMzc5MA==", "bodyText": "range.getBegin() >= 0 -> range.getBegin() != INVALID_KAFKA_RANGE_INDEX", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495113790", "createdAt": "2020-09-25T16:53:23Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExNDU4MA==", "bodyText": "Make signature:\nprivate static Set<Long> filterValuesByDomain(Set<Long> values, Domain domain)", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495114580", "createdAt": "2020-09-25T16:54:49Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        final long transferTimestamp = floorDiv(timestamp, MICROSECONDS_PER_MILLISECOND);\n+        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, transferTimestamp));\n+        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestampInclusive(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        Optional<Long> offsetsByTimestamp = findOffsetsByTimestamp(kafkaConsumer, topicPartition, timestamp - 1);\n+        return offsetsByTimestamp.map(aLong -> aLong + 1);\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionBeginOffsets(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> overrideFunction)\n+    {\n+        ImmutableMap.Builder<TopicPartition, Long> partitionFilteredBeginOffsetsBuilder = new ImmutableMap.Builder<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> newOffset = overrideFunction.apply(partition);\n+            partitionFilteredBeginOffsetsBuilder.put(partition, newOffset.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsetsBuilder.build();\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionEndOffsets(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                         Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        ImmutableMap.Builder<TopicPartition, Long> partitionFilteredEndOffsetsBuilder = new ImmutableMap.Builder<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> newOffset = computeOffset.apply(partition);\n+            partitionFilteredEndOffsetsBuilder.put(partition, newOffset.map(index -> Long.min(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredEndOffsetsBuilder.build();\n+    }\n+\n+    private static Optional<Range> getRange(Domain domain)\n+    {\n+        Long low = INVALID_KAFKA_RANGE_INDEX;\n+        Long high = INVALID_KAFKA_RANGE_INDEX;\n+        if (domain.isSingleValue()) {\n+            low = (long) domain.getSingleValue();\n+            high = (long) domain.getSingleValue();\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    List<Long> values = rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .collect(toImmutableList());\n+                    low = Collections.min(values);\n+                    high = Collections.max(values);\n+                }\n+                else {\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = getLowByLowMark(lowMark).orElse(low);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = getHighByHighMark(highMark).orElse(high);\n+                }\n+            }\n+        }\n+        if (high != INVALID_KAFKA_RANGE_INDEX) {\n+            high = high + 1;\n+        }\n+        return Optional.of(new Range(low, high));\n+    }\n+\n+    private static Set<Long> getSingleSet(Domain domain, Set<Long> sourceValues)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 239}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExODUzNg==", "bodyText": "Also this is a bit tricky. Could you write unit-test for it? To do that make it non-private and tag with @VisibleForTesting", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495118536", "createdAt": "2020-09-25T17:02:13Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = getRange(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = getSingleSet(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = getRange(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        p -> (range.getBegin() >= 0) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        p -> (range.getEnd() >= 0) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            p -> findOffsetsByTimestamp(kafkaConsumer, p, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                p -> findOffsetsByTimestampInclusive(kafkaConsumer, p, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> config = describeResult.all().get();\n+\n+            if (config != null) {\n+                Config c = config.get(topicResource);\n+                String timestampType = c.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get topic(%s) config\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestamp(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        final long transferTimestamp = floorDiv(timestamp, MICROSECONDS_PER_MILLISECOND);\n+        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, transferTimestamp));\n+        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+    }\n+\n+    private static Optional<Long> findOffsetsByTimestampInclusive(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        Optional<Long> offsetsByTimestamp = findOffsetsByTimestamp(kafkaConsumer, topicPartition, timestamp - 1);\n+        return offsetsByTimestamp.map(aLong -> aLong + 1);\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionBeginOffsets(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> overrideFunction)\n+    {\n+        ImmutableMap.Builder<TopicPartition, Long> partitionFilteredBeginOffsetsBuilder = new ImmutableMap.Builder<>();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> newOffset = overrideFunction.apply(partition);\n+            partitionFilteredBeginOffsetsBuilder.put(partition, newOffset.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsetsBuilder.build();\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionEndOffsets(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                         Function<TopicPartition, Optional<Long>> computeOffset)\n+    {\n+        ImmutableMap.Builder<TopicPartition, Long> partitionFilteredEndOffsetsBuilder = new ImmutableMap.Builder<>();\n+        partitionEndOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> newOffset = computeOffset.apply(partition);\n+            partitionFilteredEndOffsetsBuilder.put(partition, newOffset.map(index -> Long.min(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredEndOffsetsBuilder.build();\n+    }\n+\n+    private static Optional<Range> getRange(Domain domain)\n+    {\n+        Long low = INVALID_KAFKA_RANGE_INDEX;\n+        Long high = INVALID_KAFKA_RANGE_INDEX;\n+        if (domain.isSingleValue()) {\n+            low = (long) domain.getSingleValue();\n+            high = (long) domain.getSingleValue();\n+        }\n+        else {\n+            ValueSet valueSet = domain.getValues();\n+            if (valueSet instanceof SortedRangeSet) {\n+                Ranges ranges = ((SortedRangeSet) valueSet).getRanges();\n+                List<io.prestosql.spi.predicate.Range> rangeList = ranges.getOrderedRanges();\n+                if (rangeList.stream().allMatch(io.prestosql.spi.predicate.Range::isSingleValue)) {\n+                    List<Long> values = rangeList.stream()\n+                            .map(range -> (Long) range.getSingleValue())\n+                            .collect(toImmutableList());\n+                    low = Collections.min(values);\n+                    high = Collections.max(values);\n+                }\n+                else {\n+                    Marker lowMark = ranges.getSpan().getLow();\n+                    low = getLowByLowMark(lowMark).orElse(low);\n+                    Marker highMark = ranges.getSpan().getHigh();\n+                    high = getHighByHighMark(highMark).orElse(high);\n+                }\n+            }\n+        }\n+        if (high != INVALID_KAFKA_RANGE_INDEX) {\n+            high = high + 1;\n+        }\n+        return Optional.of(new Range(low, high));\n+    }\n+\n+    private static Set<Long> getSingleSet(Domain domain, Set<Long> sourceValues)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExNDU4MA=="}, "originalCommit": null, "originalPosition": 239}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTExOTg1NQ==", "bodyText": "sessionWithUpperBoundPushDownEnabled", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495119855", "createdAt": "2020-09-25T17:04:35Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.Session;\n+import io.prestosql.execution.QueryInfo;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.ResultWithQueryId;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+import org.testng.internal.collections.Pair;\n+\n+import java.util.UUID;\n+import java.util.concurrent.Future;\n+\n+import static io.prestosql.plugin.kafka.util.TestUtils.createEmptyTopicDescription;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestKafkaIntegrationPushDown\n+        extends AbstractTestQueryFramework\n+{\n+    private static final int MESSAGE_NUM = 1000;\n+    private static final int TIMESTAMP_TEST_COUNT = 5;\n+    private static final int TIMESTAMP_TEST_START_INDEX = 2;\n+    private static final int TIMESTAMP_TEST_END_INDEX = 4;\n+\n+    private TestingKafka testingKafka;\n+    private String topicNamePartition;\n+    private String topicNameOffset;\n+    private String topicNameCreateTime;\n+    private String topicNameLogAppend;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        testingKafka = new TestingKafka();\n+        topicNamePartition = \"test_push_down_partition_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameOffset = \"test_push_down_offset_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameCreateTime = \"test_push_down_create_time_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameLogAppend = \"test_push_down_log_append_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+\n+        QueryRunner queryRunner = KafkaQueryRunner.builder(testingKafka)\n+                .setExtraTopicDescription(ImmutableMap.<SchemaTableName, KafkaTopicDescription>builder()\n+                        .put(createEmptyTopicDescription(topicNamePartition, new SchemaTableName(\"default\", topicNamePartition)))\n+                        .put(createEmptyTopicDescription(topicNameOffset, new SchemaTableName(\"default\", topicNameOffset)))\n+                        .put(createEmptyTopicDescription(topicNameCreateTime, new SchemaTableName(\"default\", topicNameCreateTime)))\n+                        .put(createEmptyTopicDescription(topicNameLogAppend, new SchemaTableName(\"default\", topicNameLogAppend)))\n+                        .build())\n+                .setExtraKafkaProperties(ImmutableMap.<String, String>builder()\n+                        .put(\"kafka.messages-per-split\", \"100\")\n+                        .build())\n+                .build();\n+        testingKafka.createTopicWithConfig(2, 1, topicNamePartition, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameOffset, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameCreateTime, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameLogAppend, true);\n+        return queryRunner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void stopKafka()\n+    {\n+        if (testingKafka != null) {\n+            testingKafka.close();\n+            testingKafka = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionPushDown()\n+    {\n+        createMessages(topicNamePartition);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_id=1\",\n+                topicNamePartition);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), MESSAGE_NUM / 2);\n+    }\n+\n+    @Test\n+    public void testOffsetPushDown()\n+    {\n+        createMessages(topicNameOffset);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+                topicNameOffset);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+    }\n+\n+    @Test\n+    public void testTimestampCreateTimeModePushDown()\n+    {\n+        Pair<Long, Long> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= from_unixtime(%s)\" insure including index 2, \"< from_unixtime(%s)\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= from_unixtime(%s) and _timestamp < from_unixtime(%s)\",\n+                topicNameCreateTime, timePair.first() / 1000, timePair.second() / 1000);\n+\n+        // timestamp_upper_bound_force_push_down_enabled default as false.\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+\n+        // timestamp_upper_bound_force_push_down_enabled set as true.\n+        Session enableTmUpperBoundPushDownSession = Session.builder(getSession())", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEyMDc0OQ==", "bodyText": "do not catch", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495120749", "createdAt": "2020-09-25T17:06:14Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.Session;\n+import io.prestosql.execution.QueryInfo;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.ResultWithQueryId;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+import org.testng.internal.collections.Pair;\n+\n+import java.util.UUID;\n+import java.util.concurrent.Future;\n+\n+import static io.prestosql.plugin.kafka.util.TestUtils.createEmptyTopicDescription;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestKafkaIntegrationPushDown\n+        extends AbstractTestQueryFramework\n+{\n+    private static final int MESSAGE_NUM = 1000;\n+    private static final int TIMESTAMP_TEST_COUNT = 5;\n+    private static final int TIMESTAMP_TEST_START_INDEX = 2;\n+    private static final int TIMESTAMP_TEST_END_INDEX = 4;\n+\n+    private TestingKafka testingKafka;\n+    private String topicNamePartition;\n+    private String topicNameOffset;\n+    private String topicNameCreateTime;\n+    private String topicNameLogAppend;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        testingKafka = new TestingKafka();\n+        topicNamePartition = \"test_push_down_partition_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameOffset = \"test_push_down_offset_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameCreateTime = \"test_push_down_create_time_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameLogAppend = \"test_push_down_log_append_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+\n+        QueryRunner queryRunner = KafkaQueryRunner.builder(testingKafka)\n+                .setExtraTopicDescription(ImmutableMap.<SchemaTableName, KafkaTopicDescription>builder()\n+                        .put(createEmptyTopicDescription(topicNamePartition, new SchemaTableName(\"default\", topicNamePartition)))\n+                        .put(createEmptyTopicDescription(topicNameOffset, new SchemaTableName(\"default\", topicNameOffset)))\n+                        .put(createEmptyTopicDescription(topicNameCreateTime, new SchemaTableName(\"default\", topicNameCreateTime)))\n+                        .put(createEmptyTopicDescription(topicNameLogAppend, new SchemaTableName(\"default\", topicNameLogAppend)))\n+                        .build())\n+                .setExtraKafkaProperties(ImmutableMap.<String, String>builder()\n+                        .put(\"kafka.messages-per-split\", \"100\")\n+                        .build())\n+                .build();\n+        testingKafka.createTopicWithConfig(2, 1, topicNamePartition, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameOffset, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameCreateTime, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameLogAppend, true);\n+        return queryRunner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void stopKafka()\n+    {\n+        if (testingKafka != null) {\n+            testingKafka.close();\n+            testingKafka = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionPushDown()\n+    {\n+        createMessages(topicNamePartition);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_id=1\",\n+                topicNamePartition);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), MESSAGE_NUM / 2);\n+    }\n+\n+    @Test\n+    public void testOffsetPushDown()\n+    {\n+        createMessages(topicNameOffset);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+                topicNameOffset);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+    }\n+\n+    @Test\n+    public void testTimestampCreateTimeModePushDown()\n+    {\n+        Pair<Long, Long> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= from_unixtime(%s)\" insure including index 2, \"< from_unixtime(%s)\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= from_unixtime(%s) and _timestamp < from_unixtime(%s)\",\n+                topicNameCreateTime, timePair.first() / 1000, timePair.second() / 1000);\n+\n+        // timestamp_upper_bound_force_push_down_enabled default as false.\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+\n+        // timestamp_upper_bound_force_push_down_enabled set as true.\n+        Session enableTmUpperBoundPushDownSession = Session.builder(getSession())\n+                .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n+                .build();\n+\n+        queryResult = queryRunner.executeWithQueryId(enableTmUpperBoundPushDownSession, sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 4);\n+    }\n+\n+    @Test\n+    public void testTimestampLogAppendModePushDown()\n+    {\n+        Pair<Long, Long> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= from_unixtime(%s)\" insure including index 2, \"< from_unixtime(%s)\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= from_unixtime(%s) and _timestamp < from_unixtime(%s)\",\n+                topicNameLogAppend, timePair.first() / 1000, timePair.second() / 1000);\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 4);\n+    }\n+\n+    private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n+    }\n+\n+    private Pair<Long, Long> createTimestampTestMessages(String topicName)\n+    {\n+        long startTime = -1;\n+        long endTime = -1;\n+        try (KafkaProducer<Long, Object> producer = testingKafka.createProducer()) {\n+            for (long j = 0; j < MESSAGE_NUM; j++) {\n+                Future<RecordMetadata> send = producer.send(new ProducerRecord<>(topicName, j, j));\n+                try {\n+                    if (j < TIMESTAMP_TEST_COUNT) {\n+                        if (j == TIMESTAMP_TEST_START_INDEX) {\n+                            RecordMetadata r = send.get();\n+                            startTime = r.timestamp();\n+                        }\n+                        else if (j == TIMESTAMP_TEST_END_INDEX) {\n+                            RecordMetadata r = send.get();\n+                            endTime = r.timestamp();\n+                        }\n+                        else {\n+                            send.get();\n+                        }\n+                        Thread.sleep(1100);\n+                    }\n+                }\n+                catch (Exception e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEyMTYwMA==", "bodyText": "I totally do not understand what is happening. Here.\nAlso why do we need such long sleeps here. How much time can this loop take.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495121600", "createdAt": "2020-09-25T17:07:41Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.Session;\n+import io.prestosql.execution.QueryInfo;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.ResultWithQueryId;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+import org.testng.internal.collections.Pair;\n+\n+import java.util.UUID;\n+import java.util.concurrent.Future;\n+\n+import static io.prestosql.plugin.kafka.util.TestUtils.createEmptyTopicDescription;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestKafkaIntegrationPushDown\n+        extends AbstractTestQueryFramework\n+{\n+    private static final int MESSAGE_NUM = 1000;\n+    private static final int TIMESTAMP_TEST_COUNT = 5;\n+    private static final int TIMESTAMP_TEST_START_INDEX = 2;\n+    private static final int TIMESTAMP_TEST_END_INDEX = 4;\n+\n+    private TestingKafka testingKafka;\n+    private String topicNamePartition;\n+    private String topicNameOffset;\n+    private String topicNameCreateTime;\n+    private String topicNameLogAppend;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        testingKafka = new TestingKafka();\n+        topicNamePartition = \"test_push_down_partition_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameOffset = \"test_push_down_offset_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameCreateTime = \"test_push_down_create_time_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameLogAppend = \"test_push_down_log_append_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+\n+        QueryRunner queryRunner = KafkaQueryRunner.builder(testingKafka)\n+                .setExtraTopicDescription(ImmutableMap.<SchemaTableName, KafkaTopicDescription>builder()\n+                        .put(createEmptyTopicDescription(topicNamePartition, new SchemaTableName(\"default\", topicNamePartition)))\n+                        .put(createEmptyTopicDescription(topicNameOffset, new SchemaTableName(\"default\", topicNameOffset)))\n+                        .put(createEmptyTopicDescription(topicNameCreateTime, new SchemaTableName(\"default\", topicNameCreateTime)))\n+                        .put(createEmptyTopicDescription(topicNameLogAppend, new SchemaTableName(\"default\", topicNameLogAppend)))\n+                        .build())\n+                .setExtraKafkaProperties(ImmutableMap.<String, String>builder()\n+                        .put(\"kafka.messages-per-split\", \"100\")\n+                        .build())\n+                .build();\n+        testingKafka.createTopicWithConfig(2, 1, topicNamePartition, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameOffset, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameCreateTime, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameLogAppend, true);\n+        return queryRunner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void stopKafka()\n+    {\n+        if (testingKafka != null) {\n+            testingKafka.close();\n+            testingKafka = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionPushDown()\n+    {\n+        createMessages(topicNamePartition);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_id=1\",\n+                topicNamePartition);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), MESSAGE_NUM / 2);\n+    }\n+\n+    @Test\n+    public void testOffsetPushDown()\n+    {\n+        createMessages(topicNameOffset);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+                topicNameOffset);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+    }\n+\n+    @Test\n+    public void testTimestampCreateTimeModePushDown()\n+    {\n+        Pair<Long, Long> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= from_unixtime(%s)\" insure including index 2, \"< from_unixtime(%s)\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= from_unixtime(%s) and _timestamp < from_unixtime(%s)\",\n+                topicNameCreateTime, timePair.first() / 1000, timePair.second() / 1000);\n+\n+        // timestamp_upper_bound_force_push_down_enabled default as false.\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+\n+        // timestamp_upper_bound_force_push_down_enabled set as true.\n+        Session enableTmUpperBoundPushDownSession = Session.builder(getSession())\n+                .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n+                .build();\n+\n+        queryResult = queryRunner.executeWithQueryId(enableTmUpperBoundPushDownSession, sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 4);\n+    }\n+\n+    @Test\n+    public void testTimestampLogAppendModePushDown()\n+    {\n+        Pair<Long, Long> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= from_unixtime(%s)\" insure including index 2, \"< from_unixtime(%s)\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= from_unixtime(%s) and _timestamp < from_unixtime(%s)\",\n+                topicNameLogAppend, timePair.first() / 1000, timePair.second() / 1000);\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 4);\n+    }\n+\n+    private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n+    }\n+\n+    private Pair<Long, Long> createTimestampTestMessages(String topicName)\n+    {\n+        long startTime = -1;\n+        long endTime = -1;\n+        try (KafkaProducer<Long, Object> producer = testingKafka.createProducer()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEyNDI1OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private void createMessages(String topicName)\n          \n          \n            \n                {\n          \n          \n            \n                    try (KafkaProducer<Long, Object> producer = testingKafka.createProducer()) {\n          \n          \n            \n                        for (long j = 0; j < MESSAGE_NUM; j++) {\n          \n          \n            \n                            Future<RecordMetadata> send = producer.send(new ProducerRecord<>(topicName, j, j));\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n          \n            \n                }\n          \n          \n            \n                private void createMessages(String topicName)\n          \n          \n            \n                        throws ExecutionException, InterruptedException\n          \n          \n            \n                {\n          \n          \n            \n                    Future<RecordMetadata> lastSendFuture = Futures.immediateFuture(null);\n          \n          \n            \n                    try (KafkaProducer<Long, Object> producer = testingKafka.createProducer()) {\n          \n          \n            \n                        for (long messageNum = 0; messageNum < MESSAGE_NUM; messageNum++) {\n          \n          \n            \n                            long key = messageNum;\n          \n          \n            \n                            long value = messageNum;\n          \n          \n            \n                            lastSendFuture = producer.send(new ProducerRecord<>(topicName, key, value));\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n          \n            \n                    lastSendFuture.get();\n          \n          \n            \n                }\n          \n      \n    \n    \n  \n\nA little more code makes it more readable IMO", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r495124259", "createdAt": "2020-09-25T17:13:09Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.Session;\n+import io.prestosql.execution.QueryInfo;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.ResultWithQueryId;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+import org.testng.internal.collections.Pair;\n+\n+import java.util.UUID;\n+import java.util.concurrent.Future;\n+\n+import static io.prestosql.plugin.kafka.util.TestUtils.createEmptyTopicDescription;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestKafkaIntegrationPushDown\n+        extends AbstractTestQueryFramework\n+{\n+    private static final int MESSAGE_NUM = 1000;\n+    private static final int TIMESTAMP_TEST_COUNT = 5;\n+    private static final int TIMESTAMP_TEST_START_INDEX = 2;\n+    private static final int TIMESTAMP_TEST_END_INDEX = 4;\n+\n+    private TestingKafka testingKafka;\n+    private String topicNamePartition;\n+    private String topicNameOffset;\n+    private String topicNameCreateTime;\n+    private String topicNameLogAppend;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        testingKafka = new TestingKafka();\n+        topicNamePartition = \"test_push_down_partition_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameOffset = \"test_push_down_offset_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameCreateTime = \"test_push_down_create_time_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameLogAppend = \"test_push_down_log_append_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+\n+        QueryRunner queryRunner = KafkaQueryRunner.builder(testingKafka)\n+                .setExtraTopicDescription(ImmutableMap.<SchemaTableName, KafkaTopicDescription>builder()\n+                        .put(createEmptyTopicDescription(topicNamePartition, new SchemaTableName(\"default\", topicNamePartition)))\n+                        .put(createEmptyTopicDescription(topicNameOffset, new SchemaTableName(\"default\", topicNameOffset)))\n+                        .put(createEmptyTopicDescription(topicNameCreateTime, new SchemaTableName(\"default\", topicNameCreateTime)))\n+                        .put(createEmptyTopicDescription(topicNameLogAppend, new SchemaTableName(\"default\", topicNameLogAppend)))\n+                        .build())\n+                .setExtraKafkaProperties(ImmutableMap.<String, String>builder()\n+                        .put(\"kafka.messages-per-split\", \"100\")\n+                        .build())\n+                .build();\n+        testingKafka.createTopicWithConfig(2, 1, topicNamePartition, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameOffset, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameCreateTime, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameLogAppend, true);\n+        return queryRunner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void stopKafka()\n+    {\n+        if (testingKafka != null) {\n+            testingKafka.close();\n+            testingKafka = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionPushDown()\n+    {\n+        createMessages(topicNamePartition);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_id=1\",\n+                topicNamePartition);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), MESSAGE_NUM / 2);\n+    }\n+\n+    @Test\n+    public void testOffsetPushDown()\n+    {\n+        createMessages(topicNameOffset);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+                topicNameOffset);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+    }\n+\n+    @Test\n+    public void testTimestampCreateTimeModePushDown()\n+    {\n+        Pair<Long, Long> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= from_unixtime(%s)\" insure including index 2, \"< from_unixtime(%s)\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= from_unixtime(%s) and _timestamp < from_unixtime(%s)\",\n+                topicNameCreateTime, timePair.first() / 1000, timePair.second() / 1000);\n+\n+        // timestamp_upper_bound_force_push_down_enabled default as false.\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+\n+        // timestamp_upper_bound_force_push_down_enabled set as true.\n+        Session enableTmUpperBoundPushDownSession = Session.builder(getSession())\n+                .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n+                .build();\n+\n+        queryResult = queryRunner.executeWithQueryId(enableTmUpperBoundPushDownSession, sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 4);\n+    }\n+\n+    @Test\n+    public void testTimestampLogAppendModePushDown()\n+    {\n+        Pair<Long, Long> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= from_unixtime(%s)\" insure including index 2, \"< from_unixtime(%s)\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= from_unixtime(%s) and _timestamp < from_unixtime(%s)\",\n+                topicNameLogAppend, timePair.first() / 1000, timePair.second() / 1000);\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 4);\n+    }\n+\n+    private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n+    }\n+\n+    private Pair<Long, Long> createTimestampTestMessages(String topicName)\n+    {\n+        long startTime = -1;\n+        long endTime = -1;\n+        try (KafkaProducer<Long, Object> producer = testingKafka.createProducer()) {\n+            for (long j = 0; j < MESSAGE_NUM; j++) {\n+                Future<RecordMetadata> send = producer.send(new ProducerRecord<>(topicName, j, j));\n+                try {\n+                    if (j < TIMESTAMP_TEST_COUNT) {\n+                        if (j == TIMESTAMP_TEST_START_INDEX) {\n+                            RecordMetadata r = send.get();\n+                            startTime = r.timestamp();\n+                        }\n+                        else if (j == TIMESTAMP_TEST_END_INDEX) {\n+                            RecordMetadata r = send.get();\n+                            endTime = r.timestamp();\n+                        }\n+                        else {\n+                            send.get();\n+                        }\n+                        Thread.sleep(1100);\n+                    }\n+                }\n+                catch (Exception e) {\n+                    e.printStackTrace();\n+                }\n+            }\n+        }\n+        return Pair.of(startTime, endTime);\n+    }\n+\n+    private void createMessages(String topicName)\n+    {\n+        try (KafkaProducer<Long, Object> producer = testingKafka.createProducer()) {\n+            for (long j = 0; j < MESSAGE_NUM; j++) {\n+                Future<RecordMetadata> send = producer.send(new ProducerRecord<>(topicName, j, j));\n+            }\n+        }\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 204}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ee66e841ce0e5710ca2aa3e36c9ba9065f600be", "author": {"user": {"login": "wangli-td", "name": null}}, "url": "https://github.com/trinodb/trino/commit/1ee66e841ce0e5710ca2aa3e36c9ba9065f600be", "committedDate": "2020-09-28T11:23:40Z", "message": "Add internal Kafka column for OFFSET_TIMESTAMP_FIELD\n\nSigned-off-by: Li Wang <wangli@thinkingdata.cn>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4MjEzNjE3", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-498213617", "createdAt": "2020-09-29T08:09:13Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODowOToxM1rOHZgTTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwODo0NzoxMlrOHZiwlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUwNTY3Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ``kafka.timestamp-upper-bound-force-push-down-enabled``    Controls whether push down For topics using ``CreateTime`` mode\n          \n          \n            \n            ``kafka.timestamp-upper-bound-force-push-down-enabled``    Controls if upper bound timestamp push down is enabled for topics using ``CreateTime`` mode", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r496505676", "createdAt": "2020-09-29T08:09:13Z", "author": {"login": "losipiuk"}, "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -56,17 +56,18 @@ Configuration Properties\n \n The following configuration properties are available:\n \n-=============================== ==============================================================\n-Property Name                   Description\n-=============================== ==============================================================\n-``kafka.table-names``           List of all tables provided by the catalog\n-``kafka.default-schema``        Default schema name for tables\n-``kafka.nodes``                 List of nodes in the Kafka cluster\n-``kafka.buffer-size``           Kafka read buffer size\n-``kafka.table-description-dir`` Directory containing topic description files\n-``kafka.hide-internal-columns`` Controls whether internal columns are part of the table schema or not\n-``kafka.messages-per-split``    Number of messages that are processed by each Presto split, defaults to 100000\n-=============================== ==============================================================\n+========================================================== ==============================================================================\n+Property Name                                              Description\n+========================================================== ==============================================================================\n+``kafka.table-names``                                      List of all tables provided by the catalog\n+``kafka.default-schema``                                   Default schema name for tables\n+``kafka.nodes``                                            List of nodes in the Kafka cluster\n+``kafka.buffer-size``                                      Kafka read buffer size\n+``kafka.table-description-dir``                            Directory containing topic description files\n+``kafka.hide-internal-columns``                            Controls whether internal columns are part of the table schema or not\n+``kafka.messages-per-split``                               Number of messages that are processed by each Presto split, defaults to 100000\n+``kafka.timestamp-upper-bound-force-push-down-enabled``    Controls whether push down For topics using ``CreateTime`` mode", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUwNzAyOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ``kafka.timestamp-upper-bound-force-push-down-enabled``\n          \n          \n            \n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n          \n          \n            \n            \n          \n          \n            \n            The upper bound predicate is pushed down only for topics using ``LogAppendTime``\n          \n          \n            \n            mode.\n          \n          \n            \n            \n          \n          \n            \n            For topics using ``CreateTime`` mode, upper bound push down must be explicitly\n          \n          \n            \n            allowed via ``kafka.timestamp-upper-bound-force-push-down-enabled`` config property\n          \n          \n            \n            or ``timestamp_upper_bound_force_push_down_enabled`` session property.\n          \n          \n            \n            \n          \n          \n            \n            This property is optional; the default is ``false``.\n          \n          \n            \n            ``kafka.timestamp-upper-bound-force-push-down-enabled``\n          \n          \n            \n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n          \n          \n            \n            \n          \n          \n            \n            The upper bound predicate on ``_timestamp`` column \n          \n          \n            \n            is pushed down only for topics using ``LogAppendTime`` mode.\n          \n          \n            \n            \n          \n          \n            \n            For topics using ``CreateTime`` mode, upper bound push down must be explicitly\n          \n          \n            \n            allowed via ``kafka.timestamp-upper-bound-force-push-down-enabled`` config property\n          \n          \n            \n            or ``timestamp_upper_bound_force_push_down_enabled`` session property.\n          \n          \n            \n            \n          \n          \n            \n            This property is optional; the default is ``false``.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r496507028", "createdAt": "2020-09-29T08:10:31Z", "author": {"login": "losipiuk"}, "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -121,6 +122,18 @@ files (must end with ``.json``) which contain table description files.\n \n This property is optional; the default is ``etc/kafka``.\n \n+``kafka.timestamp-upper-bound-force-push-down-enabled``\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+The upper bound predicate is pushed down only for topics using ``LogAppendTime``\n+mode.\n+\n+For topics using ``CreateTime`` mode, upper bound push down must be explicitly\n+allowed via ``kafka.timestamp-upper-bound-force-push-down-enabled`` config property\n+or ``timestamp_upper_bound_force_push_down_enabled`` session property.\n+\n+This property is optional; the default is ``false``.\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUwODk2MA==", "bodyText": "rename computeOffset to overrideFunction for consistency.", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r496508960", "createdAt": "2020-09-29T08:12:16Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaFilterManager.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableMap;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Marker;\n+import io.prestosql.spi.predicate.Ranges;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import io.prestosql.spi.predicate.ValueSet;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigResource;\n+\n+import javax.inject.Inject;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.KafkaErrorCode.KAFKA_SPLIT_ERROR;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.OFFSET_TIMESTAMP_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_ID_FIELD;\n+import static io.prestosql.plugin.kafka.KafkaInternalFieldManager.PARTITION_OFFSET_FIELD;\n+import static io.prestosql.spi.type.Timestamps.MICROSECONDS_PER_MILLISECOND;\n+import static java.lang.Math.floorDiv;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaFilterManager\n+{\n+    private static final long INVALID_KAFKA_RANGE_INDEX = -1;\n+    private static final String TOPIC_CONFIG_TIMESTAMP_KEY = \"message.timestamp.type\";\n+    private static final String TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME = \"LogAppendTime\";\n+\n+    private final KafkaConsumerFactory consumerFactory;\n+    private final KafkaAdminFactory adminFactory;\n+\n+    @Inject\n+    public KafkaFilterManager(KafkaConsumerFactory consumerFactory, KafkaAdminFactory adminFactory)\n+    {\n+        this.consumerFactory = requireNonNull(consumerFactory, \"consumerManager is null\");\n+        this.adminFactory = requireNonNull(adminFactory, \"adminFactory is null\");\n+    }\n+\n+    public KafkaFilteringResult getKafkaFilterResult(\n+            ConnectorSession session,\n+            KafkaTableHandle kafkaTableHandle,\n+            List<PartitionInfo> partitionInfos,\n+            Map<TopicPartition, Long> partitionBeginOffsets,\n+            Map<TopicPartition, Long> partitionEndOffsets)\n+    {\n+        requireNonNull(session, \"session is null\");\n+        requireNonNull(kafkaTableHandle, \"kafkaTableHandle is null\");\n+        requireNonNull(partitionInfos, \"partitionInfos is null\");\n+        requireNonNull(partitionBeginOffsets, \"partitionBeginOffsets is null\");\n+        requireNonNull(partitionEndOffsets, \"partitionEndOffsets is null\");\n+\n+        TupleDomain<ColumnHandle> constraint = kafkaTableHandle.getConstraint();\n+        verify(!constraint.isNone(), \"constraint is none\");\n+\n+        if (!constraint.isAll()) {\n+            Set<Long> partitionIds = partitionInfos.stream().map(partitionInfo -> (long) partitionInfo.partition()).collect(toImmutableSet());\n+            Optional<Range> offsetRanged = Optional.empty();\n+            Optional<Range> offsetTimestampRanged = Optional.empty();\n+            Set<Long> partitionIdsFiltered = partitionIds;\n+            Optional<Map<ColumnHandle, Domain>> domains = constraint.getDomains();\n+\n+            for (Map.Entry<ColumnHandle, Domain> entry : domains.get().entrySet()) {\n+                KafkaColumnHandle columnHandle = (KafkaColumnHandle) entry.getKey();\n+                if (!columnHandle.isInternal()) {\n+                    continue;\n+                }\n+                switch (columnHandle.getName()) {\n+                    case PARTITION_OFFSET_FIELD:\n+                        offsetRanged = filterRangeByDomain(entry.getValue());\n+                        break;\n+                    case PARTITION_ID_FIELD:\n+                        partitionIdsFiltered = filterValuesByDomain(entry.getValue(), partitionIds);\n+                        break;\n+                    case OFFSET_TIMESTAMP_FIELD:\n+                        offsetTimestampRanged = filterRangeByDomain(entry.getValue());\n+                        break;\n+                    default:\n+                        break;\n+                }\n+            }\n+\n+            // push down offset\n+            if (offsetRanged.isPresent()) {\n+                Range range = offsetRanged.get();\n+                partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                        partition -> (range.getBegin() != INVALID_KAFKA_RANGE_INDEX) ? Optional.of(range.getBegin()) : Optional.empty());\n+                partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                        partition -> (range.getEnd() != INVALID_KAFKA_RANGE_INDEX) ? Optional.of(range.getEnd()) : Optional.empty());\n+            }\n+\n+            // push down timestamp if possible\n+            if (offsetTimestampRanged.isPresent()) {\n+                try (KafkaConsumer<byte[], byte[]> kafkaConsumer = consumerFactory.create()) {\n+                    Optional<Range> finalOffsetTimestampRanged = offsetTimestampRanged;\n+                    partitionBeginOffsets = overridePartitionBeginOffsets(partitionBeginOffsets,\n+                            partition -> findOffsetsForTimestampGreaterOrEqual(kafkaConsumer, partition, finalOffsetTimestampRanged.get().getBegin()));\n+                    if (isTimestampUpperBoundPushdownEnabled(session, kafkaTableHandle.getTopicName())) {\n+                        partitionEndOffsets = overridePartitionEndOffsets(partitionEndOffsets,\n+                                partition -> findOffsetsForTimestampGreaterOrEqual(kafkaConsumer, partition, finalOffsetTimestampRanged.get().getEnd()));\n+                    }\n+                }\n+            }\n+\n+            // push down partitions\n+            final Set<Long> finalPartitionIdsFiltered = partitionIdsFiltered;\n+            List<PartitionInfo> partitionFilteredInfos = partitionInfos.stream()\n+                    .filter(partitionInfo -> finalPartitionIdsFiltered.contains((long) partitionInfo.partition()))\n+                    .collect(toImmutableList());\n+            return new KafkaFilteringResult(partitionFilteredInfos, partitionBeginOffsets, partitionEndOffsets);\n+        }\n+        return new KafkaFilteringResult(partitionInfos, partitionBeginOffsets, partitionEndOffsets);\n+    }\n+\n+    private boolean isTimestampUpperBoundPushdownEnabled(ConnectorSession session, String topic)\n+    {\n+        try (AdminClient adminClient = adminFactory.create()) {\n+            ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);\n+\n+            DescribeConfigsResult describeResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+            Map<ConfigResource, Config> configMap = describeResult.all().get();\n+\n+            if (configMap != null) {\n+                Config config = configMap.get(topicResource);\n+                String timestampType = config.get(TOPIC_CONFIG_TIMESTAMP_KEY).value();\n+                if (TOPIC_CONFIG_TIMESTAMP_VALUE_LOG_APPEND_TIME.equals(timestampType)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(KAFKA_SPLIT_ERROR, format(\"Failed to get configuration for topic '%s'\", topic), e);\n+        }\n+        return KafkaSessionProperties.isTimestampUpperBoundPushdownEnabled(session);\n+    }\n+\n+    private static Optional<Long> findOffsetsForTimestampGreaterOrEqual(KafkaConsumer<byte[], byte[]> kafkaConsumer, TopicPartition topicPartition, long timestamp)\n+    {\n+        final long transferTimestamp = floorDiv(timestamp, MICROSECONDS_PER_MILLISECOND);\n+        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsets = kafkaConsumer.offsetsForTimes(ImmutableMap.of(topicPartition, transferTimestamp));\n+        return Optional.ofNullable(getOnlyElement(topicPartitionOffsets.values(), null)).map(OffsetAndTimestamp::offset);\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionBeginOffsets(Map<TopicPartition, Long> partitionBeginOffsets,\n+                                                                           Function<TopicPartition, Optional<Long>> overrideFunction)\n+    {\n+        ImmutableMap.Builder<TopicPartition, Long> partitionFilteredBeginOffsetsBuilder = ImmutableMap.builder();\n+        partitionBeginOffsets.forEach((partition, partitionIndex) -> {\n+            Optional<Long> newOffset = overrideFunction.apply(partition);\n+            partitionFilteredBeginOffsetsBuilder.put(partition, newOffset.map(index -> Long.max(partitionIndex, index)).orElse(partitionIndex));\n+        });\n+        return partitionFilteredBeginOffsetsBuilder.build();\n+    }\n+\n+    private static Map<TopicPartition, Long> overridePartitionEndOffsets(Map<TopicPartition, Long> partitionEndOffsets,\n+                                                                         Function<TopicPartition, Optional<Long>> computeOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 190}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUzNzc1Mg==", "bodyText": "Can you make createTopic call out to createTopicWithConfig?", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r496537752", "createdAt": "2020-09-29T08:38:38Z", "author": {"login": "losipiuk"}, "path": "presto-testing-kafka/src/main/java/io/prestosql/testing/kafka/TestingKafka.java", "diffHunk": "@@ -72,6 +72,32 @@ private void createTopic(int partitions, int replication, String topic)\n         }\n     }\n \n+    public void createTopicWithConfig(int partitions, int replication, String topic, boolean enableLogAppendTime)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjUzODU0Nw==", "bodyText": "Raneme to TestKafkaFilterManager", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r496538547", "createdAt": "2020-09-29T08:39:23Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaPushDownStatic.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.predicate.Domain;\n+import io.prestosql.spi.predicate.Range;\n+import io.prestosql.spi.predicate.SortedRangeSet;\n+import org.testng.annotations.Test;\n+\n+import java.util.Set;\n+\n+import static io.prestosql.spi.predicate.Domain.multipleValues;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertTrue;\n+\n+public class TestKafkaPushDownStatic", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU0NTk0MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                // Make timestamp more expected.\n          \n          \n            \n                                Thread.sleep(20);\n          \n          \n            \n                                // Sleep for a while to ensure different timestamps for different messages.\n          \n          \n            \n                                Thread.sleep(20);\n          \n      \n    \n    \n  \n\nActually you can add an assertion that the timestamps for different messages are different in the tests which use createTimestampTestMessages", "url": "https://github.com/trinodb/trino/pull/4805#discussion_r496545941", "createdAt": "2020-09-29T08:47:12Z", "author": {"login": "losipiuk"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationPushDown.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.util.concurrent.Futures;\n+import io.prestosql.Session;\n+import io.prestosql.execution.QueryInfo;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.ResultWithQueryId;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+import org.testng.internal.collections.Pair;\n+\n+import java.time.Instant;\n+import java.time.LocalDateTime;\n+import java.time.ZoneId;\n+import java.time.format.DateTimeFormatter;\n+import java.util.UUID;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+\n+import static io.prestosql.plugin.kafka.util.TestUtils.createEmptyTopicDescription;\n+import static java.util.Objects.requireNonNull;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestKafkaIntegrationPushDown\n+        extends AbstractTestQueryFramework\n+{\n+    private static final int MESSAGE_NUM = 1000;\n+    private static final int TIMESTAMP_TEST_COUNT = 5;\n+    private static final int TIMESTAMP_TEST_START_INDEX = 2;\n+    private static final int TIMESTAMP_TEST_END_INDEX = 4;\n+\n+    private TestingKafka testingKafka;\n+    private String topicNamePartition;\n+    private String topicNameOffset;\n+    private String topicNameCreateTime;\n+    private String topicNameLogAppend;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        testingKafka = new TestingKafka();\n+        topicNamePartition = \"test_push_down_partition_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameOffset = \"test_push_down_offset_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameCreateTime = \"test_push_down_create_time_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        topicNameLogAppend = \"test_push_down_log_append_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+\n+        QueryRunner queryRunner = KafkaQueryRunner.builder(testingKafka)\n+                .setExtraTopicDescription(ImmutableMap.<SchemaTableName, KafkaTopicDescription>builder()\n+                        .put(createEmptyTopicDescription(topicNamePartition, new SchemaTableName(\"default\", topicNamePartition)))\n+                        .put(createEmptyTopicDescription(topicNameOffset, new SchemaTableName(\"default\", topicNameOffset)))\n+                        .put(createEmptyTopicDescription(topicNameCreateTime, new SchemaTableName(\"default\", topicNameCreateTime)))\n+                        .put(createEmptyTopicDescription(topicNameLogAppend, new SchemaTableName(\"default\", topicNameLogAppend)))\n+                        .build())\n+                .setExtraKafkaProperties(ImmutableMap.<String, String>builder()\n+                        .put(\"kafka.messages-per-split\", \"100\")\n+                        .build())\n+                .build();\n+        testingKafka.createTopicWithConfig(2, 1, topicNamePartition, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameOffset, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameCreateTime, false);\n+        testingKafka.createTopicWithConfig(2, 1, topicNameLogAppend, true);\n+        return queryRunner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void stopKafka()\n+    {\n+        if (testingKafka != null) {\n+            testingKafka.close();\n+            testingKafka = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionPushDown() throws ExecutionException, InterruptedException\n+    {\n+        createMessages(topicNamePartition);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_id=1\",\n+                topicNamePartition);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), MESSAGE_NUM / 2);\n+    }\n+\n+    @Test\n+    public void testOffsetPushDown() throws ExecutionException, InterruptedException\n+    {\n+        createMessages(topicNameOffset);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset between 2 and 10\",\n+                topicNameOffset);\n+\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 18);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset > 2 and _partition_offset < 10\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 14);\n+\n+        sql = String.format(\"SELECT count(*) FROM default.%s WHERE _partition_offset = 3\",\n+                topicNameOffset);\n+\n+        queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+    }\n+\n+    @Test\n+    public void testTimestampCreateTimeModePushDown() throws ExecutionException, InterruptedException\n+    {\n+        Pair<String, String> timePair = createTimestampTestMessages(topicNameCreateTime);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameCreateTime, timePair.first(), timePair.second());\n+\n+        // timestamp_upper_bound_force_push_down_enabled default as false.\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 998);\n+\n+        // timestamp_upper_bound_force_push_down_enabled set as true.\n+        Session sessionWithUpperBoundPushDownEnabled = Session.builder(getSession())\n+                .setSystemProperty(\"kafka.timestamp_upper_bound_force_push_down_enabled\", \"true\")\n+                .build();\n+\n+        queryResult = queryRunner.executeWithQueryId(sessionWithUpperBoundPushDownEnabled, sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+    }\n+\n+    @Test\n+    public void testTimestampLogAppendModePushDown() throws ExecutionException, InterruptedException\n+    {\n+        Pair<String, String> timePair = createTimestampTestMessages(topicNameLogAppend);\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) getQueryRunner();\n+        // \">= startTime\" insure including index 2, \"< endTime\"  insure excluding index 4;\n+        String sql = String.format(\"SELECT count(*) FROM default.%s WHERE _timestamp >= timestamp '%s' and _timestamp < timestamp '%s'\",\n+                topicNameLogAppend, timePair.first(), timePair.second());\n+        ResultWithQueryId<MaterializedResult> queryResult = queryRunner.executeWithQueryId(getSession(), sql);\n+        assertEquals(getQueryInfo(queryRunner, queryResult).getQueryStats().getProcessedInputPositions(), 2);\n+    }\n+\n+    private QueryInfo getQueryInfo(DistributedQueryRunner queryRunner, ResultWithQueryId<MaterializedResult> queryResult)\n+    {\n+        return queryRunner.getCoordinator().getQueryManager().getFullQueryInfo(queryResult.getQueryId());\n+    }\n+\n+    private Pair<String, String> createTimestampTestMessages(String topicName) throws ExecutionException, InterruptedException\n+    {\n+        String startTime = null;\n+        String endTime = null;\n+        Future<RecordMetadata> lastSendFuture = Futures.immediateFuture(null);\n+        try (KafkaProducer<Long, Object> producer = testingKafka.createProducer()) {\n+            for (long messageNum = 0; messageNum < MESSAGE_NUM; messageNum++) {\n+                long key = messageNum;\n+                long value = messageNum;\n+                lastSendFuture = producer.send(new ProducerRecord<>(topicName, key, value));\n+                // Record timestamp to build expected timestamp\n+                if (messageNum < TIMESTAMP_TEST_COUNT) {\n+                    if (messageNum == TIMESTAMP_TEST_START_INDEX) {\n+                        RecordMetadata r = lastSendFuture.get();\n+                        startTime = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\")\n+                                .format(LocalDateTime.ofInstant(Instant.ofEpochMilli(r.timestamp()), ZoneId.of(\"UTC\")));\n+                    }\n+                    else if (messageNum == TIMESTAMP_TEST_END_INDEX) {\n+                        RecordMetadata r = lastSendFuture.get();\n+                        endTime = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\")\n+                                .format(LocalDateTime.ofInstant(Instant.ofEpochMilli(r.timestamp()), ZoneId.of(\"UTC\")));\n+                    }\n+                    else {\n+                        lastSendFuture.get();\n+                    }\n+                    // Make timestamp more expected.\n+                    Thread.sleep(20);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 199}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "599c11fbeb41b83f4274fac21c90cbec3d3e251b", "author": {"user": {"login": "wangli-td", "name": null}}, "url": "https://github.com/trinodb/trino/commit/599c11fbeb41b83f4274fac21c90cbec3d3e251b", "committedDate": "2020-09-29T09:43:00Z", "message": "Add predicate push down support for internal Kafka columns.\n\nAdd support for predicate pushdown for following Kafka internal columns\n * _timestamp\n * _partition_offset\n * _partition_id\n\n If predicate specifies lower bound on _timestamp column (_timestamp >\n    XXXX), it is always pushed down.\n The upper bound predicate is pushed down only for topics using ``LogAppendTime`` mode.\n For topics using ``CreateTime`` mode, upper bound pushdown must be explicitly\n    allowed via ``kafka.timestamp-upper-bound-force-push-down-enabled`` config property\n    or ``timestamp_upper_bound_force_push_down_enabled`` session property.\n\nSigned-off-by: Li Wang <wangli@thinkingdata.cn>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "71ccf492010fb20475ba92a217e5d5ea0af67961", "author": {"user": {"login": "wangli-td", "name": null}}, "url": "https://github.com/trinodb/trino/commit/71ccf492010fb20475ba92a217e5d5ea0af67961", "committedDate": "2020-09-29T09:43:01Z", "message": "kafka connector: add filter pushing down integration test\n\nSigned-off-by: Li Wang <wangli@thinkingdata.cn>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "71ccf492010fb20475ba92a217e5d5ea0af67961", "author": {"user": {"login": "wangli-td", "name": null}}, "url": "https://github.com/trinodb/trino/commit/71ccf492010fb20475ba92a217e5d5ea0af67961", "committedDate": "2020-09-29T09:43:01Z", "message": "kafka connector: add filter pushing down integration test\n\nSigned-off-by: Li Wang <wangli@thinkingdata.cn>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4MzM4NTc2", "url": "https://github.com/trinodb/trino/pull/4805#pullrequestreview-498338576", "createdAt": "2020-09-29T10:43:04Z", "commit": {"oid": "71ccf492010fb20475ba92a217e5d5ea0af67961"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4358, "cost": 1, "resetAt": "2021-10-28T20:13:43Z"}}}