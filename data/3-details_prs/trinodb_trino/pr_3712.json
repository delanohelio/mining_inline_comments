{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE3MDkwNDE1", "number": 3712, "title": "S3 streaming upload with multipart upload api", "bodyText": "Implements S3 streaming upload with multipart upload api.\nThe original implementation upload file after the all query result is fetched (which was stored in a file), it might be a bottleneck if the query result is slow or very large.\nIt's a draft, please kindly let me know if you have any suggestion, thanks!", "createdAt": "2020-05-13T03:23:13Z", "url": "https://github.com/trinodb/trino/pull/3712", "merged": true, "mergeCommit": {"oid": "f681708aab96c85a6ab20abfdcca4d7ff04a7744"}, "closed": true, "closedAt": "2020-12-04T00:26:48Z", "author": {"login": "chhsiao90"}, "timelineItems": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcuisK6gFqTQzNjg0NzUyOQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdiaSZOgH2gAyNDE3MDkwNDE1OmQyYmFlZDc4ODY0N2MwYmU0ZWFmOGI1MzVhNzdjMzJmMTk5NDYyY2I=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2ODQ3NTI5", "url": "https://github.com/trinodb/trino/pull/3712#pullrequestreview-436847529", "createdAt": "2020-06-24T17:13:12Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxNzozNToxNlrOGocSmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMzoyODo0MVrOGomcjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA1OTczNw==", "bodyText": "Let's add a new config hive.s3.streaming.part-size for this instead of multiPartUploadMinFileSize", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445059737", "createdAt": "2020-06-24T17:35:16Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -404,12 +427,22 @@ public FSDataOutputStream create(Path path, FsPermission permission, boolean ove\n         if (!stagingDirectory.isDirectory()) {\n             throw new IOException(\"Configured staging path is not a directory: \" + stagingDirectory);\n         }\n-        File tempFile = createTempFile(stagingDirectory.toPath(), \"presto-s3-\", \".tmp\").toFile();\n \n+        String bucketName = getBucketName(uri);\n         String key = keyFromPath(qualifiedPath(path));\n-        return new FSDataOutputStream(\n-                new PrestoS3OutputStream(s3, getBucketName(uri), key, tempFile, sseEnabled, sseType, sseKmsKeyId, multiPartUploadMinFileSize, multiPartUploadMinPartSize, s3AclType, requesterPaysEnabled, s3StorageClass),\n-                statistics);\n+\n+        if (streamingUploadEnabled) {\n+            String uploadId = initMultipartUpload(bucketName, key).getUploadId();\n+            return new FSDataOutputStream(\n+                    new PrestoS3StreamingOutputStream(s3, bucketName, key, uploadId, uploadExecutor, multiPartUploadMinPartSize),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4MDM5MQ==", "bodyText": "We can use the new bucketName variable here", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445080391", "createdAt": "2020-06-24T18:12:02Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -404,12 +427,22 @@ public FSDataOutputStream create(Path path, FsPermission permission, boolean ove\n         if (!stagingDirectory.isDirectory()) {\n             throw new IOException(\"Configured staging path is not a directory: \" + stagingDirectory);\n         }\n-        File tempFile = createTempFile(stagingDirectory.toPath(), \"presto-s3-\", \".tmp\").toFile();\n \n+        String bucketName = getBucketName(uri);\n         String key = keyFromPath(qualifiedPath(path));\n-        return new FSDataOutputStream(\n-                new PrestoS3OutputStream(s3, getBucketName(uri), key, tempFile, sseEnabled, sseType, sseKmsKeyId, multiPartUploadMinFileSize, multiPartUploadMinPartSize, s3AclType, requesterPaysEnabled, s3StorageClass),\n-                statistics);\n+\n+        if (streamingUploadEnabled) {\n+            String uploadId = initMultipartUpload(bucketName, key).getUploadId();\n+            return new FSDataOutputStream(\n+                    new PrestoS3StreamingOutputStream(s3, bucketName, key, uploadId, uploadExecutor, multiPartUploadMinPartSize),\n+                    statistics);\n+        }\n+        else {\n+            File tempFile = createTempFile(stagingDirectory.toPath(), \"presto-s3-\", \".tmp\").toFile();\n+            return new FSDataOutputStream(\n+                    new PrestoS3StagingOutputStream(s3, getBucketName(uri), key, tempFile, sseEnabled, sseType, sseKmsKeyId, multiPartUploadMinFileSize, multiPartUploadMinPartSize, s3AclType, requesterPaysEnabled, s3StorageClass),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4MTUxMg==", "bodyText": "It might make sense to structure this like\nOutputStream out;\nif (streamingUploadEnabled) {\n    out = new PrestoS3StreamingOutputStream(...);\n}\nelse {\n    out = new PrestoS3StagingOutputStream(...);\n}\n\nreturn new FSDataOutputStream(out, statistics);", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445081512", "createdAt": "2020-06-24T18:14:05Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -404,12 +427,22 @@ public FSDataOutputStream create(Path path, FsPermission permission, boolean ove\n         if (!stagingDirectory.isDirectory()) {\n             throw new IOException(\"Configured staging path is not a directory: \" + stagingDirectory);\n         }\n-        File tempFile = createTempFile(stagingDirectory.toPath(), \"presto-s3-\", \".tmp\").toFile();\n \n+        String bucketName = getBucketName(uri);\n         String key = keyFromPath(qualifiedPath(path));\n-        return new FSDataOutputStream(\n-                new PrestoS3OutputStream(s3, getBucketName(uri), key, tempFile, sseEnabled, sseType, sseKmsKeyId, multiPartUploadMinFileSize, multiPartUploadMinPartSize, s3AclType, requesterPaysEnabled, s3StorageClass),\n-                statistics);\n+\n+        if (streamingUploadEnabled) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4MzE5Nw==", "bodyText": "Add requireNonNull checks for all of these (not strictly required since this is a private class, but it's best practice and improves code readability)", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445083197", "createdAt": "2020-06-24T18:17:07Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4NDMyNA==", "bodyText": "No need to make these synchronized. OutputStream is not thread safe and thus is expected to be used from a single thread (or used with external synchronization).", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445084324", "createdAt": "2020-06-24T18:19:00Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 236}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NDQwMg==", "bodyText": "We should implement write(byte[], int, int) the variant for performance.", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445174402", "createdAt": "2020-06-24T21:11:48Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4NDMyNA=="}, "originalCommit": null, "originalPosition": 236}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NDkxMA==", "bodyText": "We avoid using increment in an expression. Make it a separate statement:\nbuf[count] = (byte) b;\ncount++;", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445174910", "createdAt": "2020-06-24T21:12:44Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 241}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NTM3OQ==", "bodyText": "Make this\ncurrentPartNumber++;\nThen we can use it below.", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445175379", "createdAt": "2020-06-24T21:13:46Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;\n+        }\n+\n+        @Override\n+        public synchronized void flush()\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            flushBuffer(true);\n+\n+            if (!uploadFutures.isEmpty()) {\n+                ListenableFuture<List<UploadPartResult>> partsFuture = allAsList(uploadFutures);\n+                ListenableFuture<?> finishFuture = transform(partsFuture, this::finishUpload, uploadExecutor);\n+\n+                try {\n+                    // Sync call to block presto from doing any other thing before the file is uploaded.\n+                    finishFuture.get();\n+                }\n+                catch (InterruptedException e) {\n+                    abortUpload(true);\n+                    Thread.currentThread().interrupt();\n+                    throw new InterruptedIOException();\n+                }\n+                catch (ExecutionException e) {\n+                    abortUpload(true);\n+                    throw new IOException(e.getCause());\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = Arrays.copyOf(buf, count);\n+                ListenableFuture<UploadPartResult> future = uploadExecutor.submit(\n+                        () -> uploadPage(data));\n+                uploadFutures.add(future);\n+                count = 0;\n+            }\n+        }\n+\n+        private UploadPartResult uploadPage(byte[] data)\n+        {\n+            int partNumber = ++currentPartNumber;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 297}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NjIxMg==", "bodyText": "Let's format this like\nList<PartETag> etags =  result.stream()\n        .map(UploadPartResult::getPartETag)\n        .collect(toList());\n\nCompleteMultipartUploadResult result = s3.completeMultipartUpload(\n        new CompleteMultipartUploadRequest(bucketName, key, uploadId, etags));", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445176212", "createdAt": "2020-06-24T21:15:40Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;\n+        }\n+\n+        @Override\n+        public synchronized void flush()\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            flushBuffer(true);\n+\n+            if (!uploadFutures.isEmpty()) {\n+                ListenableFuture<List<UploadPartResult>> partsFuture = allAsList(uploadFutures);\n+                ListenableFuture<?> finishFuture = transform(partsFuture, this::finishUpload, uploadExecutor);\n+\n+                try {\n+                    // Sync call to block presto from doing any other thing before the file is uploaded.\n+                    finishFuture.get();\n+                }\n+                catch (InterruptedException e) {\n+                    abortUpload(true);\n+                    Thread.currentThread().interrupt();\n+                    throw new InterruptedIOException();\n+                }\n+                catch (ExecutionException e) {\n+                    abortUpload(true);\n+                    throw new IOException(e.getCause());\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = Arrays.copyOf(buf, count);\n+                ListenableFuture<UploadPartResult> future = uploadExecutor.submit(\n+                        () -> uploadPage(data));\n+                uploadFutures.add(future);\n+                count = 0;\n+            }\n+        }\n+\n+        private UploadPartResult uploadPage(byte[] data)\n+        {\n+            int partNumber = ++currentPartNumber;\n+            UploadPartRequest uploadRequest = new UploadPartRequest()\n+                    .withBucketName(bucketName)\n+                    .withKey(key)\n+                    .withUploadId(uploadId)\n+                    .withPartNumber(partNumber)\n+                    .withInputStream(new ByteArrayInputStream(data))\n+                    .withPartSize(data.length);\n+\n+            return s3.uploadPart(uploadRequest);\n+        }\n+\n+        private CompleteMultipartUploadResult finishUpload(List<UploadPartResult> result)\n+        {\n+            CompleteMultipartUploadRequest request = new CompleteMultipartUploadRequest(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 311}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3ODk5OA==", "bodyText": "This will only a single upload for the entire server. We can use a cached pool since we will use exactly one thread per writer. We should also name the threads:\nprivate final ListeningExecutorService uploadExecutor = listeningDecorator(newCachedThreadPool(\"s3-upload-%s));", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445178998", "createdAt": "2020-06-24T21:21:51Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -202,8 +220,11 @@\n     private PrestoS3AclType s3AclType;\n     private boolean skipGlacierObjects;\n     private boolean requesterPaysEnabled;\n+    private boolean streamingUploadEnabled;\n     private PrestoS3StorageClass s3StorageClass;\n \n+    private final ListeningExecutorService uploadExecutor = listeningDecorator(newSingleThreadExecutor());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3OTUwMg==", "bodyText": "Change to a single future and wait for it if it exists (ensuring we only have one outstanding upload)", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445179502", "createdAt": "2020-06-24T21:22:58Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;\n+        }\n+\n+        @Override\n+        public synchronized void flush()\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            flushBuffer(true);\n+\n+            if (!uploadFutures.isEmpty()) {\n+                ListenableFuture<List<UploadPartResult>> partsFuture = allAsList(uploadFutures);\n+                ListenableFuture<?> finishFuture = transform(partsFuture, this::finishUpload, uploadExecutor);\n+\n+                try {\n+                    // Sync call to block presto from doing any other thing before the file is uploaded.\n+                    finishFuture.get();\n+                }\n+                catch (InterruptedException e) {\n+                    abortUpload(true);\n+                    Thread.currentThread().interrupt();\n+                    throw new InterruptedIOException();\n+                }\n+                catch (ExecutionException e) {\n+                    abortUpload(true);\n+                    throw new IOException(e.getCause());\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 283}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE4MDExOQ==", "bodyText": "No need to copy the buffer. Make it non-final and create a new array for the next batch.", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445180119", "createdAt": "2020-06-24T21:24:14Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;\n+        }\n+\n+        @Override\n+        public synchronized void flush()\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            flushBuffer(true);\n+\n+            if (!uploadFutures.isEmpty()) {\n+                ListenableFuture<List<UploadPartResult>> partsFuture = allAsList(uploadFutures);\n+                ListenableFuture<?> finishFuture = transform(partsFuture, this::finishUpload, uploadExecutor);\n+\n+                try {\n+                    // Sync call to block presto from doing any other thing before the file is uploaded.\n+                    finishFuture.get();\n+                }\n+                catch (InterruptedException e) {\n+                    abortUpload(true);\n+                    Thread.currentThread().interrupt();\n+                    throw new InterruptedIOException();\n+                }\n+                catch (ExecutionException e) {\n+                    abortUpload(true);\n+                    throw new IOException(e.getCause());\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = Arrays.copyOf(buf, count);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 287}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE4MDM0Mw==", "bodyText": "No need to wrap this or the logging", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445180343", "createdAt": "2020-06-24T21:24:43Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;\n+        }\n+\n+        @Override\n+        public synchronized void flush()\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            flushBuffer(true);\n+\n+            if (!uploadFutures.isEmpty()) {\n+                ListenableFuture<List<UploadPartResult>> partsFuture = allAsList(uploadFutures);\n+                ListenableFuture<?> finishFuture = transform(partsFuture, this::finishUpload, uploadExecutor);\n+\n+                try {\n+                    // Sync call to block presto from doing any other thing before the file is uploaded.\n+                    finishFuture.get();\n+                }\n+                catch (InterruptedException e) {\n+                    abortUpload(true);\n+                    Thread.currentThread().interrupt();\n+                    throw new InterruptedIOException();\n+                }\n+                catch (ExecutionException e) {\n+                    abortUpload(true);\n+                    throw new IOException(e.getCause());\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = Arrays.copyOf(buf, count);\n+                ListenableFuture<UploadPartResult> future = uploadExecutor.submit(\n+                        () -> uploadPage(data));\n+                uploadFutures.add(future);\n+                count = 0;\n+            }\n+        }\n+\n+        private UploadPartResult uploadPage(byte[] data)\n+        {\n+            int partNumber = ++currentPartNumber;\n+            UploadPartRequest uploadRequest = new UploadPartRequest()\n+                    .withBucketName(bucketName)\n+                    .withKey(key)\n+                    .withUploadId(uploadId)\n+                    .withPartNumber(partNumber)\n+                    .withInputStream(new ByteArrayInputStream(data))\n+                    .withPartSize(data.length);\n+\n+            return s3.uploadPart(uploadRequest);\n+        }\n+\n+        private CompleteMultipartUploadResult finishUpload(List<UploadPartResult> result)\n+        {\n+            CompleteMultipartUploadRequest request = new CompleteMultipartUploadRequest(\n+                    bucketName,\n+                    key,\n+                    uploadId,\n+                    result.stream().map(UploadPartResult::getPartETag).collect(toList()));\n+\n+            CompleteMultipartUploadResult response = s3.completeMultipartUpload(request);\n+\n+            STATS.uploadSuccessful();\n+\n+            return response;\n+        }\n+\n+        private void abortUpload(boolean silent)\n+                throws IOException\n+        {\n+            STATS.uploadFailed();\n+\n+            try {\n+                s3.abortMultipartUpload(new AbortMultipartUploadRequest(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 330}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTIyNDk3OA==", "bodyText": "We should add the MD5 so that S3 can verify the integrity of the upload\n.withMD5Digest(getMd5AsBase64(data))\nprivate static String getMd5AsBase64(byte[] data)\n{\n    @SuppressWarnings(\"deprecation\")\n    byte[] md5 = md5().hashBytes(data).asBytes();\n    return Base64.getEncoder().encodeToString(md5);\n}", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445224978", "createdAt": "2020-06-24T23:25:09Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;\n+        }\n+\n+        @Override\n+        public synchronized void flush()\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            flushBuffer(true);\n+\n+            if (!uploadFutures.isEmpty()) {\n+                ListenableFuture<List<UploadPartResult>> partsFuture = allAsList(uploadFutures);\n+                ListenableFuture<?> finishFuture = transform(partsFuture, this::finishUpload, uploadExecutor);\n+\n+                try {\n+                    // Sync call to block presto from doing any other thing before the file is uploaded.\n+                    finishFuture.get();\n+                }\n+                catch (InterruptedException e) {\n+                    abortUpload(true);\n+                    Thread.currentThread().interrupt();\n+                    throw new InterruptedIOException();\n+                }\n+                catch (ExecutionException e) {\n+                    abortUpload(true);\n+                    throw new IOException(e.getCause());\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = Arrays.copyOf(buf, count);\n+                ListenableFuture<UploadPartResult> future = uploadExecutor.submit(\n+                        () -> uploadPage(data));\n+                uploadFutures.add(future);\n+                count = 0;\n+            }\n+        }\n+\n+        private UploadPartResult uploadPage(byte[] data)\n+        {\n+            int partNumber = ++currentPartNumber;\n+            UploadPartRequest uploadRequest = new UploadPartRequest()\n+                    .withBucketName(bucketName)\n+                    .withKey(key)\n+                    .withUploadId(uploadId)\n+                    .withPartNumber(partNumber)\n+                    .withInputStream(new ByteArrayInputStream(data))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 303}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTIyNjEyNw==", "bodyText": "If this throws, we won't clean up the upload.", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445226127", "createdAt": "2020-06-24T23:28:41Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;\n+        }\n+\n+        @Override\n+        public synchronized void flush()\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            flushBuffer(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 254}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxNDMxNzQ2", "url": "https://github.com/trinodb/trino/pull/3712#pullrequestreview-541431746", "createdAt": "2020-12-01T01:54:51Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwMTo1NDo1MVrOH8VDBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwMjozODoyN1rOH8V4Hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMTQ0Ng==", "bodyText": "I'm thinking we should make this larger by default, say 16MB or 32MB, since this controls the maximum file size (part size * 10,000 max parts). Using 16MB would increase the max from 50GB to 160GB, while still being acceptable from a memory standpoint (writers can use substantial memory for buffers).", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533021446", "createdAt": "2020-12-01T01:54:51Z", "author": {"login": "electrum"}, "path": "presto-docs/src/main/sphinx/connector/hive-s3.rst", "diffHunk": "@@ -81,6 +81,11 @@ Property Name                                Description\n ``hive.s3.skip-glacier-objects``             Ignore Glacier objects rather than failing the query. This\n                                              skips data that may be expected to be part of the table\n                                              or partition. Defaults to ``false``.\n+\n+``hive.s3.streaming.enabled``                Use S3 multipart upload API to upload file in streaming way,\n+                                             without staging file to be created in the local file system.\n+\n+``hive.s3.streaming.part-size``              The part size for S3 streaming upload. Defaults to ``5MB``.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMzIwMg==", "bodyText": "Nit: move this method below, to be directly before PrestoS3InputStream. The methods in this section are related to credentials and creating the S3 client.", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533023202", "createdAt": "2020-12-01T02:00:22Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -835,6 +869,34 @@ private AmazonS3 createAmazonS3Client(Configuration hadoopConfig, ClientConfigur\n         return clientBuilder.build();\n     }\n \n+    private InitiateMultipartUploadResult initMultipartUpload(String bucket, String key)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNDczMA==", "bodyText": "Nit: we avoid abbreviations. Name this buffer", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533024730", "createdAt": "2020-12-01T02:05:09Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNDg2Mg==", "bodyText": "This initialization can move to the field declaration", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533024862", "createdAt": "2020-12-01T02:05:33Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNDk5Mg==", "bodyText": "Add requireNonNull for all of these\n(this is a private class, so not strictly necessary, but best to be consistent)", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533024992", "createdAt": "2020-12-01T02:06:01Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 238}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNTg0MA==", "bodyText": "Shorten this name to minPartSize and change to int", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533025840", "createdAt": "2020-12-01T02:08:56Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNjA1OA==", "bodyText": "You can remove this cast and inline this after changing the parameter to an int", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533026058", "createdAt": "2020-12-01T02:09:33Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNjM1Ng==", "bodyText": "Change streamingUploadPartSize to be an int and use Math.toIntExact()\nthis.streamingUploadPartSize = toIntExact(conf.getLong(S3_STREAMING_UPLOAD_PART_SIZE, defaults.getS3StreamingPartSize().toBytes()));", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533026356", "createdAt": "2020-12-01T02:10:29Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -254,6 +278,8 @@ public void initialize(URI uri, Configuration conf)\n         String userAgentPrefix = conf.get(S3_USER_AGENT_PREFIX, defaults.getS3UserAgentPrefix());\n         this.skipGlacierObjects = conf.getBoolean(S3_SKIP_GLACIER_OBJECTS, defaults.isSkipGlacierObjects());\n         this.requesterPaysEnabled = conf.getBoolean(S3_REQUESTER_PAYS_ENABLED, defaults.isRequesterPaysEnabled());\n+        this.streamingUploadEnabled = conf.getBoolean(S3_STREAMING_UPLOAD_ENABLED, defaults.isS3StreamingUploadEnabled());\n+        this.streamingUploadPartSize = conf.getLong(S3_STREAMING_UPLOAD_PART_SIZE, defaults.getS3StreamingPartSize().toBytes());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNjU3MA==", "bodyText": "Let's add a max size\n@MaxDataSize(\"256MB\")", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533026570", "createdAt": "2020-12-01T02:11:08Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/HiveS3Config.java", "diffHunk": "@@ -462,4 +464,31 @@ public HiveS3Config setRequesterPaysEnabled(boolean requesterPaysEnabled)\n         this.requesterPaysEnabled = requesterPaysEnabled;\n         return this;\n     }\n+\n+    public boolean isS3StreamingUploadEnabled()\n+    {\n+        return s3StreamingUploadEnabled;\n+    }\n+\n+    @Config(\"hive.s3.streaming.enabled\")\n+    public HiveS3Config setS3StreamingUploadEnabled(boolean s3StreamingUploadEnabled)\n+    {\n+        this.s3StreamingUploadEnabled = s3StreamingUploadEnabled;\n+        return this;\n+    }\n+\n+    @NotNull\n+    @MinDataSize(\"5MB\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNzE3MA==", "bodyText": "Let's call this bufferSize so that it's paired with buffer", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533027170", "createdAt": "2020-12-01T02:12:54Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNzkwNg==", "bodyText": "Let's name the parameters offset and length, then remove these local variables. It's confusing to have a copy of the input parameters (I had to check if the originals were also used below)", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533027906", "createdAt": "2020-12-01T02:15:14Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+\n+            buf[count] = (byte) b;\n+            count++;\n+        }\n+\n+        @Override\n+        public void write(byte[] bytes, int off, int len)\n+                throws IOException\n+        {\n+            int offset = off;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDI2Mg==", "bodyText": "This can unconditionally call flushBuffer(false) since that method checks if the buffer is full -- no need to check here", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533030262", "createdAt": "2020-12-01T02:23:01Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 251}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDk5NA==", "bodyText": "I think we can simplify this as follows:\nwhile (remain > 0) {\n    int copied = min(buf.length - count, remain);\n    ...\n}\nSince flushBuffer(false) will flush if the buffer is full, there's no need to avoid calling it when not full, or for the check/copy after the loop. Each loop iteration will copy the least of the available buffer space or the remaining bytes.", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533030994", "createdAt": "2020-12-01T02:25:25Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+\n+            buf[count] = (byte) b;\n+            count++;\n+        }\n+\n+        @Override\n+        public void write(byte[] bytes, int off, int len)\n+                throws IOException\n+        {\n+            int offset = off;\n+            int remain = len;\n+            while ((buf.length - count) <= remain) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 265}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTk2OQ==", "bodyText": "This can catch RuntimeException since finishUpload() doesn't throw checked exceptions", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533031969", "createdAt": "2020-12-01T02:28:30Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+\n+            buf[count] = (byte) b;\n+            count++;\n+        }\n+\n+        @Override\n+        public void write(byte[] bytes, int off, int len)\n+                throws IOException\n+        {\n+            int offset = off;\n+            int remain = len;\n+            while ((buf.length - count) <= remain) {\n+                int copied = buf.length - count;\n+                arraycopy(bytes, offset, buf, count, copied);\n+                count += copied;\n+\n+                flushBuffer(false);\n+\n+                offset += copied;\n+                remain -= copied;\n+            }\n+\n+            if (remain > 0) {\n+                arraycopy(bytes, offset, buf, count, remain);\n+                count += remain;\n+            }\n+        }\n+\n+        @Override\n+        public void flush()\n+                throws IOException\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            try {\n+                flushBuffer(true);\n+                waitForPreviousUploadFinish();\n+            }\n+            catch (IOException e) {\n+                abortUpload(true);\n+                throw e;\n+            }\n+\n+            if (!parts.isEmpty()) {\n+                try {\n+                    finishUpload(parts);\n+                }\n+                catch (Exception e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMjE2OQ==", "bodyText": "This probably needs to catch IOException | RuntimeException so that we always cleanup", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533032169", "createdAt": "2020-12-01T02:29:12Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+\n+            buf[count] = (byte) b;\n+            count++;\n+        }\n+\n+        @Override\n+        public void write(byte[] bytes, int off, int len)\n+                throws IOException\n+        {\n+            int offset = off;\n+            int remain = len;\n+            while ((buf.length - count) <= remain) {\n+                int copied = buf.length - count;\n+                arraycopy(bytes, offset, buf, count, copied);\n+                count += copied;\n+\n+                flushBuffer(false);\n+\n+                offset += copied;\n+                remain -= copied;\n+            }\n+\n+            if (remain > 0) {\n+                arraycopy(bytes, offset, buf, count, remain);\n+                count += remain;\n+            }\n+        }\n+\n+        @Override\n+        public void flush()\n+                throws IOException\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            try {\n+                flushBuffer(true);\n+                waitForPreviousUploadFinish();\n+            }\n+            catch (IOException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 297}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMjk3NA==", "bodyText": "I think we can make this just ExecutorService (and remove listeningDecorator) since we only call get() on the future and don't use the listening part.", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533032974", "createdAt": "2020-12-01T02:31:37Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -210,8 +230,12 @@\n     private PrestoS3AclType s3AclType;\n     private boolean skipGlacierObjects;\n     private boolean requesterPaysEnabled;\n+    private boolean streamingUploadEnabled;\n+    private long streamingUploadPartSize;\n     private PrestoS3StorageClass s3StorageClass;\n \n+    private final ListeningExecutorService uploadExecutor = listeningDecorator(newCachedThreadPool(threadsNamed(\"s3-upload-%s\")));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMzE1NA==", "bodyText": "Nit: use post increment\ncurrentPartNumber++;", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533033154", "createdAt": "2020-12-01T02:32:17Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+\n+            buf[count] = (byte) b;\n+            count++;\n+        }\n+\n+        @Override\n+        public void write(byte[] bytes, int off, int len)\n+                throws IOException\n+        {\n+            int offset = off;\n+            int remain = len;\n+            while ((buf.length - count) <= remain) {\n+                int copied = buf.length - count;\n+                arraycopy(bytes, offset, buf, count, copied);\n+                count += copied;\n+\n+                flushBuffer(false);\n+\n+                offset += copied;\n+                remain -= copied;\n+            }\n+\n+            if (remain > 0) {\n+                arraycopy(bytes, offset, buf, count, remain);\n+                count += remain;\n+            }\n+        }\n+\n+        @Override\n+        public void flush()\n+                throws IOException\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            try {\n+                flushBuffer(true);\n+                waitForPreviousUploadFinish();\n+            }\n+            catch (IOException e) {\n+                abortUpload(true);\n+                throw e;\n+            }\n+\n+            if (!parts.isEmpty()) {\n+                try {\n+                    finishUpload(parts);\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+                throws IOException\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = buf;\n+                int length = count;\n+                this.buf = new byte[buf.length];\n+                count = 0;\n+\n+                waitForPreviousUploadFinish();\n+\n+                inProgressUploadFuture = uploadExecutor.submit(() -> uploadPage(data, length));\n+            }\n+        }\n+\n+        private void waitForPreviousUploadFinish()\n+                throws IOException\n+        {\n+            if (inProgressUploadFuture == null) {\n+                return;\n+            }\n+\n+            try {\n+                inProgressUploadFuture.get();\n+            }\n+            catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                throw new InterruptedIOException();\n+            }\n+            catch (ExecutionException e) {\n+                throw new IOException(\"Streaming upload failed\", e.getCause());\n+            }\n+        }\n+\n+        private UploadPartResult uploadPage(byte[] data, int length)\n+        {\n+            ++currentPartNumber;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 353}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMzcyMg==", "bodyText": "It's better to throw e here so that we retain the full stack trace. While the ExecutionException is not really relevant to the underlying exception, it contains the Future.get() call in the stack. If you discard it, you see a stack trace from a different thread, which can be confusing.\nSee https://github.com/google/guava/wiki/Why-we-deprecated-Throwables.propagate#encourages-unwrapping-exceptions-from-other-threads which changed my opinion on this", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533033722", "createdAt": "2020-12-01T02:34:23Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+\n+            buf[count] = (byte) b;\n+            count++;\n+        }\n+\n+        @Override\n+        public void write(byte[] bytes, int off, int len)\n+                throws IOException\n+        {\n+            int offset = off;\n+            int remain = len;\n+            while ((buf.length - count) <= remain) {\n+                int copied = buf.length - count;\n+                arraycopy(bytes, offset, buf, count, copied);\n+                count += copied;\n+\n+                flushBuffer(false);\n+\n+                offset += copied;\n+                remain -= copied;\n+            }\n+\n+            if (remain > 0) {\n+                arraycopy(bytes, offset, buf, count, remain);\n+                count += remain;\n+            }\n+        }\n+\n+        @Override\n+        public void flush()\n+                throws IOException\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            try {\n+                flushBuffer(true);\n+                waitForPreviousUploadFinish();\n+            }\n+            catch (IOException e) {\n+                abortUpload(true);\n+                throw e;\n+            }\n+\n+            if (!parts.isEmpty()) {\n+                try {\n+                    finishUpload(parts);\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+                throws IOException\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = buf;\n+                int length = count;\n+                this.buf = new byte[buf.length];\n+                count = 0;\n+\n+                waitForPreviousUploadFinish();\n+\n+                inProgressUploadFuture = uploadExecutor.submit(() -> uploadPage(data, length));\n+            }\n+        }\n+\n+        private void waitForPreviousUploadFinish()\n+                throws IOException\n+        {\n+            if (inProgressUploadFuture == null) {\n+                return;\n+            }\n+\n+            try {\n+                inProgressUploadFuture.get();\n+            }\n+            catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                throw new InterruptedIOException();\n+            }\n+            catch (ExecutionException e) {\n+                throw new IOException(\"Streaming upload failed\", e.getCause());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 347}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzNTAzOA==", "bodyText": "Nit: no need to wrap here", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533035038", "createdAt": "2020-12-01T02:38:27Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+\n+            buf[count] = (byte) b;\n+            count++;\n+        }\n+\n+        @Override\n+        public void write(byte[] bytes, int off, int len)\n+                throws IOException\n+        {\n+            int offset = off;\n+            int remain = len;\n+            while ((buf.length - count) <= remain) {\n+                int copied = buf.length - count;\n+                arraycopy(bytes, offset, buf, count, copied);\n+                count += copied;\n+\n+                flushBuffer(false);\n+\n+                offset += copied;\n+                remain -= copied;\n+            }\n+\n+            if (remain > 0) {\n+                arraycopy(bytes, offset, buf, count, remain);\n+                count += remain;\n+            }\n+        }\n+\n+        @Override\n+        public void flush()\n+                throws IOException\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            try {\n+                flushBuffer(true);\n+                waitForPreviousUploadFinish();\n+            }\n+            catch (IOException e) {\n+                abortUpload(true);\n+                throw e;\n+            }\n+\n+            if (!parts.isEmpty()) {\n+                try {\n+                    finishUpload(parts);\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+                throws IOException\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = buf;\n+                int length = count;\n+                this.buf = new byte[buf.length];\n+                count = 0;\n+\n+                waitForPreviousUploadFinish();\n+\n+                inProgressUploadFuture = uploadExecutor.submit(() -> uploadPage(data, length));\n+            }\n+        }\n+\n+        private void waitForPreviousUploadFinish()\n+                throws IOException\n+        {\n+            if (inProgressUploadFuture == null) {\n+                return;\n+            }\n+\n+            try {\n+                inProgressUploadFuture.get();\n+            }\n+            catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                throw new InterruptedIOException();\n+            }\n+            catch (ExecutionException e) {\n+                throw new IOException(\"Streaming upload failed\", e.getCause());\n+            }\n+        }\n+\n+        private UploadPartResult uploadPage(byte[] data, int length)\n+        {\n+            ++currentPartNumber;\n+            UploadPartRequest uploadRequest = new UploadPartRequest()\n+                    .withBucketName(bucketName)\n+                    .withKey(key)\n+                    .withUploadId(uploadId)\n+                    .withPartNumber(currentPartNumber)\n+                    .withInputStream(new ByteArrayInputStream(data, 0, length))\n+                    .withPartSize(length)\n+                    .withMD5Digest(getMd5AsBase64(data, 0, length));\n+\n+            UploadPartResult partResult = s3.uploadPart(uploadRequest);\n+            parts.add(partResult);\n+            return partResult;\n+        }\n+\n+        private void finishUpload(List<UploadPartResult> result)\n+        {\n+            List<PartETag> etags = result.stream()\n+                    .map(UploadPartResult::getPartETag)\n+                    .collect(toList());\n+            s3.completeMultipartUpload(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 373}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxNDUwNzA3", "url": "https://github.com/trinodb/trino/pull/3712#pullrequestreview-541450707", "createdAt": "2020-12-01T02:49:46Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwMjo0OTo0NlrOH8WF0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwMjo0OTo0NlrOH8WF0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzODU0NA==", "bodyText": "Should we enable this by default? Are you using this change in production already?", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533038544", "createdAt": "2020-12-01T02:49:46Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/HiveS3Config.java", "diffHunk": "@@ -65,6 +65,8 @@\n     private PrestoS3AclType s3AclType = PrestoS3AclType.PRIVATE;\n     private boolean skipGlacierObjects;\n     private boolean requesterPaysEnabled;\n+    private boolean s3StreamingUploadEnabled;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2baed788647c0be4eaf8b535a77c32f199462cb", "author": {"user": {"login": "chhsiao90", "name": "Chun-Han, Hsiao"}}, "url": "https://github.com/trinodb/trino/commit/d2baed788647c0be4eaf8b535a77c32f199462cb", "committedDate": "2020-12-03T03:08:17Z", "message": "Implement s3 streaming upload"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1487, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}