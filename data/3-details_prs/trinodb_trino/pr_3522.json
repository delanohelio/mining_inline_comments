{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA3NzQ5MTkz", "number": 3522, "title": "Presto Druid connector", "bodyText": "Co-authored-by: Puneet Jaiswal punit.kj@gmail.com\nThis PR overtook the work done by Puneet as part of #532.\nThis is a V1 version of the connector without aggregation pushdown. While some of the work from Puneet's PR has been carried over, a lot of code was removed since it is no longer needed. Note that this PR mandates that the Druid cluster runs Druid 0.18 or newer.\nThe other bulk of the work done was for setting up the TestingDruidServer to run various Druid components, ingesting TPCH data by writing to a TSV file and then invoking Druid indexer job and, adding a smoke test.", "createdAt": "2020-04-23T08:04:10Z", "url": "https://github.com/trinodb/trino/pull/3522", "merged": true, "mergeCommit": {"oid": "fcb91631a655f8af7c6aeb803861d1c738f67281"}, "closed": true, "closedAt": "2020-06-20T11:08:50Z", "author": {"login": "samarthjain"}, "timelineItems": {"totalCount": 44, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcaYXxaABqjMyNjM5MDY3MDI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcsxR0vAFqTQzNDAwMDgzNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk4ODYxODc3", "url": "https://github.com/trinodb/trino/pull/3522#pullrequestreview-398861877", "createdAt": "2020-04-23T08:13:41Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QwODoxNDoyNFrOGKcmxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QwODoyMDoxM1rOGKc2AA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzYwNzYyMw==", "bodyText": "Can we move this below getSqlTimeZone , similarly for other setter method also", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r413607623", "createdAt": "2020-04-23T08:14:24Z", "author": {"login": "Praveen2112"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidConfig.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.airlift.configuration.Config;\n+\n+public class DruidConfig\n+{\n+    private String sqlTimeZone = \"UTC\";\n+    private boolean useApproximateCountDistinct = true;\n+    private boolean useApproximateTopN = true;\n+\n+    public String getSqlTimeZone()\n+    {\n+        return this.sqlTimeZone;\n+    }\n+\n+    public boolean getUseApproxCountDistinct()\n+    {\n+        return useApproximateCountDistinct;\n+    }\n+\n+    public boolean getUseApproxTopN()\n+    {\n+        return useApproximateTopN;\n+    }\n+\n+    @Config(\"druid.sql-time-zone\")\n+    public DruidConfig setSqlTimeZone(String sqlTimeZone)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzYxMDIzOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                static final String DRUID_SCHEM = \"druid\";\n          \n          \n            \n                static final String DRUID_SCHEMA = \"druid\";", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r413610238", "createdAt": "2020-04-23T08:18:18Z", "author": {"login": "Praveen2112"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    static final String DRUID_CATALOG = \"druid\";\n+    static final String DRUID_SCHEM = \"druid\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzYxMTUyMA==", "bodyText": "If the schema name is restricted to druid we might have to implement them in getSchemaNames", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r413611520", "createdAt": "2020-04-23T08:20:13Z", "author": {"login": "Praveen2112"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    static final String DRUID_CATALOG = \"druid\";\n+    static final String DRUID_SCHEM = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEM,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 72}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4ODg2MjMw", "url": "https://github.com/trinodb/trino/pull/3522#pullrequestreview-418886230", "createdAt": "2020-05-27T06:19:53Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwNjoxOTo1M1rOGa66Sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwNjozNjowM1rOGa7SNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4MTM1NQ==", "bodyText": "how much time does this group take now, and how much time druid adds?", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430881355", "createdAt": "2020-05-27T06:19:53Z", "author": {"login": "findepi"}, "path": ".github/workflows/ci.yml", "diffHunk": "@@ -160,7 +161,7 @@ jobs:\n           - \"presto-sqlserver,presto-postgresql,presto-mysql\"\n           - \"presto-oracle\"\n           - \"presto-kudu\"\n-          - \"presto-phoenix,presto-iceberg\"\n+          - \"presto-phoenix,presto-iceberg,presto-druid\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4MTQ0Mw==", "bodyText": "move to !presto-phoenix,!presto-iceberg, line", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430881443", "createdAt": "2020-05-27T06:20:05Z", "author": {"login": "findepi"}, "path": ".github/workflows/ci.yml", "diffHunk": "@@ -139,7 +139,8 @@ jobs:\n             !presto-oracle,\n             !presto-kudu,\n             !presto-phoenix,!presto-iceberg,\n-            !presto-docs,!presto-server,!presto-server-rpm'\n+            !presto-docs,!presto-server,!presto-server-rpm,\n+            !presto-druid'", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4MTc5OA==", "bodyText": "What are semantics of this setting? Why do we need it?\n(we don't\u00a0need this in other JDBC-based connectors)", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430881798", "createdAt": "2020-05-27T06:20:52Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidConfig.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.airlift.configuration.Config;\n+\n+public class DruidConfig\n+{\n+    private String sqlTimeZone = \"UTC\";\n+    private boolean useApproximateCountDistinct = true;\n+    private boolean useApproximateTopN = true;\n+\n+    public String getSqlTimeZone()\n+    {\n+        return this.sqlTimeZone;\n+    }\n+\n+    @Config(\"druid.sql-time-zone\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4MjA1Mw==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430882053", "createdAt": "2020-05-27T06:21:35Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    static final String DRUID_CATALOG = \"druid\";\n+    static final String DRUID_SCHEMA = \"druid\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4Mjg4Mg==", "bodyText": "does it mean druid does not support multiple catalogs, schemas?\nwhat would happen if you passed null for catalog and schema (correct from API perspective)?\n\nalso, if if only druid schema is allowed, then add this:\ncheckArgument(schemaName.isEmpty() || schemaName.get().equals(\"druid\")\", ....)", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430882882", "createdAt": "2020-05-27T06:23:53Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    static final String DRUID_CATALOG = \"druid\";\n+    static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4MzA0NA==", "bodyText": "Needs to be escaped. see io.prestosql.plugin.jdbc.BaseJdbcClient#escapeNamePattern", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430883044", "createdAt": "2020-05-27T06:24:21Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    static final String DRUID_CATALOG = \"druid\";\n+    static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,\n+                tableName.orElse(null),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4Mzg1Ng==", "bodyText": "inine", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430883856", "createdAt": "2020-05-27T06:26:26Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    static final String DRUID_CATALOG = \"druid\";\n+    static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,\n+                tableName.orElse(null),\n+                null);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    throw new PrestoException(NOT_SUPPORTED, \"Multiple tables matched: \" + schemaTableName);\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<ColumnMapping> toPrestoType(ConnectorSession session, Connection connection, JdbcTypeHandle typeHandle)\n+    {\n+        switch (typeHandle.getJdbcType()) {\n+            case Types.VARCHAR:\n+                return jdbcTypeToPrestoType(typeHandle);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4NDA2Ng==", "bodyText": "?", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430884066", "createdAt": "2020-05-27T06:26:56Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    static final String DRUID_CATALOG = \"druid\";\n+    static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,\n+                tableName.orElse(null),\n+                null);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    throw new PrestoException(NOT_SUPPORTED, \"Multiple tables matched: \" + schemaTableName);\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<ColumnMapping> toPrestoType(ConnectorSession session, Connection connection, JdbcTypeHandle typeHandle)\n+    {\n+        switch (typeHandle.getJdbcType()) {\n+            case Types.VARCHAR:\n+                return jdbcTypeToPrestoType(typeHandle);\n+            default:\n+                return super.toPrestoType(session, connection, typeHandle);\n+        }\n+    }\n+\n+    private Optional<ColumnMapping> jdbcTypeToPrestoType(JdbcTypeHandle type)\n+    {\n+        int columnSize = type.getColumnSize();\n+        if (columnSize > VarcharType.MAX_LENGTH || columnSize == -1) {\n+            return Optional.of(varcharColumnMapping(createUnboundedVarcharType()));\n+        }\n+        return Optional.of(varcharColumnMapping(createVarcharType(columnSize)));\n+    }\n+\n+    @Override\n+    public List<JdbcColumnHandle> getColumns(ConnectorSession session, JdbcTableHandle tableHandle)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(JdbcIdentity.from(session))) {\n+            try (ResultSet resultSet = getDruidColumns(tableHandle, connection.getMetaData())) {\n+                List<JdbcColumnHandle> columns = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    JdbcTypeHandle typeHandle = new JdbcTypeHandle(\n+                            resultSet.getInt(\"DATA_TYPE\"),\n+                            // type_name isn't supported in Druid\n+                            Optional.of(resultSet.getString(\"TYPE_NAME\")),\n+                            resultSet.getInt(\"COLUMN_SIZE\"),\n+                            resultSet.getInt(\"DECIMAL_DIGITS\"),\n+                            // array_dimensions isn't supported in Druid\n+                            Optional.ofNullable(null));\n+                    Optional<ColumnMapping> columnMapping = toPrestoType(session, connection, typeHandle);\n+                    // skip unsupported column types\n+                    if (columnMapping.isPresent()) {\n+                        String columnName = resultSet.getString(\"COLUMN_NAME\");\n+                        boolean nullable = (resultSet.getInt(\"NULLABLE\") != columnNoNulls);\n+                        columns.add(new JdbcColumnHandle(columnName, typeHandle, columnMapping.get().getType(), nullable));\n+                    }\n+                }\n+                if (columns.isEmpty()) {\n+                    // In rare cases (e.g. PostgreSQL) a table might have no columns.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4NDMwMw==", "bodyText": "Why is this method overridden?", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430884303", "createdAt": "2020-05-27T06:27:36Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    static final String DRUID_CATALOG = \"druid\";\n+    static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,\n+                tableName.orElse(null),\n+                null);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    throw new PrestoException(NOT_SUPPORTED, \"Multiple tables matched: \" + schemaTableName);\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<ColumnMapping> toPrestoType(ConnectorSession session, Connection connection, JdbcTypeHandle typeHandle)\n+    {\n+        switch (typeHandle.getJdbcType()) {\n+            case Types.VARCHAR:\n+                return jdbcTypeToPrestoType(typeHandle);\n+            default:\n+                return super.toPrestoType(session, connection, typeHandle);\n+        }\n+    }\n+\n+    private Optional<ColumnMapping> jdbcTypeToPrestoType(JdbcTypeHandle type)\n+    {\n+        int columnSize = type.getColumnSize();\n+        if (columnSize > VarcharType.MAX_LENGTH || columnSize == -1) {\n+            return Optional.of(varcharColumnMapping(createUnboundedVarcharType()));\n+        }\n+        return Optional.of(varcharColumnMapping(createVarcharType(columnSize)));\n+    }\n+\n+    @Override\n+    public List<JdbcColumnHandle> getColumns(ConnectorSession session, JdbcTableHandle tableHandle)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4NTMyMw==", "bodyText": "Then it might be not correct -- when asking for some_table, you should also get columns from somertable, (unless Druid JDBC not only does not support escaping, but also does not support patterns).\nIf I am right, and there really is no way to escape, we need to filter on our side as well.", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430885323", "createdAt": "2020-05-27T06:30:28Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    static final String DRUID_CATALOG = \"druid\";\n+    static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,\n+                tableName.orElse(null),\n+                null);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    throw new PrestoException(NOT_SUPPORTED, \"Multiple tables matched: \" + schemaTableName);\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<ColumnMapping> toPrestoType(ConnectorSession session, Connection connection, JdbcTypeHandle typeHandle)\n+    {\n+        switch (typeHandle.getJdbcType()) {\n+            case Types.VARCHAR:\n+                return jdbcTypeToPrestoType(typeHandle);\n+            default:\n+                return super.toPrestoType(session, connection, typeHandle);\n+        }\n+    }\n+\n+    private Optional<ColumnMapping> jdbcTypeToPrestoType(JdbcTypeHandle type)\n+    {\n+        int columnSize = type.getColumnSize();\n+        if (columnSize > VarcharType.MAX_LENGTH || columnSize == -1) {\n+            return Optional.of(varcharColumnMapping(createUnboundedVarcharType()));\n+        }\n+        return Optional.of(varcharColumnMapping(createVarcharType(columnSize)));\n+    }\n+\n+    @Override\n+    public List<JdbcColumnHandle> getColumns(ConnectorSession session, JdbcTableHandle tableHandle)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(JdbcIdentity.from(session))) {\n+            try (ResultSet resultSet = getDruidColumns(tableHandle, connection.getMetaData())) {\n+                List<JdbcColumnHandle> columns = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    JdbcTypeHandle typeHandle = new JdbcTypeHandle(\n+                            resultSet.getInt(\"DATA_TYPE\"),\n+                            // type_name isn't supported in Druid\n+                            Optional.of(resultSet.getString(\"TYPE_NAME\")),\n+                            resultSet.getInt(\"COLUMN_SIZE\"),\n+                            resultSet.getInt(\"DECIMAL_DIGITS\"),\n+                            // array_dimensions isn't supported in Druid\n+                            Optional.ofNullable(null));\n+                    Optional<ColumnMapping> columnMapping = toPrestoType(session, connection, typeHandle);\n+                    // skip unsupported column types\n+                    if (columnMapping.isPresent()) {\n+                        String columnName = resultSet.getString(\"COLUMN_NAME\");\n+                        boolean nullable = (resultSet.getInt(\"NULLABLE\") != columnNoNulls);\n+                        columns.add(new JdbcColumnHandle(columnName, typeHandle, columnMapping.get().getType(), nullable));\n+                    }\n+                }\n+                if (columns.isEmpty()) {\n+                    // In rare cases (e.g. PostgreSQL) a table might have no columns.\n+                    throw new TableNotFoundException(tableHandle.getSchemaTableName());\n+                }\n+                return ImmutableList.copyOf(columns);\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    /**\n+     * Custom way of retrieving column names since the {@link BaseJdbcClient#getColumns(JdbcTableHandle, DatabaseMetaData)}\n+     * method uses character escaping that doesn't jive well with Druid SQL.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4NTYyOQ==", "bodyText": "We can modify the QueryBuilder to eg SELECT 1. Would that help?", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430885629", "createdAt": "2020-05-27T06:31:19Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    static final String DRUID_CATALOG = \"druid\";\n+    static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,\n+                tableName.orElse(null),\n+                null);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    throw new PrestoException(NOT_SUPPORTED, \"Multiple tables matched: \" + schemaTableName);\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<ColumnMapping> toPrestoType(ConnectorSession session, Connection connection, JdbcTypeHandle typeHandle)\n+    {\n+        switch (typeHandle.getJdbcType()) {\n+            case Types.VARCHAR:\n+                return jdbcTypeToPrestoType(typeHandle);\n+            default:\n+                return super.toPrestoType(session, connection, typeHandle);\n+        }\n+    }\n+\n+    private Optional<ColumnMapping> jdbcTypeToPrestoType(JdbcTypeHandle type)\n+    {\n+        int columnSize = type.getColumnSize();\n+        if (columnSize > VarcharType.MAX_LENGTH || columnSize == -1) {\n+            return Optional.of(varcharColumnMapping(createUnboundedVarcharType()));\n+        }\n+        return Optional.of(varcharColumnMapping(createVarcharType(columnSize)));\n+    }\n+\n+    @Override\n+    public List<JdbcColumnHandle> getColumns(ConnectorSession session, JdbcTableHandle tableHandle)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(JdbcIdentity.from(session))) {\n+            try (ResultSet resultSet = getDruidColumns(tableHandle, connection.getMetaData())) {\n+                List<JdbcColumnHandle> columns = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    JdbcTypeHandle typeHandle = new JdbcTypeHandle(\n+                            resultSet.getInt(\"DATA_TYPE\"),\n+                            // type_name isn't supported in Druid\n+                            Optional.of(resultSet.getString(\"TYPE_NAME\")),\n+                            resultSet.getInt(\"COLUMN_SIZE\"),\n+                            resultSet.getInt(\"DECIMAL_DIGITS\"),\n+                            // array_dimensions isn't supported in Druid\n+                            Optional.ofNullable(null));\n+                    Optional<ColumnMapping> columnMapping = toPrestoType(session, connection, typeHandle);\n+                    // skip unsupported column types\n+                    if (columnMapping.isPresent()) {\n+                        String columnName = resultSet.getString(\"COLUMN_NAME\");\n+                        boolean nullable = (resultSet.getInt(\"NULLABLE\") != columnNoNulls);\n+                        columns.add(new JdbcColumnHandle(columnName, typeHandle, columnMapping.get().getType(), nullable));\n+                    }\n+                }\n+                if (columns.isEmpty()) {\n+                    // In rare cases (e.g. PostgreSQL) a table might have no columns.\n+                    throw new TableNotFoundException(tableHandle.getSchemaTableName());\n+                }\n+                return ImmutableList.copyOf(columns);\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    /**\n+     * Custom way of retrieving column names since the {@link BaseJdbcClient#getColumns(JdbcTableHandle, DatabaseMetaData)}\n+     * method uses character escaping that doesn't jive well with Druid SQL.\n+     */\n+    private static ResultSet getDruidColumns(JdbcTableHandle tableHandle, DatabaseMetaData metadata)\n+            throws SQLException\n+    {\n+        return metadata.getColumns(\n+                tableHandle.getCatalogName(),\n+                tableHandle.getSchemaName(),\n+                tableHandle.getTableName(),\n+                null);\n+    }\n+\n+    /**\n+     * Overriding this method to handle following weirdness/issues in Druid:\n+     * 1) Druid doesn't like table names to be qualified with catalog names in the SQL query. So setting it to an empty string.\n+     * 2) Need to use DruidQueryBuilder since Druid doesn't like SELECT NULL queries when there are no columns projected.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4NjIxOA==", "bodyText": "target/storage?\n/tmp/$USER-druid-test-storage?", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430886218", "createdAt": "2020-05-27T06:33:00Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/DruidIntegrationSmokeTest.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.testing.AbstractTestIntegrationSmokeTest;\n+import io.prestosql.testing.QueryRunner;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+\n+import static io.prestosql.tpch.TpchTable.ORDERS;\n+\n+@Test\n+public class DruidIntegrationSmokeTest\n+        extends AbstractTestIntegrationSmokeTest\n+{\n+    private TestingDruidServer druidServer;\n+\n+    public DruidIntegrationSmokeTest()\n+    {\n+        try {\n+            Path storage = Paths.get(\"storage\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4NjY1Ng==", "bodyText": "move IO and instnatiation of the server to createQueryRunner method.\nTest constructors should be as simple as possible, because they are run eagerly at the very start of all tests.\nAlso, the error reporting for ctor failures is suboptimal.", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430886656", "createdAt": "2020-05-27T06:34:00Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/DruidIntegrationSmokeTest.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.testing.AbstractTestIntegrationSmokeTest;\n+import io.prestosql.testing.QueryRunner;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+\n+import static io.prestosql.tpch.TpchTable.ORDERS;\n+\n+@Test\n+public class DruidIntegrationSmokeTest\n+        extends AbstractTestIntegrationSmokeTest\n+{\n+    private TestingDruidServer druidServer;\n+\n+    public DruidIntegrationSmokeTest()\n+    {\n+        try {\n+            Path storage = Paths.get(\"storage\");\n+            if (Files.exists(storage)) {\n+                druidServer = new TestingDruidServer(storage.toAbsolutePath().toString());\n+            }\n+            else {\n+                druidServer = new TestingDruidServer(Files.createDirectory(storage).toAbsolutePath().toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4NzExNg==", "bodyText": "Document why.\nSee also https://prestosql.slack.com/archives/CP1MUNEUX/p1588690413495700 how we would like to suppress tests now. This also solves the documentation needs.", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430887116", "createdAt": "2020-05-27T06:35:12Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/DruidIntegrationSmokeTest.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.testing.AbstractTestIntegrationSmokeTest;\n+import io.prestosql.testing.QueryRunner;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+\n+import static io.prestosql.tpch.TpchTable.ORDERS;\n+\n+@Test\n+public class DruidIntegrationSmokeTest\n+        extends AbstractTestIntegrationSmokeTest\n+{\n+    private TestingDruidServer druidServer;\n+\n+    public DruidIntegrationSmokeTest()\n+    {\n+        try {\n+            Path storage = Paths.get(\"storage\");\n+            if (Files.exists(storage)) {\n+                druidServer = new TestingDruidServer(storage.toAbsolutePath().toString());\n+            }\n+            else {\n+                druidServer = new TestingDruidServer(Files.createDirectory(storage).toAbsolutePath().toString());\n+            }\n+        }\n+        catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return DruidQueryRunner.createDruidQueryRunnerTpch(druidServer, ORDERS);\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void destroy()\n+    {\n+        druidServer.close();\n+    }\n+\n+    @Test\n+    @Override\n+    public void testShowSchemas()\n+    {\n+        assertQuery(\"SHOW SCHEMAS FROM druid\", \"VALUES 'druid', 'information_schema', 'sys', 'lookup'\");\n+    }\n+\n+    @Test\n+    @Override\n+    public void testDescribeTable()\n+    {\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4NzQ3OQ==", "bodyText": "Since the connector supports inserts too, add AbstractTestDistributedQueries", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r430887479", "createdAt": "2020-05-27T06:36:03Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/DruidIntegrationSmokeTest.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.testing.AbstractTestIntegrationSmokeTest;\n+import io.prestosql.testing.QueryRunner;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+\n+import static io.prestosql.tpch.TpchTable.ORDERS;\n+\n+@Test\n+public class DruidIntegrationSmokeTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIwMDk1MDI2", "url": "https://github.com/trinodb/trino/pull/3522#pullrequestreview-420095026", "createdAt": "2020-05-28T13:16:10Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxMzoxNjoxMFrOGb0kSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxMzoxNzo1NFrOGb0oog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTgyNTk5Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        queryRunner.createCatalog(DRUID_CATALOG, DRUID_SCHEMA, connectorProperties);\n          \n          \n            \n                        queryRunner.createCatalog(\"druid\", \"druid\", connectorProperties);\n          \n      \n    \n    \n  \n\n\nDruidJdbcClient. DRUID_CATALOG describes fixed catalog on Druid side; on Presto side we could choose any catalog name we want (like \"sales_data\"), and we just chose to name the catalog \"druid\" by convention\nDruidJdbcClient. DRUID_SCHEMA describes fixed schema on Druid side; here this is connector name (which incidentally is also \"druid\")", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r431825993", "createdAt": "2020-05-28T13:16:10Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/DruidQueryRunner.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.Session;\n+import io.prestosql.metadata.QualifiedObjectName;\n+import io.prestosql.plugin.tpch.TpchPlugin;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedRow;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.tpch.TpchTable;\n+import org.intellij.lang.annotations.Language;\n+\n+import java.io.BufferedWriter;\n+import java.io.File;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static io.airlift.testing.Closeables.closeAllSuppress;\n+import static io.prestosql.plugin.druid.DruidJdbcClient.DRUID_CATALOG;\n+import static io.prestosql.plugin.druid.DruidJdbcClient.DRUID_SCHEMA;\n+import static io.prestosql.plugin.tpch.TpchMetadata.TINY_SCHEMA_NAME;\n+import static io.prestosql.testing.TestingSession.testSessionBuilder;\n+import static java.lang.String.format;\n+\n+public class DruidQueryRunner\n+{\n+    private DruidQueryRunner() {}\n+\n+    public static QueryRunner createDruidQueryRunnerTpch(TestingDruidServer testingDruidServer, TpchTable table)\n+            throws Exception\n+    {\n+        DistributedQueryRunner queryRunner = null;\n+        try {\n+            queryRunner = DistributedQueryRunner.builder(createSession()).setNodeCount(3).build();\n+            queryRunner.installPlugin(new TpchPlugin());\n+            queryRunner.createCatalog(\"tpch\", \"tpch\");\n+\n+            Map<String, String> connectorProperties = new HashMap<>();\n+            connectorProperties.putIfAbsent(\"connection-url\", testingDruidServer.getJdbcUrl());\n+            queryRunner.installPlugin(new DruidJdbcPlugin());\n+            queryRunner.createCatalog(DRUID_CATALOG, DRUID_SCHEMA, connectorProperties);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTgyNzEwNg==", "bodyText": "DRUID_CATALOG must be private (explanation there)\nif Druid does not support schemas (always one schema), the DRUID_SCHEMA can stay visible", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r431827106", "createdAt": "2020-05-28T13:17:54Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    static final String DRUID_CATALOG = \"druid\";\n+    static final String DRUID_SCHEMA = \"druid\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4MjA1Mw=="}, "originalCommit": null, "originalPosition": 57}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyMzI4MjEw", "url": "https://github.com/trinodb/trino/pull/3522#pullrequestreview-432328210", "createdAt": "2020-06-17T11:59:16Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 32, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxMTo1OToxNlrOGlCdXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxMzoyNTozOFrOGlFnXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ5MDc4MA==", "bodyText": "the modules here should be in the same order as in the test job definition, to allow eyeballing the differences quickly (linearly \ud83d\ude09 )\nalso, i don't think druid needs a separate job run today, so\n\nadd this to !presto-phoenix,!presto-iceberg,!presto-druid here\n\"presto-phoenix,presto-iceberg,presto-druid\" below", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441490780", "createdAt": "2020-06-17T11:59:16Z", "author": {"login": "findepi"}, "path": ".github/workflows/ci.yml", "diffHunk": "@@ -139,7 +139,8 @@ jobs:\n             !presto-oracle,\n             !presto-kudu,\n             !presto-phoenix,!presto-iceberg,\n-            !presto-docs,!presto-server,!presto-server-rpm'\n+            !presto-docs,!presto-server,!presto-server-rpm,\n+            !presto-druid'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4MTQ0Mw=="}, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ5MzA1NA==", "bodyText": "move to presto-server-main/etc/catalog", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441493054", "createdAt": "2020-06-17T12:03:42Z", "author": {"login": "findepi"}, "path": "presto-main/etc/catalog/druid.properties", "diffHunk": "@@ -0,0 +1,7 @@\n+connector.name=druid", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ5ODY5OQ==", "bodyText": "move this out of SPI section.\nremove <version>", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441498699", "createdAt": "2020-06-17T12:14:44Z", "author": {"login": "findepi"}, "path": "presto-druid/pom.xml", "diffHunk": "@@ -0,0 +1,166 @@\n+<?xml version=\"1.0\"?>\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+    <modelVersion>4.0.0</modelVersion>\n+\n+    <parent>\n+        <groupId>io.prestosql</groupId>\n+        <artifactId>presto-root</artifactId>\n+        <version>336-SNAPSHOT</version>\n+    </parent>\n+\n+    <artifactId>presto-druid</artifactId>\n+    <description>Presto - Druid Jdbc Connector</description>\n+    <packaging>presto-plugin</packaging>\n+\n+    <properties>\n+        <air.main.basedir>${project.parent.basedir}</air.main.basedir>\n+    </properties>\n+\n+    <dependencies>\n+        <dependency>\n+            <groupId>io.prestosql</groupId>\n+            <artifactId>presto-base-jdbc</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.airlift</groupId>\n+            <artifactId>configuration</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.google.guava</groupId>\n+            <artifactId>guava</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.google.inject</groupId>\n+            <artifactId>guice</artifactId>\n+        </dependency>\n+\n+        <!-- Druid JDBC Connector -->\n+        <dependency>\n+            <groupId>org.apache.calcite.avatica</groupId>\n+            <artifactId>avatica-core</artifactId>\n+            <version>1.16.0</version>\n+            <!-- Excluded as they bring in duplicate classes -->\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>commons-logging</groupId>\n+                    <artifactId>commons-logging</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>javax.inject</groupId>\n+            <artifactId>javax.inject</artifactId>\n+        </dependency>\n+\n+        <!-- SPI -->\n+        <dependency>\n+            <groupId>io.prestosql</groupId>\n+            <artifactId>presto-spi</artifactId>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.airlift</groupId>\n+            <artifactId>slice</artifactId>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.fasterxml.jackson.core</groupId>\n+            <artifactId>jackson-annotations</artifactId>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.openjdk.jol</groupId>\n+            <artifactId>jol-core</artifactId>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.fasterxml.jackson.core</groupId>\n+            <artifactId>jackson-databind</artifactId>\n+            <version>2.10.3</version>\n+            <scope>compile</scope>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUwMzIxNA==", "bodyText": "why Xms? remove?", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441503214", "createdAt": "2020-06-17T12:22:51Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/resources/middleManager.config", "diffHunk": "@@ -0,0 +1,30 @@\n+druid.service=druid/middleManager\n+druid.plaintextPort=8091\n+\n+# Number of tasks per middleManager\n+druid.worker.capacity=1\n+\n+# Task launch parameters\n+druid.indexer.runner.javaOpts=-server -Xms256m -Xmx384m -XX:MaxDirectMemorySize=200m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+ExitOnOutOfMemoryError -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUwMzk0NA==", "bodyText": "is coordinator-jvm.config for druid's coordinator?\nmaybe rename to druid-.. to avoid confusion w/ presto coordinator", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441503944", "createdAt": "2020-06-17T12:24:10Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/resources/coordinator-jvm.config", "diffHunk": "@@ -0,0 +1,10 @@\n+-server", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUwNDU1MQ==", "bodyText": "can we check when the condition is satisfied?", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441504551", "createdAt": "2020-06-17T12:25:17Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestingDruidServer.java", "diffHunk": "@@ -0,0 +1,286 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.io.Closer;\n+import com.google.common.io.Resources;\n+import io.prestosql.testing.assertions.Assert;\n+import io.prestosql.tests.product.launcher.testcontainers.SelectedPortWaitStrategy;\n+import org.testcontainers.containers.BindMode;\n+import org.testcontainers.containers.GenericContainer;\n+import org.testcontainers.containers.Network;\n+import org.testcontainers.containers.startupcheck.IsRunningStartupCheckStrategy;\n+import org.testcontainers.containers.wait.strategy.Wait;\n+import org.testcontainers.shaded.okhttp3.OkHttpClient;\n+import org.testcontainers.shaded.okhttp3.Request;\n+import org.testcontainers.shaded.okhttp3.RequestBody;\n+import org.testcontainers.shaded.okhttp3.Response;\n+\n+import java.io.Closeable;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.Charset;\n+import java.util.Map;\n+\n+import static com.google.common.io.Resources.getResource;\n+import static java.lang.String.format;\n+import static org.testcontainers.utility.MountableFile.forHostPath;\n+\n+public class TestingDruidServer\n+        implements Closeable\n+{\n+    private final String hostWorkingDirectory;\n+    private final GenericContainer broker;\n+    private final GenericContainer coordinator;\n+    private final GenericContainer historical;\n+    private final GenericContainer middleManager;\n+    private final GenericContainer zookeeper;\n+    private final OkHttpClient httpClient;\n+\n+    private static final int DRUID_COORDINATOR_PORT = 8081;\n+    private static final int DRUID_BROKER_PORT = 8082;\n+    private static final int DRUID_HISTORICAL_PORT = 8083;\n+    private static final int DRUID_MIDDLE_MANAGER_PORT = 8091;\n+\n+    private static final String DRUID_DOCKER_IMAGE = \"apache/druid:0.18.0\";\n+\n+    public TestingDruidServer()\n+    {\n+        try {\n+            this.hostWorkingDirectory = TestingDruidServer.class.getClassLoader().getResource(\"druid-test-storage/\").getPath();\n+            File f = new File(hostWorkingDirectory);\n+            cleanDirectory(f);\n+            // Enable read/write/exec access for the services running in containers\n+            f.setWritable(true, false);\n+            f.setReadable(true, false);\n+            f.setExecutable(true, false);\n+            this.httpClient = new OkHttpClient();\n+            Network network = Network.newNetwork();\n+            this.zookeeper = new GenericContainer(\"zookeeper\")\n+                    .withNetwork(network)\n+                    .withNetworkAliases(\"zookeeper\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .waitingFor(new SelectedPortWaitStrategy(2181));\n+            zookeeper.start();\n+\n+            this.coordinator = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_COORDINATOR_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"coordinator\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .dependsOn(zookeeper)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"coordinator.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/master/coordinator-overlord/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"coordinator-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/master/coordinator-overlord/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            coordinator.start();\n+\n+            this.broker = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_BROKER_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"broker\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .dependsOn(zookeeper, coordinator)\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"broker.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/query/broker/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"broker-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/query/broker/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            broker.start();\n+\n+            this.historical = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_HISTORICAL_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"historical\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .dependsOn(zookeeper, coordinator)\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"historical.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/historical/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"historical-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/historical/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            historical.start();\n+\n+            this.middleManager = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_MIDDLE_MANAGER_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"middleManager\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .dependsOn(zookeeper, coordinator)\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"middleManager.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/middleManager/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"middleManager-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/middleManager/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            middleManager.start();\n+        }\n+        catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    public String getHostWorkingDirectory()\n+    {\n+        return hostWorkingDirectory;\n+    }\n+\n+    @Override\n+    public void close()\n+    {\n+        try (Closer closer = Closer.create()) {\n+            closer.register(broker::stop);\n+            closer.register(historical::stop);\n+            closer.register(middleManager::stop);\n+            closer.register(coordinator::stop);\n+            closer.register(zookeeper::stop);\n+        }\n+        catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+        finally {\n+            cleanDirectory(new File(hostWorkingDirectory));\n+        }\n+    }\n+\n+    public String getJdbcUrl()\n+    {\n+        return getJdbcUrl(broker.getMappedPort(DRUID_BROKER_PORT));\n+    }\n+\n+    public int getCoordinatorOverlordPort()\n+    {\n+        return coordinator.getMappedPort(DRUID_COORDINATOR_PORT);\n+    }\n+\n+    private static String getJdbcUrl(int port)\n+    {\n+        return format(\"jdbc:avatica:remote:url=http://localhost:%s/druid/v2/sql/avatica/\", port);\n+    }\n+\n+    void ingestData(String datasource, String indexTaskFile, String dataFilePath)\n+            throws IOException, InterruptedException\n+    {\n+        middleManager.withCopyFileToContainer(forHostPath(dataFilePath),\n+                getMiddleManagerContainerPathForDataFile(dataFilePath));\n+        String indexTask = Resources.toString(getResource(indexTaskFile), Charset.defaultCharset());\n+\n+        Request.Builder requestBuilder = new Request.Builder();\n+        requestBuilder.addHeader(\"content-type\", \"application/json;charset=utf-8\")\n+                .url(\"http://localhost:\" + getCoordinatorOverlordPort() + \"/druid/indexer/v1/task\")\n+                .post(RequestBody.create(null, indexTask));\n+        Request ingestionRequest = requestBuilder.build();\n+        Response response = null;\n+        try {\n+            response = httpClient.newCall(ingestionRequest).execute();\n+            Assert.assertTrue(checkDatasourceAvailable(datasource), \"Datasource \" + datasource + \" not loaded\");\n+        }\n+        finally {\n+            if (response != null) {\n+                response.close();\n+            }\n+        }\n+    }\n+\n+    private boolean checkDatasourceAvailable(String datasource)\n+            throws IOException, InterruptedException\n+    {\n+        Map<String, Double> datasourceAvailabilityDetails = null;\n+        boolean datasourceNotLoaded = true;\n+        int attempts = 10;\n+        while (datasourceNotLoaded && attempts > 0) {\n+            Request.Builder requestBuilder = new Request.Builder();\n+            requestBuilder.url(\"http://localhost:\" + getCoordinatorOverlordPort() + \"/druid/coordinator/v1/loadstatus\")\n+                    .get();\n+            Request datasourceAvailabilityRequest = requestBuilder.build();\n+            try (Response response = httpClient.newCall(datasourceAvailabilityRequest).execute()) {\n+                ObjectMapper mapper = new ObjectMapper();\n+                datasourceAvailabilityDetails = mapper.readValue(response.body().string(), Map.class);\n+                datasourceNotLoaded = datasourceAvailabilityDetails.get(datasource) == null || Double.compare(datasourceAvailabilityDetails.get(datasource), 100.0) < 0;\n+                if (datasourceNotLoaded) {\n+                    attempts--;\n+                    // Wait for some time since it can take a while for coordinator to load the ingested segments\n+                    Thread.sleep(15000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 243}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyNDI5Mw==", "bodyText": "Why depend on presto-product-tests-launcher?\n(in any case, the version should be defined in top level pom.xml)", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441524293", "createdAt": "2020-06-17T12:58:21Z", "author": {"login": "findepi"}, "path": "presto-druid/pom.xml", "diffHunk": "@@ -0,0 +1,166 @@\n+<?xml version=\"1.0\"?>\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+    <modelVersion>4.0.0</modelVersion>\n+\n+    <parent>\n+        <groupId>io.prestosql</groupId>\n+        <artifactId>presto-root</artifactId>\n+        <version>336-SNAPSHOT</version>\n+    </parent>\n+\n+    <artifactId>presto-druid</artifactId>\n+    <description>Presto - Druid Jdbc Connector</description>\n+    <packaging>presto-plugin</packaging>\n+\n+    <properties>\n+        <air.main.basedir>${project.parent.basedir}</air.main.basedir>\n+    </properties>\n+\n+    <dependencies>\n+        <dependency>\n+            <groupId>io.prestosql</groupId>\n+            <artifactId>presto-base-jdbc</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.airlift</groupId>\n+            <artifactId>configuration</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.google.guava</groupId>\n+            <artifactId>guava</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.google.inject</groupId>\n+            <artifactId>guice</artifactId>\n+        </dependency>\n+\n+        <!-- Druid JDBC Connector -->\n+        <dependency>\n+            <groupId>org.apache.calcite.avatica</groupId>\n+            <artifactId>avatica-core</artifactId>\n+            <version>1.16.0</version>\n+            <!-- Excluded as they bring in duplicate classes -->\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>commons-logging</groupId>\n+                    <artifactId>commons-logging</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>javax.inject</groupId>\n+            <artifactId>javax.inject</artifactId>\n+        </dependency>\n+\n+        <!-- SPI -->\n+        <dependency>\n+            <groupId>io.prestosql</groupId>\n+            <artifactId>presto-spi</artifactId>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.airlift</groupId>\n+            <artifactId>slice</artifactId>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.fasterxml.jackson.core</groupId>\n+            <artifactId>jackson-annotations</artifactId>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.openjdk.jol</groupId>\n+            <artifactId>jol-core</artifactId>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.fasterxml.jackson.core</groupId>\n+            <artifactId>jackson-databind</artifactId>\n+            <version>2.10.3</version>\n+            <scope>compile</scope>\n+        </dependency>\n+\n+        <!-- for testing -->\n+        <dependency>\n+            <groupId>org.testng</groupId>\n+            <artifactId>testng</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.prestosql</groupId>\n+            <artifactId>presto-main</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.prestosql</groupId>\n+            <artifactId>presto-tpch</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.prestosql</groupId>\n+            <artifactId>presto-tests</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.testcontainers</groupId>\n+            <artifactId>testcontainers</artifactId>\n+            <scope>test</scope>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>org.slf4j</groupId>\n+                    <artifactId>jcl-over-slf4j</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.prestosql</groupId>\n+            <artifactId>presto-testing</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.airlift</groupId>\n+            <artifactId>testing</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>javax.ws.rs</groupId>\n+            <artifactId>javax.ws.rs-api</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.prestosql</groupId>\n+            <artifactId>presto-product-tests-launcher</artifactId>\n+            <version>336-SNAPSHOT</version>\n+            <scope>test</scope>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyNTAyNA==", "bodyText": "Shouldn't this be false by default, as this affects query results?", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441525024", "createdAt": "2020-06-17T12:59:35Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidConfig.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.airlift.configuration.Config;\n+\n+public class DruidConfig\n+{\n+    private boolean useApproximateCountDistinct = true;\n+    private boolean useApproximateTopN = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyNTA5Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public boolean getUseApproxCountDistinct()\n          \n          \n            \n                public boolean isUseApproxCountDistinct()", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441525092", "createdAt": "2020-06-17T12:59:42Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidConfig.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.airlift.configuration.Config;\n+\n+public class DruidConfig\n+{\n+    private boolean useApproximateCountDistinct = true;\n+    private boolean useApproximateTopN = true;\n+\n+    public boolean getUseApproxCountDistinct()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyNTE2Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public boolean getUseApproxTopN()\n          \n          \n            \n                public boolean isUseApproxTopN()", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441525162", "createdAt": "2020-06-17T12:59:49Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidConfig.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.airlift.configuration.Config;\n+\n+public class DruidConfig\n+{\n+    private boolean useApproximateCountDistinct = true;\n+    private boolean useApproximateTopN = true;\n+\n+    public boolean getUseApproxCountDistinct()\n+    {\n+        return useApproximateCountDistinct;\n+    }\n+\n+    @Config(\"druid.use-approx-count-distinct\")\n+    public DruidConfig setUseApproxCountDistinct(boolean useApproxCountDistinct)\n+    {\n+        this.useApproximateCountDistinct = useApproxCountDistinct;\n+        return this;\n+    }\n+\n+    public boolean getUseApproxTopN()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyNTI5MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                @Config(\"druid.use-approx-topn\")\n          \n          \n            \n                @Config(\"druid.use-approximate-top-n\")", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441525290", "createdAt": "2020-06-17T13:00:03Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidConfig.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.airlift.configuration.Config;\n+\n+public class DruidConfig\n+{\n+    private boolean useApproximateCountDistinct = true;\n+    private boolean useApproximateTopN = true;\n+\n+    public boolean getUseApproxCountDistinct()\n+    {\n+        return useApproximateCountDistinct;\n+    }\n+\n+    @Config(\"druid.use-approx-count-distinct\")\n+    public DruidConfig setUseApproxCountDistinct(boolean useApproxCountDistinct)\n+    {\n+        this.useApproximateCountDistinct = useApproxCountDistinct;\n+        return this;\n+    }\n+\n+    public boolean getUseApproxTopN()\n+    {\n+        return useApproximateTopN;\n+    }\n+\n+    @Config(\"druid.use-approx-topn\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyNTgxNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                @Config(\"druid.use-approx-count-distinct\")\n          \n          \n            \n                @Config(\"druid.use-approximate-count-distinct\")\n          \n      \n    \n    \n  \n\nspell out approximate (especially that druid jdbc config also does not use abbrev)", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441525814", "createdAt": "2020-06-17T13:00:50Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidConfig.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.airlift.configuration.Config;\n+\n+public class DruidConfig\n+{\n+    private boolean useApproximateCountDistinct = true;\n+    private boolean useApproximateTopN = true;\n+\n+    public boolean getUseApproxCountDistinct()\n+    {\n+        return useApproximateCountDistinct;\n+    }\n+\n+    @Config(\"druid.use-approx-count-distinct\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyNjYwMA==", "bodyText": "Thanks, these are helpful comments!", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441526600", "createdAt": "2020-06-17T13:02:02Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcOutputTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.plugin.jdbc.QueryBuilder;\n+import io.prestosql.plugin.jdbc.UnsupportedTypeHandling;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.plugin.jdbc.TypeHandlingJdbcSessionProperties.getUnsupportedTypeHandling;\n+import static io.prestosql.plugin.jdbc.UnsupportedTypeHandling.IGNORE;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.lang.String.format;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    // Druid maintains its datasources related metadata by setting the catalog name as \"druid\"\n+    // Note that while a user may name the catalog name as something else, metadata queries made\n+    // to druid will always have the TABLE_CATALOG set to DRUID_CATALOG\n+    private static final String DRUID_CATALOG = \"druid\";\n+    // All the datasources in Druid are created under schema \"druid\"\n+    public static final String DRUID_SCHEMA = \"druid\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyNzg4OA==", "bodyText": "It looks like this doesn't have to be overridden.", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441527888", "createdAt": "2020-06-17T13:04:08Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcOutputTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.plugin.jdbc.QueryBuilder;\n+import io.prestosql.plugin.jdbc.UnsupportedTypeHandling;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.plugin.jdbc.TypeHandlingJdbcSessionProperties.getUnsupportedTypeHandling;\n+import static io.prestosql.plugin.jdbc.UnsupportedTypeHandling.IGNORE;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.lang.String.format;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    // Druid maintains its datasources related metadata by setting the catalog name as \"druid\"\n+    // Note that while a user may name the catalog name as something else, metadata queries made\n+    // to druid will always have the TABLE_CATALOG set to DRUID_CATALOG\n+    private static final String DRUID_CATALOG = \"druid\";\n+    // All the datasources in Druid are created under schema \"druid\"\n+    public static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyOTI1MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                /**\n          \n          \n            \n                 * Overriden to filter out tables that don't match @param schemaTableName\n          \n          \n            \n                 */\n          \n          \n            \n                // Overridden to filter out tables that don't match schemaTableName\n          \n      \n    \n    \n  \n\n(typoe, plus it's not a javadoc)", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441529250", "createdAt": "2020-06-17T13:06:10Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcOutputTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.plugin.jdbc.QueryBuilder;\n+import io.prestosql.plugin.jdbc.UnsupportedTypeHandling;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.plugin.jdbc.TypeHandlingJdbcSessionProperties.getUnsupportedTypeHandling;\n+import static io.prestosql.plugin.jdbc.UnsupportedTypeHandling.IGNORE;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.lang.String.format;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    // Druid maintains its datasources related metadata by setting the catalog name as \"druid\"\n+    // Note that while a user may name the catalog name as something else, metadata queries made\n+    // to druid will always have the TABLE_CATALOG set to DRUID_CATALOG\n+    private static final String DRUID_CATALOG = \"druid\";\n+    // All the datasources in Druid are created under schema \"druid\"\n+    public static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    protected Collection<String> listSchemas(Connection connection)\n+    {\n+        return ImmutableList.of(DRUID_SCHEMA);\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match @param schemaTableName\n+     */", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzMDI5Mg==", "bodyText": "the filtering should be applied regardless whether we have 1 candidate or many\n(if we have 1, it may be a wrong one)", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441530292", "createdAt": "2020-06-17T13:07:47Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcOutputTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.plugin.jdbc.QueryBuilder;\n+import io.prestosql.plugin.jdbc.UnsupportedTypeHandling;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.plugin.jdbc.TypeHandlingJdbcSessionProperties.getUnsupportedTypeHandling;\n+import static io.prestosql.plugin.jdbc.UnsupportedTypeHandling.IGNORE;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.lang.String.format;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    // Druid maintains its datasources related metadata by setting the catalog name as \"druid\"\n+    // Note that while a user may name the catalog name as something else, metadata queries made\n+    // to druid will always have the TABLE_CATALOG set to DRUID_CATALOG\n+    private static final String DRUID_CATALOG = \"druid\";\n+    // All the datasources in Druid are created under schema \"druid\"\n+    public static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    protected Collection<String> listSchemas(Connection connection)\n+    {\n+        return ImmutableList.of(DRUID_SCHEMA);\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match @param schemaTableName\n+     */\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzMDQ2NA==", "bodyText": "Suggested change", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441530464", "createdAt": "2020-06-17T13:08:03Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcOutputTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.plugin.jdbc.QueryBuilder;\n+import io.prestosql.plugin.jdbc.UnsupportedTypeHandling;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.plugin.jdbc.TypeHandlingJdbcSessionProperties.getUnsupportedTypeHandling;\n+import static io.prestosql.plugin.jdbc.UnsupportedTypeHandling.IGNORE;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.lang.String.format;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    // Druid maintains its datasources related metadata by setting the catalog name as \"druid\"\n+    // Note that while a user may name the catalog name as something else, metadata queries made\n+    // to druid will always have the TABLE_CATALOG set to DRUID_CATALOG\n+    private static final String DRUID_CATALOG = \"druid\";\n+    // All the datasources in Druid are created under schema \"druid\"\n+    public static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    protected Collection<String> listSchemas(Connection connection)\n+    {\n+        return ImmutableList.of(DRUID_SCHEMA);\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match @param schemaTableName\n+     */\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    return Optional.of(\n+                            getOnlyElement(\n+                                    tableHandles\n+                                            .stream()\n+                                            .filter(\n+                                                    jdbcTableHandle ->\n+                                                            Objects.equals(jdbcTableHandle.getSchemaName(), schemaTableName.getSchemaName())\n+                                                                    && Objects.equals(jdbcTableHandle.getTableName(), schemaTableName.getTableName()))\n+                                            .collect(Collectors.toList())));\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    /**\n+     * Overriden since the {@link BaseJdbcClient#getTables(Connection, Optional, Optional)}\n+     * method uses character escaping that doesn't work well with Druid's Avatica handler.\n+     * Unfortunately, because we can't escape search characters like '_' and '%\", this call\n+     * ends up retrieving metadata for all tables that match the search\n+     * pattern. For ex - LIKE some_table matches somertable, somextable and some_table.\n+     * <p>\n+     * See {@link DruidJdbcClient#getTableHandle(JdbcIdentity, SchemaTableName)} to look at\n+     * how tables are filtered.\n+     */\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzMTYzNw==", "bodyText": "Javadoc defines method behavior, here you explain technical choice. While this could go as @implNote, it is better to just turn this into /* comment\nalso, Overriden -> Overridden", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441531637", "createdAt": "2020-06-17T13:09:56Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcOutputTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.plugin.jdbc.QueryBuilder;\n+import io.prestosql.plugin.jdbc.UnsupportedTypeHandling;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.plugin.jdbc.TypeHandlingJdbcSessionProperties.getUnsupportedTypeHandling;\n+import static io.prestosql.plugin.jdbc.UnsupportedTypeHandling.IGNORE;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.lang.String.format;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    // Druid maintains its datasources related metadata by setting the catalog name as \"druid\"\n+    // Note that while a user may name the catalog name as something else, metadata queries made\n+    // to druid will always have the TABLE_CATALOG set to DRUID_CATALOG\n+    private static final String DRUID_CATALOG = \"druid\";\n+    // All the datasources in Druid are created under schema \"druid\"\n+    public static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    protected Collection<String> listSchemas(Connection connection)\n+    {\n+        return ImmutableList.of(DRUID_SCHEMA);\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match @param schemaTableName\n+     */\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    return Optional.of(\n+                            getOnlyElement(\n+                                    tableHandles\n+                                            .stream()\n+                                            .filter(\n+                                                    jdbcTableHandle ->\n+                                                            Objects.equals(jdbcTableHandle.getSchemaName(), schemaTableName.getSchemaName())\n+                                                                    && Objects.equals(jdbcTableHandle.getTableName(), schemaTableName.getTableName()))\n+                                            .collect(Collectors.toList())));\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    /**\n+     * Overriden since the {@link BaseJdbcClient#getTables(Connection, Optional, Optional)}", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzMjI3MA==", "bodyText": "move the default out of switch, as last line of the method", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441532270", "createdAt": "2020-06-17T13:10:51Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcOutputTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.plugin.jdbc.QueryBuilder;\n+import io.prestosql.plugin.jdbc.UnsupportedTypeHandling;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.plugin.jdbc.TypeHandlingJdbcSessionProperties.getUnsupportedTypeHandling;\n+import static io.prestosql.plugin.jdbc.UnsupportedTypeHandling.IGNORE;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.lang.String.format;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    // Druid maintains its datasources related metadata by setting the catalog name as \"druid\"\n+    // Note that while a user may name the catalog name as something else, metadata queries made\n+    // to druid will always have the TABLE_CATALOG set to DRUID_CATALOG\n+    private static final String DRUID_CATALOG = \"druid\";\n+    // All the datasources in Druid are created under schema \"druid\"\n+    public static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    protected Collection<String> listSchemas(Connection connection)\n+    {\n+        return ImmutableList.of(DRUID_SCHEMA);\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match @param schemaTableName\n+     */\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    return Optional.of(\n+                            getOnlyElement(\n+                                    tableHandles\n+                                            .stream()\n+                                            .filter(\n+                                                    jdbcTableHandle ->\n+                                                            Objects.equals(jdbcTableHandle.getSchemaName(), schemaTableName.getSchemaName())\n+                                                                    && Objects.equals(jdbcTableHandle.getTableName(), schemaTableName.getTableName()))\n+                                            .collect(Collectors.toList())));\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    /**\n+     * Overriden since the {@link BaseJdbcClient#getTables(Connection, Optional, Optional)}\n+     * method uses character escaping that doesn't work well with Druid's Avatica handler.\n+     * Unfortunately, because we can't escape search characters like '_' and '%\", this call\n+     * ends up retrieving metadata for all tables that match the search\n+     * pattern. For ex - LIKE some_table matches somertable, somextable and some_table.\n+     * <p>\n+     * See {@link DruidJdbcClient#getTableHandle(JdbcIdentity, SchemaTableName)} to look at\n+     * how tables are filtered.\n+     */\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,\n+                tableName.orElse(null),\n+                null);\n+    }\n+\n+    @Override\n+    public Optional<ColumnMapping> toPrestoType(ConnectorSession session, Connection connection, JdbcTypeHandle typeHandle)\n+    {\n+        switch (typeHandle.getJdbcType()) {\n+            case Types.VARCHAR:\n+                int columnSize = typeHandle.getColumnSize();\n+                if (columnSize > VarcharType.MAX_LENGTH || columnSize == -1) {\n+                    return Optional.of(varcharColumnMapping(createUnboundedVarcharType()));\n+                }\n+                return Optional.of(varcharColumnMapping(createVarcharType(columnSize)));\n+            default:\n+                return super.toPrestoType(session, connection, typeHandle);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzMzAxNw==", "bodyText": "why is \"\" schema OK here?\nMaybe just:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    checkArgument(schemaName.isEmpty() || schemaName.equals(\"druid\"), \"Only \\\"druid\\\" schema is supported\");\n          \n          \n            \n                    checkArgument(\"druid\".equals(schemaName), \"Only \\\"druid\\\" schema is supported\");", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441533017", "createdAt": "2020-06-17T13:12:00Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcOutputTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.plugin.jdbc.QueryBuilder;\n+import io.prestosql.plugin.jdbc.UnsupportedTypeHandling;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.plugin.jdbc.TypeHandlingJdbcSessionProperties.getUnsupportedTypeHandling;\n+import static io.prestosql.plugin.jdbc.UnsupportedTypeHandling.IGNORE;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.lang.String.format;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    // Druid maintains its datasources related metadata by setting the catalog name as \"druid\"\n+    // Note that while a user may name the catalog name as something else, metadata queries made\n+    // to druid will always have the TABLE_CATALOG set to DRUID_CATALOG\n+    private static final String DRUID_CATALOG = \"druid\";\n+    // All the datasources in Druid are created under schema \"druid\"\n+    public static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    protected Collection<String> listSchemas(Connection connection)\n+    {\n+        return ImmutableList.of(DRUID_SCHEMA);\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match @param schemaTableName\n+     */\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    return Optional.of(\n+                            getOnlyElement(\n+                                    tableHandles\n+                                            .stream()\n+                                            .filter(\n+                                                    jdbcTableHandle ->\n+                                                            Objects.equals(jdbcTableHandle.getSchemaName(), schemaTableName.getSchemaName())\n+                                                                    && Objects.equals(jdbcTableHandle.getTableName(), schemaTableName.getTableName()))\n+                                            .collect(Collectors.toList())));\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    /**\n+     * Overriden since the {@link BaseJdbcClient#getTables(Connection, Optional, Optional)}\n+     * method uses character escaping that doesn't work well with Druid's Avatica handler.\n+     * Unfortunately, because we can't escape search characters like '_' and '%\", this call\n+     * ends up retrieving metadata for all tables that match the search\n+     * pattern. For ex - LIKE some_table matches somertable, somextable and some_table.\n+     * <p>\n+     * See {@link DruidJdbcClient#getTableHandle(JdbcIdentity, SchemaTableName)} to look at\n+     * how tables are filtered.\n+     */\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,\n+                tableName.orElse(null),\n+                null);\n+    }\n+\n+    @Override\n+    public Optional<ColumnMapping> toPrestoType(ConnectorSession session, Connection connection, JdbcTypeHandle typeHandle)\n+    {\n+        switch (typeHandle.getJdbcType()) {\n+            case Types.VARCHAR:\n+                int columnSize = typeHandle.getColumnSize();\n+                if (columnSize > VarcharType.MAX_LENGTH || columnSize == -1) {\n+                    return Optional.of(varcharColumnMapping(createUnboundedVarcharType()));\n+                }\n+                return Optional.of(varcharColumnMapping(createVarcharType(columnSize)));\n+            default:\n+                return super.toPrestoType(session, connection, typeHandle);\n+        }\n+    }\n+\n+    /**\n+     * Druid doesn't like table names to be qualified with catalog names in the SQL query.\n+     * Hence, overriding this method to pass catalog as an empty string.\n+     */\n+    @Override\n+    public PreparedStatement buildSql(ConnectorSession session, Connection connection, JdbcSplit split, JdbcTableHandle table, List<JdbcColumnHandle> columns)\n+            throws SQLException\n+    {\n+        String schemaName = table.getSchemaName();\n+        checkArgument(schemaName.isEmpty() || schemaName.equals(\"druid\"), \"Only \\\"druid\\\" schema is supported\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 193}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzMzMwNQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                /**\n          \n          \n            \n                 * Druid doesn't like table names to be qualified with catalog names in the SQL query.\n          \n          \n            \n                 * Hence, overriding this method to pass catalog as an empty string.\n          \n          \n            \n                 */\n          \n          \n            \n                // Druid doesn't like table names to be qualified with catalog names in the SQL query.\n          \n          \n            \n                // Hence, overriding this method to pass catalog as an empty string.", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441533305", "createdAt": "2020-06-17T13:12:27Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcOutputTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.plugin.jdbc.QueryBuilder;\n+import io.prestosql.plugin.jdbc.UnsupportedTypeHandling;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.plugin.jdbc.TypeHandlingJdbcSessionProperties.getUnsupportedTypeHandling;\n+import static io.prestosql.plugin.jdbc.UnsupportedTypeHandling.IGNORE;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.lang.String.format;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    // Druid maintains its datasources related metadata by setting the catalog name as \"druid\"\n+    // Note that while a user may name the catalog name as something else, metadata queries made\n+    // to druid will always have the TABLE_CATALOG set to DRUID_CATALOG\n+    private static final String DRUID_CATALOG = \"druid\";\n+    // All the datasources in Druid are created under schema \"druid\"\n+    public static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    protected Collection<String> listSchemas(Connection connection)\n+    {\n+        return ImmutableList.of(DRUID_SCHEMA);\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match @param schemaTableName\n+     */\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    return Optional.of(\n+                            getOnlyElement(\n+                                    tableHandles\n+                                            .stream()\n+                                            .filter(\n+                                                    jdbcTableHandle ->\n+                                                            Objects.equals(jdbcTableHandle.getSchemaName(), schemaTableName.getSchemaName())\n+                                                                    && Objects.equals(jdbcTableHandle.getTableName(), schemaTableName.getTableName()))\n+                                            .collect(Collectors.toList())));\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    /**\n+     * Overriden since the {@link BaseJdbcClient#getTables(Connection, Optional, Optional)}\n+     * method uses character escaping that doesn't work well with Druid's Avatica handler.\n+     * Unfortunately, because we can't escape search characters like '_' and '%\", this call\n+     * ends up retrieving metadata for all tables that match the search\n+     * pattern. For ex - LIKE some_table matches somertable, somextable and some_table.\n+     * <p>\n+     * See {@link DruidJdbcClient#getTableHandle(JdbcIdentity, SchemaTableName)} to look at\n+     * how tables are filtered.\n+     */\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,\n+                tableName.orElse(null),\n+                null);\n+    }\n+\n+    @Override\n+    public Optional<ColumnMapping> toPrestoType(ConnectorSession session, Connection connection, JdbcTypeHandle typeHandle)\n+    {\n+        switch (typeHandle.getJdbcType()) {\n+            case Types.VARCHAR:\n+                int columnSize = typeHandle.getColumnSize();\n+                if (columnSize > VarcharType.MAX_LENGTH || columnSize == -1) {\n+                    return Optional.of(varcharColumnMapping(createUnboundedVarcharType()));\n+                }\n+                return Optional.of(varcharColumnMapping(createVarcharType(columnSize)));\n+            default:\n+                return super.toPrestoType(session, connection, typeHandle);\n+        }\n+    }\n+\n+    /**\n+     * Druid doesn't like table names to be qualified with catalog names in the SQL query.\n+     * Hence, overriding this method to pass catalog as an empty string.\n+     */", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzMzk5NQ==", "bodyText": "i think null would be more appriopriate\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    \"\",\n          \n          \n            \n                                    null,\n          \n      \n    \n    \n  \n\n(please also update method comment)", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441533995", "createdAt": "2020-06-17T13:13:31Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcOutputTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.plugin.jdbc.QueryBuilder;\n+import io.prestosql.plugin.jdbc.UnsupportedTypeHandling;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.plugin.jdbc.TypeHandlingJdbcSessionProperties.getUnsupportedTypeHandling;\n+import static io.prestosql.plugin.jdbc.UnsupportedTypeHandling.IGNORE;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.lang.String.format;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    // Druid maintains its datasources related metadata by setting the catalog name as \"druid\"\n+    // Note that while a user may name the catalog name as something else, metadata queries made\n+    // to druid will always have the TABLE_CATALOG set to DRUID_CATALOG\n+    private static final String DRUID_CATALOG = \"druid\";\n+    // All the datasources in Druid are created under schema \"druid\"\n+    public static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    protected Collection<String> listSchemas(Connection connection)\n+    {\n+        return ImmutableList.of(DRUID_SCHEMA);\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match @param schemaTableName\n+     */\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    return Optional.of(\n+                            getOnlyElement(\n+                                    tableHandles\n+                                            .stream()\n+                                            .filter(\n+                                                    jdbcTableHandle ->\n+                                                            Objects.equals(jdbcTableHandle.getSchemaName(), schemaTableName.getSchemaName())\n+                                                                    && Objects.equals(jdbcTableHandle.getTableName(), schemaTableName.getTableName()))\n+                                            .collect(Collectors.toList())));\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    /**\n+     * Overriden since the {@link BaseJdbcClient#getTables(Connection, Optional, Optional)}\n+     * method uses character escaping that doesn't work well with Druid's Avatica handler.\n+     * Unfortunately, because we can't escape search characters like '_' and '%\", this call\n+     * ends up retrieving metadata for all tables that match the search\n+     * pattern. For ex - LIKE some_table matches somertable, somextable and some_table.\n+     * <p>\n+     * See {@link DruidJdbcClient#getTableHandle(JdbcIdentity, SchemaTableName)} to look at\n+     * how tables are filtered.\n+     */\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,\n+                tableName.orElse(null),\n+                null);\n+    }\n+\n+    @Override\n+    public Optional<ColumnMapping> toPrestoType(ConnectorSession session, Connection connection, JdbcTypeHandle typeHandle)\n+    {\n+        switch (typeHandle.getJdbcType()) {\n+            case Types.VARCHAR:\n+                int columnSize = typeHandle.getColumnSize();\n+                if (columnSize > VarcharType.MAX_LENGTH || columnSize == -1) {\n+                    return Optional.of(varcharColumnMapping(createUnboundedVarcharType()));\n+                }\n+                return Optional.of(varcharColumnMapping(createVarcharType(columnSize)));\n+            default:\n+                return super.toPrestoType(session, connection, typeHandle);\n+        }\n+    }\n+\n+    /**\n+     * Druid doesn't like table names to be qualified with catalog names in the SQL query.\n+     * Hence, overriding this method to pass catalog as an empty string.\n+     */\n+    @Override\n+    public PreparedStatement buildSql(ConnectorSession session, Connection connection, JdbcSplit split, JdbcTableHandle table, List<JdbcColumnHandle> columns)\n+            throws SQLException\n+    {\n+        String schemaName = table.getSchemaName();\n+        checkArgument(schemaName.isEmpty() || schemaName.equals(\"druid\"), \"Only \\\"druid\\\" schema is supported\");\n+        return new QueryBuilder(identifierQuote)\n+                .buildSql(\n+                        this,\n+                        session,\n+                        connection,\n+                        \"\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzNDE4Nw==", "bodyText": "same here", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441534187", "createdAt": "2020-06-17T13:13:48Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcOutputTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.plugin.jdbc.QueryBuilder;\n+import io.prestosql.plugin.jdbc.UnsupportedTypeHandling;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.plugin.jdbc.TypeHandlingJdbcSessionProperties.getUnsupportedTypeHandling;\n+import static io.prestosql.plugin.jdbc.UnsupportedTypeHandling.IGNORE;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.lang.String.format;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    // Druid maintains its datasources related metadata by setting the catalog name as \"druid\"\n+    // Note that while a user may name the catalog name as something else, metadata queries made\n+    // to druid will always have the TABLE_CATALOG set to DRUID_CATALOG\n+    private static final String DRUID_CATALOG = \"druid\";\n+    // All the datasources in Druid are created under schema \"druid\"\n+    public static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    protected Collection<String> listSchemas(Connection connection)\n+    {\n+        return ImmutableList.of(DRUID_SCHEMA);\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match @param schemaTableName\n+     */\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    return Optional.of(\n+                            getOnlyElement(\n+                                    tableHandles\n+                                            .stream()\n+                                            .filter(\n+                                                    jdbcTableHandle ->\n+                                                            Objects.equals(jdbcTableHandle.getSchemaName(), schemaTableName.getSchemaName())\n+                                                                    && Objects.equals(jdbcTableHandle.getTableName(), schemaTableName.getTableName()))\n+                                            .collect(Collectors.toList())));\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    /**\n+     * Overriden since the {@link BaseJdbcClient#getTables(Connection, Optional, Optional)}\n+     * method uses character escaping that doesn't work well with Druid's Avatica handler.\n+     * Unfortunately, because we can't escape search characters like '_' and '%\", this call\n+     * ends up retrieving metadata for all tables that match the search\n+     * pattern. For ex - LIKE some_table matches somertable, somextable and some_table.\n+     * <p>\n+     * See {@link DruidJdbcClient#getTableHandle(JdbcIdentity, SchemaTableName)} to look at\n+     * how tables are filtered.\n+     */\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,\n+                tableName.orElse(null),\n+                null);\n+    }\n+\n+    @Override\n+    public Optional<ColumnMapping> toPrestoType(ConnectorSession session, Connection connection, JdbcTypeHandle typeHandle)\n+    {\n+        switch (typeHandle.getJdbcType()) {\n+            case Types.VARCHAR:\n+                int columnSize = typeHandle.getColumnSize();\n+                if (columnSize > VarcharType.MAX_LENGTH || columnSize == -1) {\n+                    return Optional.of(varcharColumnMapping(createUnboundedVarcharType()));\n+                }\n+                return Optional.of(varcharColumnMapping(createVarcharType(columnSize)));\n+            default:\n+                return super.toPrestoType(session, connection, typeHandle);\n+        }\n+    }\n+\n+    /**\n+     * Druid doesn't like table names to be qualified with catalog names in the SQL query.\n+     * Hence, overriding this method to pass catalog as an empty string.\n+     */\n+    @Override\n+    public PreparedStatement buildSql(ConnectorSession session, Connection connection, JdbcSplit split, JdbcTableHandle table, List<JdbcColumnHandle> columns)\n+            throws SQLException\n+    {\n+        String schemaName = table.getSchemaName();\n+        checkArgument(schemaName.isEmpty() || schemaName.equals(\"druid\"), \"Only \\\"druid\\\" schema is supported\");\n+        return new QueryBuilder(identifierQuote)\n+                .buildSql(\n+                        this,\n+                        session,\n+                        connection,\n+                        \"\",\n+                        schemaName,\n+                        table.getTableName(),\n+                        columns,\n+                        table.getConstraint(),\n+                        split.getAdditionalPredicate(),\n+                        tryApplyLimit(table.getLimit()));\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match the @param tableHandle", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 209}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzNDgzNg==", "bodyText": "Is this the only thing being added here?\nIf we added this in BaseJdbcClient, would it allow us not to override this (lengthy) method?", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441534836", "createdAt": "2020-06-17T13:14:47Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcOutputTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.plugin.jdbc.QueryBuilder;\n+import io.prestosql.plugin.jdbc.UnsupportedTypeHandling;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.plugin.jdbc.TypeHandlingJdbcSessionProperties.getUnsupportedTypeHandling;\n+import static io.prestosql.plugin.jdbc.UnsupportedTypeHandling.IGNORE;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.lang.String.format;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    // Druid maintains its datasources related metadata by setting the catalog name as \"druid\"\n+    // Note that while a user may name the catalog name as something else, metadata queries made\n+    // to druid will always have the TABLE_CATALOG set to DRUID_CATALOG\n+    private static final String DRUID_CATALOG = \"druid\";\n+    // All the datasources in Druid are created under schema \"druid\"\n+    public static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    protected Collection<String> listSchemas(Connection connection)\n+    {\n+        return ImmutableList.of(DRUID_SCHEMA);\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match @param schemaTableName\n+     */\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    return Optional.of(\n+                            getOnlyElement(\n+                                    tableHandles\n+                                            .stream()\n+                                            .filter(\n+                                                    jdbcTableHandle ->\n+                                                            Objects.equals(jdbcTableHandle.getSchemaName(), schemaTableName.getSchemaName())\n+                                                                    && Objects.equals(jdbcTableHandle.getTableName(), schemaTableName.getTableName()))\n+                                            .collect(Collectors.toList())));\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    /**\n+     * Overriden since the {@link BaseJdbcClient#getTables(Connection, Optional, Optional)}\n+     * method uses character escaping that doesn't work well with Druid's Avatica handler.\n+     * Unfortunately, because we can't escape search characters like '_' and '%\", this call\n+     * ends up retrieving metadata for all tables that match the search\n+     * pattern. For ex - LIKE some_table matches somertable, somextable and some_table.\n+     * <p>\n+     * See {@link DruidJdbcClient#getTableHandle(JdbcIdentity, SchemaTableName)} to look at\n+     * how tables are filtered.\n+     */\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,\n+                tableName.orElse(null),\n+                null);\n+    }\n+\n+    @Override\n+    public Optional<ColumnMapping> toPrestoType(ConnectorSession session, Connection connection, JdbcTypeHandle typeHandle)\n+    {\n+        switch (typeHandle.getJdbcType()) {\n+            case Types.VARCHAR:\n+                int columnSize = typeHandle.getColumnSize();\n+                if (columnSize > VarcharType.MAX_LENGTH || columnSize == -1) {\n+                    return Optional.of(varcharColumnMapping(createUnboundedVarcharType()));\n+                }\n+                return Optional.of(varcharColumnMapping(createVarcharType(columnSize)));\n+            default:\n+                return super.toPrestoType(session, connection, typeHandle);\n+        }\n+    }\n+\n+    /**\n+     * Druid doesn't like table names to be qualified with catalog names in the SQL query.\n+     * Hence, overriding this method to pass catalog as an empty string.\n+     */\n+    @Override\n+    public PreparedStatement buildSql(ConnectorSession session, Connection connection, JdbcSplit split, JdbcTableHandle table, List<JdbcColumnHandle> columns)\n+            throws SQLException\n+    {\n+        String schemaName = table.getSchemaName();\n+        checkArgument(schemaName.isEmpty() || schemaName.equals(\"druid\"), \"Only \\\"druid\\\" schema is supported\");\n+        return new QueryBuilder(identifierQuote)\n+                .buildSql(\n+                        this,\n+                        session,\n+                        connection,\n+                        \"\",\n+                        schemaName,\n+                        table.getTableName(),\n+                        columns,\n+                        table.getConstraint(),\n+                        split.getAdditionalPredicate(),\n+                        tryApplyLimit(table.getLimit()));\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match the @param tableHandle\n+     * <p>\n+     * See {@link #getColumns(JdbcTableHandle, DatabaseMetaData)}\n+     */\n+    @Override\n+    public List<JdbcColumnHandle> getColumns(ConnectorSession session, JdbcTableHandle tableHandle)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(JdbcIdentity.from(session));\n+                ResultSet resultSet = getColumns(tableHandle, connection.getMetaData())) {\n+            int allColumns = 0;\n+            List<JdbcColumnHandle> columns = new ArrayList<>();\n+            while (resultSet.next()) {\n+                // skip if table doesn't match expected\n+                if (!(Objects.equals(tableHandle.getCatalogName(), resultSet.getString(\"TABLE_CAT\"))\n+                        && Objects.equals(tableHandle.getSchemaName(), resultSet.getString(\"TABLE_SCHEM\"))\n+                        && Objects.equals(tableHandle.getTableName(), resultSet.getString(\"TABLE_NAME\")))) {\n+                    continue;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzNTQyMQ==", "bodyText": "PostgreSQL -> ...\ni think this my also happen when table is removed concurrently, so just remove the comment and leave the check & throw as is", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441535421", "createdAt": "2020-06-17T13:15:39Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcOutputTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.plugin.jdbc.QueryBuilder;\n+import io.prestosql.plugin.jdbc.UnsupportedTypeHandling;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.plugin.jdbc.TypeHandlingJdbcSessionProperties.getUnsupportedTypeHandling;\n+import static io.prestosql.plugin.jdbc.UnsupportedTypeHandling.IGNORE;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.lang.String.format;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    // Druid maintains its datasources related metadata by setting the catalog name as \"druid\"\n+    // Note that while a user may name the catalog name as something else, metadata queries made\n+    // to druid will always have the TABLE_CATALOG set to DRUID_CATALOG\n+    private static final String DRUID_CATALOG = \"druid\";\n+    // All the datasources in Druid are created under schema \"druid\"\n+    public static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    protected Collection<String> listSchemas(Connection connection)\n+    {\n+        return ImmutableList.of(DRUID_SCHEMA);\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match @param schemaTableName\n+     */\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    return Optional.of(\n+                            getOnlyElement(\n+                                    tableHandles\n+                                            .stream()\n+                                            .filter(\n+                                                    jdbcTableHandle ->\n+                                                            Objects.equals(jdbcTableHandle.getSchemaName(), schemaTableName.getSchemaName())\n+                                                                    && Objects.equals(jdbcTableHandle.getTableName(), schemaTableName.getTableName()))\n+                                            .collect(Collectors.toList())));\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    /**\n+     * Overriden since the {@link BaseJdbcClient#getTables(Connection, Optional, Optional)}\n+     * method uses character escaping that doesn't work well with Druid's Avatica handler.\n+     * Unfortunately, because we can't escape search characters like '_' and '%\", this call\n+     * ends up retrieving metadata for all tables that match the search\n+     * pattern. For ex - LIKE some_table matches somertable, somextable and some_table.\n+     * <p>\n+     * See {@link DruidJdbcClient#getTableHandle(JdbcIdentity, SchemaTableName)} to look at\n+     * how tables are filtered.\n+     */\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,\n+                tableName.orElse(null),\n+                null);\n+    }\n+\n+    @Override\n+    public Optional<ColumnMapping> toPrestoType(ConnectorSession session, Connection connection, JdbcTypeHandle typeHandle)\n+    {\n+        switch (typeHandle.getJdbcType()) {\n+            case Types.VARCHAR:\n+                int columnSize = typeHandle.getColumnSize();\n+                if (columnSize > VarcharType.MAX_LENGTH || columnSize == -1) {\n+                    return Optional.of(varcharColumnMapping(createUnboundedVarcharType()));\n+                }\n+                return Optional.of(varcharColumnMapping(createVarcharType(columnSize)));\n+            default:\n+                return super.toPrestoType(session, connection, typeHandle);\n+        }\n+    }\n+\n+    /**\n+     * Druid doesn't like table names to be qualified with catalog names in the SQL query.\n+     * Hence, overriding this method to pass catalog as an empty string.\n+     */\n+    @Override\n+    public PreparedStatement buildSql(ConnectorSession session, Connection connection, JdbcSplit split, JdbcTableHandle table, List<JdbcColumnHandle> columns)\n+            throws SQLException\n+    {\n+        String schemaName = table.getSchemaName();\n+        checkArgument(schemaName.isEmpty() || schemaName.equals(\"druid\"), \"Only \\\"druid\\\" schema is supported\");\n+        return new QueryBuilder(identifierQuote)\n+                .buildSql(\n+                        this,\n+                        session,\n+                        connection,\n+                        \"\",\n+                        schemaName,\n+                        table.getTableName(),\n+                        columns,\n+                        table.getConstraint(),\n+                        split.getAdditionalPredicate(),\n+                        tryApplyLimit(table.getLimit()));\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match the @param tableHandle\n+     * <p>\n+     * See {@link #getColumns(JdbcTableHandle, DatabaseMetaData)}\n+     */\n+    @Override\n+    public List<JdbcColumnHandle> getColumns(ConnectorSession session, JdbcTableHandle tableHandle)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(JdbcIdentity.from(session));\n+                ResultSet resultSet = getColumns(tableHandle, connection.getMetaData())) {\n+            int allColumns = 0;\n+            List<JdbcColumnHandle> columns = new ArrayList<>();\n+            while (resultSet.next()) {\n+                // skip if table doesn't match expected\n+                if (!(Objects.equals(tableHandle.getCatalogName(), resultSet.getString(\"TABLE_CAT\"))\n+                        && Objects.equals(tableHandle.getSchemaName(), resultSet.getString(\"TABLE_SCHEM\"))\n+                        && Objects.equals(tableHandle.getTableName(), resultSet.getString(\"TABLE_NAME\")))) {\n+                    continue;\n+                }\n+                allColumns++;\n+                String columnName = resultSet.getString(\"COLUMN_NAME\");\n+                JdbcTypeHandle typeHandle = new JdbcTypeHandle(\n+                        resultSet.getInt(\"DATA_TYPE\"),\n+                        Optional.ofNullable(resultSet.getString(\"TYPE_NAME\")),\n+                        resultSet.getInt(\"COLUMN_SIZE\"),\n+                        resultSet.getInt(\"DECIMAL_DIGITS\"),\n+                        Optional.empty());\n+                Optional<ColumnMapping> columnMapping = toPrestoType(session, connection, typeHandle);\n+                // skip unsupported column types\n+                boolean nullable = (resultSet.getInt(\"NULLABLE\") != columnNoNulls);\n+                // Note: some databases (e.g. SQL Server) do not return column remarks/comment here.\n+                Optional<String> comment = Optional.ofNullable(emptyToNull(resultSet.getString(\"REMARKS\")));\n+                if (columnMapping.isPresent()) {\n+                    columns.add(JdbcColumnHandle.builder()\n+                            .setColumnName(columnName)\n+                            .setJdbcTypeHandle(typeHandle)\n+                            .setColumnType(columnMapping.get().getType())\n+                            .setNullable(nullable)\n+                            .setComment(comment)\n+                            .build());\n+                }\n+                if (columnMapping.isEmpty()) {\n+                    UnsupportedTypeHandling unsupportedTypeHandling = getUnsupportedTypeHandling(session);\n+                    verify(unsupportedTypeHandling == IGNORE, \"Unsupported type handling is set to %s, but toPrestoType() returned empty\", unsupportedTypeHandling);\n+                }\n+            }\n+            if (columns.isEmpty()) {\n+                // A table may have no supported columns. In rare cases (e.g. PostgreSQL) a table might have no columns at all.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 255}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzNTU3Ng==", "bodyText": "same", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441535576", "createdAt": "2020-06-17T13:15:50Z", "author": {"login": "findepi"}, "path": "presto-druid/src/main/java/io/prestosql/plugin/druid/DruidJdbcClient.java", "diffHunk": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.jdbc.BaseJdbcClient;\n+import io.prestosql.plugin.jdbc.BaseJdbcConfig;\n+import io.prestosql.plugin.jdbc.ColumnMapping;\n+import io.prestosql.plugin.jdbc.ConnectionFactory;\n+import io.prestosql.plugin.jdbc.JdbcColumnHandle;\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcOutputTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcSplit;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.plugin.jdbc.JdbcTypeHandle;\n+import io.prestosql.plugin.jdbc.QueryBuilder;\n+import io.prestosql.plugin.jdbc.UnsupportedTypeHandling;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.ConnectorTableMetadata;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.connector.TableNotFoundException;\n+import io.prestosql.spi.type.VarcharType;\n+\n+import javax.inject.Inject;\n+\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.jdbc.JdbcErrorCode.JDBC_ERROR;\n+import static io.prestosql.plugin.jdbc.StandardColumnMappings.varcharColumnMapping;\n+import static io.prestosql.plugin.jdbc.TypeHandlingJdbcSessionProperties.getUnsupportedTypeHandling;\n+import static io.prestosql.plugin.jdbc.UnsupportedTypeHandling.IGNORE;\n+import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n+import static io.prestosql.spi.type.VarcharType.createVarcharType;\n+import static java.lang.String.format;\n+import static java.sql.DatabaseMetaData.columnNoNulls;\n+\n+public class DruidJdbcClient\n+        extends BaseJdbcClient\n+{\n+    // Druid maintains its datasources related metadata by setting the catalog name as \"druid\"\n+    // Note that while a user may name the catalog name as something else, metadata queries made\n+    // to druid will always have the TABLE_CATALOG set to DRUID_CATALOG\n+    private static final String DRUID_CATALOG = \"druid\";\n+    // All the datasources in Druid are created under schema \"druid\"\n+    public static final String DRUID_SCHEMA = \"druid\";\n+\n+    @Inject\n+    public DruidJdbcClient(BaseJdbcConfig config, ConnectionFactory connectionFactory)\n+            throws SQLException\n+    {\n+        super(config, \"\\\"\", connectionFactory);\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> getTableNames(JdbcIdentity identity, Optional<String> schema)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            try (ResultSet resultSet = getTables(connection, schema, Optional.empty())) {\n+                ImmutableList.Builder<SchemaTableName> list = ImmutableList.builder();\n+                while (resultSet.next()) {\n+                    list.add(new SchemaTableName(\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                return list.build();\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    protected Collection<String> listSchemas(Connection connection)\n+    {\n+        return ImmutableList.of(DRUID_SCHEMA);\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match @param schemaTableName\n+     */\n+    @Override\n+    public Optional<JdbcTableHandle> getTableHandle(JdbcIdentity identity, SchemaTableName schemaTableName)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(identity)) {\n+            String jdbcSchemaName = schemaTableName.getSchemaName();\n+            String jdbcTableName = schemaTableName.getTableName();\n+            try (ResultSet resultSet = getTables(connection, Optional.of(jdbcSchemaName), Optional.of(jdbcTableName))) {\n+                List<JdbcTableHandle> tableHandles = new ArrayList<>();\n+                while (resultSet.next()) {\n+                    tableHandles.add(new JdbcTableHandle(\n+                            schemaTableName,\n+                            DRUID_CATALOG,\n+                            resultSet.getString(\"TABLE_SCHEM\"),\n+                            resultSet.getString(\"TABLE_NAME\")));\n+                }\n+                if (tableHandles.isEmpty()) {\n+                    return Optional.empty();\n+                }\n+                if (tableHandles.size() > 1) {\n+                    return Optional.of(\n+                            getOnlyElement(\n+                                    tableHandles\n+                                            .stream()\n+                                            .filter(\n+                                                    jdbcTableHandle ->\n+                                                            Objects.equals(jdbcTableHandle.getSchemaName(), schemaTableName.getSchemaName())\n+                                                                    && Objects.equals(jdbcTableHandle.getTableName(), schemaTableName.getTableName()))\n+                                            .collect(Collectors.toList())));\n+                }\n+                return Optional.of(getOnlyElement(tableHandles));\n+            }\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    /**\n+     * Overriden since the {@link BaseJdbcClient#getTables(Connection, Optional, Optional)}\n+     * method uses character escaping that doesn't work well with Druid's Avatica handler.\n+     * Unfortunately, because we can't escape search characters like '_' and '%\", this call\n+     * ends up retrieving metadata for all tables that match the search\n+     * pattern. For ex - LIKE some_table matches somertable, somextable and some_table.\n+     * <p>\n+     * See {@link DruidJdbcClient#getTableHandle(JdbcIdentity, SchemaTableName)} to look at\n+     * how tables are filtered.\n+     */\n+\n+    @Override\n+    protected ResultSet getTables(Connection connection, Optional<String> schemaName, Optional<String> tableName)\n+            throws SQLException\n+    {\n+        DatabaseMetaData metadata = connection.getMetaData();\n+        return metadata.getTables(DRUID_CATALOG,\n+                DRUID_SCHEMA,\n+                tableName.orElse(null),\n+                null);\n+    }\n+\n+    @Override\n+    public Optional<ColumnMapping> toPrestoType(ConnectorSession session, Connection connection, JdbcTypeHandle typeHandle)\n+    {\n+        switch (typeHandle.getJdbcType()) {\n+            case Types.VARCHAR:\n+                int columnSize = typeHandle.getColumnSize();\n+                if (columnSize > VarcharType.MAX_LENGTH || columnSize == -1) {\n+                    return Optional.of(varcharColumnMapping(createUnboundedVarcharType()));\n+                }\n+                return Optional.of(varcharColumnMapping(createVarcharType(columnSize)));\n+            default:\n+                return super.toPrestoType(session, connection, typeHandle);\n+        }\n+    }\n+\n+    /**\n+     * Druid doesn't like table names to be qualified with catalog names in the SQL query.\n+     * Hence, overriding this method to pass catalog as an empty string.\n+     */\n+    @Override\n+    public PreparedStatement buildSql(ConnectorSession session, Connection connection, JdbcSplit split, JdbcTableHandle table, List<JdbcColumnHandle> columns)\n+            throws SQLException\n+    {\n+        String schemaName = table.getSchemaName();\n+        checkArgument(schemaName.isEmpty() || schemaName.equals(\"druid\"), \"Only \\\"druid\\\" schema is supported\");\n+        return new QueryBuilder(identifierQuote)\n+                .buildSql(\n+                        this,\n+                        session,\n+                        connection,\n+                        \"\",\n+                        schemaName,\n+                        table.getTableName(),\n+                        columns,\n+                        table.getConstraint(),\n+                        split.getAdditionalPredicate(),\n+                        tryApplyLimit(table.getLimit()));\n+    }\n+\n+    /**\n+     * Overriden to filter out tables that don't match the @param tableHandle\n+     * <p>\n+     * See {@link #getColumns(JdbcTableHandle, DatabaseMetaData)}\n+     */\n+    @Override\n+    public List<JdbcColumnHandle> getColumns(ConnectorSession session, JdbcTableHandle tableHandle)\n+    {\n+        try (Connection connection = connectionFactory.openConnection(JdbcIdentity.from(session));\n+                ResultSet resultSet = getColumns(tableHandle, connection.getMetaData())) {\n+            int allColumns = 0;\n+            List<JdbcColumnHandle> columns = new ArrayList<>();\n+            while (resultSet.next()) {\n+                // skip if table doesn't match expected\n+                if (!(Objects.equals(tableHandle.getCatalogName(), resultSet.getString(\"TABLE_CAT\"))\n+                        && Objects.equals(tableHandle.getSchemaName(), resultSet.getString(\"TABLE_SCHEM\"))\n+                        && Objects.equals(tableHandle.getTableName(), resultSet.getString(\"TABLE_NAME\")))) {\n+                    continue;\n+                }\n+                allColumns++;\n+                String columnName = resultSet.getString(\"COLUMN_NAME\");\n+                JdbcTypeHandle typeHandle = new JdbcTypeHandle(\n+                        resultSet.getInt(\"DATA_TYPE\"),\n+                        Optional.ofNullable(resultSet.getString(\"TYPE_NAME\")),\n+                        resultSet.getInt(\"COLUMN_SIZE\"),\n+                        resultSet.getInt(\"DECIMAL_DIGITS\"),\n+                        Optional.empty());\n+                Optional<ColumnMapping> columnMapping = toPrestoType(session, connection, typeHandle);\n+                // skip unsupported column types\n+                boolean nullable = (resultSet.getInt(\"NULLABLE\") != columnNoNulls);\n+                // Note: some databases (e.g. SQL Server) do not return column remarks/comment here.\n+                Optional<String> comment = Optional.ofNullable(emptyToNull(resultSet.getString(\"REMARKS\")));\n+                if (columnMapping.isPresent()) {\n+                    columns.add(JdbcColumnHandle.builder()\n+                            .setColumnName(columnName)\n+                            .setJdbcTypeHandle(typeHandle)\n+                            .setColumnType(columnMapping.get().getType())\n+                            .setNullable(nullable)\n+                            .setComment(comment)\n+                            .build());\n+                }\n+                if (columnMapping.isEmpty()) {\n+                    UnsupportedTypeHandling unsupportedTypeHandling = getUnsupportedTypeHandling(session);\n+                    verify(unsupportedTypeHandling == IGNORE, \"Unsupported type handling is set to %s, but toPrestoType() returned empty\", unsupportedTypeHandling);\n+                }\n+            }\n+            if (columns.isEmpty()) {\n+                // A table may have no supported columns. In rare cases (e.g. PostgreSQL) a table might have no columns at all.\n+                throw new TableNotFoundException(\n+                        tableHandle.getSchemaTableName(),\n+                        format(\"Table '%s' has no supported columns (all %s columns are not supported)\", tableHandle.getSchemaTableName(), allColumns));\n+            }\n+            return ImmutableList.copyOf(columns);\n+        }\n+        catch (SQLException e) {\n+            throw new PrestoException(JDBC_ERROR, e);\n+        }\n+    }\n+\n+    /**\n+     * Overriden since the {@link BaseJdbcClient#getColumns(JdbcTableHandle, DatabaseMetaData)}", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 268}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzNjg0NA==", "bodyText": "inline", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441536844", "createdAt": "2020-06-17T13:17:44Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/DruidQueryRunner.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.Session;\n+import io.prestosql.metadata.QualifiedObjectName;\n+import io.prestosql.plugin.tpch.TpchPlugin;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedRow;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.tpch.TpchTable;\n+import org.intellij.lang.annotations.Language;\n+\n+import java.io.BufferedWriter;\n+import java.io.File;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static io.airlift.testing.Closeables.closeAllSuppress;\n+import static io.prestosql.plugin.tpch.TpchMetadata.TINY_SCHEMA_NAME;\n+import static io.prestosql.testing.TestingSession.testSessionBuilder;\n+import static java.lang.String.format;\n+\n+public class DruidQueryRunner\n+{\n+    private DruidQueryRunner() {}\n+\n+    public static QueryRunner createDruidQueryRunnerTpch(TestingDruidServer testingDruidServer)\n+            throws Exception\n+    {\n+        DistributedQueryRunner queryRunner = null;\n+        try {\n+            queryRunner = DistributedQueryRunner.builder(createSession()).setNodeCount(3).build();\n+            queryRunner.installPlugin(new TpchPlugin());\n+            queryRunner.createCatalog(\"tpch\", \"tpch\");\n+\n+            Map<String, String> connectorProperties = new HashMap<>();\n+            connectorProperties.putIfAbsent(\"connection-url\", testingDruidServer.getJdbcUrl());\n+            queryRunner.installPlugin(new DruidJdbcPlugin());\n+            queryRunner.createCatalog(\"druid\", \"druid\", connectorProperties);\n+            return queryRunner;\n+        }\n+        catch (Throwable e) {\n+            closeAllSuppress(e, queryRunner);\n+            throw e;\n+        }\n+    }\n+\n+    public static void copyAndIngestTpchData(TestingDruidServer testingDruidServer, TpchTable table, QueryRunner queryRunner, String druidDatasource, Integer limit)\n+            throws IOException, InterruptedException\n+    {\n+        QualifiedObjectName source = new QualifiedObjectName(\"tpch\", TINY_SCHEMA_NAME, table.getTableName());\n+        String tsvFileLocation = getTSVFileLocation(testingDruidServer.getHostWorkingDirectory(), druidDatasource);\n+        writeDataAsTSV(queryRunner, source, createSession(), tsvFileLocation, limit);\n+        testingDruidServer.ingestData(druidDatasource, getIngestionSpecFileName(druidDatasource), tsvFileLocation);\n+    }\n+\n+    private static String getTSVFileLocation(String directory, String datasource)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzNzA1MQ==", "bodyText": "writeDataAsTsv", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441537051", "createdAt": "2020-06-17T13:18:01Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/DruidQueryRunner.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.Session;\n+import io.prestosql.metadata.QualifiedObjectName;\n+import io.prestosql.plugin.tpch.TpchPlugin;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedRow;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.tpch.TpchTable;\n+import org.intellij.lang.annotations.Language;\n+\n+import java.io.BufferedWriter;\n+import java.io.File;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static io.airlift.testing.Closeables.closeAllSuppress;\n+import static io.prestosql.plugin.tpch.TpchMetadata.TINY_SCHEMA_NAME;\n+import static io.prestosql.testing.TestingSession.testSessionBuilder;\n+import static java.lang.String.format;\n+\n+public class DruidQueryRunner\n+{\n+    private DruidQueryRunner() {}\n+\n+    public static QueryRunner createDruidQueryRunnerTpch(TestingDruidServer testingDruidServer)\n+            throws Exception\n+    {\n+        DistributedQueryRunner queryRunner = null;\n+        try {\n+            queryRunner = DistributedQueryRunner.builder(createSession()).setNodeCount(3).build();\n+            queryRunner.installPlugin(new TpchPlugin());\n+            queryRunner.createCatalog(\"tpch\", \"tpch\");\n+\n+            Map<String, String> connectorProperties = new HashMap<>();\n+            connectorProperties.putIfAbsent(\"connection-url\", testingDruidServer.getJdbcUrl());\n+            queryRunner.installPlugin(new DruidJdbcPlugin());\n+            queryRunner.createCatalog(\"druid\", \"druid\", connectorProperties);\n+            return queryRunner;\n+        }\n+        catch (Throwable e) {\n+            closeAllSuppress(e, queryRunner);\n+            throw e;\n+        }\n+    }\n+\n+    public static void copyAndIngestTpchData(TestingDruidServer testingDruidServer, TpchTable table, QueryRunner queryRunner, String druidDatasource, Integer limit)\n+            throws IOException, InterruptedException\n+    {\n+        QualifiedObjectName source = new QualifiedObjectName(\"tpch\", TINY_SCHEMA_NAME, table.getTableName());\n+        String tsvFileLocation = getTSVFileLocation(testingDruidServer.getHostWorkingDirectory(), druidDatasource);\n+        writeDataAsTSV(queryRunner, source, createSession(), tsvFileLocation, limit);\n+        testingDruidServer.ingestData(druidDatasource, getIngestionSpecFileName(druidDatasource), tsvFileLocation);\n+    }\n+\n+    private static String getTSVFileLocation(String directory, String datasource)\n+    {\n+        String fileName = String.format(\"%s.tsv\", datasource);\n+        return directory + fileName;\n+    }\n+\n+    private static String getIngestionSpecFileName(String datasource)\n+    {\n+        return String.format(\"druid-tpch-ingest-%s.json\", datasource);\n+    }\n+\n+    private static Session createSession()\n+    {\n+        return testSessionBuilder()\n+                .setCatalog(\"druid\")\n+                .setSchema(\"druid\")\n+                .build();\n+    }\n+\n+    private static void writeDataAsTSV(QueryRunner queryRunner, QualifiedObjectName table, Session session, String dataFile, Integer limit)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzNzQ1OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return data.stream().map(o -> String.valueOf(o))\n          \n          \n            \n                    return data.stream()\n          \n          \n            \n                        .map(String::valueOf)", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441537458", "createdAt": "2020-06-17T13:18:38Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/DruidQueryRunner.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.Session;\n+import io.prestosql.metadata.QualifiedObjectName;\n+import io.prestosql.plugin.tpch.TpchPlugin;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.MaterializedRow;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.tpch.TpchTable;\n+import org.intellij.lang.annotations.Language;\n+\n+import java.io.BufferedWriter;\n+import java.io.File;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static io.airlift.testing.Closeables.closeAllSuppress;\n+import static io.prestosql.plugin.tpch.TpchMetadata.TINY_SCHEMA_NAME;\n+import static io.prestosql.testing.TestingSession.testSessionBuilder;\n+import static java.lang.String.format;\n+\n+public class DruidQueryRunner\n+{\n+    private DruidQueryRunner() {}\n+\n+    public static QueryRunner createDruidQueryRunnerTpch(TestingDruidServer testingDruidServer)\n+            throws Exception\n+    {\n+        DistributedQueryRunner queryRunner = null;\n+        try {\n+            queryRunner = DistributedQueryRunner.builder(createSession()).setNodeCount(3).build();\n+            queryRunner.installPlugin(new TpchPlugin());\n+            queryRunner.createCatalog(\"tpch\", \"tpch\");\n+\n+            Map<String, String> connectorProperties = new HashMap<>();\n+            connectorProperties.putIfAbsent(\"connection-url\", testingDruidServer.getJdbcUrl());\n+            queryRunner.installPlugin(new DruidJdbcPlugin());\n+            queryRunner.createCatalog(\"druid\", \"druid\", connectorProperties);\n+            return queryRunner;\n+        }\n+        catch (Throwable e) {\n+            closeAllSuppress(e, queryRunner);\n+            throw e;\n+        }\n+    }\n+\n+    public static void copyAndIngestTpchData(TestingDruidServer testingDruidServer, TpchTable table, QueryRunner queryRunner, String druidDatasource, Integer limit)\n+            throws IOException, InterruptedException\n+    {\n+        QualifiedObjectName source = new QualifiedObjectName(\"tpch\", TINY_SCHEMA_NAME, table.getTableName());\n+        String tsvFileLocation = getTSVFileLocation(testingDruidServer.getHostWorkingDirectory(), druidDatasource);\n+        writeDataAsTSV(queryRunner, source, createSession(), tsvFileLocation, limit);\n+        testingDruidServer.ingestData(druidDatasource, getIngestionSpecFileName(druidDatasource), tsvFileLocation);\n+    }\n+\n+    private static String getTSVFileLocation(String directory, String datasource)\n+    {\n+        String fileName = String.format(\"%s.tsv\", datasource);\n+        return directory + fileName;\n+    }\n+\n+    private static String getIngestionSpecFileName(String datasource)\n+    {\n+        return String.format(\"druid-tpch-ingest-%s.json\", datasource);\n+    }\n+\n+    private static Session createSession()\n+    {\n+        return testSessionBuilder()\n+                .setCatalog(\"druid\")\n+                .setSchema(\"druid\")\n+                .build();\n+    }\n+\n+    private static void writeDataAsTSV(QueryRunner queryRunner, QualifiedObjectName table, Session session, String dataFile, Integer limit)\n+            throws IOException\n+    {\n+        File file = new File(dataFile);\n+        try (BufferedWriter bw = new BufferedWriter(new FileWriter(file))) {\n+            @Language(\"SQL\") String sql = format(\"SELECT * FROM %s\", table);\n+            if (limit != null) {\n+                sql = sql + \" LIMIT \" + limit;\n+            }\n+            List<MaterializedRow> rows = queryRunner.execute(session, sql).getMaterializedRows();\n+            for (MaterializedRow row : rows) {\n+                bw.write(convertToTSV(row.getFields()));\n+                bw.newLine();\n+            }\n+        }\n+    }\n+\n+    private static String convertToTSV(List<Object> data)\n+    {\n+        return data.stream().map(o -> String.valueOf(o))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzODY0Nw==", "bodyText": "This affects many current & future tests.\nIf druid requires __time column, please copy orderdate column to have both: __time and orderdate.", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441538647", "createdAt": "2020-06-17T13:20:25Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestDruidIntegrationSmokeTest.java", "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestIntegrationSmokeTest;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.assertions.Assert;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+\n+import static io.prestosql.plugin.druid.DruidQueryRunner.copyAndIngestTpchData;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static io.prestosql.tpch.TpchTable.ORDERS;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertTrue;\n+\n+@Test\n+public class TestDruidIntegrationSmokeTest\n+        extends AbstractTestIntegrationSmokeTest\n+{\n+    private TestingDruidServer druidServer;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        this.druidServer = new TestingDruidServer();\n+        QueryRunner queryRunner = DruidQueryRunner.createDruidQueryRunnerTpch(druidServer);\n+        copyAndIngestTpchData(this.druidServer, ORDERS, queryRunner, ORDERS.getTableName(), null);\n+        return queryRunner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void destroy()\n+    {\n+        if (druidServer != null) {\n+            druidServer.close();\n+        }\n+    }\n+\n+    @Test\n+    @Override\n+    public void testDescribeTable()\n+    {\n+        MaterializedResult expectedColumns = MaterializedResult.resultBuilder(getQueryRunner().getDefaultSession(), VARCHAR, VARCHAR, VARCHAR, VARCHAR)\n+                .row(\"__time\", \"timestamp(3)\", \"\", \"\")\n+                .row(\"clerk\", \"varchar\", \"\", \"\") // String columns are reported only as varchar\n+                .row(\"comment\", \"varchar\", \"\", \"\")\n+                .row(\"custkey\", \"bigint\", \"\", \"\") // Long columns are reported as bigint\n+                .row(\"orderkey\", \"bigint\", \"\", \"\")\n+                .row(\"orderpriority\", \"varchar\", \"\", \"\")\n+                .row(\"orderstatus\", \"varchar\", \"\", \"\")\n+                .row(\"shippriority\", \"bigint\", \"\", \"\") // Druid doesn't support int type\n+                .row(\"totalprice\", \"double\", \"\", \"\")\n+                .build();\n+        MaterializedResult actualColumns = computeActual(\"DESCRIBE orders\");\n+        Assert.assertEquals(actualColumns, expectedColumns);\n+    }\n+\n+    @Test\n+    @Override\n+    public void testShowCreateTable()\n+    {\n+        assertThat((String) computeActual(\"SHOW CREATE TABLE orders\").getOnlyValue())\n+                // If the connector reports additional column properties, the expected value needs to be adjusted in the test subclass\n+                .matches(\"CREATE TABLE \\\\w+\\\\.\\\\w+\\\\.orders \\\\Q(\\n\" +\n+                        \"   __time timestamp(3) NOT NULL,\\n\" +\n+                        \"   clerk varchar,\\n\" +\n+                        \"   comment varchar,\\n\" +\n+                        \"   custkey bigint NOT NULL,\\n\" +\n+                        \"   orderkey bigint NOT NULL,\\n\" +\n+                        \"   orderpriority varchar,\\n\" +\n+                        \"   orderstatus varchar,\\n\" +\n+                        \"   shippriority bigint NOT NULL,\\n\" +\n+                        \"   totalprice double NOT NULL\\n\" +\n+                        \")\");\n+    }\n+\n+    @Test\n+    @Override\n+    public void testMultipleRangesPredicate()\n+    {\n+        // Removed orderdate column from projection since druid stores orderdate as __time column", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUzOTQ2Mw==", "bodyText": "follow io.prestosql.elasticsearch.BaseElasticsearchSmokeTest#testSelectAll example", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441539463", "createdAt": "2020-06-17T13:21:36Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestDruidIntegrationSmokeTest.java", "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestIntegrationSmokeTest;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.assertions.Assert;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+\n+import static io.prestosql.plugin.druid.DruidQueryRunner.copyAndIngestTpchData;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static io.prestosql.tpch.TpchTable.ORDERS;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertTrue;\n+\n+@Test\n+public class TestDruidIntegrationSmokeTest\n+        extends AbstractTestIntegrationSmokeTest\n+{\n+    private TestingDruidServer druidServer;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        this.druidServer = new TestingDruidServer();\n+        QueryRunner queryRunner = DruidQueryRunner.createDruidQueryRunnerTpch(druidServer);\n+        copyAndIngestTpchData(this.druidServer, ORDERS, queryRunner, ORDERS.getTableName(), null);\n+        return queryRunner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void destroy()\n+    {\n+        if (druidServer != null) {\n+            druidServer.close();\n+        }\n+    }\n+\n+    @Test\n+    @Override\n+    public void testDescribeTable()\n+    {\n+        MaterializedResult expectedColumns = MaterializedResult.resultBuilder(getQueryRunner().getDefaultSession(), VARCHAR, VARCHAR, VARCHAR, VARCHAR)\n+                .row(\"__time\", \"timestamp(3)\", \"\", \"\")\n+                .row(\"clerk\", \"varchar\", \"\", \"\") // String columns are reported only as varchar\n+                .row(\"comment\", \"varchar\", \"\", \"\")\n+                .row(\"custkey\", \"bigint\", \"\", \"\") // Long columns are reported as bigint\n+                .row(\"orderkey\", \"bigint\", \"\", \"\")\n+                .row(\"orderpriority\", \"varchar\", \"\", \"\")\n+                .row(\"orderstatus\", \"varchar\", \"\", \"\")\n+                .row(\"shippriority\", \"bigint\", \"\", \"\") // Druid doesn't support int type\n+                .row(\"totalprice\", \"double\", \"\", \"\")\n+                .build();\n+        MaterializedResult actualColumns = computeActual(\"DESCRIBE orders\");\n+        Assert.assertEquals(actualColumns, expectedColumns);\n+    }\n+\n+    @Test\n+    @Override\n+    public void testShowCreateTable()\n+    {\n+        assertThat((String) computeActual(\"SHOW CREATE TABLE orders\").getOnlyValue())\n+                // If the connector reports additional column properties, the expected value needs to be adjusted in the test subclass\n+                .matches(\"CREATE TABLE \\\\w+\\\\.\\\\w+\\\\.orders \\\\Q(\\n\" +\n+                        \"   __time timestamp(3) NOT NULL,\\n\" +\n+                        \"   clerk varchar,\\n\" +\n+                        \"   comment varchar,\\n\" +\n+                        \"   custkey bigint NOT NULL,\\n\" +\n+                        \"   orderkey bigint NOT NULL,\\n\" +\n+                        \"   orderpriority varchar,\\n\" +\n+                        \"   orderstatus varchar,\\n\" +\n+                        \"   shippriority bigint NOT NULL,\\n\" +\n+                        \"   totalprice double NOT NULL\\n\" +\n+                        \")\");\n+    }\n+\n+    @Test\n+    @Override\n+    public void testMultipleRangesPredicate()\n+    {\n+        // Removed orderdate column from projection since druid stores orderdate as __time column\n+        assertQuery(\"\" +\n+                \"SELECT orderkey, custkey, orderstatus, totalprice, orderpriority, clerk, shippriority, comment \" +\n+                \"FROM orders \" +\n+                \"WHERE orderkey BETWEEN 10 AND 50\");\n+    }\n+\n+    @Test\n+    @Override\n+    public void testRangePredicate()\n+    {\n+        // Removed orderdate column from projection since druid stores orderdate as __time column\n+        assertQuery(\"\" +\n+                \"SELECT orderkey, custkey, orderstatus, totalprice, orderpriority, clerk, shippriority, comment \" +\n+                \"FROM orders \" +\n+                \"WHERE orderkey BETWEEN 10 AND 50\");\n+    }\n+\n+    @Test\n+    @Override\n+    public void testSelectInformationSchemaColumns()\n+    {\n+        String catalog = getSession().getCatalog().get();\n+        String schema = getSession().getSchema().get();\n+        String schemaPattern = schema.replaceAll(\".$\", \"_\");\n+\n+        @Language(\"SQL\") String ordersTableWithColumns = \"VALUES \" +\n+                \"('orders', 'orderkey'), \" +\n+                \"('orders', 'custkey'), \" +\n+                \"('orders', 'orderstatus'), \" +\n+                \"('orders', 'totalprice'), \" +\n+                \"('orders', '__time'), \" + // orderdate is mapped to __time column in Druid\n+                \"('orders', 'orderpriority'), \" +\n+                \"('orders', 'clerk'), \" +\n+                \"('orders', 'shippriority'), \" +\n+                \"('orders', 'comment')\";\n+\n+        assertQuery(\"SELECT table_schema FROM information_schema.columns WHERE table_schema = '\" + schema + \"' GROUP BY table_schema\", \"VALUES '\" + schema + \"'\");\n+        assertQuery(\"SELECT table_name FROM information_schema.columns WHERE table_name = 'orders' GROUP BY table_name\", \"VALUES 'orders'\");\n+        assertQuery(\"SELECT table_name, column_name FROM information_schema.columns WHERE table_schema = '\" + schema + \"' AND table_name = 'orders'\", ordersTableWithColumns);\n+        assertQuery(\"SELECT table_name, column_name FROM information_schema.columns WHERE table_schema = '\" + schema + \"' AND table_name LIKE '%rders'\", ordersTableWithColumns);\n+        assertQuery(\"SELECT table_name, column_name FROM information_schema.columns WHERE table_schema LIKE '\" + schemaPattern + \"' AND table_name LIKE '_rder_'\", ordersTableWithColumns);\n+        assertQuery(\n+                \"SELECT table_name, column_name FROM information_schema.columns \" +\n+                        \"WHERE table_catalog = '\" + catalog + \"' AND table_schema = '\" + schema + \"' AND table_name LIKE '%orders%'\",\n+                ordersTableWithColumns);\n+\n+        assertQuerySucceeds(\"SELECT * FROM information_schema.columns\");\n+        assertQuery(\"SELECT DISTINCT table_name, column_name FROM information_schema.columns WHERE table_name LIKE '_rders'\", ordersTableWithColumns);\n+        assertQuerySucceeds(\"SELECT * FROM information_schema.columns WHERE table_catalog = '\" + catalog + \"'\");\n+        assertQuerySucceeds(\"SELECT * FROM information_schema.columns WHERE table_catalog = '\" + catalog + \"' AND table_schema = '\" + schema + \"'\");\n+        assertQuery(\"SELECT table_name, column_name FROM information_schema.columns WHERE table_catalog = '\" + catalog + \"' AND table_schema = '\" + schema + \"' AND table_name LIKE '_rders'\", ordersTableWithColumns);\n+        assertQuerySucceeds(\"SELECT * FROM information_schema.columns WHERE table_catalog = '\" + catalog + \"' AND table_name LIKE '%'\");\n+        assertQuery(\"SELECT column_name FROM information_schema.columns WHERE table_catalog = 'something_else'\", \"SELECT '' WHERE false\");\n+    }\n+\n+    @Test\n+    @Override\n+    // Ignoring the test for now. For some reason the \"expected\" query is failing\n+    public void testSelectAll()\n+    {\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 163}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU0MjQ5NQ==", "bodyText": "// Cannot use Files.createTempDirectory() because on Mac by default it uses /var/folders/ which is not visible to Docker for Mac\nPath hostWorkingDirectory = Files.createDirectory(Paths.get(\"/tmp/druid-workdir-\" + randomUUID().toString()));\n\nand remove druid-test-storage from resources", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r441542495", "createdAt": "2020-06-17T13:25:38Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestingDruidServer.java", "diffHunk": "@@ -0,0 +1,286 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.io.Closer;\n+import com.google.common.io.Resources;\n+import io.prestosql.testing.assertions.Assert;\n+import io.prestosql.tests.product.launcher.testcontainers.SelectedPortWaitStrategy;\n+import org.testcontainers.containers.BindMode;\n+import org.testcontainers.containers.GenericContainer;\n+import org.testcontainers.containers.Network;\n+import org.testcontainers.containers.startupcheck.IsRunningStartupCheckStrategy;\n+import org.testcontainers.containers.wait.strategy.Wait;\n+import org.testcontainers.shaded.okhttp3.OkHttpClient;\n+import org.testcontainers.shaded.okhttp3.Request;\n+import org.testcontainers.shaded.okhttp3.RequestBody;\n+import org.testcontainers.shaded.okhttp3.Response;\n+\n+import java.io.Closeable;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.Charset;\n+import java.util.Map;\n+\n+import static com.google.common.io.Resources.getResource;\n+import static java.lang.String.format;\n+import static org.testcontainers.utility.MountableFile.forHostPath;\n+\n+public class TestingDruidServer\n+        implements Closeable\n+{\n+    private final String hostWorkingDirectory;\n+    private final GenericContainer broker;\n+    private final GenericContainer coordinator;\n+    private final GenericContainer historical;\n+    private final GenericContainer middleManager;\n+    private final GenericContainer zookeeper;\n+    private final OkHttpClient httpClient;\n+\n+    private static final int DRUID_COORDINATOR_PORT = 8081;\n+    private static final int DRUID_BROKER_PORT = 8082;\n+    private static final int DRUID_HISTORICAL_PORT = 8083;\n+    private static final int DRUID_MIDDLE_MANAGER_PORT = 8091;\n+\n+    private static final String DRUID_DOCKER_IMAGE = \"apache/druid:0.18.0\";\n+\n+    public TestingDruidServer()\n+    {\n+        try {\n+            this.hostWorkingDirectory = TestingDruidServer.class.getClassLoader().getResource(\"druid-test-storage/\").getPath();\n+            File f = new File(hostWorkingDirectory);\n+            cleanDirectory(f);\n+            // Enable read/write/exec access for the services running in containers\n+            f.setWritable(true, false);\n+            f.setReadable(true, false);\n+            f.setExecutable(true, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 68}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzMTcyMzE3", "url": "https://github.com/trinodb/trino/pull/3522#pullrequestreview-433172317", "createdAt": "2020-06-18T11:01:50Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxMTowMTo1MFrOGlqXeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxMToxNDozNVrOGlqvNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE0NDYzNA==", "bodyText": "I did this:\n// Cannot use Files.createTempDirectory() because on Mac by default it uses /var/folders/ which is not visible to Docker for Mac\nhostWorkingDirectory = Files.createDirectory(Paths.get(\"/tmp/docker-files-\" + randomUUID().toString()));\n\nthen\n.withFileSystemBind(hostWorkingDirectory.toString(), \"/opt/druid/var\", BindMode.READ_WRITE)\n\nit was correctly bound, but something didn't work quite well (the data import).\nPlease try to complete this this way. If you cannot, add a TODO -- using classpath resource as R/W dir is not a good idea.", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r442144634", "createdAt": "2020-06-18T11:01:50Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestingDruidServer.java", "diffHunk": "@@ -0,0 +1,286 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.io.Closer;\n+import com.google.common.io.Resources;\n+import io.prestosql.testing.assertions.Assert;\n+import io.prestosql.tests.product.launcher.testcontainers.SelectedPortWaitStrategy;\n+import org.testcontainers.containers.BindMode;\n+import org.testcontainers.containers.GenericContainer;\n+import org.testcontainers.containers.Network;\n+import org.testcontainers.containers.startupcheck.IsRunningStartupCheckStrategy;\n+import org.testcontainers.containers.wait.strategy.Wait;\n+import org.testcontainers.shaded.okhttp3.OkHttpClient;\n+import org.testcontainers.shaded.okhttp3.Request;\n+import org.testcontainers.shaded.okhttp3.RequestBody;\n+import org.testcontainers.shaded.okhttp3.Response;\n+\n+import java.io.Closeable;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.Charset;\n+import java.util.Map;\n+\n+import static com.google.common.io.Resources.getResource;\n+import static java.lang.String.format;\n+import static org.testcontainers.utility.MountableFile.forHostPath;\n+\n+public class TestingDruidServer\n+        implements Closeable\n+{\n+    private final String hostWorkingDirectory;\n+    private final GenericContainer broker;\n+    private final GenericContainer coordinator;\n+    private final GenericContainer historical;\n+    private final GenericContainer middleManager;\n+    private final GenericContainer zookeeper;\n+    private final OkHttpClient httpClient;\n+\n+    private static final int DRUID_COORDINATOR_PORT = 8081;\n+    private static final int DRUID_BROKER_PORT = 8082;\n+    private static final int DRUID_HISTORICAL_PORT = 8083;\n+    private static final int DRUID_MIDDLE_MANAGER_PORT = 8091;\n+\n+    private static final String DRUID_DOCKER_IMAGE = \"apache/druid:0.18.0\";\n+\n+    public TestingDruidServer()\n+    {\n+        try {\n+            this.hostWorkingDirectory = TestingDruidServer.class.getClassLoader().getResource(\"druid-test-storage/\").getPath();\n+            File f = new File(hostWorkingDirectory);\n+            cleanDirectory(f);\n+            // Enable read/write/exec access for the services running in containers\n+            f.setWritable(true, false);\n+            f.setReadable(true, false);\n+            f.setExecutable(true, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU0MjQ5NQ=="}, "originalCommit": null, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE0NDY4Ng==", "bodyText": "forClasspathResource", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r442144686", "createdAt": "2020-06-18T11:01:59Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestingDruidServer.java", "diffHunk": "@@ -0,0 +1,286 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.io.Closer;\n+import com.google.common.io.Resources;\n+import io.prestosql.testing.assertions.Assert;\n+import io.prestosql.tests.product.launcher.testcontainers.SelectedPortWaitStrategy;\n+import org.testcontainers.containers.BindMode;\n+import org.testcontainers.containers.GenericContainer;\n+import org.testcontainers.containers.Network;\n+import org.testcontainers.containers.startupcheck.IsRunningStartupCheckStrategy;\n+import org.testcontainers.containers.wait.strategy.Wait;\n+import org.testcontainers.shaded.okhttp3.OkHttpClient;\n+import org.testcontainers.shaded.okhttp3.Request;\n+import org.testcontainers.shaded.okhttp3.RequestBody;\n+import org.testcontainers.shaded.okhttp3.Response;\n+\n+import java.io.Closeable;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.Charset;\n+import java.util.Map;\n+\n+import static com.google.common.io.Resources.getResource;\n+import static java.lang.String.format;\n+import static org.testcontainers.utility.MountableFile.forHostPath;\n+\n+public class TestingDruidServer\n+        implements Closeable\n+{\n+    private final String hostWorkingDirectory;\n+    private final GenericContainer broker;\n+    private final GenericContainer coordinator;\n+    private final GenericContainer historical;\n+    private final GenericContainer middleManager;\n+    private final GenericContainer zookeeper;\n+    private final OkHttpClient httpClient;\n+\n+    private static final int DRUID_COORDINATOR_PORT = 8081;\n+    private static final int DRUID_BROKER_PORT = 8082;\n+    private static final int DRUID_HISTORICAL_PORT = 8083;\n+    private static final int DRUID_MIDDLE_MANAGER_PORT = 8091;\n+\n+    private static final String DRUID_DOCKER_IMAGE = \"apache/druid:0.18.0\";\n+\n+    public TestingDruidServer()\n+    {\n+        try {\n+            this.hostWorkingDirectory = TestingDruidServer.class.getClassLoader().getResource(\"druid-test-storage/\").getPath();\n+            File f = new File(hostWorkingDirectory);\n+            cleanDirectory(f);\n+            // Enable read/write/exec access for the services running in containers\n+            f.setWritable(true, false);\n+            f.setReadable(true, false);\n+            f.setExecutable(true, false);\n+            this.httpClient = new OkHttpClient();\n+            Network network = Network.newNetwork();\n+            this.zookeeper = new GenericContainer(\"zookeeper\")\n+                    .withNetwork(network)\n+                    .withNetworkAliases(\"zookeeper\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .waitingFor(new SelectedPortWaitStrategy(2181));\n+            zookeeper.start();\n+\n+            this.coordinator = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_COORDINATOR_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"coordinator\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .dependsOn(zookeeper)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE0NTE0OA==", "bodyText": "Register this in closer as well:\n closer.register(() -> deleteRecursively(hostWorkingDirectory, ALLOW_INSECURE));\n\n(first, so it's get called last)", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r442145148", "createdAt": "2020-06-18T11:03:06Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestingDruidServer.java", "diffHunk": "@@ -0,0 +1,286 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.io.Closer;\n+import com.google.common.io.Resources;\n+import io.prestosql.testing.assertions.Assert;\n+import io.prestosql.tests.product.launcher.testcontainers.SelectedPortWaitStrategy;\n+import org.testcontainers.containers.BindMode;\n+import org.testcontainers.containers.GenericContainer;\n+import org.testcontainers.containers.Network;\n+import org.testcontainers.containers.startupcheck.IsRunningStartupCheckStrategy;\n+import org.testcontainers.containers.wait.strategy.Wait;\n+import org.testcontainers.shaded.okhttp3.OkHttpClient;\n+import org.testcontainers.shaded.okhttp3.Request;\n+import org.testcontainers.shaded.okhttp3.RequestBody;\n+import org.testcontainers.shaded.okhttp3.Response;\n+\n+import java.io.Closeable;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.Charset;\n+import java.util.Map;\n+\n+import static com.google.common.io.Resources.getResource;\n+import static java.lang.String.format;\n+import static org.testcontainers.utility.MountableFile.forHostPath;\n+\n+public class TestingDruidServer\n+        implements Closeable\n+{\n+    private final String hostWorkingDirectory;\n+    private final GenericContainer broker;\n+    private final GenericContainer coordinator;\n+    private final GenericContainer historical;\n+    private final GenericContainer middleManager;\n+    private final GenericContainer zookeeper;\n+    private final OkHttpClient httpClient;\n+\n+    private static final int DRUID_COORDINATOR_PORT = 8081;\n+    private static final int DRUID_BROKER_PORT = 8082;\n+    private static final int DRUID_HISTORICAL_PORT = 8083;\n+    private static final int DRUID_MIDDLE_MANAGER_PORT = 8091;\n+\n+    private static final String DRUID_DOCKER_IMAGE = \"apache/druid:0.18.0\";\n+\n+    public TestingDruidServer()\n+    {\n+        try {\n+            this.hostWorkingDirectory = TestingDruidServer.class.getClassLoader().getResource(\"druid-test-storage/\").getPath();\n+            File f = new File(hostWorkingDirectory);\n+            cleanDirectory(f);\n+            // Enable read/write/exec access for the services running in containers\n+            f.setWritable(true, false);\n+            f.setReadable(true, false);\n+            f.setExecutable(true, false);\n+            this.httpClient = new OkHttpClient();\n+            Network network = Network.newNetwork();\n+            this.zookeeper = new GenericContainer(\"zookeeper\")\n+                    .withNetwork(network)\n+                    .withNetworkAliases(\"zookeeper\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .waitingFor(new SelectedPortWaitStrategy(2181));\n+            zookeeper.start();\n+\n+            this.coordinator = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_COORDINATOR_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"coordinator\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .dependsOn(zookeeper)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"druid-coordinator.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/master/coordinator-overlord/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"druid-coordinator-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/master/coordinator-overlord/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            coordinator.start();\n+\n+            this.broker = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_BROKER_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"broker\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .dependsOn(zookeeper, coordinator)\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"broker.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/query/broker/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"broker-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/query/broker/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            broker.start();\n+\n+            this.historical = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_HISTORICAL_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"historical\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .dependsOn(zookeeper, coordinator)\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"historical.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/historical/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"historical-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/historical/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            historical.start();\n+\n+            this.middleManager = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_MIDDLE_MANAGER_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"middleManager\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .dependsOn(zookeeper, coordinator)\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"middleManager.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/middleManager/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"middleManager-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/middleManager/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            middleManager.start();\n+        }\n+        catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    public String getHostWorkingDirectory()\n+    {\n+        return hostWorkingDirectory;\n+    }\n+\n+    @Override\n+    public void close()\n+    {\n+        try (Closer closer = Closer.create()) {\n+            closer.register(broker::stop);\n+            closer.register(historical::stop);\n+            closer.register(middleManager::stop);\n+            closer.register(coordinator::stop);\n+            closer.register(zookeeper::stop);\n+        }\n+        catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+        finally {\n+            cleanDirectory(new File(hostWorkingDirectory));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE0NTY1MQ==", "bodyText": "[com.google.common.io.MoreFiles.]deleteRecursively(path, ALLOW_INSECURE);", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r442145651", "createdAt": "2020-06-18T11:04:06Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestingDruidServer.java", "diffHunk": "@@ -0,0 +1,286 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.io.Closer;\n+import com.google.common.io.Resources;\n+import io.prestosql.testing.assertions.Assert;\n+import io.prestosql.tests.product.launcher.testcontainers.SelectedPortWaitStrategy;\n+import org.testcontainers.containers.BindMode;\n+import org.testcontainers.containers.GenericContainer;\n+import org.testcontainers.containers.Network;\n+import org.testcontainers.containers.startupcheck.IsRunningStartupCheckStrategy;\n+import org.testcontainers.containers.wait.strategy.Wait;\n+import org.testcontainers.shaded.okhttp3.OkHttpClient;\n+import org.testcontainers.shaded.okhttp3.Request;\n+import org.testcontainers.shaded.okhttp3.RequestBody;\n+import org.testcontainers.shaded.okhttp3.Response;\n+\n+import java.io.Closeable;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.Charset;\n+import java.util.Map;\n+\n+import static com.google.common.io.Resources.getResource;\n+import static java.lang.String.format;\n+import static org.testcontainers.utility.MountableFile.forHostPath;\n+\n+public class TestingDruidServer\n+        implements Closeable\n+{\n+    private final String hostWorkingDirectory;\n+    private final GenericContainer broker;\n+    private final GenericContainer coordinator;\n+    private final GenericContainer historical;\n+    private final GenericContainer middleManager;\n+    private final GenericContainer zookeeper;\n+    private final OkHttpClient httpClient;\n+\n+    private static final int DRUID_COORDINATOR_PORT = 8081;\n+    private static final int DRUID_BROKER_PORT = 8082;\n+    private static final int DRUID_HISTORICAL_PORT = 8083;\n+    private static final int DRUID_MIDDLE_MANAGER_PORT = 8091;\n+\n+    private static final String DRUID_DOCKER_IMAGE = \"apache/druid:0.18.0\";\n+\n+    public TestingDruidServer()\n+    {\n+        try {\n+            this.hostWorkingDirectory = TestingDruidServer.class.getClassLoader().getResource(\"druid-test-storage/\").getPath();\n+            File f = new File(hostWorkingDirectory);\n+            cleanDirectory(f);\n+            // Enable read/write/exec access for the services running in containers\n+            f.setWritable(true, false);\n+            f.setReadable(true, false);\n+            f.setExecutable(true, false);\n+            this.httpClient = new OkHttpClient();\n+            Network network = Network.newNetwork();\n+            this.zookeeper = new GenericContainer(\"zookeeper\")\n+                    .withNetwork(network)\n+                    .withNetworkAliases(\"zookeeper\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .waitingFor(new SelectedPortWaitStrategy(2181));\n+            zookeeper.start();\n+\n+            this.coordinator = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_COORDINATOR_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"coordinator\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .dependsOn(zookeeper)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"druid-coordinator.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/master/coordinator-overlord/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"druid-coordinator-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/master/coordinator-overlord/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            coordinator.start();\n+\n+            this.broker = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_BROKER_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"broker\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .dependsOn(zookeeper, coordinator)\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"broker.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/query/broker/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"broker-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/query/broker/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            broker.start();\n+\n+            this.historical = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_HISTORICAL_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"historical\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .dependsOn(zookeeper, coordinator)\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"historical.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/historical/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"historical-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/historical/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            historical.start();\n+\n+            this.middleManager = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_MIDDLE_MANAGER_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"middleManager\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .dependsOn(zookeeper, coordinator)\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"middleManager.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/middleManager/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"middleManager-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/middleManager/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            middleManager.start();\n+        }\n+        catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    public String getHostWorkingDirectory()\n+    {\n+        return hostWorkingDirectory;\n+    }\n+\n+    @Override\n+    public void close()\n+    {\n+        try (Closer closer = Closer.create()) {\n+            closer.register(broker::stop);\n+            closer.register(historical::stop);\n+            closer.register(middleManager::stop);\n+            closer.register(coordinator::stop);\n+            closer.register(zookeeper::stop);\n+        }\n+        catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+        finally {\n+            cleanDirectory(new File(hostWorkingDirectory));\n+        }\n+    }\n+\n+    public String getJdbcUrl()\n+    {\n+        return getJdbcUrl(broker.getMappedPort(DRUID_BROKER_PORT));\n+    }\n+\n+    public int getCoordinatorOverlordPort()\n+    {\n+        return coordinator.getMappedPort(DRUID_COORDINATOR_PORT);\n+    }\n+\n+    private static String getJdbcUrl(int port)\n+    {\n+        return format(\"jdbc:avatica:remote:url=http://localhost:%s/druid/v2/sql/avatica/\", port);\n+    }\n+\n+    void ingestData(String datasource, String indexTaskFile, String dataFilePath)\n+            throws IOException, InterruptedException\n+    {\n+        middleManager.withCopyFileToContainer(forHostPath(dataFilePath),\n+                getMiddleManagerContainerPathForDataFile(dataFilePath));\n+        String indexTask = Resources.toString(getResource(indexTaskFile), Charset.defaultCharset());\n+\n+        Request.Builder requestBuilder = new Request.Builder();\n+        requestBuilder.addHeader(\"content-type\", \"application/json;charset=utf-8\")\n+                .url(\"http://localhost:\" + getCoordinatorOverlordPort() + \"/druid/indexer/v1/task\")\n+                .post(RequestBody.create(null, indexTask));\n+        Request ingestionRequest = requestBuilder.build();\n+        Response response = null;\n+        try {\n+            response = httpClient.newCall(ingestionRequest).execute();\n+            Assert.assertTrue(checkDatasourceAvailable(datasource), \"Datasource \" + datasource + \" not loaded\");\n+        }\n+        finally {\n+            if (response != null) {\n+                response.close();\n+            }\n+        }\n+    }\n+\n+    private boolean checkDatasourceAvailable(String datasource)\n+            throws IOException, InterruptedException\n+    {\n+        Map<String, Double> datasourceAvailabilityDetails = null;\n+        boolean datasourceNotLoaded = true;\n+        int attempts = 10;\n+        while (datasourceNotLoaded && attempts > 0) {\n+            Request.Builder requestBuilder = new Request.Builder();\n+            requestBuilder.url(\"http://localhost:\" + getCoordinatorOverlordPort() + \"/druid/coordinator/v1/loadstatus\")\n+                    .get();\n+            Request datasourceAvailabilityRequest = requestBuilder.build();\n+            try (Response response = httpClient.newCall(datasourceAvailabilityRequest).execute()) {\n+                ObjectMapper mapper = new ObjectMapper();\n+                datasourceAvailabilityDetails = mapper.readValue(response.body().string(), Map.class);\n+                datasourceNotLoaded = datasourceAvailabilityDetails.get(datasource) == null || Double.compare(datasourceAvailabilityDetails.get(datasource), 100.0) < 0;\n+                if (datasourceNotLoaded) {\n+                    attempts--;\n+                    // Wait for some time since it can take a while for coordinator to load the ingested segments\n+                    Thread.sleep(15000);\n+                }\n+            }\n+        }\n+        return !datasourceNotLoaded;\n+    }\n+\n+    private static String getMiddleManagerContainerPathForDataFile(String dataFilePath)\n+    {\n+        return String.format(\"/opt/druid/var/%s\", new File(dataFilePath).getName());\n+    }\n+\n+    // Clean up the contents of the directory\n+    private static void cleanDirectory(File directoryToBeCleaned)\n+    {\n+        File[] allContents = directoryToBeCleaned.listFiles();\n+        if (allContents != null) {\n+            for (File file : allContents) {\n+                if (file.isDirectory()) {\n+                    deleteRecursively(file);\n+                }\n+                else {\n+                    file.delete();\n+                }\n+            }\n+        }\n+    }\n+\n+    private static void deleteRecursively(File directory)\n+    {\n+        File[] allContents = directory.listFiles();\n+        if (allContents != null) {\n+            for (File file : allContents) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 275}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE0NTk1MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return String.format(\"/opt/druid/var/%s\", new File(dataFilePath).getName());\n          \n          \n            \n                    return \"/opt/druid/var/\" + dataFilePath;\n          \n      \n    \n    \n  \n\n?", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r442145950", "createdAt": "2020-06-18T11:04:44Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestingDruidServer.java", "diffHunk": "@@ -0,0 +1,286 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.io.Closer;\n+import com.google.common.io.Resources;\n+import io.prestosql.testing.assertions.Assert;\n+import io.prestosql.tests.product.launcher.testcontainers.SelectedPortWaitStrategy;\n+import org.testcontainers.containers.BindMode;\n+import org.testcontainers.containers.GenericContainer;\n+import org.testcontainers.containers.Network;\n+import org.testcontainers.containers.startupcheck.IsRunningStartupCheckStrategy;\n+import org.testcontainers.containers.wait.strategy.Wait;\n+import org.testcontainers.shaded.okhttp3.OkHttpClient;\n+import org.testcontainers.shaded.okhttp3.Request;\n+import org.testcontainers.shaded.okhttp3.RequestBody;\n+import org.testcontainers.shaded.okhttp3.Response;\n+\n+import java.io.Closeable;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.Charset;\n+import java.util.Map;\n+\n+import static com.google.common.io.Resources.getResource;\n+import static java.lang.String.format;\n+import static org.testcontainers.utility.MountableFile.forHostPath;\n+\n+public class TestingDruidServer\n+        implements Closeable\n+{\n+    private final String hostWorkingDirectory;\n+    private final GenericContainer broker;\n+    private final GenericContainer coordinator;\n+    private final GenericContainer historical;\n+    private final GenericContainer middleManager;\n+    private final GenericContainer zookeeper;\n+    private final OkHttpClient httpClient;\n+\n+    private static final int DRUID_COORDINATOR_PORT = 8081;\n+    private static final int DRUID_BROKER_PORT = 8082;\n+    private static final int DRUID_HISTORICAL_PORT = 8083;\n+    private static final int DRUID_MIDDLE_MANAGER_PORT = 8091;\n+\n+    private static final String DRUID_DOCKER_IMAGE = \"apache/druid:0.18.0\";\n+\n+    public TestingDruidServer()\n+    {\n+        try {\n+            this.hostWorkingDirectory = TestingDruidServer.class.getClassLoader().getResource(\"druid-test-storage/\").getPath();\n+            File f = new File(hostWorkingDirectory);\n+            cleanDirectory(f);\n+            // Enable read/write/exec access for the services running in containers\n+            f.setWritable(true, false);\n+            f.setReadable(true, false);\n+            f.setExecutable(true, false);\n+            this.httpClient = new OkHttpClient();\n+            Network network = Network.newNetwork();\n+            this.zookeeper = new GenericContainer(\"zookeeper\")\n+                    .withNetwork(network)\n+                    .withNetworkAliases(\"zookeeper\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .waitingFor(new SelectedPortWaitStrategy(2181));\n+            zookeeper.start();\n+\n+            this.coordinator = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_COORDINATOR_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"coordinator\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .dependsOn(zookeeper)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"druid-coordinator.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/master/coordinator-overlord/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"druid-coordinator-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/master/coordinator-overlord/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            coordinator.start();\n+\n+            this.broker = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_BROKER_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"broker\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .dependsOn(zookeeper, coordinator)\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"broker.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/query/broker/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"broker-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/query/broker/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            broker.start();\n+\n+            this.historical = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_HISTORICAL_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"historical\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .dependsOn(zookeeper, coordinator)\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"historical.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/historical/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"historical-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/historical/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            historical.start();\n+\n+            this.middleManager = new GenericContainer(DRUID_DOCKER_IMAGE)\n+                    .withExposedPorts(DRUID_MIDDLE_MANAGER_PORT)\n+                    .withNetwork(network)\n+                    .withCommand(\"middleManager\")\n+                    .withWorkingDirectory(\"/opt/druid\")\n+                    .dependsOn(zookeeper, coordinator)\n+                    .withClasspathResourceMapping(\"druid-test-storage/\", \"/opt/druid/var\", BindMode.READ_WRITE)\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"common.runtime.properties\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/_common/common.runtime.properties\")\n+                    .withStartupCheckStrategy(new IsRunningStartupCheckStrategy())\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"middleManager.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/middleManager/runtime.properties\")\n+                    .withCopyFileToContainer(\n+                            forHostPath(getResource(\"middleManager-jvm.config\").getPath()),\n+                            \"/opt/druid/conf/druid/cluster/data/middleManager/jvm.config\")\n+                    .waitingFor(Wait.forHttp(\"/status/selfDiscovered\"));\n+            middleManager.start();\n+        }\n+        catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    public String getHostWorkingDirectory()\n+    {\n+        return hostWorkingDirectory;\n+    }\n+\n+    @Override\n+    public void close()\n+    {\n+        try (Closer closer = Closer.create()) {\n+            closer.register(broker::stop);\n+            closer.register(historical::stop);\n+            closer.register(middleManager::stop);\n+            closer.register(coordinator::stop);\n+            closer.register(zookeeper::stop);\n+        }\n+        catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+        finally {\n+            cleanDirectory(new File(hostWorkingDirectory));\n+        }\n+    }\n+\n+    public String getJdbcUrl()\n+    {\n+        return getJdbcUrl(broker.getMappedPort(DRUID_BROKER_PORT));\n+    }\n+\n+    public int getCoordinatorOverlordPort()\n+    {\n+        return coordinator.getMappedPort(DRUID_COORDINATOR_PORT);\n+    }\n+\n+    private static String getJdbcUrl(int port)\n+    {\n+        return format(\"jdbc:avatica:remote:url=http://localhost:%s/druid/v2/sql/avatica/\", port);\n+    }\n+\n+    void ingestData(String datasource, String indexTaskFile, String dataFilePath)\n+            throws IOException, InterruptedException\n+    {\n+        middleManager.withCopyFileToContainer(forHostPath(dataFilePath),\n+                getMiddleManagerContainerPathForDataFile(dataFilePath));\n+        String indexTask = Resources.toString(getResource(indexTaskFile), Charset.defaultCharset());\n+\n+        Request.Builder requestBuilder = new Request.Builder();\n+        requestBuilder.addHeader(\"content-type\", \"application/json;charset=utf-8\")\n+                .url(\"http://localhost:\" + getCoordinatorOverlordPort() + \"/druid/indexer/v1/task\")\n+                .post(RequestBody.create(null, indexTask));\n+        Request ingestionRequest = requestBuilder.build();\n+        Response response = null;\n+        try {\n+            response = httpClient.newCall(ingestionRequest).execute();\n+            Assert.assertTrue(checkDatasourceAvailable(datasource), \"Datasource \" + datasource + \" not loaded\");\n+        }\n+        finally {\n+            if (response != null) {\n+                response.close();\n+            }\n+        }\n+    }\n+\n+    private boolean checkDatasourceAvailable(String datasource)\n+            throws IOException, InterruptedException\n+    {\n+        Map<String, Double> datasourceAvailabilityDetails = null;\n+        boolean datasourceNotLoaded = true;\n+        int attempts = 10;\n+        while (datasourceNotLoaded && attempts > 0) {\n+            Request.Builder requestBuilder = new Request.Builder();\n+            requestBuilder.url(\"http://localhost:\" + getCoordinatorOverlordPort() + \"/druid/coordinator/v1/loadstatus\")\n+                    .get();\n+            Request datasourceAvailabilityRequest = requestBuilder.build();\n+            try (Response response = httpClient.newCall(datasourceAvailabilityRequest).execute()) {\n+                ObjectMapper mapper = new ObjectMapper();\n+                datasourceAvailabilityDetails = mapper.readValue(response.body().string(), Map.class);\n+                datasourceNotLoaded = datasourceAvailabilityDetails.get(datasource) == null || Double.compare(datasourceAvailabilityDetails.get(datasource), 100.0) < 0;\n+                if (datasourceNotLoaded) {\n+                    attempts--;\n+                    // Wait for some time since it can take a while for coordinator to load the ingested segments\n+                    Thread.sleep(15000);\n+                }\n+            }\n+        }\n+        return !datasourceNotLoaded;\n+    }\n+\n+    private static String getMiddleManagerContainerPathForDataFile(String dataFilePath)\n+    {\n+        return String.format(\"/opt/druid/var/%s\", new File(dataFilePath).getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 252}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE0Njc2NQ==", "bodyText": "It's written this way in superclass, as it's generic. In concrete subclass use\nassertThat(computeActual(\"SHOW CREATE TABLE orders\").getOnlyValue())\n        .isEqualTo(\"CREATE TABLE druid.druid.orders (\\n\" +\n              ....", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r442146765", "createdAt": "2020-06-18T11:06:14Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestDruidIntegrationSmokeTest.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestIntegrationSmokeTest;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.assertions.Assert;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+\n+import static io.prestosql.plugin.druid.DruidQueryRunner.copyAndIngestTpchData;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static io.prestosql.tpch.TpchTable.ORDERS;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+@Test\n+public class TestDruidIntegrationSmokeTest\n+        extends AbstractTestIntegrationSmokeTest\n+{\n+    private TestingDruidServer druidServer;\n+    private static final String SELECT_FROM_ORDERS = \"SELECT \" +\n+            \"orderdate, \" +\n+            \"orderdate AS orderdate_druid_ts, \" + // Druid stores the orderdate_druid_ts column as __time column.\n+            \"orderkey, \" +\n+            \"custkey, \" +\n+            \"orderstatus, \" +\n+            \"totalprice, \" +\n+            \"orderpriority, \" +\n+            \"clerk, \" +\n+            \"shippriority, \" +\n+            \"comment \" +\n+            \"FROM \" + (\"tpch.tiny.\" + ORDERS.getTableName());\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        this.druidServer = new TestingDruidServer();\n+        QueryRunner runner = DruidQueryRunner.createDruidQueryRunnerTpch(druidServer);\n+        copyAndIngestTpchData(runner.execute(SELECT_FROM_ORDERS), this.druidServer, ORDERS.getTableName());\n+        return runner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void destroy()\n+    {\n+        if (druidServer != null) {\n+            druidServer.close();\n+        }\n+    }\n+\n+    @Test\n+    @Override\n+    public void testDescribeTable()\n+    {\n+        MaterializedResult expectedColumns = MaterializedResult.resultBuilder(getQueryRunner().getDefaultSession(), VARCHAR, VARCHAR, VARCHAR, VARCHAR)\n+                .row(\"__time\", \"timestamp(3)\", \"\", \"\")\n+                .row(\"clerk\", \"varchar\", \"\", \"\") // String columns are reported only as varchar\n+                .row(\"comment\", \"varchar\", \"\", \"\")\n+                .row(\"custkey\", \"bigint\", \"\", \"\") // Long columns are reported as bigint\n+                .row(\"orderdate\", \"varchar\", \"\", \"\")\n+                .row(\"orderkey\", \"bigint\", \"\", \"\")\n+                .row(\"orderpriority\", \"varchar\", \"\", \"\")\n+                .row(\"orderstatus\", \"varchar\", \"\", \"\")\n+                .row(\"shippriority\", \"bigint\", \"\", \"\") // Druid doesn't support int type\n+                .row(\"totalprice\", \"double\", \"\", \"\")\n+                .build();\n+        MaterializedResult actualColumns = computeActual(\"DESCRIBE orders\");\n+        Assert.assertEquals(actualColumns, expectedColumns);\n+    }\n+\n+    @Test\n+    @Override\n+    public void testShowCreateTable()\n+    {\n+        assertThat((String) computeActual(\"SHOW CREATE TABLE orders\").getOnlyValue())\n+                // If the connector reports additional column properties, the expected value needs to be adjusted in the test subclass\n+                .matches(\"CREATE TABLE \\\\w+\\\\.\\\\w+\\\\.orders \\\\Q(\\n\" +\n+                        \"   __time timestamp(3) NOT NULL,\\n\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE0Njk0MA==", "bodyText": "nit\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    assertThatThrownBy(() -> super.testJoin())\n          \n          \n            \n                    assertThatThrownBy(super::testJoin)", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r442146940", "createdAt": "2020-06-18T11:06:39Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestDruidIntegrationSmokeTest.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestIntegrationSmokeTest;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.assertions.Assert;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+\n+import static io.prestosql.plugin.druid.DruidQueryRunner.copyAndIngestTpchData;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static io.prestosql.tpch.TpchTable.ORDERS;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+@Test\n+public class TestDruidIntegrationSmokeTest\n+        extends AbstractTestIntegrationSmokeTest\n+{\n+    private TestingDruidServer druidServer;\n+    private static final String SELECT_FROM_ORDERS = \"SELECT \" +\n+            \"orderdate, \" +\n+            \"orderdate AS orderdate_druid_ts, \" + // Druid stores the orderdate_druid_ts column as __time column.\n+            \"orderkey, \" +\n+            \"custkey, \" +\n+            \"orderstatus, \" +\n+            \"totalprice, \" +\n+            \"orderpriority, \" +\n+            \"clerk, \" +\n+            \"shippriority, \" +\n+            \"comment \" +\n+            \"FROM \" + (\"tpch.tiny.\" + ORDERS.getTableName());\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        this.druidServer = new TestingDruidServer();\n+        QueryRunner runner = DruidQueryRunner.createDruidQueryRunnerTpch(druidServer);\n+        copyAndIngestTpchData(runner.execute(SELECT_FROM_ORDERS), this.druidServer, ORDERS.getTableName());\n+        return runner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void destroy()\n+    {\n+        if (druidServer != null) {\n+            druidServer.close();\n+        }\n+    }\n+\n+    @Test\n+    @Override\n+    public void testDescribeTable()\n+    {\n+        MaterializedResult expectedColumns = MaterializedResult.resultBuilder(getQueryRunner().getDefaultSession(), VARCHAR, VARCHAR, VARCHAR, VARCHAR)\n+                .row(\"__time\", \"timestamp(3)\", \"\", \"\")\n+                .row(\"clerk\", \"varchar\", \"\", \"\") // String columns are reported only as varchar\n+                .row(\"comment\", \"varchar\", \"\", \"\")\n+                .row(\"custkey\", \"bigint\", \"\", \"\") // Long columns are reported as bigint\n+                .row(\"orderdate\", \"varchar\", \"\", \"\")\n+                .row(\"orderkey\", \"bigint\", \"\", \"\")\n+                .row(\"orderpriority\", \"varchar\", \"\", \"\")\n+                .row(\"orderstatus\", \"varchar\", \"\", \"\")\n+                .row(\"shippriority\", \"bigint\", \"\", \"\") // Druid doesn't support int type\n+                .row(\"totalprice\", \"double\", \"\", \"\")\n+                .build();\n+        MaterializedResult actualColumns = computeActual(\"DESCRIBE orders\");\n+        Assert.assertEquals(actualColumns, expectedColumns);\n+    }\n+\n+    @Test\n+    @Override\n+    public void testShowCreateTable()\n+    {\n+        assertThat((String) computeActual(\"SHOW CREATE TABLE orders\").getOnlyValue())\n+                // If the connector reports additional column properties, the expected value needs to be adjusted in the test subclass\n+                .matches(\"CREATE TABLE \\\\w+\\\\.\\\\w+\\\\.orders \\\\Q(\\n\" +\n+                        \"   __time timestamp(3) NOT NULL,\\n\" +\n+                        \"   clerk varchar,\\n\" +\n+                        \"   comment varchar,\\n\" +\n+                        \"   custkey bigint NOT NULL,\\n\" +\n+                        \"   orderdate varchar,\\n\" +\n+                        \"   orderkey bigint NOT NULL,\\n\" +\n+                        \"   orderpriority varchar,\\n\" +\n+                        \"   orderstatus varchar,\\n\" +\n+                        \"   shippriority bigint NOT NULL,\\n\" +\n+                        \"   totalprice double NOT NULL\\n\" +\n+                        \")\");\n+    }\n+\n+    @Test\n+    @Override\n+    public void testSelectInformationSchemaColumns()\n+    {\n+        String catalog = getSession().getCatalog().get();\n+        String schema = getSession().getSchema().get();\n+        String schemaPattern = schema.replaceAll(\".$\", \"_\");\n+\n+        @Language(\"SQL\") String ordersTableWithColumns = \"VALUES \" +\n+                \"('orders', 'orderkey'), \" +\n+                \"('orders', 'custkey'), \" +\n+                \"('orders', 'orderstatus'), \" +\n+                \"('orders', 'totalprice'), \" +\n+                \"('orders', 'orderdate'), \" +\n+                \"('orders', '__time'), \" +\n+                \"('orders', 'orderpriority'), \" +\n+                \"('orders', 'clerk'), \" +\n+                \"('orders', 'shippriority'), \" +\n+                \"('orders', 'comment')\";\n+\n+        assertQuery(\"SELECT table_schema FROM information_schema.columns WHERE table_schema = '\" + schema + \"' GROUP BY table_schema\", \"VALUES '\" + schema + \"'\");\n+        assertQuery(\"SELECT table_name FROM information_schema.columns WHERE table_name = 'orders' GROUP BY table_name\", \"VALUES 'orders'\");\n+        assertQuery(\"SELECT table_name, column_name FROM information_schema.columns WHERE table_schema = '\" + schema + \"' AND table_name = 'orders'\", ordersTableWithColumns);\n+        assertQuery(\"SELECT table_name, column_name FROM information_schema.columns WHERE table_schema = '\" + schema + \"' AND table_name LIKE '%rders'\", ordersTableWithColumns);\n+        assertQuery(\"SELECT table_name, column_name FROM information_schema.columns WHERE table_schema LIKE '\" + schemaPattern + \"' AND table_name LIKE '_rder_'\", ordersTableWithColumns);\n+        assertQuery(\n+                \"SELECT table_name, column_name FROM information_schema.columns \" +\n+                        \"WHERE table_catalog = '\" + catalog + \"' AND table_schema = '\" + schema + \"' AND table_name LIKE '%orders%'\",\n+                ordersTableWithColumns);\n+\n+        assertQuerySucceeds(\"SELECT * FROM information_schema.columns\");\n+        assertQuery(\"SELECT DISTINCT table_name, column_name FROM information_schema.columns WHERE table_name LIKE '_rders'\", ordersTableWithColumns);\n+        assertQuerySucceeds(\"SELECT * FROM information_schema.columns WHERE table_catalog = '\" + catalog + \"'\");\n+        assertQuerySucceeds(\"SELECT * FROM information_schema.columns WHERE table_catalog = '\" + catalog + \"' AND table_schema = '\" + schema + \"'\");\n+        assertQuery(\"SELECT table_name, column_name FROM information_schema.columns WHERE table_catalog = '\" + catalog + \"' AND table_schema = '\" + schema + \"' AND table_name LIKE '_rders'\", ordersTableWithColumns);\n+        assertQuerySucceeds(\"SELECT * FROM information_schema.columns WHERE table_catalog = '\" + catalog + \"' AND table_name LIKE '%'\");\n+        assertQuery(\"SELECT column_name FROM information_schema.columns WHERE table_catalog = 'something_else'\", \"SELECT '' WHERE false\");\n+    }\n+\n+    @Test\n+    @Override\n+    public void testSelectAll()\n+    {\n+        // List columns explicitly, as we ingest orderdate column twice in Druid\n+        assertQuery(\"SELECT orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment  FROM orders\");\n+    }\n+\n+    @Test\n+    @Override\n+    // nation, region, customer tables don't have any date/time in the rows.\n+    // For a table to be ingested into Druid, a date/time/timestamp column is required.\n+    //TODO: This test needs to be adjusted to possibly join between lineitems and orders datasources\n+    public void testJoin()\n+    {\n+        assertThatThrownBy(() -> super.testJoin())", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE0NzE5MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        \"FROM \" + (\"tpch.tiny.\" + ORDERS.getTableName());\n          \n          \n            \n                        \"FROM tpch.tiny.orders\";", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r442147191", "createdAt": "2020-06-18T11:07:05Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestDruidIntegrationSmokeTest.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestIntegrationSmokeTest;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.assertions.Assert;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+\n+import static io.prestosql.plugin.druid.DruidQueryRunner.copyAndIngestTpchData;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static io.prestosql.tpch.TpchTable.ORDERS;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+@Test\n+public class TestDruidIntegrationSmokeTest\n+        extends AbstractTestIntegrationSmokeTest\n+{\n+    private TestingDruidServer druidServer;\n+    private static final String SELECT_FROM_ORDERS = \"SELECT \" +\n+            \"orderdate, \" +\n+            \"orderdate AS orderdate_druid_ts, \" + // Druid stores the orderdate_druid_ts column as __time column.\n+            \"orderkey, \" +\n+            \"custkey, \" +\n+            \"orderstatus, \" +\n+            \"totalprice, \" +\n+            \"orderpriority, \" +\n+            \"clerk, \" +\n+            \"shippriority, \" +\n+            \"comment \" +\n+            \"FROM \" + (\"tpch.tiny.\" + ORDERS.getTableName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE0NzI5NQ==", "bodyText": "move below the constant", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r442147295", "createdAt": "2020-06-18T11:07:20Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestDruidIntegrationSmokeTest.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestIntegrationSmokeTest;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.assertions.Assert;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+\n+import static io.prestosql.plugin.druid.DruidQueryRunner.copyAndIngestTpchData;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static io.prestosql.tpch.TpchTable.ORDERS;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+@Test\n+public class TestDruidIntegrationSmokeTest\n+        extends AbstractTestIntegrationSmokeTest\n+{\n+    private TestingDruidServer druidServer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE1MDcwOA==", "bodyText": "We could ingest these tables with some fake time column.\nA follow-up", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r442150708", "createdAt": "2020-06-18T11:14:35Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestDruidIntegrationSmokeTest.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.druid;\n+\n+import io.prestosql.plugin.jdbc.JdbcIdentity;\n+import io.prestosql.plugin.jdbc.JdbcTableHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.testing.AbstractTestIntegrationSmokeTest;\n+import io.prestosql.testing.MaterializedResult;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.assertions.Assert;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+\n+import static io.prestosql.plugin.druid.DruidQueryRunner.copyAndIngestTpchData;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static io.prestosql.tpch.TpchTable.ORDERS;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+@Test\n+public class TestDruidIntegrationSmokeTest\n+        extends AbstractTestIntegrationSmokeTest\n+{\n+    private TestingDruidServer druidServer;\n+    private static final String SELECT_FROM_ORDERS = \"SELECT \" +\n+            \"orderdate, \" +\n+            \"orderdate AS orderdate_druid_ts, \" + // Druid stores the orderdate_druid_ts column as __time column.\n+            \"orderkey, \" +\n+            \"custkey, \" +\n+            \"orderstatus, \" +\n+            \"totalprice, \" +\n+            \"orderpriority, \" +\n+            \"clerk, \" +\n+            \"shippriority, \" +\n+            \"comment \" +\n+            \"FROM \" + (\"tpch.tiny.\" + ORDERS.getTableName());\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        this.druidServer = new TestingDruidServer();\n+        QueryRunner runner = DruidQueryRunner.createDruidQueryRunnerTpch(druidServer);\n+        copyAndIngestTpchData(runner.execute(SELECT_FROM_ORDERS), this.druidServer, ORDERS.getTableName());\n+        return runner;\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void destroy()\n+    {\n+        if (druidServer != null) {\n+            druidServer.close();\n+        }\n+    }\n+\n+    @Test\n+    @Override\n+    public void testDescribeTable()\n+    {\n+        MaterializedResult expectedColumns = MaterializedResult.resultBuilder(getQueryRunner().getDefaultSession(), VARCHAR, VARCHAR, VARCHAR, VARCHAR)\n+                .row(\"__time\", \"timestamp(3)\", \"\", \"\")\n+                .row(\"clerk\", \"varchar\", \"\", \"\") // String columns are reported only as varchar\n+                .row(\"comment\", \"varchar\", \"\", \"\")\n+                .row(\"custkey\", \"bigint\", \"\", \"\") // Long columns are reported as bigint\n+                .row(\"orderdate\", \"varchar\", \"\", \"\")\n+                .row(\"orderkey\", \"bigint\", \"\", \"\")\n+                .row(\"orderpriority\", \"varchar\", \"\", \"\")\n+                .row(\"orderstatus\", \"varchar\", \"\", \"\")\n+                .row(\"shippriority\", \"bigint\", \"\", \"\") // Druid doesn't support int type\n+                .row(\"totalprice\", \"double\", \"\", \"\")\n+                .build();\n+        MaterializedResult actualColumns = computeActual(\"DESCRIBE orders\");\n+        Assert.assertEquals(actualColumns, expectedColumns);\n+    }\n+\n+    @Test\n+    @Override\n+    public void testShowCreateTable()\n+    {\n+        assertThat((String) computeActual(\"SHOW CREATE TABLE orders\").getOnlyValue())\n+                // If the connector reports additional column properties, the expected value needs to be adjusted in the test subclass\n+                .matches(\"CREATE TABLE \\\\w+\\\\.\\\\w+\\\\.orders \\\\Q(\\n\" +\n+                        \"   __time timestamp(3) NOT NULL,\\n\" +\n+                        \"   clerk varchar,\\n\" +\n+                        \"   comment varchar,\\n\" +\n+                        \"   custkey bigint NOT NULL,\\n\" +\n+                        \"   orderdate varchar,\\n\" +\n+                        \"   orderkey bigint NOT NULL,\\n\" +\n+                        \"   orderpriority varchar,\\n\" +\n+                        \"   orderstatus varchar,\\n\" +\n+                        \"   shippriority bigint NOT NULL,\\n\" +\n+                        \"   totalprice double NOT NULL\\n\" +\n+                        \")\");\n+    }\n+\n+    @Test\n+    @Override\n+    public void testSelectInformationSchemaColumns()\n+    {\n+        String catalog = getSession().getCatalog().get();\n+        String schema = getSession().getSchema().get();\n+        String schemaPattern = schema.replaceAll(\".$\", \"_\");\n+\n+        @Language(\"SQL\") String ordersTableWithColumns = \"VALUES \" +\n+                \"('orders', 'orderkey'), \" +\n+                \"('orders', 'custkey'), \" +\n+                \"('orders', 'orderstatus'), \" +\n+                \"('orders', 'totalprice'), \" +\n+                \"('orders', 'orderdate'), \" +\n+                \"('orders', '__time'), \" +\n+                \"('orders', 'orderpriority'), \" +\n+                \"('orders', 'clerk'), \" +\n+                \"('orders', 'shippriority'), \" +\n+                \"('orders', 'comment')\";\n+\n+        assertQuery(\"SELECT table_schema FROM information_schema.columns WHERE table_schema = '\" + schema + \"' GROUP BY table_schema\", \"VALUES '\" + schema + \"'\");\n+        assertQuery(\"SELECT table_name FROM information_schema.columns WHERE table_name = 'orders' GROUP BY table_name\", \"VALUES 'orders'\");\n+        assertQuery(\"SELECT table_name, column_name FROM information_schema.columns WHERE table_schema = '\" + schema + \"' AND table_name = 'orders'\", ordersTableWithColumns);\n+        assertQuery(\"SELECT table_name, column_name FROM information_schema.columns WHERE table_schema = '\" + schema + \"' AND table_name LIKE '%rders'\", ordersTableWithColumns);\n+        assertQuery(\"SELECT table_name, column_name FROM information_schema.columns WHERE table_schema LIKE '\" + schemaPattern + \"' AND table_name LIKE '_rder_'\", ordersTableWithColumns);\n+        assertQuery(\n+                \"SELECT table_name, column_name FROM information_schema.columns \" +\n+                        \"WHERE table_catalog = '\" + catalog + \"' AND table_schema = '\" + schema + \"' AND table_name LIKE '%orders%'\",\n+                ordersTableWithColumns);\n+\n+        assertQuerySucceeds(\"SELECT * FROM information_schema.columns\");\n+        assertQuery(\"SELECT DISTINCT table_name, column_name FROM information_schema.columns WHERE table_name LIKE '_rders'\", ordersTableWithColumns);\n+        assertQuerySucceeds(\"SELECT * FROM information_schema.columns WHERE table_catalog = '\" + catalog + \"'\");\n+        assertQuerySucceeds(\"SELECT * FROM information_schema.columns WHERE table_catalog = '\" + catalog + \"' AND table_schema = '\" + schema + \"'\");\n+        assertQuery(\"SELECT table_name, column_name FROM information_schema.columns WHERE table_catalog = '\" + catalog + \"' AND table_schema = '\" + schema + \"' AND table_name LIKE '_rders'\", ordersTableWithColumns);\n+        assertQuerySucceeds(\"SELECT * FROM information_schema.columns WHERE table_catalog = '\" + catalog + \"' AND table_name LIKE '%'\");\n+        assertQuery(\"SELECT column_name FROM information_schema.columns WHERE table_catalog = 'something_else'\", \"SELECT '' WHERE false\");\n+    }\n+\n+    @Test\n+    @Override\n+    public void testSelectAll()\n+    {\n+        // List columns explicitly, as we ingest orderdate column twice in Druid\n+        assertQuery(\"SELECT orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment  FROM orders\");\n+    }\n+\n+    @Test\n+    @Override\n+    // nation, region, customer tables don't have any date/time in the rows.\n+    // For a table to be ingested into Druid, a date/time/timestamp column is required.\n+    //TODO: This test needs to be adjusted to possibly join between lineitems and orders datasources", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 161}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzNjM5MzQ4", "url": "https://github.com/trinodb/trino/pull/3522#pullrequestreview-433639348", "createdAt": "2020-06-18T20:53:59Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMDo1Mzo1OVrOGl_2GA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMDo1Mzo1OVrOGl_2GA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ5NjUzNg==", "bodyText": "you probably need to remove pom.xml dep now", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r442496536", "createdAt": "2020-06-18T20:53:59Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestingDruidServer.java", "diffHunk": "@@ -17,7 +17,6 @@\n import com.google.common.io.Closer;\n import com.google.common.io.Resources;\n import io.prestosql.testing.assertions.Assert;\n-import io.prestosql.tests.product.launcher.testcontainers.SelectedPortWaitStrategy;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzNjQyNTU1", "url": "https://github.com/trinodb/trino/pull/3522#pullrequestreview-433642555", "createdAt": "2020-06-18T20:58:48Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMDo1ODo0OFrOGl__ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMDo1ODo0OFrOGl__ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ5ODkyMg==", "bodyText": "This fails on CI.\nCould it be that previous code was failing silently too?", "url": "https://github.com/trinodb/trino/pull/3522#discussion_r442498922", "createdAt": "2020-06-18T20:58:48Z", "author": {"login": "findepi"}, "path": "presto-druid/src/test/java/io/prestosql/plugin/druid/TestingDruidServer.java", "diffHunk": "@@ -169,6 +176,8 @@ public String getHostWorkingDirectory()\n     public void close()\n     {\n         try (Closer closer = Closer.create()) {\n+            closer.register(() ->\n+                    deleteDirectoryContents(Path.of(hostWorkingDirectory), ALLOW_INSECURE));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 122}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzOTQxMDE0", "url": "https://github.com/trinodb/trino/pull/3522#pullrequestreview-433941014", "createdAt": "2020-06-19T09:41:54Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e5663d1a2948c6a9e1e1decc922928ccfcf2808f", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/trinodb/trino/commit/e5663d1a2948c6a9e1e1decc922928ccfcf2808f", "committedDate": "2020-06-19T09:50:03Z", "message": "Presto Druid connector\n\nCo-authored-by: Puneet Jaiswal <punit.kj@gmail.com>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "e5663d1a2948c6a9e1e1decc922928ccfcf2808f", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/trinodb/trino/commit/e5663d1a2948c6a9e1e1decc922928ccfcf2808f", "committedDate": "2020-06-19T09:50:03Z", "message": "Presto Druid connector\n\nCo-authored-by: Puneet Jaiswal <punit.kj@gmail.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0MDAwODM3", "url": "https://github.com/trinodb/trino/pull/3522#pullrequestreview-434000837", "createdAt": "2020-06-19T11:23:34Z", "commit": {"oid": "e5663d1a2948c6a9e1e1decc922928ccfcf2808f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1625, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}