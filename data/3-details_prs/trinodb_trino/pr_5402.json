{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk3MzE0NDg2", "number": 5402, "title": "Hive ACID row-level insert and delete", "bodyText": "This commit adds support for row-level insert and delete for Hive ACID tables,\nand product tests that verify that row-level insert (and delete where allowed)\nare working correctly for this test matrix:\n(partitioned | unpartitioned) X\n(bucketed | unbucketed) X\n(normal | insert-only) X\n(original files | non-original files) X\n(Presto inserts | Hive inserts) X\n(first insert | subsequent inserts) X\n(Presto deletes | Hive deletes) X\n(first delete | subsequent deletes) X\n(Hive selects to verify | Presto selects to verify)\nThe tests also verify that metadata delete still works correctly for non-ACID\ntables, and that row-level delete is always used for ACID tables.  Hive ACID\ninsert and delete make metastore updates using the delayed commit paradigm\nprovided by SemiTransactionalHiveMetastore.\nACID insert and delete need four Hive metastore methods not previously\nused, and this commit adds the plumbing through the many metastore layers\nfor those methods.\nRecords to be deleted flow through HiveUpdatablePageSource, which feeds an OrcWriter\nthe three columns - - originalTransaction, bucketId, rowId - - that identify an\nACID row to be deleted.  The writer builds the ACID delete_delta bucket, adding\nthe Orc ACID operation column specifying delete and the currentTransaction column\nto each row.\nA complete delete scan will in general read many Orc files from earlier transactions.\nThe delete implementation will create a HiveUpdatablePageSource for each file.\nEach will write a delete_delta file, distinguished by different statementIds.\nThese delete_delta bucket files and directories will be combined by the Hive\nACID compactor as the accumulate.\nACID insert is simpler than delete - - it just adds the 5 ACID columns needed\nto make insertion transactional, and creates delta directories to hold bucket files.", "createdAt": "2020-10-03T17:57:06Z", "url": "https://github.com/trinodb/trino/pull/5402", "merged": true, "mergeCommit": {"oid": "58ee2329fe769042b72974e5b683332e09fc973e"}, "closed": true, "closedAt": "2020-10-22T15:40:12Z", "author": {"login": "djsstarburst"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdO-g4KAFqTUwMTU2MzU4NA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABejq8TWgFqTY5MTE0OTQzNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNTYzNTg0", "url": "https://github.com/trinodb/trino/pull/5402#pullrequestreview-501563584", "createdAt": "2020-10-03T18:02:11Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxODowMjoxMVrOHcC2Bg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxODowMjoxMVrOHcC2Bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2ODc3NA==", "bodyText": "I like this helper function, and the order and economy it brings to the tests I've added.  If there is no objection, I'll follow this PR with a PR that makes no semantic change but updates the previously existing tests to use this helper function.", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r499168774", "createdAt": "2020-10-03T18:02:11Z", "author": {"login": "djsstarburst"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/hive/TestHiveTransactionalTable.java", "diffHunk": "@@ -345,16 +447,378 @@ public void testCtasAcidTable(boolean isPartitioned, BucketingType bucketingType\n     @Test(groups = HIVE_TRANSACTIONAL, dataProvider = \"testCreateAcidTableDataProvider\")\n     public void testCreateAcidTable(boolean isPartitioned, BucketingType bucketingType)\n     {\n-        if (getHiveVersionMajor() < 3) {\n-            throw new SkipException(\"Hive transactional tables are supported with Hive version 3 or above\");\n-        }\n-\n-        try (TemporaryHiveTable table = TemporaryHiveTable.temporaryHiveTable(format(\"create_transactional_%s\", randomTableSuffix()))) {\n-            String tableName = table.getName();\n+        withTemporaryTable(\"create_transactional\", true, isPartitioned, bucketingType, tableName -> {\n             query(\"CREATE TABLE \" + tableName + \" (col INTEGER, fcol INTEGER, partcol INTEGER)\" +\n                     prestoTableProperties(ACID, isPartitioned, bucketingType));\n \n-            assertThat(() -> query(\"INSERT INTO \" + tableName + \" VALUES (1,2,3)\")).failsWithMessageMatching(\".*Writes to Hive transactional tables are not supported.*\");\n+            query(\"INSERT INTO \" + tableName + \" VALUES (1, 2, 3)\");\n+            assertThat(query(\"SELECT * FROM \" + tableName)).containsOnly(row(1, 2, 3));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL)\n+    public void testSimpleUnpartitionedTransactionalInsert()\n+    {\n+        withTemporaryTable(\"unpartitioned_transactional_insert\", true, false, NONE, tableName -> {\n+            onPresto().executeQuery(format(\"CREATE TABLE %s (column1 INT, column2 BIGINT) WITH (transactional = true)\", tableName));\n+\n+            onPresto().executeQuery(format(\"INSERT INTO %s VALUES (11, 100), (12, 200), (13, 300)\", tableName));\n+\n+            verifySelectForPrestoAndHive(\"SELECT * FROM \" + tableName, \"true\", row(11, 100L), row(12, 200L), row(13, 300L));\n+\n+            onPresto().executeQuery(format(\"INSERT INTO %s VALUES (14, 400), (15, 500), (16, 600)\", tableName));\n+\n+            verifySelectForPrestoAndHive(\"SELECT * FROM \" + tableName, \"true\", row(11, 100L), row(12, 200L), row(13, 300L), row(14, 400L), row(15, 500L), row(16, 600L));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL)\n+    public void testTransactionalPartitionInsert()\n+    {\n+        withTemporaryTable(\"transactional_partition_insert\", true, true, NONE, tableName -> {\n+            onPresto().executeQuery(format(\"CREATE TABLE %s (column1 INT, column2 BIGINT) WITH (transactional = true, partitioned_by = ARRAY['column2'])\", tableName));\n+\n+            onPresto().executeQuery(format(\"INSERT INTO %s (column2, column1) VALUES %s, %s\",\n+                    tableName,\n+                    makeInsertValues(1, 1, 20),\n+                    makeInsertValues(2, 1, 20)));\n+\n+            verifySelectForPrestoAndHive(format(\"SELECT COUNT(*) FROM %s\", tableName), \"column1 > 10\", row(20));\n+\n+            onPresto().executeQuery(format(\"INSERT INTO %s (column2, column1) VALUES %s, %s\",\n+                    tableName,\n+                    makeInsertValues(1, 21, 30),\n+                    makeInsertValues(2, 21, 30)));\n+\n+            verifySelectForPrestoAndHive(format(\"SELECT COUNT(*) FROM %s\", tableName), \"column1 > 15 AND column1 <= 25\", row(20));\n+\n+            onHive().executeQuery(format(\"DELETE FROM %s WHERE column1 > 15 AND column1 <= 25\", tableName));\n+\n+            verifySelectForPrestoAndHive(format(\"SELECT COUNT(*) FROM %s\", tableName), \"column1 > 15 AND column1 <= 25\", row(0));\n+\n+            onPresto().executeQuery(format(\"INSERT INTO %s (column2, column1) VALUES %s, %s\",\n+                    tableName,\n+                    makeInsertValues(1, 20, 23),\n+                    makeInsertValues(2, 20, 23)));\n+\n+            verifySelectForPrestoAndHive(format(\"SELECT COUNT(*) FROM %s\", tableName), \"column1 > 15 AND column1 <= 25\", row(8));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL)\n+    public void testTransactionalBucketedPartitionedInsert()\n+    {\n+        testTransactionalBucketedPartitioned(false);\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL)\n+    public void testTransactionalBucketedPartitionedInsertOnly()\n+    {\n+        testTransactionalBucketedPartitioned(true);\n+    }\n+\n+    private void testTransactionalBucketedPartitioned(boolean insertOnly)\n+    {\n+        withTemporaryTable(\"bucketed_partitioned_insert_only\", true, true, BUCKETED_V2, tableName -> {\n+            String insertOnlyProperty = insertOnly ? \", 'transactional_properties'='insert_only'\" : \"\";\n+            onHive().executeQuery(format(\"CREATE TABLE %s (purchase STRING) PARTITIONED BY (customer STRING) CLUSTERED BY (purchase) INTO 3 BUCKETS\" +\n+                            \" STORED AS ORC TBLPROPERTIES ('transactional' = 'true'%s)\",\n+                    tableName, insertOnlyProperty));\n+\n+            onPresto().executeQuery(format(\"INSERT INTO %s (customer, purchase) VALUES\", tableName) +\n+                    \" ('Fred', 'cards'), ('Fred', 'cereal'), ('Fred', 'limes'), ('Fred', 'chips'),\" +\n+                    \" ('Ann', 'cards'), ('Ann', 'cereal'), ('Ann', 'lemons'), ('Ann', 'chips'),\" +\n+                    \" ('Lou', 'cards'), ('Lou', 'cereal'), ('Lou', 'lemons'), ('Lou', 'chips')\");\n+\n+            verifySelectForPrestoAndHive(format(\"SELECT customer FROM %s\", tableName), \"purchase = 'lemons'\", row(\"Ann\"), row(\"Lou\"));\n+\n+            verifySelectForPrestoAndHive(format(\"SELECT purchase FROM %s\", tableName), \"customer = 'Fred'\", row(\"cards\"), row(\"cereal\"), row(\"limes\"), row(\"chips\"));\n+\n+            onPresto().executeQuery(format(\"INSERT INTO %s (customer, purchase) VALUES\", tableName) +\n+                    \" ('Ernie', 'cards'), ('Ernie', 'cereal'),\" +\n+                    \" ('Debby', 'corn'), ('Debby', 'chips'),\" +\n+                    \" ('Joe', 'corn'), ('Joe', 'lemons'), ('Joe', 'candy')\");\n+\n+            verifySelectForPrestoAndHive(format(\"SELECT customer FROM %s\", tableName), \"purchase = 'corn'\", row(\"Debby\"), row(\"Joe\"));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL, dataProvider = \"inserterAndDeleterProvider\", timeOut = TEST_TIMEOUT)\n+    public void testTransactionalUnpartitionedDelete(HiveOrPresto inserter, HiveOrPresto deleter)\n+    {\n+        withTemporaryTable(\"unpartitioned_delete\", true, false, NONE, tableName -> {\n+            onPresto().executeQuery(format(\"CREATE TABLE %s (column1 INTEGER, column2 BIGINT) WITH (format = 'ORC', transactional = true)\", tableName));\n+            execute(inserter, format(\"INSERT INTO %s (column1, column2) VALUES (1, 100), (2, 200), (3, 300), (4, 400), (5, 500)\", tableName));\n+            execute(deleter, format(\"DELETE FROM %s WHERE column2 = 100\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT * FROM \" + tableName, \"true\", row(2, 200), row(3, 300), row(4, 400), row(5, 500));\n+\n+            execute(inserter, format(\"INSERT INTO %s VALUES (6, 600), (7, 700)\", tableName));\n+            execute(deleter, format(\"DELETE FROM %s WHERE column1 = 4\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT * FROM \" + tableName, \"true\", row(2, 200), row(3, 300), row(5, 500), row(6, 600), row(7, 700));\n+\n+            execute(deleter, format(\"DELETE FROM %s WHERE column1 <= 3 OR column1 = 6\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT * FROM \" + tableName, \"true\", row(5, 500), row(7, 700));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL, dataProvider = \"inserterAndDeleterProvider\", timeOut = TEST_TIMEOUT)\n+    public void testMultiDelete(HiveOrPresto inserter, HiveOrPresto deleter)\n+    {\n+        withTemporaryTable(\"unpartitioned_multi_delete\", true, false, NONE, tableName -> {\n+            onPresto().executeQuery(format(\"CREATE TABLE %s (column1 INT, column2 BIGINT) WITH (transactional = true)\", tableName));\n+            execute(inserter, format(\"INSERT INTO %s VALUES (1, 100), (2, 200), (3, 300), (4, 400), (5, 500)\", tableName));\n+            execute(inserter, format(\"INSERT INTO %s VALUES (6, 600), (7, 700), (8, 800), (9, 900), (10, 1000)\", tableName));\n+\n+            execute(deleter, format(\"DELETE FROM %s WHERE column1 = 9\", tableName));\n+            execute(deleter, format(\"DELETE FROM %s WHERE column1 = 2 OR column1 = 3\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT * FROM \" + tableName, \"true\", row(1, 100), row(4, 400), row(5, 500), row(6, 600), row(7, 700), row(8, 800), row(10, 1000));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL, dataProvider = \"inserterAndDeleterProvider\", timeOut = TEST_TIMEOUT)\n+    public void testTransactionalMetadataDelete(HiveOrPresto inserter, HiveOrPresto deleter)\n+    {\n+        withTemporaryTable(\"metadata_delete\", true, true, NONE, tableName -> {\n+            onPresto().executeQuery(format(\"CREATE TABLE %s (column1 INT, column2 BIGINT) WITH (transactional = true, partitioned_by = ARRAY['column2'])\", tableName));\n+            execute(inserter, format(\"INSERT INTO %s (column2, column1) VALUES %s, %s\",\n+                    tableName,\n+                    makeInsertValues(1, 1, 20),\n+                    makeInsertValues(2, 1, 20)));\n+\n+            execute(deleter, format(\"DELETE FROM %s WHERE column2 = 1\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT COUNT(*) FROM \" + tableName, \"column2 = 1\", row(0));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL, timeOut = TEST_TIMEOUT)\n+    public void testNonTransactionalMetadataDelete()\n+    {\n+        withTemporaryTable(\"non_transactional_metadata_delete\", false, true, NONE, tableName -> {\n+            onPresto().executeQuery(format(\"CREATE TABLE %s (column2 BIGINT, column1 INT) WITH (partitioned_by = ARRAY['column1'])\", tableName));\n+\n+            execute(HiveOrPresto.PRESTO, format(\"INSERT INTO %s (column1, column2) VALUES %s, %s\",\n+                    tableName,\n+                    makeInsertValues(1, 1, 10),\n+                    makeInsertValues(2, 1, 10)));\n+\n+            execute(HiveOrPresto.PRESTO, format(\"INSERT INTO %s (column1, column2) VALUES %s, %s\",\n+                    tableName,\n+                    makeInsertValues(1, 11, 20),\n+                    makeInsertValues(2, 11, 20)));\n+\n+            execute(HiveOrPresto.PRESTO, format(\"DELETE FROM %s WHERE column1 = 1\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT COUNT(*) FROM \" + tableName, \"column1 = 1\", row(0));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL, dataProvider = \"inserterAndDeleterProvider\", timeOut = TEST_TIMEOUT)\n+    public void testUnpartitionedDeleteAll(HiveOrPresto inserter, HiveOrPresto deleter)\n+    {\n+        withTemporaryTable(\"unpartitioned_delete_all\", true, false, NONE, tableName -> {\n+            onPresto().executeQuery(format(\"CREATE TABLE %s (column1 INT, column2 BIGINT) WITH (transactional = true)\", tableName));\n+            execute(inserter, format(\"INSERT INTO %s VALUES (1, 100), (2, 200), (3, 300), (4, 400), (5, 500)\", tableName));\n+            execute(deleter, \"DELETE FROM \" + tableName);\n+            verifySelectForPrestoAndHive(\"SELECT COUNT(*) FROM \" + tableName, \"true\", row(0));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL, dataProvider = \"inserterAndDeleterProvider\", timeOut = TEST_TIMEOUT)\n+    public void testMultiColumnDelete(HiveOrPresto inserter, HiveOrPresto deleter)\n+    {\n+        withTemporaryTable(\"multi_column_delete\", true, false, NONE, tableName -> {\n+            onPresto().executeQuery(format(\"CREATE TABLE %s (column1 INT, column2 BIGINT) WITH (transactional = true)\", tableName));\n+            execute(inserter, format(\"INSERT INTO %s VALUES (1, 100), (2, 200), (3, 300), (4, 400), (5, 500)\", tableName));\n+            String where = \" WHERE column1 >= 2 AND column2 <= 400\";\n+            execute(deleter, format(\"DELETE FROM %s %s\", tableName, where));\n+            verifySelectForPrestoAndHive(\"SELECT * FROM \" + tableName, \"column1 IN (1, 5)\", row(1, 100), row(5, 500));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL, dataProvider = \"inserterAndDeleterProvider\", timeOut = TEST_TIMEOUT)\n+    public void testPartitionAndRowsDelete(HiveOrPresto inserter, HiveOrPresto deleter)\n+    {\n+        withTemporaryTable(\"partition_and_rows_delete\", true, true, NONE, tableName -> {\n+            onPresto().executeQuery(\"CREATE TABLE \" + tableName +\n+                    \" (column2 BIGINT, column1 INT) WITH (transactional = true, partitioned_by = ARRAY['column1'])\");\n+            execute(inserter, format(\"INSERT INTO %s (column1, column2) VALUES (1, 100), (1, 200), (2, 300), (2, 400), (2, 500)\", tableName));\n+            String where = \" WHERE column1 = 2 OR column2 = 200\";\n+            execute(deleter, format(\"DELETE FROM %s %s\", tableName, where));\n+            verifySelectForPrestoAndHive(\"SELECT column1, column2 FROM \" + tableName, \"true\", row(1, 100));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL, dataProvider = \"inserterAndDeleterProvider\", timeOut = TEST_TIMEOUT)\n+    public void testPartitionedInsertAndRowLevelDelete(HiveOrPresto inserter, HiveOrPresto deleter)\n+    {\n+        withTemporaryTable(\"partitioned_row_level_delete\", true, true, NONE, tableName -> {\n+            onPresto().executeQuery(format(\"CREATE TABLE %s (column2 INT, column1 BIGINT) WITH (transactional = true, partitioned_by = ARRAY['column1'])\", tableName));\n+\n+            execute(inserter, format(\"INSERT INTO %s (column1, column2) VALUES %s, %s\",\n+                    tableName,\n+                    makeInsertValues(1, 1, 20),\n+                    makeInsertValues(2, 1, 20)));\n+            execute(inserter, format(\"INSERT INTO %s (column1, column2) VALUES %s, %s\",\n+                    tableName,\n+                    makeInsertValues(1, 21, 40),\n+                    makeInsertValues(2, 21, 40)));\n+\n+            verifySelectForPrestoAndHive(\"SELECT COUNT(*) FROM \" + tableName, \"column2 > 10 AND column2 <= 30\", row(40));\n+\n+            execute(deleter, format(\"DELETE FROM %s WHERE column2 > 10 AND column2 <= 30\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT COUNT(*) FROM \" + tableName, \"column2 > 10 AND column2 <= 30\", row(0));\n+            verifySelectForPrestoAndHive(\"SELECT COUNT(*) FROM \" + tableName, \"true\", row(40));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL, dataProvider = \"inserterAndDeleterProvider\", timeOut = TEST_TIMEOUT)\n+    public void testBucketedPartitionedDelete(HiveOrPresto inserter, HiveOrPresto deleter)\n+    {\n+        withTemporaryTable(\"bucketed_partitioned_delete\", true, true, NONE, tableName -> {\n+            onHive().executeQuery(format(\"CREATE TABLE %s (purchase STRING) PARTITIONED BY (customer STRING) CLUSTERED BY (purchase) INTO 3 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional' = 'true')\", tableName));\n+\n+            execute(inserter, format(\"INSERT INTO %s (customer, purchase) VALUES\", tableName) +\n+                    \" ('Fred', 'cards'), ('Fred', 'cereal'), ('Fred', 'limes'), ('Fred', 'chips'),\" +\n+                    \" ('Ann', 'cards'), ('Ann', 'cereal'), ('Ann', 'lemons'), ('Ann', 'chips'),\" +\n+                    \" ('Lou', 'cards'), ('Lou', 'cereal'), ('Lou', 'lemons'), ('Lou', 'chips')\");\n+\n+            verifySelectForPrestoAndHive(format(\"SELECT customer FROM %s\", tableName), \"purchase = 'lemons'\", row(\"Ann\"), row(\"Lou\"));\n+\n+            verifySelectForPrestoAndHive(format(\"SELECT purchase FROM %s\", tableName), \"customer = 'Fred'\", row(\"cards\"), row(\"cereal\"), row(\"limes\"), row(\"chips\"));\n+\n+            execute(inserter, format(\"INSERT INTO %s (customer, purchase) VALUES\", tableName) +\n+                    \" ('Ernie', 'cards'), ('Ernie', 'cereal'),\" +\n+                    \" ('Debby', 'corn'), ('Debby', 'chips'),\" +\n+                    \" ('Joe', 'corn'), ('Joe', 'lemons'), ('Joe', 'candy')\");\n+\n+            verifySelectForPrestoAndHive(\"SELECT customer FROM \" + tableName, \"purchase = 'corn'\", row(\"Debby\"), row(\"Joe\"));\n+\n+            execute(deleter, format(\"DELETE FROM %s WHERE purchase = 'lemons'\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT purchase FROM \" + tableName, \"customer = 'Ann'\", row(\"cards\"), row(\"cereal\"), row(\"chips\"));\n+\n+            execute(deleter, format(\"DELETE FROM %s WHERE purchase like('c%%')\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT customer, purchase FROM \" + tableName, \"true\", row(\"Fred\", \"limes\"));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL, dataProvider = \"inserterAndDeleterProvider\", timeOut = TEST_TIMEOUT)\n+    public void testBucketedUnpartitionedDelete(HiveOrPresto inserter, HiveOrPresto deleter)\n+    {\n+        withTemporaryTable(\"bucketed_unpartitioned_delete\", true, true, NONE, tableName -> {\n+            onHive().executeQuery(format(\"CREATE TABLE %s (customer STRING, purchase STRING) CLUSTERED BY (purchase) INTO 3 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional' = 'true')\", tableName));\n+\n+            execute(inserter, format(\"INSERT INTO %s (customer, purchase) VALUES\", tableName) +\n+                    \" ('Fred', 'cards'), ('Fred', 'cereal'), ('Fred', 'limes'), ('Fred', 'chips'),\" +\n+                    \" ('Ann', 'cards'), ('Ann', 'cereal'), ('Ann', 'lemons'), ('Ann', 'chips'),\" +\n+                    \" ('Lou', 'cards'), ('Lou', 'cereal'), ('Lou', 'lemons'), ('Lou', 'chips')\");\n+\n+            verifySelectForPrestoAndHive(format(\"SELECT customer FROM %s\", tableName), \"purchase = 'lemons'\", row(\"Ann\"), row(\"Lou\"));\n+\n+            verifySelectForPrestoAndHive(format(\"SELECT purchase FROM %s\", tableName), \"customer = 'Fred'\", row(\"cards\"), row(\"cereal\"), row(\"limes\"), row(\"chips\"));\n+\n+            execute(inserter, format(\"INSERT INTO %s (customer, purchase) VALUES\", tableName) +\n+                    \" ('Ernie', 'cards'), ('Ernie', 'cereal'),\" +\n+                    \" ('Debby', 'corn'), ('Debby', 'chips'),\" +\n+                    \" ('Joe', 'corn'), ('Joe', 'lemons'), ('Joe', 'candy')\");\n+\n+            verifySelectForPrestoAndHive(\"SELECT customer FROM \" + tableName, \"purchase = 'corn'\", row(\"Debby\"), row(\"Joe\"));\n+\n+            execute(deleter, format(\"DELETE FROM %s WHERE purchase = 'lemons'\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT purchase FROM \" + tableName, \"customer = 'Ann'\", row(\"cards\"), row(\"cereal\"), row(\"chips\"));\n+\n+            execute(deleter, format(\"DELETE FROM %s WHERE purchase like('c%%')\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT customer, purchase FROM \" + tableName, \"true\", row(\"Fred\", \"limes\"));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL, dataProvider = \"inserterAndDeleterProvider\", timeOut = TEST_TIMEOUT)\n+    public void testCorrectSelectCountStar(HiveOrPresto inserter, HiveOrPresto deleter)\n+    {\n+        withTemporaryTable(\"select_count_star_delete\", true, true, NONE, tableName -> {\n+            onHive().executeQuery(format(\"CREATE TABLE %s (col1 INT, col2 BIGINT) PARTITIONED BY (col3 STRING) STORED AS ORC TBLPROPERTIES ('transactional'='true')\", tableName));\n+\n+            execute(inserter, format(\"INSERT INTO %s VALUES (1, 100, 'a'), (2, 200, 'b'), (3, 300, 'c'), (4, 400, 'a'), (5, 500, 'b'), (6, 600, 'c')\", tableName));\n+            execute(deleter, format(\"DELETE FROM %s WHERE col2 = 200\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT COUNT(*) FROM \" + tableName, \"true\", row(5));\n+        });\n+    }\n+\n+    @Test(groups = HIVE_TRANSACTIONAL, dataProvider = \"insertersProvider\", timeOut = TEST_TIMEOUT)\n+    public void testInsertOnlyMultipleWriters(boolean bucketed, HiveOrPresto inserter1, HiveOrPresto inserter2)\n+    {\n+        log.info(\"testInsertOnlyMultipleWriters bucketed %s, inserter1 %s, inserter2 %s\", bucketed, inserter1, inserter2);\n+        withTemporaryTable(\"insert_only_partitioned\", true, true, NONE, tableName -> {\n+            onHive().executeQuery(format(\"CREATE TABLE %s (col1 INT, col2 BIGINT) PARTITIONED BY (col3 STRING) %s STORED AS ORC TBLPROPERTIES ('transactional'='true', 'transactional_properties'='insert_only')\",\n+                    tableName, bucketed ? \"CLUSTERED BY (col2) INTO 3 BUCKETS\" : \"\"));\n+\n+            execute(inserter1, format(\"INSERT INTO %s VALUES (1, 100, 'a'), (2, 200, 'b')\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT * FROM \" + tableName, \"true\", row(1, 100, \"a\"), row(2, 200, \"b\"));\n+\n+            execute(inserter2, format(\"INSERT INTO %s VALUES (3, 300, 'c'), (4, 400, 'a')\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT * FROM \" + tableName, \"true\", row(1, 100, \"a\"), row(2, 200, \"b\"), row(3, 300, \"c\"), row(4, 400, \"a\"));\n+\n+            execute(inserter1, format(\"INSERT INTO %s VALUES (5, 500, 'b'), (6, 600, 'c')\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT * FROM \" + tableName, \"true\", row(1, 100, \"a\"), row(2, 200, \"b\"), row(3, 300, \"c\"), row(4, 400, \"a\"), row(5, 500, \"b\"), row(6, 600, \"c\"));\n+            verifySelectForPrestoAndHive(\"SELECT * FROM \" + tableName, \"col2 > 300\", row(4, 400, \"a\"), row(5, 500, \"b\"), row(6, 600, \"c\"));\n+\n+            execute(inserter2, format(\"INSERT INTO %s VALUES (7, 700, 'b'), (8, 800, 'c')\", tableName));\n+            verifySelectForPrestoAndHive(\"SELECT * FROM \" + tableName, \"true\", row(1, 100, \"a\"), row(2, 200, \"b\"), row(3, 300, \"c\"), row(4, 400, \"a\"), row(5, 500, \"b\"), row(6, 600, \"c\"), row(7, 700, \"b\"), row(8, 800, \"c\"));\n+            verifySelectForPrestoAndHive(\"SELECT * FROM \" + tableName, \"col3 = 'c'\", row(3, 300, \"c\"), row(6, 600, \"c\"), row(8, 800, \"c\"));\n+        });\n+    }\n+\n+    @DataProvider\n+    public Object[][] insertersProvider()\n+    {\n+        return new Object[][] {\n+                {false, HiveOrPresto.HIVE, HiveOrPresto.PRESTO},\n+                {false, HiveOrPresto.PRESTO, HiveOrPresto.PRESTO},\n+                {true, HiveOrPresto.HIVE, HiveOrPresto.PRESTO},\n+                {true, HiveOrPresto.PRESTO, HiveOrPresto.PRESTO},\n+        };\n+    }\n+\n+    private enum HiveOrPresto\n+    {\n+        HIVE,\n+        PRESTO\n+    }\n+\n+    private static QueryResult execute(HiveOrPresto hiveOrPresto, String sql, QueryExecutor.QueryParam... params)\n+    {\n+        return executorFor(hiveOrPresto).executeQuery(sql, params);\n+    }\n+\n+    private static QueryExecutor executorFor(HiveOrPresto hiveOrPresto)\n+    {\n+        switch (hiveOrPresto) {\n+            case HIVE:\n+                return onHive();\n+            case PRESTO:\n+                return onPresto();\n+            default:\n+                throw new IllegalStateException(\"Unknown enum value \" + hiveOrPresto);\n+        }\n+    }\n+\n+    @DataProvider\n+    public Object[][] inserterAndDeleterProvider()\n+    {\n+        return new Object[][] {\n+                {HiveOrPresto.HIVE, HiveOrPresto.PRESTO},\n+                {HiveOrPresto.PRESTO, HiveOrPresto.PRESTO},\n+                {HiveOrPresto.PRESTO, HiveOrPresto.HIVE}\n+        };\n+    }\n+\n+    void withTemporaryTable(String rootName, boolean transactional, boolean isPartitioned, BucketingType bucketingType, Consumer<String> testRunner)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 512}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAyODY2MjUw", "url": "https://github.com/trinodb/trino/pull/5402#pullrequestreview-502866250", "createdAt": "2020-10-06T12:04:47Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxMjowNDo0OFrOHdDCTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNToxNDowM1rOHgr3sQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIyMDQ5NQ==", "bodyText": "static import", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r500220495", "createdAt": "2020-10-06T12:04:48Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/AcidSchema.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.Type;\n+import org.apache.hadoop.hive.ql.io.IOConstants;\n+\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static io.prestosql.plugin.hive.HiveType.HIVE_INT;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_LONG;\n+import static io.prestosql.plugin.hive.orc.OrcPageSourceFactory.ACID_COLUMN_BUCKET;\n+import static io.prestosql.plugin.hive.orc.OrcPageSourceFactory.ACID_COLUMN_CURRENT_TRANSACTION;\n+import static io.prestosql.plugin.hive.orc.OrcPageSourceFactory.ACID_COLUMN_OPERATION;\n+import static io.prestosql.plugin.hive.orc.OrcPageSourceFactory.ACID_COLUMN_ORIGINAL_TRANSACTION;\n+import static io.prestosql.plugin.hive.orc.OrcPageSourceFactory.ACID_COLUMN_ROW_ID;\n+import static io.prestosql.plugin.hive.orc.OrcPageSourceFactory.ACID_COLUMN_ROW_STRUCT;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.joining;\n+\n+public final class AcidSchema\n+{\n+    public static final List<String> ACID_COLUMN_NAMES = ImmutableList.of(\n+            ACID_COLUMN_OPERATION,\n+            ACID_COLUMN_ORIGINAL_TRANSACTION,\n+            ACID_COLUMN_BUCKET,\n+            ACID_COLUMN_ROW_ID,\n+            ACID_COLUMN_CURRENT_TRANSACTION,\n+            ACID_COLUMN_ROW_STRUCT);\n+\n+    private AcidSchema() {}\n+\n+    public static Properties createAcidSchema(HiveType rowType)\n+    {\n+        Properties hiveAcidSchema = new Properties();\n+        hiveAcidSchema.setProperty(IOConstants.COLUMNS, String.join(\",\", ACID_COLUMN_NAMES));\n+        // We must supply an accurate row type, because Apache ORC code we don't control has a consistency\n+        // check that the layout of this \"row\" must agree with the layout of an inserted row.\n+        hiveAcidSchema.setProperty(IOConstants.COLUMNS_TYPES, createAcidColumnHiveTypes(rowType).stream()\n+                .map(HiveType::getHiveTypeName)\n+                .map(HiveTypeName::toString)\n+                .collect(joining(\":\")));\n+        return hiveAcidSchema;\n+    }\n+\n+    public static Type createRowType(List<String> names, List<Type> types)\n+    {\n+        requireNonNull(names, \"names is null\");\n+        requireNonNull(types, \"types is null\");\n+        Preconditions.checkArgument(names.size() == types.size(), \"names size %s differs from types size %s\", names.size(), types.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzNTkxNQ==", "bodyText": "nit: static import", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r500235915", "createdAt": "2020-10-06T12:30:27Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveBasicStatistics.java", "diffHunk": "@@ -84,6 +85,12 @@ public OptionalLong getOnDiskDataSizeInBytes()\n         return onDiskDataSizeInBytes;\n     }\n \n+    public HiveBasicStatistics withAdjustedRowCount(long adjustment)\n+    {\n+        Preconditions.checkArgument(rowCount.isPresent(), \"rowCount isn't present\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI0MTk3NQ==", "bodyText": "I wonder if we should sometimes decide to drop all statistics after insert/delete. E.g. if the ratio of number of inserted rows to table size is large. Is there a place where we can track how many rows were added/deleted to the table since table was last analyzed. (I do not expect this to be addressed in this PR, just something to think of).", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r500241975", "createdAt": "2020-10-06T12:40:05Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/SemiTransactionalHiveMetastore.java", "diffHunk": "@@ -1365,6 +1524,97 @@ private void prepareInsertExistingTable(HdfsContext context, TableAndMore tableA\n                     Optional.empty(),\n                     tableAndMore.getStatisticsUpdate(),\n                     true));\n+\n+            if (isAcidTransactionRunning()) {\n+                AcidTransaction transaction = getCurrentAcidTransaction();\n+                updateTableWriteId(tableAndMore.getIdentity(), table.getDatabaseName(), table.getTableName(), transaction.getAcidTransactionId(), transaction.getWriteId(), OptionalLong.empty());\n+            }\n+        }\n+\n+        private void prepareDeleteRowsFromExistingTable(HdfsContext context, TableAndMore tableAndMore)\n+        {\n+            TableAndDeleteDirectories deletionState = (TableAndDeleteDirectories) tableAndMore;\n+            List<PartitionAndStatementId> partitionAndStatementIds = deletionState.getPartitionAndStatementIds();\n+            checkArgument(!partitionAndStatementIds.isEmpty(), \"partitionAndStatementIds is empty\");\n+\n+            deleteOnly = false;\n+            Table table = deletionState.getTable();\n+            checkArgument(currentHiveTransaction.isPresent(), \"currentHiveTransaction isn't present\");\n+            AcidTransaction transaction = currentHiveTransaction.get().getTransaction();\n+            checkArgument(transaction.isDelete(), \"transaction should be delete, but is \" + transaction);\n+\n+            cleanUpTasksForAbort.addAll(deletionState.getPartitionAndStatementIds().stream()\n+                    .map(ps -> new DirectoryCleanUpTask(context, new Path(ps.getDeleteDeltaDirectory()), true))\n+                    .collect(Collectors.toUnmodifiableList()));\n+\n+            Map<String, Long> partitionRowCounts = new HashMap<>(partitionAndStatementIds.size());\n+            int totalRowsDeleted = 0;\n+            for (PartitionAndStatementId ps : partitionAndStatementIds) {\n+                long rowCount = ps.getRowCount();\n+                partitionRowCounts.compute(ps.getPartitionName(), (k, count) -> count != null ? count + rowCount : rowCount);\n+            }\n+\n+            HiveIdentity identity = deletionState.getIdentity();\n+            String databaseName = table.getDatabaseName();\n+            String tableName = table.getTableName();\n+\n+            // Update the table statistics\n+            PartitionStatistics tableStatistics = getTableStatistics(identity, databaseName, tableName);\n+            HiveBasicStatistics basicStatistics = tableStatistics.getBasicStatistics();\n+            if (basicStatistics.getRowCount().isPresent()) {\n+                tableStatistics = tableStatistics.withAdjustedRowCount(-totalRowsDeleted);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 508}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI0NzA4NA==", "bodyText": "nit: static import isTransactionalTable", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r500247084", "createdAt": "2020-10-06T12:48:09Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveMetadata.java", "diffHunk": "@@ -1544,6 +1564,9 @@ public HiveInsertTableHandle beginInsert(ConnectorSession session, ConnectorTabl\n             throw new PrestoException(NOT_SUPPORTED, format(\"Inserting into Hive table with %s property not supported\", SKIP_FOOTER_COUNT_KEY));\n         }\n         LocationHandle locationHandle = locationService.forExistingTable(metastore, session, table);\n+\n+        AcidTransaction transaction = AcidUtils.isTransactionalTable(table.getParameters()) ? metastore.beginInsert(session, table) : NO_ACID_TRANSACTION;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI1MjkxMQ==", "bodyText": "nit: we can pass empty statistics here so there is just one place where statistics for partition are update. It is happening below anyway, right?", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r500252911", "createdAt": "2020-10-06T12:56:51Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveMetadata.java", "diffHunk": "@@ -1584,6 +1608,10 @@ public HiveInsertTableHandle beginInsert(ConnectorSession session, ConnectorTabl\n             partitionUpdates = PartitionUpdate.mergePartitionUpdates(concat(partitionUpdates, partitionUpdatesForMissingBuckets));\n             for (PartitionUpdate partitionUpdate : partitionUpdatesForMissingBuckets) {\n                 Optional<Partition> partition = table.getPartitionColumns().isEmpty() ? Optional.empty() : Optional.of(buildPartitionObject(session, table, partitionUpdate));\n+                if (handle.isTransactional() && partition.isPresent()) {\n+                    PartitionStatistics statistics = PartitionStatistics.builder().setBasicStatistics(partitionUpdate.getStatistics()).build();\n+                    metastore.addPartition(session, handle.getSchemaName(), handle.getTableName(), partition.get(), partitionUpdate.getWritePath(), statistics);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzkwNTkyMQ==", "bodyText": "WriteId makes sense in context of single table, right? If multiple tables are modified in single Hive transactions those operations would have different writeIds attached.\nI guess this is fine to have it in AcidTransaction class as Presto originated Hive transaction spans only single modification in Hive for single table, but it would be nice to have a comment here to state that.", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r503905921", "createdAt": "2020-10-13T12:23:13Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/AcidTransaction.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import io.prestosql.orc.OrcWriter.OrcOperation;\n+\n+import static com.google.common.base.MoreObjects.toStringHelper;\n+import static io.prestosql.plugin.hive.AcidOperation.CREATE_TABLE;\n+import static io.prestosql.plugin.hive.AcidOperation.DELETE;\n+import static io.prestosql.plugin.hive.AcidOperation.INSERT;\n+import static io.prestosql.plugin.hive.AcidOperation.NONE;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AcidTransaction\n+{\n+    public static final AcidTransaction NO_ACID_TRANSACTION = new AcidTransaction(NONE, 0, 0);\n+\n+    private final AcidOperation operation;\n+    private final long transactionId;\n+    private final long writeId;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzkxMDE1MQ==", "bodyText": "nit: I would rename to updateRowIdColumnHandle or rowIdColumnHandle", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r503910151", "createdAt": "2020-10-13T12:30:03Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveColumnHandle.java", "diffHunk": "@@ -244,13 +257,7 @@ public String toString()\n \n     public static HiveColumnHandle updateRowIdHandle()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzkxNjQ4Mw==", "bodyText": "I do not really undestand what is happening here, but it does not seem to be inline with a comment.", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r503916483", "createdAt": "2020-10-13T12:39:44Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HivePageSourceProvider.java", "diffHunk": "@@ -379,6 +445,22 @@ public int getIndex()\n                         columnMappings.add(empty(column));\n                     }\n                 }\n+                else if (isRowIdColumnHandle(column)) {\n+                    baseColumnHiveIndices.add(column.getBaseHiveColumnIndex());\n+                    checkArgument(\n+                            projectionsForColumn.computeIfAbsent(column.getBaseHiveColumnIndex(), index -> new HashSet()).add(column.getHiveColumnProjectionInfo()),\n+                            \"duplicate column in columns list\");\n+\n+                    // Add regular mapping if projection is valid for partition schema, otherwise add an empty mapping", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 188}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzkyMTExMA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    Map<String, String> parms = new HashMap<>(parameters);\n          \n          \n            \n                    String existingRowCount = parameters.get(NUM_ROWS);\n          \n          \n            \n                    if (existingRowCount == null) {\n          \n          \n            \n                        return parameters;\n          \n          \n            \n                    }\n          \n          \n            \n                    checkArgument(DIGITS.matcher(existingRowCount).matches(), \"For %s, the existing row count (%s) is not a digit string\", description, existingRowCount);\n          \n          \n            \n                    long newRowCount = Long.parseLong(existingRowCount) + rowCountAdjustment;\n          \n          \n            \n                    checkArgument(newRowCount >= 0, \"For %s, the subtracted row count (%s) is less than zero, existing count %s, rows deleted %d\", description, newRowCount, existingRowCount, rowCountAdjustment);\n          \n          \n            \n                    parms.put(NUM_ROWS, String.valueOf(newRowCount));\n          \n          \n            \n                    return ImmutableMap.copyOf(parms);\n          \n          \n            \n                    String existingRowCount = parameters.get(NUM_ROWS);\n          \n          \n            \n                    if (existingRowCount == null) {\n          \n          \n            \n                        return parameters;\n          \n          \n            \n                    }\n          \n          \n            \n                    ImmutableMap.Builder<String, String> adjustedParameters = ImmutableMap.builder();\n          \n          \n            \n                    adjustedParameters.putAll(parameters);\n          \n          \n            \n                    checkArgument(DIGITS.matcher(existingRowCount).matches(), \"For %s, the existing row count (%s) is not a digit string\", description, existingRowCount);\n          \n          \n            \n                    long adjustedRowCount = Long.parseLong(existingRowCount) + rowCountAdjustment;\n          \n          \n            \n                    checkArgument(adjustedRowCount >= 0, \"For %s, the subtracted row count (%s) is less than zero, existing count %s, rows deleted %d\", description, adjustedRowCount, existingRowCount, rowCountAdjustment);\n          \n          \n            \n                    adjustedParameters.put(NUM_ROWS, String.valueOf(adjustedRowCount));\n          \n          \n            \n                    return adjustedParameters.build();", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r503921110", "createdAt": "2020-10-13T12:46:40Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/MetastoreUtil.java", "diffHunk": "@@ -444,4 +448,18 @@ else if (type instanceof TinyintType\n \n         return withColumnDomains(domains);\n     }\n+\n+    public static Map<String, String> adjustRowCount(Map<String, String> parameters, String description, long rowCountAdjustment)\n+    {\n+        Map<String, String> parms = new HashMap<>(parameters);\n+        String existingRowCount = parameters.get(NUM_ROWS);\n+        if (existingRowCount == null) {\n+            return parameters;\n+        }\n+        checkArgument(DIGITS.matcher(existingRowCount).matches(), \"For %s, the existing row count (%s) is not a digit string\", description, existingRowCount);\n+        long newRowCount = Long.parseLong(existingRowCount) + rowCountAdjustment;\n+        checkArgument(newRowCount >= 0, \"For %s, the subtracted row count (%s) is less than zero, existing count %s, rows deleted %d\", description, newRowCount, existingRowCount, rowCountAdjustment);\n+        parms.put(NUM_ROWS, String.valueOf(newRowCount));\n+        return ImmutableMap.copyOf(parms);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAxNjY3OA==", "bodyText": "why bumping openTransaction stats for this call and calls below?", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504016678", "createdAt": "2020-10-13T14:50:29Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/thrift/ThriftHiveMetastore.java", "diffHunk": "@@ -1722,6 +1768,97 @@ public String getValidWriteIds(HiveIdentity identity, List<SchemaTableName> tabl\n         }\n     }\n \n+    @Override\n+    public long allocateWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId)\n+    {\n+        try {\n+            return retry()\n+                    .stopOnIllegalExceptions()\n+                    .run(\"allocateWriteId\", stats.getOpenTransaction().wrap(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAyMzIyOQ==", "bodyText": "Shouldn't ThriftHiveMetastoreClient which is the deepest layer  only do simple transalation between Presto object model and Hive-metastore-thrift object model.\nHere we are calling out to getValidWriteIds. Maybe instead we should pass valid write ids as parameter?\npossibly this is fine but I want to double check on thinking here.", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504023229", "createdAt": "2020-10-13T14:58:47Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/thrift/ThriftHiveMetastoreClient.java", "diffHunk": "@@ -511,4 +523,56 @@ public String getDelegationToken(String userName)\n     {\n         return client.get_delegation_token(userName, userName);\n     }\n+\n+    @Override\n+    public long allocateWriteId(String dbName, String tableName, long transactionId)\n+            throws TException\n+    {\n+        AllocateTableWriteIdsRequest request = new AllocateTableWriteIdsRequest(dbName, tableName);\n+        request.setTxnIds(ImmutableList.of(transactionId));\n+        AllocateTableWriteIdsResponse response = client.allocate_table_write_ids(request);\n+        return Iterables.getOnlyElement(response.getTxnToWriteIds()).getWriteId();\n+    }\n+\n+    @Override\n+    public void updateTableWriteId(String dbName, String tableName, long transactionId, long writeId, OptionalLong rowCountChange)\n+            throws TException\n+    {\n+        Table table = getTableWithCapabilities(dbName, tableName);\n+        if (rowCountChange.isPresent()) {\n+            table.setParameters(adjustRowCount(table.getParameters(), tableName, rowCountChange.getAsLong()));\n+        }\n+        alterTransactionalTable(table, transactionId, writeId, new EnvironmentContext());\n+    }\n+\n+    @Override\n+    public void alterPartitions(String dbName, String tableName, List<Partition> partitions, long writeId)\n+            throws TException\n+    {\n+        AlterPartitionsRequest request = new AlterPartitionsRequest(dbName, tableName, partitions);\n+        request.setWriteId(writeId);\n+        client.alter_partitions_req(request);\n+    }\n+\n+    @Override\n+    public void addDynamicPartitions(String dbName, String tableName, List<String> partitionNames, long transactionId, long writeId, AcidOperation operation)\n+            throws TException\n+    {\n+        AddDynamicPartitions request = new AddDynamicPartitions(transactionId, writeId, dbName, tableName, partitionNames);\n+        request.setOperationType(operation.getMetastoreOperationType());\n+        client.add_dynamic_partitions(request);\n+    }\n+\n+    @Override\n+    public void alterTransactionalTable(Table table, long transactionId, long writeId, EnvironmentContext environmentContext)\n+            throws TException\n+    {\n+        table.setWriteId(writeId);\n+        checkArgument(writeId >= table.getWriteId(), \"The writeId supplied %s should be greater than or equal to the table writeId %s\", writeId, table.getWriteId());\n+        AlterTableRequest request = new AlterTableRequest(table.getDbName(), table.getTableName(), table);\n+        request.setValidWriteIdList(getValidWriteIds(ImmutableList.of(format(\"%s.%s\", table.getDbName(), table.getTableName())), transactionId));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAyNzcwOQ==", "bodyText": "Should we add checks that statmentId is at most 2^16 and bucketId is at most 2^12 (I guess that is the limit based on 0xfff above)", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504027709", "createdAt": "2020-10-13T15:04:28Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcFileWriter.java", "diffHunk": "@@ -191,11 +223,60 @@ public long getValidationCpuNanos()\n         return validationCpuNanos;\n     }\n \n+    public void setMaxWriteId(long maxWriteId)\n+    {\n+        this.maxWriteId = OptionalLong.of(maxWriteId);\n+    }\n+\n     @Override\n     public String toString()\n     {\n         return toStringHelper(this)\n                 .add(\"writer\", orcWriter)\n                 .toString();\n     }\n+\n+    private Block[] buildAcidColumns(Block rowBlock, AcidTransaction transaction)\n+    {\n+        int positionCount = rowBlock.getPositionCount();\n+        int bucketValue = computeBucketValue(bucketNumber.isPresent() ? bucketNumber.getAsInt() : 0, 0);\n+        // operation, originalWriteId, bucket, rowId, currentWriteId, row<>\n+        return new Block[] {\n+                RunLengthEncodedBlock.create(INTEGER, (long) getOrcOperation(transaction), positionCount),\n+                RunLengthEncodedBlock.create(BIGINT, transaction.getWriteId(), positionCount),\n+                RunLengthEncodedBlock.create(INTEGER, (long) bucketValue, positionCount),\n+                buildAcidRowIdsColumn(positionCount),\n+                RunLengthEncodedBlock.create(BIGINT, transaction.getWriteId(), positionCount),\n+                rowBlock\n+        };\n+    }\n+\n+    private int getOrcOperation(AcidTransaction transaction)\n+    {\n+        switch (transaction.getOperation()) {\n+            case INSERT:\n+                return 0;\n+            default:\n+                throw new IllegalStateException(\"In getOrcOperation, transaction isn't INSERT, transaction \" + transaction);\n+        }\n+    }\n+\n+    private Block buildAcidRowIdsColumn(int positionCount)\n+    {\n+        long[] rowIds = new long[positionCount];\n+        for (int i = 0; i < positionCount; i++) {\n+            rowIds[i] = i;\n+        }\n+        return new LongArrayBlock(positionCount, Optional.empty(), rowIds);\n+    }\n+\n+    public static int extractBucketNumber(int bucketValue)\n+    {\n+        return (bucketValue >> 16) & 0xFFF;\n+    }\n+\n+    public static int computeBucketValue(int bucketId, int statementId)\n+    {\n+        return 1 << 29 | bucketId << 16 | statementId;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAzNTI0OQ==", "bodyText": "why not always take writeId from transaction?", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504035249", "createdAt": "2020-10-13T15:14:03Z", "author": {"login": "losipiuk"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcFileWriter.java", "diffHunk": "@@ -143,6 +170,11 @@ public void appendRows(Page dataPage)\n     public void commit()\n     {\n         try {\n+            if (transaction.isAcidTransactionRunning() && useAcidSchema) {\n+                int bucketValue = computeBucketValue(bucketNumber.isEmpty() ? 0 : bucketNumber.getAsInt(), 0);\n+                long writeId = maxWriteId.isPresent() ? maxWriteId.getAsLong() : transaction.getWriteId();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 130}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3NjQyNzIx", "url": "https://github.com/trinodb/trino/pull/5402#pullrequestreview-507642721", "createdAt": "2020-10-13T16:48:59Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 29, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNjo0ODo1OVrOHgwaFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMTozOTo0NVrOHg_H1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDEwOTU4OQ==", "bodyText": "Maybe add a new acid package for these new classes (we could move AcidInfo there later)", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504109589", "createdAt": "2020-10-13T16:48:59Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/AcidOperation.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE4MDcxNg==", "bodyText": "Nit: we usually wrap arguments that have @JsonProperty", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504180716", "createdAt": "2020-10-13T18:46:31Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/AcidTransaction.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import io.prestosql.orc.OrcWriter.OrcOperation;\n+\n+import static com.google.common.base.MoreObjects.toStringHelper;\n+import static io.prestosql.plugin.hive.AcidOperation.CREATE_TABLE;\n+import static io.prestosql.plugin.hive.AcidOperation.DELETE;\n+import static io.prestosql.plugin.hive.AcidOperation.INSERT;\n+import static io.prestosql.plugin.hive.AcidOperation.NONE;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AcidTransaction\n+{\n+    public static final AcidTransaction NO_ACID_TRANSACTION = new AcidTransaction(NONE, 0, 0);\n+\n+    private final AcidOperation operation;\n+    private final long transactionId;\n+    private final long writeId;\n+\n+    @JsonCreator\n+    public AcidTransaction(@JsonProperty(\"operation\") AcidOperation operation, @JsonProperty(\"transactionId\") long transactionId, @JsonProperty(\"writeId\") long writeId)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE4MTAwNQ==", "bodyText": "Use checkState", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504181005", "createdAt": "2020-10-13T18:47:04Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/AcidTransaction.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import io.prestosql.orc.OrcWriter.OrcOperation;\n+\n+import static com.google.common.base.MoreObjects.toStringHelper;\n+import static io.prestosql.plugin.hive.AcidOperation.CREATE_TABLE;\n+import static io.prestosql.plugin.hive.AcidOperation.DELETE;\n+import static io.prestosql.plugin.hive.AcidOperation.INSERT;\n+import static io.prestosql.plugin.hive.AcidOperation.NONE;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AcidTransaction\n+{\n+    public static final AcidTransaction NO_ACID_TRANSACTION = new AcidTransaction(NONE, 0, 0);\n+\n+    private final AcidOperation operation;\n+    private final long transactionId;\n+    private final long writeId;\n+\n+    @JsonCreator\n+    public AcidTransaction(@JsonProperty(\"operation\") AcidOperation operation, @JsonProperty(\"transactionId\") long transactionId, @JsonProperty(\"writeId\") long writeId)\n+    {\n+        this.operation = requireNonNull(operation, \"operation is null\");\n+        this.transactionId = transactionId;\n+        this.writeId = writeId;\n+    }\n+\n+    @JsonProperty(\"operation\")\n+    public AcidOperation getOperation()\n+    {\n+        return operation;\n+    }\n+\n+    @JsonProperty(\"transactionId\")\n+    public long getAcidTransactionIdForSerialization()\n+    {\n+        return transactionId;\n+    }\n+\n+    @JsonProperty(\"writeId\")\n+    public long getWriteIdForSerialization()\n+    {\n+        return writeId;\n+    }\n+\n+    @JsonIgnore\n+    public boolean isAcidTransactionRunning()\n+    {\n+        return operation == INSERT || operation == DELETE;\n+    }\n+\n+    @JsonIgnore\n+    public boolean isTransactional()\n+    {\n+        return operation != AcidOperation.NONE;\n+    }\n+\n+    @JsonIgnore\n+    public OrcOperation getOrcOperation()\n+    {\n+        ensureTransactionRunning(\"accessing transactionId\");\n+        return operation.getOrcOperation();\n+    }\n+\n+    @JsonIgnore\n+    public long getAcidTransactionId()\n+    {\n+        ensureTransactionRunning(\"accessing transactionId\");\n+        return transactionId;\n+    }\n+\n+    @JsonIgnore\n+    public long getWriteId()\n+    {\n+        ensureTransactionRunning(\"accessing writeId\");\n+        return writeId;\n+    }\n+\n+    private void ensureTransactionRunning(String description)\n+    {\n+        if (!isAcidTransactionRunning()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE4NjcxNA==", "bodyText": "wrong message?", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504186714", "createdAt": "2020-10-13T18:57:54Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/AcidTransaction.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import io.prestosql.orc.OrcWriter.OrcOperation;\n+\n+import static com.google.common.base.MoreObjects.toStringHelper;\n+import static io.prestosql.plugin.hive.AcidOperation.CREATE_TABLE;\n+import static io.prestosql.plugin.hive.AcidOperation.DELETE;\n+import static io.prestosql.plugin.hive.AcidOperation.INSERT;\n+import static io.prestosql.plugin.hive.AcidOperation.NONE;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AcidTransaction\n+{\n+    public static final AcidTransaction NO_ACID_TRANSACTION = new AcidTransaction(NONE, 0, 0);\n+\n+    private final AcidOperation operation;\n+    private final long transactionId;\n+    private final long writeId;\n+\n+    @JsonCreator\n+    public AcidTransaction(@JsonProperty(\"operation\") AcidOperation operation, @JsonProperty(\"transactionId\") long transactionId, @JsonProperty(\"writeId\") long writeId)\n+    {\n+        this.operation = requireNonNull(operation, \"operation is null\");\n+        this.transactionId = transactionId;\n+        this.writeId = writeId;\n+    }\n+\n+    @JsonProperty(\"operation\")\n+    public AcidOperation getOperation()\n+    {\n+        return operation;\n+    }\n+\n+    @JsonProperty(\"transactionId\")\n+    public long getAcidTransactionIdForSerialization()\n+    {\n+        return transactionId;\n+    }\n+\n+    @JsonProperty(\"writeId\")\n+    public long getWriteIdForSerialization()\n+    {\n+        return writeId;\n+    }\n+\n+    @JsonIgnore\n+    public boolean isAcidTransactionRunning()\n+    {\n+        return operation == INSERT || operation == DELETE;\n+    }\n+\n+    @JsonIgnore\n+    public boolean isTransactional()\n+    {\n+        return operation != AcidOperation.NONE;\n+    }\n+\n+    @JsonIgnore\n+    public OrcOperation getOrcOperation()\n+    {\n+        ensureTransactionRunning(\"accessing transactionId\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE4NjgxMg==", "bodyText": "Maybe \"but\" should be \"while\"", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504186812", "createdAt": "2020-10-13T18:58:04Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/AcidTransaction.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import io.prestosql.orc.OrcWriter.OrcOperation;\n+\n+import static com.google.common.base.MoreObjects.toStringHelper;\n+import static io.prestosql.plugin.hive.AcidOperation.CREATE_TABLE;\n+import static io.prestosql.plugin.hive.AcidOperation.DELETE;\n+import static io.prestosql.plugin.hive.AcidOperation.INSERT;\n+import static io.prestosql.plugin.hive.AcidOperation.NONE;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AcidTransaction\n+{\n+    public static final AcidTransaction NO_ACID_TRANSACTION = new AcidTransaction(NONE, 0, 0);\n+\n+    private final AcidOperation operation;\n+    private final long transactionId;\n+    private final long writeId;\n+\n+    @JsonCreator\n+    public AcidTransaction(@JsonProperty(\"operation\") AcidOperation operation, @JsonProperty(\"transactionId\") long transactionId, @JsonProperty(\"writeId\") long writeId)\n+    {\n+        this.operation = requireNonNull(operation, \"operation is null\");\n+        this.transactionId = transactionId;\n+        this.writeId = writeId;\n+    }\n+\n+    @JsonProperty(\"operation\")\n+    public AcidOperation getOperation()\n+    {\n+        return operation;\n+    }\n+\n+    @JsonProperty(\"transactionId\")\n+    public long getAcidTransactionIdForSerialization()\n+    {\n+        return transactionId;\n+    }\n+\n+    @JsonProperty(\"writeId\")\n+    public long getWriteIdForSerialization()\n+    {\n+        return writeId;\n+    }\n+\n+    @JsonIgnore\n+    public boolean isAcidTransactionRunning()\n+    {\n+        return operation == INSERT || operation == DELETE;\n+    }\n+\n+    @JsonIgnore\n+    public boolean isTransactional()\n+    {\n+        return operation != AcidOperation.NONE;\n+    }\n+\n+    @JsonIgnore\n+    public OrcOperation getOrcOperation()\n+    {\n+        ensureTransactionRunning(\"accessing transactionId\");\n+        return operation.getOrcOperation();\n+    }\n+\n+    @JsonIgnore\n+    public long getAcidTransactionId()\n+    {\n+        ensureTransactionRunning(\"accessing transactionId\");\n+        return transactionId;\n+    }\n+\n+    @JsonIgnore\n+    public long getWriteId()\n+    {\n+        ensureTransactionRunning(\"accessing writeId\");\n+        return writeId;\n+    }\n+\n+    private void ensureTransactionRunning(String description)\n+    {\n+        if (!isAcidTransactionRunning()) {\n+            throw new IllegalStateException(\"Not in ACID transaction but \" + description);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE4NzAwMA==", "bodyText": "Check for null", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504187000", "createdAt": "2020-10-13T18:58:25Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/BackgroundHiveSplitLoader.java", "diffHunk": "@@ -190,6 +192,7 @@ public BackgroundHiveSplitLoader(\n             Optional<ValidWriteIdList> validWriteIds)\n     {\n         this.table = table;\n+        this.transaction = transaction;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE4NzgwNw==", "bodyText": "Let's call this $row_id to match Presto conventions", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504187807", "createdAt": "2020-10-13T18:59:49Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveColumnHandle.java", "diffHunk": "@@ -67,7 +74,13 @@\n     public static final HiveType PARTITION_HIVE_TYPE = HIVE_STRING;\n     public static final Type PARTITION_TYPE_SIGNATURE = VARCHAR;\n \n-    private static final String UPDATE_ROW_ID_COLUMN_NAME = \"$shard_row_id\";\n+    public static final int UPDATE_ROW_ID_COLUMN_INDEX = -16;\n+    public static final String UPDATE_ROW_ID_COLUMN_NAME = \"$row__id\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE5MDI0OA==", "bodyText": "The is prefix is normally reserved for getters, not variable names, so this would be better named originalFile", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504190248", "createdAt": "2020-10-13T19:04:15Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HivePageSourceFactory.java", "diffHunk": "@@ -37,7 +38,10 @@\n             Properties schema,\n             List<HiveColumnHandle> columns,\n             TupleDomain<HiveColumnHandle> effectivePredicate,\n-            Optional<AcidInfo> acidInfo);\n+            Optional<AcidInfo> acidInfo,\n+            OptionalInt bucketNumber,\n+            boolean isOriginalFile,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE5MDYwMw==", "bodyText": "originalFile", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504190603", "createdAt": "2020-10-13T19:04:51Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HivePageSourceProvider.java", "diffHunk": "@@ -83,19 +105,27 @@ public HivePageSourceProvider(\n                 .addAll(requireNonNull(cursorProviders, \"cursorProviders is null\"))\n                 .add(genericCursorProvider) // generic should be last, as a fallback option\n                 .build();\n+        this.orcFileWriterFactory = requireNonNull(orcFileWriterFactory, \"orcFileWriterFactory is null\");\n     }\n \n     @Override\n-    public ConnectorPageSource createPageSource(ConnectorTransactionHandle transaction, ConnectorSession session, ConnectorSplit split, ConnectorTableHandle table, List<ColumnHandle> columns, DynamicFilter dynamicFilter)\n+    public ConnectorPageSource createPageSource(\n+            ConnectorTransactionHandle transaction,\n+            ConnectorSession session,\n+            ConnectorSplit split,\n+            ConnectorTableHandle tableHandle,\n+            List<ColumnHandle> columns,\n+            DynamicFilter dynamicFilter)\n     {\n-        HiveTableHandle hiveTable = (HiveTableHandle) table;\n+        HiveTableHandle hiveTable = (HiveTableHandle) tableHandle;\n \n         List<HiveColumnHandle> hiveColumns = columns.stream()\n                 .map(HiveColumnHandle.class::cast)\n                 .collect(toList());\n \n         HiveSplit hiveSplit = (HiveSplit) split;\n         Path path = new Path(hiveSplit.getPath());\n+        boolean isOriginalFile = ORIGINAL_FILE_PATH_MATCHER.matcher(path.toString()).matches();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE5MTEzMg==", "bodyText": "Remove the wrapping here for this last argument (not sure why it was originally wrapped)", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504191132", "createdAt": "2020-10-13T19:05:45Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HivePartitionManager.java", "diffHunk": "@@ -195,7 +195,8 @@ public HiveTableHandle applyPartitionResult(HiveTableHandle handle, HivePartitio\n                 handle.getAnalyzePartitionValues(),\n                 handle.getAnalyzeColumnNames(),\n                 Optionals.combine(handle.getConstraintColumns(), columns,\n-                        Sets::union));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE5MTQxMw==", "bodyText": "Wrap the arguments", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504191413", "createdAt": "2020-10-13T19:06:17Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/PartitionAndStatementId.java", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import io.airlift.json.JsonCodec;\n+\n+import java.util.Objects;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class PartitionAndStatementId\n+{\n+    private final String partitionName;\n+    private final int statementId;\n+    private final long rowCount;\n+    private final String deleteDeltaDirectory;\n+\n+    @JsonCreator\n+    public PartitionAndStatementId(@JsonProperty(\"partitionName\") String partitionName, @JsonProperty(\"statementId\") int statementId, @JsonProperty(\"rowCount\") long rowCount, @JsonProperty(\"deleteDeltaDirectory\") String deleteDeltaDirectory)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE5MTcyNA==", "bodyText": "Check for null", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504191724", "createdAt": "2020-10-13T19:06:54Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/HiveTransaction.java", "diffHunk": "@@ -32,15 +33,17 @@\n     private final String queryId;\n     private final long transactionId;\n     private final ScheduledFuture<?> heartbeatTask;\n+    private final AcidTransaction transaction;\n \n     private final Map<SchemaTableName, ValidTxnWriteIdList> validHiveTransactionsForTable = new HashMap<>();\n \n-    public HiveTransaction(HiveIdentity identity, String queryId, long transactionId, ScheduledFuture<?> heartbeatTask)\n+    public HiveTransaction(HiveIdentity identity, String queryId, long transactionId, ScheduledFuture<?> heartbeatTask, AcidTransaction transaction)\n     {\n         this.identity = requireNonNull(identity, \"identity is null\");\n         this.queryId = requireNonNull(queryId, \"queryId is null\");\n         this.transactionId = transactionId;\n         this.heartbeatTask = requireNonNull(heartbeatTask, \"heartbeatTask is null\");\n+        this.transaction = transaction;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE5MzA1MQ==", "bodyText": "Nit: I think this can be a method reference", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504193051", "createdAt": "2020-10-13T19:09:22Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/thrift/BridgingHiveMetastore.java", "diffHunk": "@@ -487,4 +493,44 @@ public String getValidWriteIds(HiveIdentity identity, List<SchemaTableName> tabl\n     {\n         return delegate.getValidWriteIds(identity, tables, currentTransactionId);\n     }\n+\n+    @Override\n+    public long allocateWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId)\n+    {\n+        return delegate.allocateWriteId(identity, dbName, tableName, transactionId);\n+    }\n+\n+    @Override\n+    public void acquireTableWriteLock(HiveIdentity identity, String queryId, long transactionId, String dbName, String tableName, DataOperationType operation, boolean isDynamicPartitionWrite)\n+    {\n+        delegate.acquireTableWriteLock(identity, queryId, transactionId, dbName, tableName, operation, isDynamicPartitionWrite);\n+    }\n+\n+    @Override\n+    public void updateTableWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId, long writeId, OptionalLong rowCountChange)\n+    {\n+        delegate.updateTableWriteId(identity, dbName, tableName, transactionId, writeId, rowCountChange);\n+    }\n+\n+    @Override\n+    public void alterPartitions(HiveIdentity identity, String dbName, String tableName, List<Partition> partitions, long writeId)\n+    {\n+        List<org.apache.hadoop.hive.metastore.api.Partition> hadoopPartitions = partitions.stream()\n+                .map(partition -> toMetastoreApiPartition(partition))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIwMzIyNQ==", "bodyText": "You could move this into the stream operation using peek()", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504203225", "createdAt": "2020-10-13T19:27:54Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/thrift/BridgingHiveMetastore.java", "diffHunk": "@@ -487,4 +493,44 @@ public String getValidWriteIds(HiveIdentity identity, List<SchemaTableName> tabl\n     {\n         return delegate.getValidWriteIds(identity, tables, currentTransactionId);\n     }\n+\n+    @Override\n+    public long allocateWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId)\n+    {\n+        return delegate.allocateWriteId(identity, dbName, tableName, transactionId);\n+    }\n+\n+    @Override\n+    public void acquireTableWriteLock(HiveIdentity identity, String queryId, long transactionId, String dbName, String tableName, DataOperationType operation, boolean isDynamicPartitionWrite)\n+    {\n+        delegate.acquireTableWriteLock(identity, queryId, transactionId, dbName, tableName, operation, isDynamicPartitionWrite);\n+    }\n+\n+    @Override\n+    public void updateTableWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId, long writeId, OptionalLong rowCountChange)\n+    {\n+        delegate.updateTableWriteId(identity, dbName, tableName, transactionId, writeId, rowCountChange);\n+    }\n+\n+    @Override\n+    public void alterPartitions(HiveIdentity identity, String dbName, String tableName, List<Partition> partitions, long writeId)\n+    {\n+        List<org.apache.hadoop.hive.metastore.api.Partition> hadoopPartitions = partitions.stream()\n+                .map(partition -> toMetastoreApiPartition(partition))\n+                .collect(toImmutableList());\n+        hadoopPartitions.forEach(partition -> partition.setWriteId(writeId));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIwMzk2NA==", "bodyText": "Why is this a TODO? What are the benefits of doing it, or the drawbacks of not doing it?", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504203964", "createdAt": "2020-10-13T19:29:17Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/thrift/ThriftHiveMetastore.java", "diffHunk": "@@ -1190,6 +1221,8 @@ public void addPartitions(HiveIdentity identity, String databaseName, String tab\n         List<Partition> partitions = partitionsWithStatistics.stream()\n                 .map(ThriftMetastoreUtil::toMetastoreApiPartition)\n                 .collect(toImmutableList());\n+        // TODO: fill in the catalogName", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIwNDIyNw==", "bodyText": "Yes, this should be a different stats call", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504204227", "createdAt": "2020-10-13T19:29:47Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/thrift/ThriftHiveMetastore.java", "diffHunk": "@@ -1722,6 +1768,97 @@ public String getValidWriteIds(HiveIdentity identity, List<SchemaTableName> tabl\n         }\n     }\n \n+    @Override\n+    public long allocateWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId)\n+    {\n+        try {\n+            return retry()\n+                    .stopOnIllegalExceptions()\n+                    .run(\"allocateWriteId\", stats.getOpenTransaction().wrap(() -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAxNjY3OA=="}, "originalCommit": null, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMwNzQ3Ng==", "bodyText": "This can be simplified with ifPresent()\nrowCountChange.ifPresent(rowCount ->\n        table.setParameters(adjustRowCount(table.getParameters(), tableName, rowCount));", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504307476", "createdAt": "2020-10-13T23:04:04Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/thrift/ThriftHiveMetastoreClient.java", "diffHunk": "@@ -511,4 +523,56 @@ public String getDelegationToken(String userName)\n     {\n         return client.get_delegation_token(userName, userName);\n     }\n+\n+    @Override\n+    public long allocateWriteId(String dbName, String tableName, long transactionId)\n+            throws TException\n+    {\n+        AllocateTableWriteIdsRequest request = new AllocateTableWriteIdsRequest(dbName, tableName);\n+        request.setTxnIds(ImmutableList.of(transactionId));\n+        AllocateTableWriteIdsResponse response = client.allocate_table_write_ids(request);\n+        return Iterables.getOnlyElement(response.getTxnToWriteIds()).getWriteId();\n+    }\n+\n+    @Override\n+    public void updateTableWriteId(String dbName, String tableName, long transactionId, long writeId, OptionalLong rowCountChange)\n+            throws TException\n+    {\n+        Table table = getTableWithCapabilities(dbName, tableName);\n+        if (rowCountChange.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0NTA0NA==", "bodyText": "checkState", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504345044", "createdAt": "2020-10-14T01:17:11Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveTableHandle.java", "diffHunk": "@@ -240,6 +274,32 @@ public SchemaTableName getSchemaTableName()\n         return new SchemaTableName(schemaName, tableName);\n     }\n \n+    @JsonIgnore\n+    public boolean isAcidDelete()\n+    {\n+        return transaction.isDelete();\n+    }\n+\n+    @JsonIgnore\n+    public boolean isInAcidTransaction()\n+    {\n+        return transaction.isAcidTransactionRunning();\n+    }\n+\n+    @JsonIgnore\n+    public long getAcidTransactionId()\n+    {\n+        checkArgument(transaction.isAcidTransactionRunning(), \"The AcidTransaction is not running\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0NTA2MQ==", "bodyText": "checkState", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504345061", "createdAt": "2020-10-14T01:17:16Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveTableHandle.java", "diffHunk": "@@ -240,6 +274,32 @@ public SchemaTableName getSchemaTableName()\n         return new SchemaTableName(schemaName, tableName);\n     }\n \n+    @JsonIgnore\n+    public boolean isAcidDelete()\n+    {\n+        return transaction.isDelete();\n+    }\n+\n+    @JsonIgnore\n+    public boolean isInAcidTransaction()\n+    {\n+        return transaction.isAcidTransactionRunning();\n+    }\n+\n+    @JsonIgnore\n+    public long getAcidTransactionId()\n+    {\n+        checkArgument(transaction.isAcidTransactionRunning(), \"The AcidTransaction is not running\");\n+        return transaction.getAcidTransactionId();\n+    }\n+\n+    @JsonIgnore\n+    public long getWriteId()\n+    {\n+        checkArgument(transaction.isAcidTransactionRunning(), \"The AcidTransaction is not running\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0NTk0OQ==", "bodyText": "We could write this so it's easier to see the difference between the two cases\nString nameFormat = isInsertOnlyTable(tableParameters) ? \"%05d_0\" : \"bucket_%05d\";\nreturn new Path(subdirPath, String.format(nameFormat, bucketToUse));", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504345949", "createdAt": "2020-10-14T01:20:47Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveWriterFactory.java", "diffHunk": "@@ -565,6 +586,16 @@ else if (insertExistingPartitionsBehavior == InsertExistingPartitionsBehavior.ER\n                 hiveWriterStats);\n     }\n \n+    private static Path createHiveBucketPath(Path subdirPath, int bucketToUse, Map<String, String> tableParameters)\n+    {\n+        if (isInsertOnlyTable(tableParameters)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0ODAzNA==", "bodyText": "You could move this to the top of the class and call it just CODEC so that it's referenced as PartitionAndStatementId.CODEC. This seems better since \"fragment\" is specific to a particular usage of the constant.", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504348034", "createdAt": "2020-10-14T01:28:59Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/PartitionAndStatementId.java", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import io.airlift.json.JsonCodec;\n+\n+import java.util.Objects;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class PartitionAndStatementId\n+{\n+    private final String partitionName;\n+    private final int statementId;\n+    private final long rowCount;\n+    private final String deleteDeltaDirectory;\n+\n+    @JsonCreator\n+    public PartitionAndStatementId(@JsonProperty(\"partitionName\") String partitionName, @JsonProperty(\"statementId\") int statementId, @JsonProperty(\"rowCount\") long rowCount, @JsonProperty(\"deleteDeltaDirectory\") String deleteDeltaDirectory)\n+    {\n+        this.partitionName = requireNonNull(partitionName, \"partitionName is null\");\n+        this.statementId = statementId;\n+        this.rowCount = rowCount;\n+        this.deleteDeltaDirectory = deleteDeltaDirectory;\n+    }\n+\n+    @JsonProperty(\"partitionName\")\n+    public String getPartitionName()\n+    {\n+        return partitionName;\n+    }\n+\n+    @JsonProperty(\"statementId\")\n+    public int getStatementId()\n+    {\n+        return statementId;\n+    }\n+\n+    @JsonProperty(\"rowCount\")\n+    public long getRowCount()\n+    {\n+        return rowCount;\n+    }\n+\n+    @JsonProperty(\"deleteDeltaDirectory\")\n+    public String getDeleteDeltaDirectory()\n+    {\n+        return deleteDeltaDirectory;\n+    }\n+\n+    public static final JsonCodec<PartitionAndStatementId> FRAGMENT_CODEC = JsonCodec.jsonCodec(PartitionAndStatementId.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0ODE4OQ==", "bodyText": "We don't normally provide the name if it's the same as the default (all of these are the default names)", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504348189", "createdAt": "2020-10-14T01:29:24Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/PartitionAndStatementId.java", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import io.airlift.json.JsonCodec;\n+\n+import java.util.Objects;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class PartitionAndStatementId\n+{\n+    private final String partitionName;\n+    private final int statementId;\n+    private final long rowCount;\n+    private final String deleteDeltaDirectory;\n+\n+    @JsonCreator\n+    public PartitionAndStatementId(@JsonProperty(\"partitionName\") String partitionName, @JsonProperty(\"statementId\") int statementId, @JsonProperty(\"rowCount\") long rowCount, @JsonProperty(\"deleteDeltaDirectory\") String deleteDeltaDirectory)\n+    {\n+        this.partitionName = requireNonNull(partitionName, \"partitionName is null\");\n+        this.statementId = statementId;\n+        this.rowCount = rowCount;\n+        this.deleteDeltaDirectory = deleteDeltaDirectory;\n+    }\n+\n+    @JsonProperty(\"partitionName\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0ODY3Nw==", "bodyText": "table.getWriteId().ifPresent(result::setWriteId);", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504348677", "createdAt": "2020-10-14T01:31:15Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/thrift/ThriftMetastoreUtil.java", "diffHunk": "@@ -186,6 +186,9 @@ private ThriftMetastoreUtil() {}\n         result.setSd(makeStorageDescriptor(table.getTableName(), table.getDataColumns(), table.getStorage()));\n         result.setViewOriginalText(table.getViewOriginalText().orElse(null));\n         result.setViewExpandedText(table.getViewExpandedText().orElse(null));\n+        if (table.getWriteId().isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0ODc2Mw==", "bodyText": "writeId.ifPresent(result::setWriteId);", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504348763", "createdAt": "2020-10-14T01:31:38Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/thrift/ThriftMetastoreUtil.java", "diffHunk": "@@ -358,13 +361,21 @@ public static boolean isRoleEnabled(ConnectorIdentity identity, Function<HivePri\n     }\n \n     public static org.apache.hadoop.hive.metastore.api.Partition toMetastoreApiPartition(Partition partition)\n+    {\n+        return toMetastoreApiPartition(partition, Optional.empty());\n+    }\n+\n+    public static org.apache.hadoop.hive.metastore.api.Partition toMetastoreApiPartition(Partition partition, Optional<Long> writeId)\n     {\n         org.apache.hadoop.hive.metastore.api.Partition result = new org.apache.hadoop.hive.metastore.api.Partition();\n         result.setDbName(partition.getDatabaseName());\n         result.setTableName(partition.getTableName());\n         result.setValues(partition.getValues());\n         result.setSd(makeStorageDescriptor(partition.getTableName(), partition.getColumns(), partition.getStorage()));\n         result.setParameters(partition.getParameters());\n+        if (writeId.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0OTIwNg==", "bodyText": "Check for null", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504349206", "createdAt": "2020-10-14T01:33:33Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/InternalHiveSplitFactory.java", "diffHunk": "@@ -92,6 +98,7 @@ public InternalHiveSplitFactory(\n         this.bucketConversion = requireNonNull(bucketConversion, \"bucketConversion is null\");\n         this.forceLocalScheduling = forceLocalScheduling;\n         this.s3SelectPushdownEnabled = s3SelectPushdownEnabled;\n+        this.transaction = transaction;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0OTI2MA==", "bodyText": "int bucketNumberIndex = bucketNumber.orElse(0);", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504349260", "createdAt": "2020-10-14T01:33:50Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/util/InternalHiveSplitFactory.java", "diffHunk": "@@ -190,6 +197,13 @@ public String getPartitionName()\n             blocks = ImmutableList.of(new InternalHiveBlock(start, start + length, blocks.get(0).getAddresses()));\n         }\n \n+        int statementId = 0;\n+\n+        if (transaction.isDelete()) {\n+            int bucketNumberIndex = bucketNumber.isEmpty() ? 0 : bucketNumber.getAsInt();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM1MDE3NQ==", "bodyText": "Let's keep this final and change it to new HashMap<>(), then we can just update it normally in the update method", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504350175", "createdAt": "2020-10-14T01:37:48Z", "author": {"login": "electrum"}, "path": "presto-orc/src/main/java/io/prestosql/orc/OrcWriter.java", "diffHunk": "@@ -106,7 +107,7 @@\n     private final int stripeMaxRowCount;\n     private final int rowGroupMaxRowCount;\n     private final int maxCompressionBufferSize;\n-    private final Map<String, String> userMetadata;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM1MDUzOA==", "bodyText": "This code is specific to Hive ACID and belongs in the Hive connector. The ORC writer should have a simple method to add/update metadata entries in the map.", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504350538", "createdAt": "2020-10-14T01:39:14Z", "author": {"login": "electrum"}, "path": "presto-orc/src/main/java/io/prestosql/orc/OrcWriter.java", "diffHunk": "@@ -468,6 +469,42 @@ public void close()\n         }\n     }\n \n+    public enum OrcOperation\n+    {\n+        NONE(-1),\n+        INSERT(0),\n+        DELETE(2);\n+\n+        private final int operationNumber;\n+\n+        OrcOperation(int operationNumber)\n+        {\n+            this.operationNumber = operationNumber;\n+        }\n+\n+        public int getOperationNumber()\n+        {\n+            return operationNumber;\n+        }\n+    }\n+\n+    public void updateUserMetadata(long writeId, int bucket, OrcOperation operation)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM1MDY3OA==", "bodyText": "Use toStringHelper", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504350678", "createdAt": "2020-10-14T01:39:45Z", "author": {"login": "electrum"}, "path": "presto-orc/src/main/java/io/prestosql/orc/writer/LongColumnWriter.java", "diffHunk": "@@ -235,4 +235,13 @@ public void reset()\n         rowGroupColumnStatistics.clear();\n         statisticsBuilder = statisticsBuilderSupplier.get();\n     }\n+\n+    @Override\n+    public String toString()\n+    {\n+        return \"LongColumnWriter{\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 8}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA4NDczMDY4", "url": "https://github.com/trinodb/trino/pull/5402#pullrequestreview-508473068", "createdAt": "2020-10-14T15:31:06Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 36, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNTozMTowN1rOHhY8MQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQyMjo0MjoxMFrOHmJ_pQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc3MzY4MQ==", "bodyText": "You can use Longs.tryParse() for this", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504773681", "createdAt": "2020-10-14T15:31:07Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/MetastoreUtil.java", "diffHunk": "@@ -444,4 +448,18 @@ else if (type instanceof TinyintType\n \n         return withColumnDomains(domains);\n     }\n+\n+    public static Map<String, String> adjustRowCount(Map<String, String> parameters, String description, long rowCountAdjustment)\n+    {\n+        Map<String, String> parms = new HashMap<>(parameters);\n+        String existingRowCount = parameters.get(NUM_ROWS);\n+        if (existingRowCount == null) {\n+            return parameters;\n+        }\n+        checkArgument(DIGITS.matcher(existingRowCount).matches(), \"For %s, the existing row count (%s) is not a digit string\", description, existingRowCount);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc3NzI4NA==", "bodyText": "This should be actualColumnCount. Let's remove the acidColumnCount variable so it's not accidentally used.", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504777284", "createdAt": "2020-10-14T15:35:53Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcPageSourceFactory.java", "diffHunk": "@@ -243,9 +255,11 @@ private static ConnectorPageSource createOrcPageSource(\n             OrcReader reader = optionalOrcReader.get();\n \n             List<OrcColumn> fileColumns = reader.getRootColumn().getNestedColumns();\n-            List<OrcColumn> fileReadColumns = new ArrayList<>(columns.size() + (isFullAcid ? 2 : 0));\n-            List<Type> fileReadTypes = new ArrayList<>(columns.size() + (isFullAcid ? 2 : 0));\n-            List<OrcReader.ProjectedLayout> fileReadLayouts = new ArrayList<>(columns.size() + (isFullAcid ? 2 : 0));\n+            int acidColumnCount = 3;\n+            int actualColumnCount = columns.size() + (isFullAcid ? acidColumnCount : 0);\n+            List<OrcColumn> fileReadColumns = new ArrayList<>(acidColumnCount);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc3Nzc3Mg==", "bodyText": "Remove is prefix", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504777772", "createdAt": "2020-10-14T15:36:33Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcPageSourceFactory.java", "diffHunk": "@@ -145,7 +148,10 @@ public OrcPageSourceFactory(\n             Properties schema,\n             List<HiveColumnHandle> columns,\n             TupleDomain<HiveColumnHandle> effectivePredicate,\n-            Optional<AcidInfo> acidInfo)\n+            Optional<AcidInfo> acidInfo,\n+            OptionalInt bucketNumber,\n+            boolean isOriginalFile,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc3ODA2Mw==", "bodyText": "int bucket = bucketNumber.orElse(0);\nint startingRowId = originalFileRowId.orElse(0);", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504778063", "createdAt": "2020-10-14T15:36:57Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcPageSourceFactory.java", "diffHunk": "@@ -371,6 +389,17 @@ else if (column.getBaseHiveColumnIndex() < fileColumns.size()) {\n                             configuration,\n                             stats));\n \n+            if (transaction.isDelete()) {\n+                if (isOriginalFile) {\n+                    int bucket = bucketNumber.isEmpty() ? 0 : bucketNumber.getAsInt();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc3ODc5Mg==", "bodyText": "When are these values empty for original files?", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504778792", "createdAt": "2020-10-14T15:37:58Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcPageSourceFactory.java", "diffHunk": "@@ -371,6 +389,17 @@ else if (column.getBaseHiveColumnIndex() < fileColumns.size()) {\n                             configuration,\n                             stats));\n \n+            if (transaction.isDelete()) {\n+                if (isOriginalFile) {\n+                    int bucket = bucketNumber.isEmpty() ? 0 : bucketNumber.getAsInt();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc3ODA2Mw=="}, "originalCommit": null, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc4MTI5MQ==", "bodyText": "Add braces to be consistent with the others", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504781291", "createdAt": "2020-10-14T15:41:34Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcTypeToHiveTypeTranslator.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.orc;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.plugin.hive.HiveType;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_BINARY;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_BOOLEAN;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_BYTE;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_DATE;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_DOUBLE;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_FLOAT;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_INT;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_LONG;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_SHORT;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_STRING;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_TIMESTAMP;\n+import static io.prestosql.plugin.hive.HiveType.toHiveType;\n+import static org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getListTypeInfo;\n+import static org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getMapTypeInfo;\n+import static org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getStructTypeInfo;\n+\n+public final class OrcTypeToHiveTypeTranslator\n+{\n+    private OrcTypeToHiveTypeTranslator() {}\n+\n+    public static HiveType fromOrcTypeToHiveType(OrcType orcType, ColumnMetadata<OrcType> columnMetadata)\n+    {\n+        switch (orcType.getOrcTypeKind()) {\n+            case BOOLEAN:\n+                return HIVE_BOOLEAN;\n+\n+            case FLOAT:\n+                return HIVE_FLOAT;\n+\n+            case DOUBLE:\n+                return HIVE_DOUBLE;\n+\n+            case BYTE:\n+                return HIVE_BYTE;\n+\n+            case DATE:\n+                return HIVE_DATE;\n+\n+            case SHORT:\n+                return HIVE_SHORT;\n+\n+            case INT:\n+                return HIVE_INT;\n+\n+            case LONG:\n+                return HIVE_LONG;\n+\n+            case DECIMAL:\n+                checkArgument(orcType.getPrecision().isPresent(), \"orcType.getPrecision() is not present\");\n+                checkArgument(orcType.getScale().isPresent(), \"orcType.getScale() is not present\");\n+                return toHiveType(new DecimalTypeInfo(orcType.getPrecision().get(), orcType.getScale().get()));\n+\n+            case TIMESTAMP:\n+                return HIVE_TIMESTAMP;\n+\n+            case BINARY:\n+                return HIVE_BINARY;\n+\n+            case CHAR:\n+            case VARCHAR:\n+            case STRING:\n+                return HIVE_STRING;\n+\n+            case LIST: {\n+                HiveType elementType = fromOrcTypeToHiveType(columnMetadata.get(orcType.getFieldTypeIndex(0)), columnMetadata);\n+                return toHiveType(getListTypeInfo(elementType.getTypeInfo()));\n+            }\n+\n+            case MAP: {\n+                HiveType keyType = getHiveType(orcType, 0, columnMetadata);\n+                HiveType elementType = getHiveType(orcType, 1, columnMetadata);\n+                return toHiveType(getMapTypeInfo(keyType.getTypeInfo(), elementType.getTypeInfo()));\n+            }\n+\n+            case STRUCT:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc4MTc5NQ==", "bodyText": "throw new VerifyException(\"Unhandled ORC type: \" + orcType.getOrcTypeKind());", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504781795", "createdAt": "2020-10-14T15:42:20Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcTypeToHiveTypeTranslator.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.orc;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.plugin.hive.HiveType;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_BINARY;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_BOOLEAN;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_BYTE;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_DATE;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_DOUBLE;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_FLOAT;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_INT;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_LONG;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_SHORT;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_STRING;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_TIMESTAMP;\n+import static io.prestosql.plugin.hive.HiveType.toHiveType;\n+import static org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getListTypeInfo;\n+import static org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getMapTypeInfo;\n+import static org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getStructTypeInfo;\n+\n+public final class OrcTypeToHiveTypeTranslator\n+{\n+    private OrcTypeToHiveTypeTranslator() {}\n+\n+    public static HiveType fromOrcTypeToHiveType(OrcType orcType, ColumnMetadata<OrcType> columnMetadata)\n+    {\n+        switch (orcType.getOrcTypeKind()) {\n+            case BOOLEAN:\n+                return HIVE_BOOLEAN;\n+\n+            case FLOAT:\n+                return HIVE_FLOAT;\n+\n+            case DOUBLE:\n+                return HIVE_DOUBLE;\n+\n+            case BYTE:\n+                return HIVE_BYTE;\n+\n+            case DATE:\n+                return HIVE_DATE;\n+\n+            case SHORT:\n+                return HIVE_SHORT;\n+\n+            case INT:\n+                return HIVE_INT;\n+\n+            case LONG:\n+                return HIVE_LONG;\n+\n+            case DECIMAL:\n+                checkArgument(orcType.getPrecision().isPresent(), \"orcType.getPrecision() is not present\");\n+                checkArgument(orcType.getScale().isPresent(), \"orcType.getScale() is not present\");\n+                return toHiveType(new DecimalTypeInfo(orcType.getPrecision().get(), orcType.getScale().get()));\n+\n+            case TIMESTAMP:\n+                return HIVE_TIMESTAMP;\n+\n+            case BINARY:\n+                return HIVE_BINARY;\n+\n+            case CHAR:\n+            case VARCHAR:\n+            case STRING:\n+                return HIVE_STRING;\n+\n+            case LIST: {\n+                HiveType elementType = fromOrcTypeToHiveType(columnMetadata.get(orcType.getFieldTypeIndex(0)), columnMetadata);\n+                return toHiveType(getListTypeInfo(elementType.getTypeInfo()));\n+            }\n+\n+            case MAP: {\n+                HiveType keyType = getHiveType(orcType, 0, columnMetadata);\n+                HiveType elementType = getHiveType(orcType, 1, columnMetadata);\n+                return toHiveType(getMapTypeInfo(keyType.getTypeInfo(), elementType.getTypeInfo()));\n+            }\n+\n+            case STRUCT:\n+                ImmutableList.Builder<TypeInfo> fieldTypeInfo = ImmutableList.builder();\n+                for (int fieldId = 0; fieldId < orcType.getFieldCount(); fieldId++) {\n+                    fieldTypeInfo.add(getHiveType(orcType, fieldId, columnMetadata).getTypeInfo());\n+                }\n+                return toHiveType(getStructTypeInfo(orcType.getFieldNames(), fieldTypeInfo.build()));\n+\n+            default:\n+                throw new UnsupportedOperationException();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDk0MDYzMA==", "bodyText": "Is this used anywhere? If so, make it public", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504940630", "createdAt": "2020-10-14T20:05:31Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcPageSource.java", "diffHunk": "@@ -97,6 +112,16 @@ public boolean isFinished()\n         return closed;\n     }\n \n+    public ColumnMetadata<OrcType> getColumnTypes()\n+    {\n+        return recordReader.getColumnTypes();\n+    }\n+\n+    Optional<Long> getOriginalFileRowId()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDk0MzA2NA==", "bodyText": "Nit: you can use a trailing comma for the last item in an array initializer -- keeps the syntax more consistent", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504943064", "createdAt": "2020-10-14T20:09:51Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcPageSource.java", "diffHunk": "@@ -281,4 +316,59 @@ public Block load()\n             }\n         }\n     }\n+\n+    private static class RowIdAdaptation\n+            implements ColumnAdaptation\n+    {\n+        @Override\n+        public Block block(Page sourcePage, MaskDeletedRowsFunction maskDeletedRowsFunction, long filePosition)\n+        {\n+            Block rowBlock = maskDeletedRowsFunction.apply(fromFieldBlocks(\n+                    sourcePage.getPositionCount(),\n+                    Optional.empty(),\n+                    new Block[] {\n+                            sourcePage.getBlock(ORIGINAL_TRANSACTION_CHANNEL),\n+                            sourcePage.getBlock(ROW_ID_CHANNEL),\n+                            sourcePage.getBlock(BUCKET_CHANNEL)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDk0NjQwNA==", "bodyText": "Extra space between \"- -\"", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504946404", "createdAt": "2020-10-14T20:16:20Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcFileWriterFactory.java", "diffHunk": "@@ -173,8 +187,22 @@ public OrcWriterStats getStats()\n                 return null;\n             };\n \n+            ColumnMetadata<OrcType> rootType;\n+            if (transaction.isInsert() && useAcidSchema) {\n+                // Only add the ACID columns if the request is for INSERT - - for DELETE, the columns are", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDk0NzM1Mw==", "bodyText": "This should be IllegalArgumentException since the error is dependent on the argument value", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504947353", "createdAt": "2020-10-14T20:18:10Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcFileWriter.java", "diffHunk": "@@ -191,11 +223,60 @@ public long getValidationCpuNanos()\n         return validationCpuNanos;\n     }\n \n+    public void setMaxWriteId(long maxWriteId)\n+    {\n+        this.maxWriteId = OptionalLong.of(maxWriteId);\n+    }\n+\n     @Override\n     public String toString()\n     {\n         return toStringHelper(this)\n                 .add(\"writer\", orcWriter)\n                 .toString();\n     }\n+\n+    private Block[] buildAcidColumns(Block rowBlock, AcidTransaction transaction)\n+    {\n+        int positionCount = rowBlock.getPositionCount();\n+        int bucketValue = computeBucketValue(bucketNumber.isPresent() ? bucketNumber.getAsInt() : 0, 0);\n+        // operation, originalWriteId, bucket, rowId, currentWriteId, row<>\n+        return new Block[] {\n+                RunLengthEncodedBlock.create(INTEGER, (long) getOrcOperation(transaction), positionCount),\n+                RunLengthEncodedBlock.create(BIGINT, transaction.getWriteId(), positionCount),\n+                RunLengthEncodedBlock.create(INTEGER, (long) bucketValue, positionCount),\n+                buildAcidRowIdsColumn(positionCount),\n+                RunLengthEncodedBlock.create(BIGINT, transaction.getWriteId(), positionCount),\n+                rowBlock\n+        };\n+    }\n+\n+    private int getOrcOperation(AcidTransaction transaction)\n+    {\n+        switch (transaction.getOperation()) {\n+            case INSERT:\n+                return 0;\n+            default:\n+                throw new IllegalStateException(\"In getOrcOperation, transaction isn't INSERT, transaction \" + transaction);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDk0ODEwMg==", "bodyText": "This method and error message combination is a bit strange. We have a switch statement, implying we might later handle different types, but the error message says \"not INSERT\", implying there is only one valid type.", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r504948102", "createdAt": "2020-10-14T20:19:28Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcFileWriter.java", "diffHunk": "@@ -191,11 +223,60 @@ public long getValidationCpuNanos()\n         return validationCpuNanos;\n     }\n \n+    public void setMaxWriteId(long maxWriteId)\n+    {\n+        this.maxWriteId = OptionalLong.of(maxWriteId);\n+    }\n+\n     @Override\n     public String toString()\n     {\n         return toStringHelper(this)\n                 .add(\"writer\", orcWriter)\n                 .toString();\n     }\n+\n+    private Block[] buildAcidColumns(Block rowBlock, AcidTransaction transaction)\n+    {\n+        int positionCount = rowBlock.getPositionCount();\n+        int bucketValue = computeBucketValue(bucketNumber.isPresent() ? bucketNumber.getAsInt() : 0, 0);\n+        // operation, originalWriteId, bucket, rowId, currentWriteId, row<>\n+        return new Block[] {\n+                RunLengthEncodedBlock.create(INTEGER, (long) getOrcOperation(transaction), positionCount),\n+                RunLengthEncodedBlock.create(BIGINT, transaction.getWriteId(), positionCount),\n+                RunLengthEncodedBlock.create(INTEGER, (long) bucketValue, positionCount),\n+                buildAcidRowIdsColumn(positionCount),\n+                RunLengthEncodedBlock.create(BIGINT, transaction.getWriteId(), positionCount),\n+                rowBlock\n+        };\n+    }\n+\n+    private int getOrcOperation(AcidTransaction transaction)\n+    {\n+        switch (transaction.getOperation()) {\n+            case INSERT:\n+                return 0;\n+            default:\n+                throw new IllegalStateException(\"In getOrcOperation, transaction isn't INSERT, transaction \" + transaction);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDk0NzM1Mw=="}, "originalCommit": null, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwNjYyOA==", "bodyText": "Check for null", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r505706628", "createdAt": "2020-10-15T17:11:44Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/PartitionAndStatementId.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import io.airlift.json.JsonCodec;\n+\n+import java.util.Objects;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class PartitionAndStatementId\n+{\n+    public static final JsonCodec<PartitionAndStatementId> CODEC = JsonCodec.jsonCodec(PartitionAndStatementId.class);\n+\n+    private final String partitionName;\n+    private final int statementId;\n+    private final long rowCount;\n+    private final String deleteDeltaDirectory;\n+\n+    @JsonCreator\n+    public PartitionAndStatementId(\n+            @JsonProperty(\"partitionName\") String partitionName,\n+            @JsonProperty(\"statementId\") int statementId,\n+            @JsonProperty(\"rowCount\") long rowCount,\n+            @JsonProperty(\"deleteDeltaDirectory\") String deleteDeltaDirectory)\n+    {\n+        this.partitionName = requireNonNull(partitionName, \"partitionName is null\");\n+        this.statementId = statementId;\n+        this.rowCount = rowCount;\n+        this.deleteDeltaDirectory = deleteDeltaDirectory;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwNzI5NA==", "bodyText": "Now that we have a class for ACID schema, it would make sense to move these constants into this class", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r505707294", "createdAt": "2020-10-15T17:12:54Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/acid/AcidSchema.java", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.hive.acid;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.hive.HiveType;\n+import io.prestosql.plugin.hive.HiveTypeName;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.Type;\n+import org.apache.hadoop.hive.ql.io.IOConstants;\n+\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_INT;\n+import static io.prestosql.plugin.hive.HiveType.HIVE_LONG;\n+import static io.prestosql.plugin.hive.orc.OrcPageSourceFactory.ACID_COLUMN_BUCKET;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwODg2Nw==", "bodyText": "This needs to call invalidateTable(), similar to replaceTable()", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r505708867", "createdAt": "2020-10-15T17:15:29Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/cache/CachingHiveMetastore.java", "diffHunk": "@@ -943,6 +947,42 @@ public boolean isImpersonationEnabled()\n         return delegate.listTablePrivileges(databaseName, tableName, tableOwner, principal);\n     }\n \n+    @Override\n+    public long allocateWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId)\n+    {\n+        return delegate.allocateWriteId(identity, dbName, tableName, transactionId);\n+    }\n+\n+    @Override\n+    public void acquireTableWriteLock(HiveIdentity identity, String queryId, long transactionId, String dbName, String tableName, DataOperationType operation, boolean isDynamicPartitionWrite)\n+    {\n+        delegate.acquireTableWriteLock(identity, queryId, transactionId, dbName, tableName, operation, isDynamicPartitionWrite);\n+    }\n+\n+    @Override\n+    public void updateTableWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId, long writeId, OptionalLong rowCountChange)\n+    {\n+        delegate.updateTableWriteId(identity, dbName, tableName, transactionId, writeId, rowCountChange);\n+    }\n+\n+    @Override\n+    public void alterPartitions(HiveIdentity identity, String dbName, String tableName, List<Partition> partitions, long writeId)\n+    {\n+        delegate.alterPartitions(identity, dbName, tableName, partitions, writeId);\n+    }\n+\n+    @Override\n+    public void addDynamicPartitions(HiveIdentity identity, String dbName, String tableName, List<String> partitionNames, long transactionId, long writeId, AcidOperation operation)\n+    {\n+        delegate.addDynamicPartitions(identity, dbName, tableName, partitionNames, transactionId, writeId, operation);\n+    }\n+\n+    @Override\n+    public void alterTransactionalTable(HiveIdentity identity, Table table, long transactionId, long writeId, EnvironmentContext context, PrincipalPrivileges principalPrivileges)\n+    {\n+        delegate.alterTransactionalTable(identity, table, transactionId, writeId, context, principalPrivileges);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwOTA1Nw==", "bodyText": "This needs invalidatePartitionCache(), like addPartitions()", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r505709057", "createdAt": "2020-10-15T17:15:52Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/cache/CachingHiveMetastore.java", "diffHunk": "@@ -943,6 +947,42 @@ public boolean isImpersonationEnabled()\n         return delegate.listTablePrivileges(databaseName, tableName, tableOwner, principal);\n     }\n \n+    @Override\n+    public long allocateWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId)\n+    {\n+        return delegate.allocateWriteId(identity, dbName, tableName, transactionId);\n+    }\n+\n+    @Override\n+    public void acquireTableWriteLock(HiveIdentity identity, String queryId, long transactionId, String dbName, String tableName, DataOperationType operation, boolean isDynamicPartitionWrite)\n+    {\n+        delegate.acquireTableWriteLock(identity, queryId, transactionId, dbName, tableName, operation, isDynamicPartitionWrite);\n+    }\n+\n+    @Override\n+    public void updateTableWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId, long writeId, OptionalLong rowCountChange)\n+    {\n+        delegate.updateTableWriteId(identity, dbName, tableName, transactionId, writeId, rowCountChange);\n+    }\n+\n+    @Override\n+    public void alterPartitions(HiveIdentity identity, String dbName, String tableName, List<Partition> partitions, long writeId)\n+    {\n+        delegate.alterPartitions(identity, dbName, tableName, partitions, writeId);\n+    }\n+\n+    @Override\n+    public void addDynamicPartitions(HiveIdentity identity, String dbName, String tableName, List<String> partitionNames, long transactionId, long writeId, AcidOperation operation)\n+    {\n+        delegate.addDynamicPartitions(identity, dbName, tableName, partitionNames, transactionId, writeId, operation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwOTMxOA==", "bodyText": "This needs invalidatePartitionCache(), like alterPartition()", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r505709318", "createdAt": "2020-10-15T17:16:19Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/cache/CachingHiveMetastore.java", "diffHunk": "@@ -943,6 +947,42 @@ public boolean isImpersonationEnabled()\n         return delegate.listTablePrivileges(databaseName, tableName, tableOwner, principal);\n     }\n \n+    @Override\n+    public long allocateWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId)\n+    {\n+        return delegate.allocateWriteId(identity, dbName, tableName, transactionId);\n+    }\n+\n+    @Override\n+    public void acquireTableWriteLock(HiveIdentity identity, String queryId, long transactionId, String dbName, String tableName, DataOperationType operation, boolean isDynamicPartitionWrite)\n+    {\n+        delegate.acquireTableWriteLock(identity, queryId, transactionId, dbName, tableName, operation, isDynamicPartitionWrite);\n+    }\n+\n+    @Override\n+    public void updateTableWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId, long writeId, OptionalLong rowCountChange)\n+    {\n+        delegate.updateTableWriteId(identity, dbName, tableName, transactionId, writeId, rowCountChange);\n+    }\n+\n+    @Override\n+    public void alterPartitions(HiveIdentity identity, String dbName, String tableName, List<Partition> partitions, long writeId)\n+    {\n+        delegate.alterPartitions(identity, dbName, tableName, partitions, writeId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwOTg3MA==", "bodyText": "This probably needs to call invalidateTable(), like the other operations that modify tables", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r505709870", "createdAt": "2020-10-15T17:17:16Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/cache/CachingHiveMetastore.java", "diffHunk": "@@ -943,6 +947,42 @@ public boolean isImpersonationEnabled()\n         return delegate.listTablePrivileges(databaseName, tableName, tableOwner, principal);\n     }\n \n+    @Override\n+    public long allocateWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId)\n+    {\n+        return delegate.allocateWriteId(identity, dbName, tableName, transactionId);\n+    }\n+\n+    @Override\n+    public void acquireTableWriteLock(HiveIdentity identity, String queryId, long transactionId, String dbName, String tableName, DataOperationType operation, boolean isDynamicPartitionWrite)\n+    {\n+        delegate.acquireTableWriteLock(identity, queryId, transactionId, dbName, tableName, operation, isDynamicPartitionWrite);\n+    }\n+\n+    @Override\n+    public void updateTableWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId, long writeId, OptionalLong rowCountChange)\n+    {\n+        delegate.updateTableWriteId(identity, dbName, tableName, transactionId, writeId, rowCountChange);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcxMTg1Ng==", "bodyText": "Makes sense. Please remember to remove the comment and commented out code when updating this PR.", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r505711856", "createdAt": "2020-10-15T17:20:47Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/thrift/ThriftHiveMetastore.java", "diffHunk": "@@ -1190,6 +1221,8 @@ public void addPartitions(HiveIdentity identity, String databaseName, String tab\n         List<Partition> partitions = partitionsWithStatistics.stream()\n                 .map(ThriftMetastoreUtil::toMetastoreApiPartition)\n                 .collect(toImmutableList());\n+        // TODO: fill in the catalogName", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIwMzk2NA=="}, "originalCommit": null, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTczMzU5OQ==", "bodyText": "Add a new stats object for alterTransactionalTable so that we can track the calls separately, in case it has a different performance profile.", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r505733599", "createdAt": "2020-10-15T17:56:57Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/thrift/ThriftHiveMetastore.java", "diffHunk": "@@ -1153,6 +1160,31 @@ public void alterTable(HiveIdentity identity, String databaseName, String tableN\n         }\n     }\n \n+    @Override\n+    public void alterTransactionalTable(HiveIdentity identity, Table table, long transactionId, long writeId, EnvironmentContext context)\n+    {\n+        try {\n+            retry()\n+                    .stopOn(InvalidOperationException.class, MetaException.class)\n+                    .stopOnIllegalExceptions()\n+                    .run(\"alterTransactionalTable\", stats.getAlterTable().wrap(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTczNTMyMw==", "bodyText": "You can use getOnlyElement() from Guava Iterables. It provides good error messages.", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r505735323", "createdAt": "2020-10-15T17:59:47Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/thrift/ThriftHiveMetastore.java", "diffHunk": "@@ -1731,6 +1778,99 @@ public String getValidWriteIds(HiveIdentity identity, List<SchemaTableName> tabl\n         }\n     }\n \n+    @Override\n+    public long allocateWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId)\n+    {\n+        try {\n+            return retry()\n+                    .stopOnIllegalExceptions()\n+                    .run(\"allocateWriteId\", stats.getAllocateWriteId().wrap(() -> {\n+                        try (ThriftMetastoreClient metastoreClient = createMetastoreClient(identity)) {\n+                            List<TxnToWriteId> list = metastoreClient.allocateTableWriteIds(dbName, tableName, ImmutableList.of(transactionId));\n+                            checkArgument(list.size() == 1, \"list should have size 1\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQyODgzMA==", "bodyText": "This looks strange. I'd format like\n}, NO_ACID_TRANSACTION);", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509428830", "createdAt": "2020-10-21T16:26:43Z", "author": {"login": "electrum"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/AbstractTestHive.java", "diffHunk": "@@ -2696,7 +2698,8 @@ protected void testUpdateTableStatistics(SchemaTableName tableName, PartitionSta\n             metastoreClient.updateTableStatistics(identity, tableName.getSchemaName(), tableName.getTableName(), actualStatistics -> {\n                 assertThat(actualStatistics).isEqualTo(expectedStatistics.get());\n                 return partitionStatistics;\n-            });\n+            },", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQyODkwMw==", "bodyText": "Same", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509428903", "createdAt": "2020-10-21T16:26:47Z", "author": {"login": "electrum"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/AbstractTestHive.java", "diffHunk": "@@ -2708,7 +2711,8 @@ protected void testUpdateTableStatistics(SchemaTableName tableName, PartitionSta\n         metastoreClient.updateTableStatistics(identity, tableName.getSchemaName(), tableName.getTableName(), actualStatistics -> {\n             assertThat(actualStatistics).isEqualTo(expectedStatistics.get());\n             return initialStatistics;\n-        });\n+        },", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ0NTM1NQ==", "bodyText": "This is fairly long, you might want to wrap the arguments", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509445355", "createdAt": "2020-10-21T16:51:32Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveMetadata.java", "diffHunk": "@@ -1369,6 +1387,11 @@ public HiveOutputTableHandle beginCreateTable(ConnectorSession session, Connecto\n                 Optional<Partition> partition = table.getPartitionColumns().isEmpty() ? Optional.empty() : Optional.of(buildPartitionObject(session, table, partitionUpdate));\n                 createEmptyFiles(session, partitionUpdate.getWritePath(), table, partition, partitionUpdate.getFileNames());\n             }\n+            if (handle.isTransactional()) {\n+                AcidTransaction transaction = handle.getTransaction();\n+                List<String> partitionNames = partitionUpdates.stream().map(PartitionUpdate::getName).collect(toImmutableList());\n+                metastore.addDynamicPartitions(new HiveIdentity(session), handle.getSchemaName(), handle.getTableName(), partitionNames, transaction.getAcidTransactionId(), transaction.getWriteId(), AcidOperation.CREATE_TABLE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ0NjQyNg==", "bodyText": "Remove this, as we avoid the log-and-throw pattern. Any exception here will fail the query and present the exception to the user, so no need to log", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509446426", "createdAt": "2020-10-21T16:53:11Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveMetadata.java", "diffHunk": "@@ -1626,13 +1657,19 @@ public HiveInsertTableHandle beginInsert(ConnectorSession session, ConnectorTabl\n                 }\n                 else if (partitionUpdate.getUpdateMode() == NEW || partitionUpdate.getUpdateMode() == APPEND) {\n                     // insert into unpartitioned table\n-                    metastore.finishInsertIntoExistingTable(\n-                            session,\n-                            handle.getSchemaName(),\n-                            handle.getTableName(),\n-                            partitionUpdate.getWritePath(),\n-                            partitionUpdate.getFileNames(),\n-                            partitionStatistics);\n+                    try {\n+                        metastore.finishInsertIntoExistingTable(\n+                                session,\n+                                handle.getSchemaName(),\n+                                handle.getTableName(),\n+                                partitionUpdate.getWritePath(),\n+                                partitionUpdate.getFileNames(),\n+                                partitionStatistics);\n+                    }\n+                    catch (Exception e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ0NzEwOQ==", "bodyText": "Same comment, remove this. I think we can remove the log field after this as well.", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509447109", "createdAt": "2020-10-21T16:54:07Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveMetadata.java", "diffHunk": "@@ -1891,13 +1955,69 @@ private static boolean filterSchema(String schemaName)\n     @Override\n     public ConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n     {\n-        throw new PrestoException(NOT_SUPPORTED, \"This connector only supports delete where one or more partitions are deleted entirely\");\n+        HiveTableHandle handle = (HiveTableHandle) tableHandle;\n+        SchemaTableName tableName = handle.getSchemaTableName();\n+\n+        Table table = metastore.getTable(new HiveIdentity(session), tableName.getSchemaName(), tableName.getTableName())\n+                .orElseThrow(() -> new TableNotFoundException(tableName));\n+        ensureTableSupportsDelete(table);\n+\n+        LocationHandle locationHandle = locationService.forExistingTable(metastore, session, table);\n+\n+        AcidTransaction transaction = metastore.beginDelete(session, table);\n+\n+        WriteInfo writeInfo = locationService.getQueryWriteInfo(locationHandle);\n+        metastore.declareIntentionToWrite(session, writeInfo.getWriteMode(), writeInfo.getWritePath(), handle.getSchemaTableName());\n+\n+        return handle.withTransaction(transaction);\n+    }\n+\n+    @Override\n+    public void finishDelete(ConnectorSession session, ConnectorTableHandle tableHandle, Collection<Slice> fragments)\n+    {\n+        HiveTableHandle handle = (HiveTableHandle) tableHandle;\n+        checkArgument(handle.isAcidDelete(), \"handle should be a delete handle, but is \" + handle);\n+\n+        requireNonNull(fragments, \"fragments is null\");\n+\n+        SchemaTableName tableName = handle.getSchemaTableName();\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Table table = metastore.getTable(identity, tableName.getSchemaName(), tableName.getTableName())\n+                .orElseThrow(() -> new TableNotFoundException(tableName));\n+        ensureTableSupportsDelete(table);\n+\n+        List<PartitionAndStatementId> partitionAndStatementIds = fragments.stream()\n+                .map(Slice::getBytes)\n+                .map(CODEC::fromJson)\n+                .collect(toList());\n+\n+        HdfsContext context = new HdfsContext(session, table.getDatabaseName());\n+        if (!partitionAndStatementIds.isEmpty()) {\n+            partitionAndStatementIds.forEach(ps -> createOrcAcidVersionFile(context, new Path(ps.getDeleteDeltaDirectory())));\n+        }\n+\n+        LocationHandle locationHandle = locationService.forExistingTable(metastore, session, table);\n+        WriteInfo writeInfo = locationService.getQueryWriteInfo(locationHandle);\n+        try {\n+            metastore.finishRowLevelDelete(session, table.getDatabaseName(), table.getTableName(), writeInfo.getWritePath(), partitionAndStatementIds);\n+        }\n+        catch (Exception e) {\n+            log.error(e, \"Exception %s thrown in finishRowLevelDelete for table %s\", e, e.getMessage(), table.getTableName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 290}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ1MzAxOA==", "bodyText": "This can be a normal for-each loop", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509453018", "createdAt": "2020-10-21T17:02:33Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveMetadata.java", "diffHunk": "@@ -1676,12 +1713,32 @@ else if (partitionUpdate.getUpdateMode() == NEW || partitionUpdate.getUpdateMode\n             }\n         }\n \n+        if (isFullAcidTable(table.getParameters())) {\n+            HdfsContext context = new HdfsContext(session, table.getDatabaseName());\n+            partitionUpdates.forEach(update -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU5OTgwNg==", "bodyText": "Use %s rather than concat so that the argument is only built when the check fails", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509599806", "createdAt": "2020-10-21T19:12:25Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveMetadata.java", "diffHunk": "@@ -1891,13 +1955,69 @@ private static boolean filterSchema(String schemaName)\n     @Override\n     public ConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n     {\n-        throw new PrestoException(NOT_SUPPORTED, \"This connector only supports delete where one or more partitions are deleted entirely\");\n+        HiveTableHandle handle = (HiveTableHandle) tableHandle;\n+        SchemaTableName tableName = handle.getSchemaTableName();\n+\n+        Table table = metastore.getTable(new HiveIdentity(session), tableName.getSchemaName(), tableName.getTableName())\n+                .orElseThrow(() -> new TableNotFoundException(tableName));\n+        ensureTableSupportsDelete(table);\n+\n+        LocationHandle locationHandle = locationService.forExistingTable(metastore, session, table);\n+\n+        AcidTransaction transaction = metastore.beginDelete(session, table);\n+\n+        WriteInfo writeInfo = locationService.getQueryWriteInfo(locationHandle);\n+        metastore.declareIntentionToWrite(session, writeInfo.getWriteMode(), writeInfo.getWritePath(), handle.getSchemaTableName());\n+\n+        return handle.withTransaction(transaction);\n+    }\n+\n+    @Override\n+    public void finishDelete(ConnectorSession session, ConnectorTableHandle tableHandle, Collection<Slice> fragments)\n+    {\n+        HiveTableHandle handle = (HiveTableHandle) tableHandle;\n+        checkArgument(handle.isAcidDelete(), \"handle should be a delete handle, but is \" + handle);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 264}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYwMDE0MQ==", "bodyText": "This can be done as a normal for-each loop, and there's no need to check for empty", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509600141", "createdAt": "2020-10-21T19:13:02Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HiveMetadata.java", "diffHunk": "@@ -1891,13 +1955,69 @@ private static boolean filterSchema(String schemaName)\n     @Override\n     public ConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n     {\n-        throw new PrestoException(NOT_SUPPORTED, \"This connector only supports delete where one or more partitions are deleted entirely\");\n+        HiveTableHandle handle = (HiveTableHandle) tableHandle;\n+        SchemaTableName tableName = handle.getSchemaTableName();\n+\n+        Table table = metastore.getTable(new HiveIdentity(session), tableName.getSchemaName(), tableName.getTableName())\n+                .orElseThrow(() -> new TableNotFoundException(tableName));\n+        ensureTableSupportsDelete(table);\n+\n+        LocationHandle locationHandle = locationService.forExistingTable(metastore, session, table);\n+\n+        AcidTransaction transaction = metastore.beginDelete(session, table);\n+\n+        WriteInfo writeInfo = locationService.getQueryWriteInfo(locationHandle);\n+        metastore.declareIntentionToWrite(session, writeInfo.getWriteMode(), writeInfo.getWritePath(), handle.getSchemaTableName());\n+\n+        return handle.withTransaction(transaction);\n+    }\n+\n+    @Override\n+    public void finishDelete(ConnectorSession session, ConnectorTableHandle tableHandle, Collection<Slice> fragments)\n+    {\n+        HiveTableHandle handle = (HiveTableHandle) tableHandle;\n+        checkArgument(handle.isAcidDelete(), \"handle should be a delete handle, but is \" + handle);\n+\n+        requireNonNull(fragments, \"fragments is null\");\n+\n+        SchemaTableName tableName = handle.getSchemaTableName();\n+        HiveIdentity identity = new HiveIdentity(session);\n+        Table table = metastore.getTable(identity, tableName.getSchemaName(), tableName.getTableName())\n+                .orElseThrow(() -> new TableNotFoundException(tableName));\n+        ensureTableSupportsDelete(table);\n+\n+        List<PartitionAndStatementId> partitionAndStatementIds = fragments.stream()\n+                .map(Slice::getBytes)\n+                .map(CODEC::fromJson)\n+                .collect(toList());\n+\n+        HdfsContext context = new HdfsContext(session, table.getDatabaseName());\n+        if (!partitionAndStatementIds.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 280}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTc2MTIxNg==", "bodyText": "This pattern be\nreturn currentHiveTransaction.map(AcidTransaction::getTransaction)\n        .orElseThrow(() -> new IllegalStateException(\"currentHiveTransaction not present\"));", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509761216", "createdAt": "2020-10-21T22:32:04Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/SemiTransactionalHiveMetastore.java", "diffHunk": "@@ -835,12 +915,19 @@ public synchronized void finishInsertIntoExistingPartition(\n             case ADD:\n             case ALTER:\n             case INSERT_EXISTING:\n+            case DELETE_ROWS:\n                 throw new UnsupportedOperationException(\"Inserting into a partition that were added, altered, or inserted into in the same transaction is not supported\");\n             default:\n                 throw new IllegalStateException(\"Unknown action type\");\n         }\n     }\n \n+    private synchronized AcidTransaction getCurrentAcidTransaction()\n+    {\n+        checkArgument(currentHiveTransaction.isPresent(), \"currentHiveTransaction isn't present\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTc2MjQwOQ==", "bodyText": "Having a negative method name is a bit strange. Maybe use isFinished with opposite behavior or call it isRunning.", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509762409", "createdAt": "2020-10-21T22:33:31Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/SemiTransactionalHiveMetastore.java", "diffHunk": "@@ -961,6 +1049,11 @@ public synchronized void declareIntentionToWrite(ConnectorSession session, Write\n         declaredIntentionsToWrite.add(new DeclaredIntentionToWrite(writeMode, hdfsContext, identity, session.getQueryId(), stagingPathRoot, schemaTableName));\n     }\n \n+    public boolean notYetCommitted()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 254}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTc2NDI2Ng==", "bodyText": "We shouldn't log the full exception here since we throw it below. But we could do it like\nlog.warn(\"Rolling back due to metastore commit failure: %s\", t.getMessage());", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509764266", "createdAt": "2020-10-21T22:34:46Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/SemiTransactionalHiveMetastore.java", "diffHunk": "@@ -1147,13 +1299,14 @@ private void commitShared()\n             committer.executeAddTableOperations();\n             committer.executeAlterTableOperations();\n             committer.executeAlterPartitionOperations();\n-            committer.executeAddPartitionOperations();\n-            committer.executeUpdateStatisticsOperations();\n+            committer.executeAddPartitionOperations(transaction);\n+            committer.executeUpdateStatisticsOperations(transaction);\n         }\n         catch (Throwable t) {\n+            log.error(t, \"Metastore commit failed - - rolling back\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 423}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTc2ODUyOQ==", "bodyText": "Should this be toImmutableList()", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509768529", "createdAt": "2020-10-21T22:37:35Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/SemiTransactionalHiveMetastore.java", "diffHunk": "@@ -1365,6 +1524,97 @@ private void prepareInsertExistingTable(HdfsContext context, TableAndMore tableA\n                     Optional.empty(),\n                     tableAndMore.getStatisticsUpdate(),\n                     true));\n+\n+            if (isAcidTransactionRunning()) {\n+                AcidTransaction transaction = getCurrentAcidTransaction();\n+                updateTableWriteId(tableAndMore.getIdentity(), table.getDatabaseName(), table.getTableName(), transaction.getAcidTransactionId(), transaction.getWriteId(), OptionalLong.empty());\n+            }\n+        }\n+\n+        private void prepareDeleteRowsFromExistingTable(HdfsContext context, TableAndMore tableAndMore)\n+        {\n+            TableAndDeleteDirectories deletionState = (TableAndDeleteDirectories) tableAndMore;\n+            List<PartitionAndStatementId> partitionAndStatementIds = deletionState.getPartitionAndStatementIds();\n+            checkArgument(!partitionAndStatementIds.isEmpty(), \"partitionAndStatementIds is empty\");\n+\n+            deleteOnly = false;\n+            Table table = deletionState.getTable();\n+            checkArgument(currentHiveTransaction.isPresent(), \"currentHiveTransaction isn't present\");\n+            AcidTransaction transaction = currentHiveTransaction.get().getTransaction();\n+            checkArgument(transaction.isDelete(), \"transaction should be delete, but is \" + transaction);\n+\n+            cleanUpTasksForAbort.addAll(deletionState.getPartitionAndStatementIds().stream()\n+                    .map(ps -> new DirectoryCleanUpTask(context, new Path(ps.getDeleteDeltaDirectory()), true))\n+                    .collect(Collectors.toUnmodifiableList()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 481}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTc3MDc5OA==", "bodyText": "Use VerifyException", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509770798", "createdAt": "2020-10-21T22:40:06Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcFileWriter.java", "diffHunk": "@@ -169,6 +202,31 @@ public void commit()\n         }\n     }\n \n+    private void updateUserMetadata()\n+    {\n+        int bucketValue = computeBucketValue(bucketNumber.orElse(0), 0);\n+        long writeId = maxWriteId.isPresent() ? maxWriteId.getAsLong() : transaction.getWriteId();\n+        if (transaction.isAcidTransactionRunning()) {\n+            int stripeRowCount = orcWriter.getStripeRowCount();\n+            Map<String, String> userMetadata = new HashMap<>();\n+            OrcWriter.OrcOperation operation = transaction.getOrcOperation();\n+            switch (operation) {\n+                case INSERT:\n+                    userMetadata.put(\"hive.acid.stats\", format(\"%s,0,0\", stripeRowCount));\n+                    break;\n+                case DELETE:\n+                    userMetadata.put(\"hive.acid.stats\", format(\"0,0,%s\", stripeRowCount));\n+                    break;\n+                default:\n+                    throw new IllegalStateException(\"In updateUserMetadata, unknown OrcOperation \" + operation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTc3MTQ2Ng==", "bodyText": "bucketNumber.orElse(0)", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509771466", "createdAt": "2020-10-21T22:41:35Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcFileWriter.java", "diffHunk": "@@ -191,11 +249,77 @@ public long getValidationCpuNanos()\n         return validationCpuNanos;\n     }\n \n+    public int getStripeRowCount()\n+    {\n+        return orcWriter.getStripeRowCount();\n+    }\n+\n+    public void setMaxWriteId(long maxWriteId)\n+    {\n+        this.maxWriteId = OptionalLong.of(maxWriteId);\n+    }\n+\n+    public OptionalLong getMaxWriteId()\n+    {\n+        return maxWriteId;\n+    }\n+\n+    public void updateUserMetadata(Map<String, String> userMetadata)\n+    {\n+        orcWriter.updateUserMetadata(userMetadata);\n+    }\n+\n     @Override\n     public String toString()\n     {\n         return toStringHelper(this)\n                 .add(\"writer\", orcWriter)\n                 .toString();\n     }\n+\n+    private Block[] buildAcidColumns(Block rowBlock, AcidTransaction transaction)\n+    {\n+        int positionCount = rowBlock.getPositionCount();\n+        int bucketValue = computeBucketValue(bucketNumber.isPresent() ? bucketNumber.getAsInt() : 0, 0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTc3MTY4NQ==", "bodyText": "Use %s so that we don't compute the message on every call", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r509771685", "createdAt": "2020-10-21T22:42:10Z", "author": {"login": "electrum"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/orc/OrcFileWriter.java", "diffHunk": "@@ -191,11 +249,77 @@ public long getValidationCpuNanos()\n         return validationCpuNanos;\n     }\n \n+    public int getStripeRowCount()\n+    {\n+        return orcWriter.getStripeRowCount();\n+    }\n+\n+    public void setMaxWriteId(long maxWriteId)\n+    {\n+        this.maxWriteId = OptionalLong.of(maxWriteId);\n+    }\n+\n+    public OptionalLong getMaxWriteId()\n+    {\n+        return maxWriteId;\n+    }\n+\n+    public void updateUserMetadata(Map<String, String> userMetadata)\n+    {\n+        orcWriter.updateUserMetadata(userMetadata);\n+    }\n+\n     @Override\n     public String toString()\n     {\n         return toStringHelper(this)\n                 .add(\"writer\", orcWriter)\n                 .toString();\n     }\n+\n+    private Block[] buildAcidColumns(Block rowBlock, AcidTransaction transaction)\n+    {\n+        int positionCount = rowBlock.getPositionCount();\n+        int bucketValue = computeBucketValue(bucketNumber.isPresent() ? bucketNumber.getAsInt() : 0, 0);\n+        // operation, originalWriteId, bucket, rowId, currentWriteId, row<>\n+        return new Block[] {\n+                RunLengthEncodedBlock.create(INTEGER, (long) getOrcOperation(transaction), positionCount),\n+                RunLengthEncodedBlock.create(BIGINT, transaction.getWriteId(), positionCount),\n+                RunLengthEncodedBlock.create(INTEGER, (long) bucketValue, positionCount),\n+                buildAcidRowIdsColumn(positionCount),\n+                RunLengthEncodedBlock.create(BIGINT, transaction.getWriteId(), positionCount),\n+                rowBlock\n+        };\n+    }\n+\n+    private int getOrcOperation(AcidTransaction transaction)\n+    {\n+        switch (transaction.getOperation()) {\n+            case INSERT:\n+                return 0;\n+            default:\n+                throw new IllegalStateException(\"In getOrcOperation, transaction isn't INSERT, transaction \" + transaction);\n+        }\n+    }\n+\n+    private Block buildAcidRowIdsColumn(int positionCount)\n+    {\n+        long[] rowIds = new long[positionCount];\n+        for (int i = 0; i < positionCount; i++) {\n+            rowIds[i] = i;\n+        }\n+        return new LongArrayBlock(positionCount, Optional.empty(), rowIds);\n+    }\n+\n+    public static int extractBucketNumber(int bucketValue)\n+    {\n+        return (bucketValue >> 16) & 0xFFF;\n+    }\n+\n+    public static int computeBucketValue(int bucketId, int statementId)\n+    {\n+        checkArgument(statementId >= 0 && statementId < 1 << 16, \"statementId should be non-negative and less than 1 << 16, but is \" + statementId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 244}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "f9c7e62744b77df7b7e8d1b55e8532c03f353309", "author": {"user": {"login": "djsstarburst", "name": "David Stryker"}}, "url": "https://github.com/trinodb/trino/commit/f9c7e62744b77df7b7e8d1b55e8532c03f353309", "committedDate": "2020-10-22T01:38:20Z", "message": "Hive ACID row-level insert and delete\n\nThis commit adds support for row-level insert and delete for Hive ACID tables,\nand product tests that verify that row-level insert (and delete where allowed)\nare working correctly for this test matrix:\n\n(partitioned | unpartitioned) X\n(bucketed | unbucketed) X\n(normal | insert-only) X\n(original files | non-original files) X\n(Presto inserts | Hive inserts) X\n(first insert | subsequent inserts) X\n(Presto deletes | Hive deletes) X\n(first delete | subsequent deletes) X\n(Hive selects to verify | Presto selects to verify)\n\nThe tests also verify that metadata delete still works correctly for non-ACID\ntables, and that row-level delete is always used for ACID tables. Hive ACID\ninsert and delete make metastore updates using the delayed commit paradigm\nprovided by SemiTransactionalHiveMetastore.\n\nACID insert and delete need four Hive metastore methods not previously\nused, and this commit adds the plumbing through the many metastore layers\nfor those methods.\n\nRecords to be deleted flow through HiveUpdatablePageSource, which feeds an OrcWriter\nthe three columns - - originalTransaction, bucketId, rowId - - that identify an\nACID row to be deleted. The writer builds the ACID delete_delta bucket, adding\nthe Orc ACID operation column specifying delete and the currentTransaction column\nto each row.\n\nA complete delete scan will in general read many Orc files from earlier transactions.\nThe delete implementation will create a HiveUpdatablePageSource for each file.\nEach will write a delete_delta file, distinguished by different statementIds.\nThese delete_delta bucket files and directories will be combined by the Hive\nACID compactor as the accumulate.\n\nACID insert is simpler than delete - - it just adds the 5 ACID columns needed\nto make insertion transactional, and creates delta directories to hold bucket files."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "f9c7e62744b77df7b7e8d1b55e8532c03f353309", "author": {"user": {"login": "djsstarburst", "name": "David Stryker"}}, "url": "https://github.com/trinodb/trino/commit/f9c7e62744b77df7b7e8d1b55e8532c03f353309", "committedDate": "2020-10-22T01:38:20Z", "message": "Hive ACID row-level insert and delete\n\nThis commit adds support for row-level insert and delete for Hive ACID tables,\nand product tests that verify that row-level insert (and delete where allowed)\nare working correctly for this test matrix:\n\n(partitioned | unpartitioned) X\n(bucketed | unbucketed) X\n(normal | insert-only) X\n(original files | non-original files) X\n(Presto inserts | Hive inserts) X\n(first insert | subsequent inserts) X\n(Presto deletes | Hive deletes) X\n(first delete | subsequent deletes) X\n(Hive selects to verify | Presto selects to verify)\n\nThe tests also verify that metadata delete still works correctly for non-ACID\ntables, and that row-level delete is always used for ACID tables. Hive ACID\ninsert and delete make metastore updates using the delayed commit paradigm\nprovided by SemiTransactionalHiveMetastore.\n\nACID insert and delete need four Hive metastore methods not previously\nused, and this commit adds the plumbing through the many metastore layers\nfor those methods.\n\nRecords to be deleted flow through HiveUpdatablePageSource, which feeds an OrcWriter\nthe three columns - - originalTransaction, bucketId, rowId - - that identify an\nACID row to be deleted. The writer builds the ACID delete_delta bucket, adding\nthe Orc ACID operation column specifying delete and the currentTransaction column\nto each row.\n\nA complete delete scan will in general read many Orc files from earlier transactions.\nThe delete implementation will create a HiveUpdatablePageSource for each file.\nEach will write a delete_delta file, distinguished by different statementIds.\nThese delete_delta bucket files and directories will be combined by the Hive\nACID compactor as the accumulate.\n\nACID insert is simpler than delete - - it just adds the 5 ACID columns needed\nto make insertion transactional, and creates delta directories to hold bucket files."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUxODg2OTU5", "url": "https://github.com/trinodb/trino/pull/5402#pullrequestreview-551886959", "createdAt": "2020-12-14T20:58:13Z", "commit": {"oid": "f9c7e62744b77df7b7e8d1b55e8532c03f353309"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNFQyMDo1ODoxM1rOIFohHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNFQyMDo1ODoxM1rOIFohHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc3NzYzMA==", "bodyText": "Should this be moved before invoking the delegate?", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r542777630", "createdAt": "2020-12-14T20:58:13Z", "author": {"login": "findepi"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/cache/CachingHiveMetastore.java", "diffHunk": "@@ -943,6 +947,65 @@ public boolean isImpersonationEnabled()\n         return delegate.listTablePrivileges(databaseName, tableName, tableOwner, principal);\n     }\n \n+    @Override\n+    public long allocateWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId)\n+    {\n+        return delegate.allocateWriteId(identity, dbName, tableName, transactionId);\n+    }\n+\n+    @Override\n+    public void acquireTableWriteLock(HiveIdentity identity, String queryId, long transactionId, String dbName, String tableName, DataOperationType operation, boolean isDynamicPartitionWrite)\n+    {\n+        delegate.acquireTableWriteLock(identity, queryId, transactionId, dbName, tableName, operation, isDynamicPartitionWrite);\n+    }\n+\n+    @Override\n+    public void updateTableWriteId(HiveIdentity identity, String dbName, String tableName, long transactionId, long writeId, OptionalLong rowCountChange)\n+    {\n+        try {\n+            delegate.updateTableWriteId(identity, dbName, tableName, transactionId, writeId, rowCountChange);\n+        }\n+        finally {\n+            invalidateTable(dbName, tableName);\n+        }\n+    }\n+\n+    @Override\n+    public void alterPartitions(HiveIdentity identity, String dbName, String tableName, List<Partition> partitions, long writeId)\n+    {\n+        identity = updateIdentity(identity);\n+        try {\n+            delegate.alterPartitions(identity, dbName, tableName, partitions, writeId);\n+        }\n+        finally {\n+            invalidatePartitionCache(dbName, tableName);\n+        }\n+    }\n+\n+    @Override\n+    public void addDynamicPartitions(HiveIdentity identity, String dbName, String tableName, List<String> partitionNames, long transactionId, long writeId, AcidOperation operation)\n+    {\n+        identity = updateIdentity(identity);\n+        try {\n+            delegate.addDynamicPartitions(identity, dbName, tableName, partitionNames, transactionId, writeId, operation);\n+        }\n+        finally {\n+            invalidatePartitionCache(dbName, tableName);\n+        }\n+    }\n+\n+    @Override\n+    public void alterTransactionalTable(HiveIdentity identity, Table table, long transactionId, long writeId, EnvironmentContext context, PrincipalPrivileges principalPrivileges)\n+    {\n+        try {\n+            delegate.alterTransactionalTable(identity, table, transactionId, writeId, context, principalPrivileges);\n+        }\n+        finally {\n+            identity = updateIdentity(identity);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9c7e62744b77df7b7e8d1b55e8532c03f353309"}, "originalPosition": 90}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjkxMTQ5NDM3", "url": "https://github.com/trinodb/trino/pull/5402#pullrequestreview-691149437", "createdAt": "2021-06-23T21:17:36Z", "commit": {"oid": "f9c7e62744b77df7b7e8d1b55e8532c03f353309"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNi0yM1QyMToxNzozN1rOJzAo-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNi0yM1QyMToxNzozN1rOJzAo-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzQ2NzY0MQ==", "bodyText": "What's the reason to call commit within cleanupQuery?\nwhat if\n\nquery is part of a longer transaction (we seem to commit everything too early)\nquery is cancelled/aborted/etc -- it seems we commit instead of rolling back\n\nplease help me understand\ncc @losipiuk", "url": "https://github.com/trinodb/trino/pull/5402#discussion_r657467641", "createdAt": "2021-06-23T21:17:37Z", "author": {"login": "findepi"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/metastore/SemiTransactionalHiveMetastore.java", "diffHunk": "@@ -1068,14 +1204,15 @@ public synchronized void cleanupQuery(ConnectorSession session)\n         HiveIdentity identity = new HiveIdentity(session);\n         checkState(currentQueryId.equals(Optional.of(queryId)), \"Invalid query id %s while current query is\", queryId, currentQueryId);\n         Optional<HiveTransaction> transaction = currentHiveTransaction;\n-        currentQueryId = Optional.empty();\n-        currentHiveTransaction = Optional.empty();\n-        hiveTransactionSupplier = Optional.empty();\n \n         if (transaction.isEmpty()) {\n+            clearCurrentTransaction();\n             return;\n         }\n \n+        commit();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9c7e62744b77df7b7e8d1b55e8532c03f353309"}, "originalPosition": 364}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3592, "cost": 1, "resetAt": "2021-10-28T20:13:43Z"}}}