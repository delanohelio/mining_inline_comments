{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM1NTMwMDI3", "number": 4055, "title": "Introduce IcebergFileWriter and collect file stats directly from ORC writers", "bodyText": "Umbrella issue: #1324\nCommit Fix predicating on REAL type in Iceberg Connector:\nPreviously there was no conversion from Presto's representation of REAL type to Iceberg's, causing predicating on a REAL type to fail. Queries to reproduce the issue:\nCREATE TABLE iceberg.u_xinlin.test_real_parquet (_real REAL) WITH (format = 'parquet');\nINSERT INTO iceberg.u_xinlin.test_real_parquet VALUES 1.2;\nSELECT * FROM iceberg.u_xinlin.test_real_parquet WHERE _real = 1.2;\nThe SELECT query should output 1.2 but it would output no row.\nCommit Support Files table with nested types\nIceberg has column stats for nested columns.\nCommit Collect file stats directly from ORC writers\nWhen registering a data file with Iceberg, Presto needs to provide file statistics. Previously, Presto would write an ORC file and then reads from it to get the file stats. This commit makes Presto collect file stats directly from ORC writers.\nA related issue is that Avro files don't store file stats. So it would be highly-inefficient to re-read a written Avro file and compute the stats. With the new framework, it's possible to implement an Iceberg Avro writer that collects stats on the fly and then uses them when registering a file with Iceberg.\nIn addition, this commit removes Presto's reference to iceberg-orc. This can help us sidestep the ORC version conflict issue when upgrading to iceberg-0.8.0-incubating", "createdAt": "2020-06-17T00:13:27Z", "url": "https://github.com/trinodb/trino/pull/4055", "merged": true, "mergeCommit": {"oid": "27483269f528ab2c750b06b6e1ddcdd1f8261692"}, "closed": true, "closedAt": "2020-06-29T22:36:19Z", "author": {"login": "lxynov"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcsj16jgFqTQzMjcwMDg4Mw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcvhPMRgBqjM0ODk2MTgwMjE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyNzAwODgz", "url": "https://github.com/trinodb/trino/pull/4055#pullrequestreview-432700883", "createdAt": "2020-06-17T19:15:42Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxOToxNTo0MlrOGlT5kg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxOTo0MDoyOVrOGl9pfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc3NjUzMA==", "bodyText": "For simple iteration, it's preferable to use a normal for-each loop. Use forEach() where it's the terminal operation of a stream, or for Map.forEach() where it has the advantage of naming the key/value pair variables.", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r441776530", "createdAt": "2020-06-17T19:15:42Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/FilesTable.java", "diffHunk": "@@ -154,4 +155,20 @@ private static boolean checkNonNull(Object object, PageListBuilder pagesBuilder)\n         }\n         return true;\n     }\n+\n+    private static Map<Integer, Type> getIcebergIdToTypeMapping(Schema schema)\n+    {\n+        ImmutableMap.Builder<Integer, Type> icebergIdToTypeMapping = ImmutableMap.builder();\n+        schema.columns().forEach(field -> populateIcebergIdToTypeMapping(field, icebergIdToTypeMapping));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc4OTM4MQ==", "bodyText": "We can remove this now (since there are two branches in the switch)", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r441789381", "createdAt": "2020-06-17T19:40:41Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergPageSink.java", "diffHunk": "@@ -324,14 +323,15 @@ private WriteContext createWriter(Optional<String> partitionPath, Optional<Parti\n     }\n \n     @SuppressWarnings(\"SwitchStatementWithTooFewBranches\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc5MDM1OQ==", "bodyText": "return writeContext.getWriter().getMetrics()\n        .orElseThrow(() -> new VerifyException(\"Iceberg ORC file writers should return Iceberg metrics\"));", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r441790359", "createdAt": "2020-06-17T19:42:29Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergPageSink.java", "diffHunk": "@@ -324,14 +323,15 @@ private WriteContext createWriter(Optional<String> partitionPath, Optional<Parti\n     }\n \n     @SuppressWarnings(\"SwitchStatementWithTooFewBranches\")\n-    private Metrics readMetrics(Path path)\n+    private Metrics getMetrics(WriteContext writeContext)\n     {\n         switch (fileFormat) {\n             case PARQUET:\n-                return ParquetUtil.fileMetrics(HadoopInputFile.fromPath(path, jobConf), MetricsConfig.getDefault());\n+                return ParquetUtil.fileMetrics(HadoopInputFile.fromPath(writeContext.getPath(), jobConf), MetricsConfig.getDefault());\n             case ORC:\n-                // TODO: update Iceberg version after OrcMetrics is completed\n-                return OrcMetrics.fromInputFile(HadoopInputFile.fromPath(path, jobConf), jobConf);\n+                IcebergFileWriter fileWriter = writeContext.getWriter();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0MTc1Mg==", "bodyText": "Make this the first or last argument so that the argument ordering matches the parent class (keeps things simpler when reading the code).", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442441752", "createdAt": "2020-06-18T19:05:53Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0MjU5Ng==", "bodyText": "Maybe invert this\nif (excludedColumns.contains(orcColumnId)) {\n    continue;\n}", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442442596", "createdAt": "2020-06-18T19:07:34Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0MjczNg==", "bodyText": "The local variable could be inlined", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442442736", "createdAt": "2020-06-18T19:07:52Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0MzIxMQ==", "bodyText": "This shouldn't throw NPE. Make this\nverify(icebergSchema.findField(icebergId) != null, \"Cannot find Iceberg column with ID %s in schema %s\", icebergId, icebergSchema);", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442443211", "createdAt": "2020-06-18T19:08:57Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0NDM5Mg==", "bodyText": "Nit: also put first argument on separate line", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442444392", "createdAt": "2020-06-18T19:11:17Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0NDg4OA==", "bodyText": "Move this to the null below\nnew Metrics(\n        fileRowCount,\n        null, // TODO: Add column size accounting to ORC column writers\nOr maybe add above like\nMap<Integer, Long> columnSizes = null; // TODO: Add column size accounting to ORC column writers", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442444888", "createdAt": "2020-06-18T19:12:19Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0NTg0NA==", "bodyText": "Can shorten shouldBeExcluded to exclude\nif (exclude) {", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442445844", "createdAt": "2020-06-18T19:14:14Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0NjY5NQ==", "bodyText": "No need for empty default block", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442446695", "createdAt": "2020-06-18T19:16:01Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {\n+            excludedColumns.add(orcColumnId);\n+        }\n+        OrcType orcColumn = orcColumns.get(orcColumnId);\n+        switch (orcColumn.getOrcTypeKind()) {\n+            case LIST:\n+            case MAP:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, true, excludedColumns);\n+                }\n+                return;\n+            case STRUCT:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, shouldBeExcluded, excludedColumns);\n+                }\n+                return;\n+            default:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ0NzgzMg==", "bodyText": "String icebergId = orcColumn.getAttributes().get(ORC_ICEBERG_ID_KEY);\nverify(icebergId != null, \"ORC column %s doesn't have an associated Iceberg ID\", orcColumn);", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442447832", "createdAt": "2020-06-18T19:18:21Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {\n+            excludedColumns.add(orcColumnId);\n+        }\n+        OrcType orcColumn = orcColumns.get(orcColumnId);\n+        switch (orcColumn.getOrcTypeKind()) {\n+            case LIST:\n+            case MAP:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, true, excludedColumns);\n+                }\n+                return;\n+            case STRUCT:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, shouldBeExcluded, excludedColumns);\n+                }\n+                return;\n+            default:\n+        }\n+    }\n+\n+    private static int getIcebergId(OrcType orcColumn)\n+    {\n+        String icebergId = requireNonNull(orcColumn.getAttributes().get(ORC_ICEBERG_ID_KEY), format(\"ORC column %s doesn't have an associated Iceberg ID\", orcColumn.toString()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 172}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ1MDA0Nw==", "bodyText": "These can be declared as Long", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442450047", "createdAt": "2020-06-18T19:22:51Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {\n+            excludedColumns.add(orcColumnId);\n+        }\n+        OrcType orcColumn = orcColumns.get(orcColumnId);\n+        switch (orcColumn.getOrcTypeKind()) {\n+            case LIST:\n+            case MAP:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, true, excludedColumns);\n+                }\n+                return;\n+            case STRUCT:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, shouldBeExcluded, excludedColumns);\n+                }\n+                return;\n+            default:\n+        }\n+    }\n+\n+    private static int getIcebergId(OrcType orcColumn)\n+    {\n+        String icebergId = requireNonNull(orcColumn.getAttributes().get(ORC_ICEBERG_ID_KEY), format(\"ORC column %s doesn't have an associated Iceberg ID\", orcColumn.toString()));\n+        return Integer.parseInt(icebergId);\n+    }\n+\n+    private static Optional<IcebergMinMax> toIcebergMinMax(ColumnStatistics orcColumnStats, org.apache.iceberg.types.Type icebergType)\n+    {\n+        IntegerStatistics integerStatistics = orcColumnStats.getIntegerStatistics();\n+        if (integerStatistics != null) {\n+            Object min = integerStatistics.getMin();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ1MDEzMQ==", "bodyText": "Cast isn't needed if you change the type", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442450131", "createdAt": "2020-06-18T19:23:00Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {\n+            excludedColumns.add(orcColumnId);\n+        }\n+        OrcType orcColumn = orcColumns.get(orcColumnId);\n+        switch (orcColumn.getOrcTypeKind()) {\n+            case LIST:\n+            case MAP:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, true, excludedColumns);\n+                }\n+                return;\n+            case STRUCT:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, shouldBeExcluded, excludedColumns);\n+                }\n+                return;\n+            default:\n+        }\n+    }\n+\n+    private static int getIcebergId(OrcType orcColumn)\n+    {\n+        String icebergId = requireNonNull(orcColumn.getAttributes().get(ORC_ICEBERG_ID_KEY), format(\"ORC column %s doesn't have an associated Iceberg ID\", orcColumn.toString()));\n+        return Integer.parseInt(icebergId);\n+    }\n+\n+    private static Optional<IcebergMinMax> toIcebergMinMax(ColumnStatistics orcColumnStats, org.apache.iceberg.types.Type icebergType)\n+    {\n+        IntegerStatistics integerStatistics = orcColumnStats.getIntegerStatistics();\n+        if (integerStatistics != null) {\n+            Object min = integerStatistics.getMin();\n+            Object max = integerStatistics.getMax();\n+            if (min == null || max == null) {\n+                return Optional.empty();\n+            }\n+            if (icebergType.typeId() == org.apache.iceberg.types.Type.TypeID.INTEGER) {\n+                min = toIntExact((Long) min);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ1MDI3MQ==", "bodyText": "Double", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442450271", "createdAt": "2020-06-18T19:23:17Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {\n+            excludedColumns.add(orcColumnId);\n+        }\n+        OrcType orcColumn = orcColumns.get(orcColumnId);\n+        switch (orcColumn.getOrcTypeKind()) {\n+            case LIST:\n+            case MAP:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, true, excludedColumns);\n+                }\n+                return;\n+            case STRUCT:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, shouldBeExcluded, excludedColumns);\n+                }\n+                return;\n+            default:\n+        }\n+    }\n+\n+    private static int getIcebergId(OrcType orcColumn)\n+    {\n+        String icebergId = requireNonNull(orcColumn.getAttributes().get(ORC_ICEBERG_ID_KEY), format(\"ORC column %s doesn't have an associated Iceberg ID\", orcColumn.toString()));\n+        return Integer.parseInt(icebergId);\n+    }\n+\n+    private static Optional<IcebergMinMax> toIcebergMinMax(ColumnStatistics orcColumnStats, org.apache.iceberg.types.Type icebergType)\n+    {\n+        IntegerStatistics integerStatistics = orcColumnStats.getIntegerStatistics();\n+        if (integerStatistics != null) {\n+            Object min = integerStatistics.getMin();\n+            Object max = integerStatistics.getMax();\n+            if (min == null || max == null) {\n+                return Optional.empty();\n+            }\n+            if (icebergType.typeId() == org.apache.iceberg.types.Type.TypeID.INTEGER) {\n+                min = toIntExact((Long) min);\n+                max = toIntExact((Long) max);\n+            }\n+            return Optional.of(IcebergMinMax.builder(icebergType).setMin(min).setMax(max).build());\n+        }\n+        DoubleStatistics doubleStatistics = orcColumnStats.getDoubleStatistics();\n+        if (doubleStatistics != null) {\n+            Object min = doubleStatistics.getMin();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 193}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ1MTI4NQ==", "bodyText": "I don't think we need a builder for this. All the usages are for all arguments. So this be\nreturn Optional.of(new IcebergMinMax(icebergType, min, max));\nWhich I find more readable here.", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442451285", "createdAt": "2020-06-18T19:25:20Z", "author": {"login": "electrum"}, "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergOrcFileWriter.java", "diffHunk": "@@ -0,0 +1,293 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.slice.Slice;\n+import io.prestosql.orc.OrcDataSink;\n+import io.prestosql.orc.OrcDataSource;\n+import io.prestosql.orc.OrcWriteValidation;\n+import io.prestosql.orc.OrcWriterOptions;\n+import io.prestosql.orc.OrcWriterStats;\n+import io.prestosql.orc.metadata.ColumnMetadata;\n+import io.prestosql.orc.metadata.CompressionKind;\n+import io.prestosql.orc.metadata.OrcColumnId;\n+import io.prestosql.orc.metadata.OrcType;\n+import io.prestosql.orc.metadata.statistics.ColumnStatistics;\n+import io.prestosql.orc.metadata.statistics.DateStatistics;\n+import io.prestosql.orc.metadata.statistics.DecimalStatistics;\n+import io.prestosql.orc.metadata.statistics.DoubleStatistics;\n+import io.prestosql.orc.metadata.statistics.IntegerStatistics;\n+import io.prestosql.orc.metadata.statistics.StringStatistics;\n+import io.prestosql.plugin.hive.orc.OrcFileWriter;\n+import io.prestosql.spi.type.Type;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Conversions;\n+import org.apache.iceberg.types.Types;\n+import org.joda.time.DateTimeZone;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.function.Supplier;\n+\n+import static io.prestosql.orc.metadata.OrcColumnId.ROOT_COLUMN;\n+import static io.prestosql.plugin.iceberg.TypeConverter.ORC_ICEBERG_ID_KEY;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergOrcFileWriter\n+        extends OrcFileWriter\n+        implements IcebergFileWriter\n+{\n+    private final Schema icebergSchema;\n+    private final ColumnMetadata<OrcType> orcColumns;\n+\n+    public IcebergOrcFileWriter(\n+            OrcDataSink orcDataSink,\n+            Callable<Void> rollbackAction,\n+            Schema icebergSchema,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ColumnMetadata<OrcType> fileColumnOrcTypes,\n+            CompressionKind compression,\n+            OrcWriterOptions options,\n+            boolean writeLegacyVersion,\n+            int[] fileInputColumnIndexes,\n+            Map<String, String> metadata,\n+            DateTimeZone hiveStorageTimeZone,\n+            Optional<Supplier<OrcDataSource>> validationInputFactory,\n+            OrcWriteValidation.OrcWriteValidationMode validationMode,\n+            OrcWriterStats stats)\n+    {\n+        super(orcDataSink, rollbackAction, columnNames, fileColumnTypes, fileColumnOrcTypes, compression, options, writeLegacyVersion, fileInputColumnIndexes, metadata, hiveStorageTimeZone, validationInputFactory, validationMode, stats);\n+        this.icebergSchema = requireNonNull(icebergSchema, \"icebergSchema is null\");\n+        orcColumns = fileColumnOrcTypes;\n+    }\n+\n+    @Override\n+    public Optional<Metrics> getMetrics()\n+    {\n+        return Optional.of(computeMetrics(icebergSchema, orcColumns, orcWriter.getFileRowCount(), orcWriter.getFileStats()));\n+    }\n+\n+    private static Metrics computeMetrics(Schema icebergSchema, ColumnMetadata<OrcType> orcColumns, long fileRowCount, Optional<ColumnMetadata<ColumnStatistics>> columnStatistics)\n+    {\n+        if (columnStatistics.isEmpty()) {\n+            return new Metrics(fileRowCount, null, null, null, null, null);\n+        }\n+        // Columns that are descendants of LIST or MAP types are excluded because:\n+        // 1. Their stats are not used by Apache Iceberg to filter out data files\n+        // 2. Their record count can be larger than table-level row count. There's no good way to calculate nullCounts for them.\n+        // See https://github.com/apache/iceberg/pull/199#discussion_r429443627\n+        Set<OrcColumnId> excludedColumns = getExcludedColumns(orcColumns);\n+\n+        ImmutableMap.Builder<Integer, Long> valueCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, Long> nullCountsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> lowerBoundsBuilder = ImmutableMap.builder();\n+        ImmutableMap.Builder<Integer, ByteBuffer> upperBoundsBuilder = ImmutableMap.builder();\n+\n+        // OrcColumnId(0) is the root column that represents file-level schema\n+        for (int i = 1; i < orcColumns.size(); i++) {\n+            OrcColumnId orcColumnId = new OrcColumnId(i);\n+            if (!excludedColumns.contains(orcColumnId)) {\n+                OrcType orcColumn = orcColumns.get(orcColumnId);\n+                ColumnStatistics orcColumnStats = columnStatistics.get().get(orcColumnId);\n+                int icebergId = getIcebergId(orcColumn);\n+                Types.NestedField icebergField = requireNonNull(icebergSchema.findField(icebergId), format(\"Cannot find Iceberg column with ID %d in schema %s\", icebergId, icebergSchema));\n+\n+                valueCountsBuilder.put(icebergId, fileRowCount);\n+                if (orcColumnStats.hasNumberOfValues()) {\n+                    nullCountsBuilder.put(icebergId, fileRowCount - orcColumnStats.getNumberOfValues());\n+                }\n+                Optional<IcebergMinMax> icebergMinMax = toIcebergMinMax(orcColumnStats, icebergField.type());\n+                icebergMinMax.ifPresent(minMax -> {\n+                    lowerBoundsBuilder.put(icebergId, minMax.getMin());\n+                    upperBoundsBuilder.put(icebergId, minMax.getMax());\n+                });\n+            }\n+        }\n+        Map<Integer, Long> valueCounts = valueCountsBuilder.build();\n+        Map<Integer, Long> nullCounts = nullCountsBuilder.build();\n+        Map<Integer, ByteBuffer> lowerBounds = lowerBoundsBuilder.build();\n+        Map<Integer, ByteBuffer> upperBounds = upperBoundsBuilder.build();\n+        // TODO: Add column size accounting to ORC column writers\n+        return new Metrics(fileRowCount,\n+                null,\n+                valueCounts.isEmpty() ? null : valueCounts,\n+                nullCounts.isEmpty() ? null : nullCounts,\n+                lowerBounds.isEmpty() ? null : lowerBounds,\n+                upperBounds.isEmpty() ? null : upperBounds);\n+    }\n+\n+    private static Set<OrcColumnId> getExcludedColumns(ColumnMetadata<OrcType> orcColumns)\n+    {\n+        ImmutableSet.Builder<OrcColumnId> excludedColumns = ImmutableSet.builder();\n+        populateExcludedColumns(orcColumns, ROOT_COLUMN, false, excludedColumns);\n+        return excludedColumns.build();\n+    }\n+\n+    private static void populateExcludedColumns(ColumnMetadata<OrcType> orcColumns, OrcColumnId orcColumnId, boolean shouldBeExcluded, ImmutableSet.Builder<OrcColumnId> excludedColumns)\n+    {\n+        if (shouldBeExcluded) {\n+            excludedColumns.add(orcColumnId);\n+        }\n+        OrcType orcColumn = orcColumns.get(orcColumnId);\n+        switch (orcColumn.getOrcTypeKind()) {\n+            case LIST:\n+            case MAP:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, true, excludedColumns);\n+                }\n+                return;\n+            case STRUCT:\n+                for (OrcColumnId child : orcColumn.getFieldTypeIndexes()) {\n+                    populateExcludedColumns(orcColumns, child, shouldBeExcluded, excludedColumns);\n+                }\n+                return;\n+            default:\n+        }\n+    }\n+\n+    private static int getIcebergId(OrcType orcColumn)\n+    {\n+        String icebergId = requireNonNull(orcColumn.getAttributes().get(ORC_ICEBERG_ID_KEY), format(\"ORC column %s doesn't have an associated Iceberg ID\", orcColumn.toString()));\n+        return Integer.parseInt(icebergId);\n+    }\n+\n+    private static Optional<IcebergMinMax> toIcebergMinMax(ColumnStatistics orcColumnStats, org.apache.iceberg.types.Type icebergType)\n+    {\n+        IntegerStatistics integerStatistics = orcColumnStats.getIntegerStatistics();\n+        if (integerStatistics != null) {\n+            Object min = integerStatistics.getMin();\n+            Object max = integerStatistics.getMax();\n+            if (min == null || max == null) {\n+                return Optional.empty();\n+            }\n+            if (icebergType.typeId() == org.apache.iceberg.types.Type.TypeID.INTEGER) {\n+                min = toIntExact((Long) min);\n+                max = toIntExact((Long) max);\n+            }\n+            return Optional.of(IcebergMinMax.builder(icebergType).setMin(min).setMax(max).build());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ1NTg2Mw==", "bodyText": "Replace numberOfRows in bufferFileFooter() with fileRowCount", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442455863", "createdAt": "2020-06-18T19:33:49Z", "author": {"login": "electrum"}, "path": "presto-orc/src/main/java/io/prestosql/orc/OrcWriter.java", "diffHunk": "@@ -264,6 +269,7 @@ public void write(Page page)\n             }\n \n             writeChunk(chunk);\n+            fileRowCount += chunkRows;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ2MDU0MA==", "bodyText": "Assigning a field inside a lambda is not typical and easy to miss, so it'd be better to write like\nfileStatsRetainedBytes = fileStats.map(stats -> stats.stream()\n        .mapToLong(ColumnStatistics::getRetainedSizeInBytes)\n        .sum()).orElse(0L);", "url": "https://github.com/trinodb/trino/pull/4055#discussion_r442460540", "createdAt": "2020-06-18T19:40:29Z", "author": {"login": "electrum"}, "path": "presto-orc/src/main/java/io/prestosql/orc/OrcWriter.java", "diffHunk": "@@ -471,10 +477,11 @@ public void close()\n                 .mapToLong(stripe -> stripe.getStripeInformation().getNumberOfRows())\n                 .sum();\n \n-        Optional<ColumnMetadata<ColumnStatistics>> fileStats = toFileStats(closedStripes.stream()\n+        fileStats = toFileStats(closedStripes.stream()\n                 .map(ClosedStripe::getStatistics)\n                 .map(StripeStatistics::getColumnStatistics)\n                 .collect(toList()));\n+        fileStats.ifPresent(stats -> fileStatsRetainedBytes = stats.stream().mapToLong(ColumnStatistics::getRetainedSizeInBytes).sum());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "dc1c442a584b675276f9fc5ffe36e13b88ceea09", "author": {"user": {"login": "alexjo2144", "name": "Alexander Jo"}}, "url": "https://github.com/trinodb/trino/commit/dc1c442a584b675276f9fc5ffe36e13b88ceea09", "committedDate": "2020-06-24T15:36:36Z", "message": "Add pushdown for parquet timestamp predicate"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "e68b0e3cd39be0df83830131f65fe4bd8daf3396", "author": {"user": {"login": "lxynov", "name": "Xingyuan Lin"}}, "url": "https://github.com/trinodb/trino/commit/e68b0e3cd39be0df83830131f65fe4bd8daf3396", "committedDate": "2020-06-28T00:21:15Z", "message": "Fix predicating on REAL type in Iceberg Connector\n\nIn Apache Iceberg, a REAL type value is represented as a native Java\nfloat value."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c6e96ce69c57967c4c87b654b4392b62133ea1c0", "author": {"user": {"login": "lxynov", "name": "Xingyuan Lin"}}, "url": "https://github.com/trinodb/trino/commit/c6e96ce69c57967c4c87b654b4392b62133ea1c0", "committedDate": "2020-06-28T00:21:51Z", "message": "Fix predicating on DECIMAL type in Iceberg Connector"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff54e93a773d1d37eee17bd9fea26d95bfa6ed3d", "author": {"user": {"login": "lxynov", "name": "Xingyuan Lin"}}, "url": "https://github.com/trinodb/trino/commit/ff54e93a773d1d37eee17bd9fea26d95bfa6ed3d", "committedDate": "2020-06-28T00:22:04Z", "message": "Support Files table with nested types"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "37abf51498ccad2c3597f0cfaeec07f2cf2432bf", "author": {"user": {"login": "lxynov", "name": "Xingyuan Lin"}}, "url": "https://github.com/trinodb/trino/commit/37abf51498ccad2c3597f0cfaeec07f2cf2432bf", "committedDate": "2020-06-28T00:22:12Z", "message": "Collect file stats directly from ORC writers\n\n - Introduced IcebergFileWriter so that the file stats can be returned\n   to the page sink directly from a file writer\n - Implemented IcebergOrcFileWriter to collect file stats directly from\n   the writer\n\nThis implementation avoids re-opening an ORC file to read its stats\nafter it's written. Furthermore, the introduction of IcebergFileWriter\ncan help the implementation of Iceberg Avro writer, since Avro files\ndon't store stats for the page sink to read from."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "37abf51498ccad2c3597f0cfaeec07f2cf2432bf", "author": {"user": {"login": "lxynov", "name": "Xingyuan Lin"}}, "url": "https://github.com/trinodb/trino/commit/37abf51498ccad2c3597f0cfaeec07f2cf2432bf", "committedDate": "2020-06-28T00:22:12Z", "message": "Collect file stats directly from ORC writers\n\n - Introduced IcebergFileWriter so that the file stats can be returned\n   to the page sink directly from a file writer\n - Implemented IcebergOrcFileWriter to collect file stats directly from\n   the writer\n\nThis implementation avoids re-opening an ORC file to read its stats\nafter it's written. Furthermore, the introduction of IcebergFileWriter\ncan help the implementation of Iceberg Avro writer, since Avro files\ndon't store stats for the page sink to read from."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 401, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}