{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYxOTAyMjQ1", "number": 4671, "title": "Demonstrate Spark/Iceberg and Presto/Iceberg compatibility", "bodyText": "This commit adds a number of tests to ensure that Iceberg data\nwritten by Spark/SQL can be read by the Presto Iceberg connector,\nand vice versa.  Everything worked correctly except DECIMAL, which\nisn't yet supported in Spark/SQL, and TIMESTAMP, because\nIceberg doesn't yet support TIMESTAMP WITH TIME ZONE and Spark\nonly supports TIMESTAMP WITH TIME ZONE.", "createdAt": "2020-08-03T02:08:38Z", "url": "https://github.com/trinodb/trino/pull/4671", "merged": true, "mergeCommit": {"oid": "54343ff3d928d3734f2861b32834b98db7f12f75"}, "closed": true, "closedAt": "2020-08-19T17:31:19Z", "author": {"login": "djsstarburst"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc7JqHvABqjM2MTQxMDUyNjk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdAfD6dAFqTQ3MDY4ODk3OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU5OTU5NDk5", "url": "https://github.com/trinodb/trino/pull/4671#pullrequestreview-459959499", "createdAt": "2020-08-03T11:43:14Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMTo0MzoxNVrOG62Vqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNjowNDo0N1rOG6_dKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM2MDg3NQ==", "bodyText": "I think we should use the original name of your test (TestSparkCompatibility) or something similar.", "url": "https://github.com/trinodb/trino/pull/4671#discussion_r464360875", "createdAt": "2020-08-03T11:43:15Z", "author": {"login": "aalbu"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "diffHunk": "@@ -30,30 +29,177 @@\n public class TestIcebergBasic", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM2MzE2Nw==", "bodyText": "I think the equivalent Presto semantics would be to treat this as a timestamp (without timezone), so we could cast(_timestamp as TIMESTAMP).  In addition, we need non-legacy semantics to preserve wall clock time, so we'd need to execute SET SESSION legacy_timestamp=FALSE.  Though this area is tricky and I don't fully understand it.", "url": "https://github.com/trinodb/trino/pull/4671#discussion_r464363167", "createdAt": "2020-08-03T11:48:36Z", "author": {"login": "aalbu"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "diffHunk": "@@ -30,30 +29,177 @@\n public class TestIcebergBasic\n         extends ProductTest\n {\n+    // TODO: Spark SQL doesn't yet support decimal.  When it does add it to the test.\n+    // TODO: Spark SQL only stores TIMESTAMP WITH TIME ZONE, and Iceberg only supports\n+    // TIMESTAMP with no time zone.  The Spark writes/Presto reads test can pass by\n+    // stripping off the UTC.  However, I haven't been able to get the\n+    // Presto writes/Spark reads test TIMESTAMPs to match.\n+\n     // see spark-defaults.conf\n     private static final String SPARK_CATALOG = \"iceberg_test\";\n     private static final String PRESTO_CATALOG = \"iceberg\";\n-    private static final String TABLE_NAME = \"test_iceberg_basic\";\n-    private static final String SPARK_TABLE_NAME = format(\"%s.default.%s\", SPARK_CATALOG, TABLE_NAME);\n-    private static final String PRESTO_TABLE_NAME = format(\"%s.default.%s\", PRESTO_CATALOG, TABLE_NAME);\n \n-    @BeforeTestWithContext\n-    @AfterTestWithContext\n-    public void dropTestTables()\n+    @Test(groups = {ICEBERG, PROFILE_SPECIFIC_TESTS})\n+    public void testPrestoReadingSparkData()\n+    {\n+        String baseTableName = \"test_presto_reading_primitive_types\";\n+        String sparkTableName = sparkTableName(baseTableName);\n+\n+        String sparkTableDefinition =\n+                \"CREATE TABLE %s (\" +\n+                        \"  _string STRING\" +\n+                        \", _bigint BIGINT\" +\n+                        \", _integer INTEGER\" +\n+                        \", _real REAL\" +\n+                        \", _double DOUBLE\" +\n+                        \", _boolean BOOLEAN\" +\n+                        \", _timestamp TIMESTAMP\" +\n+                        \", _date DATE\" +\n+                        \") USING ICEBERG\";\n+        onSpark().executeQuery(format(sparkTableDefinition, sparkTableName));\n+\n+        String values = \"VALUES (\" +\n+                \"'a_string'\" +\n+                \", 1000000000000000\" +\n+                \", 1000000000\" +\n+                \", 10000000.123\" +\n+                \", 100000000000.123\" +\n+                \", true\" +\n+                \", TIMESTAMP '2020-06-28 14:16:00.456'\" +\n+                \", DATE '1950-06-28'\" +\n+                \")\";\n+        String insert = format(\"INSERT INTO %s %s\", sparkTableName, values);\n+        onSpark().executeQuery(insert);\n+\n+        Row row = row(\n+                \"a_string\",\n+                1000000000000000L,\n+                1000000000,\n+                10000000.123F,\n+                100000000000.123,\n+                true,\n+                \"2020-06-28 14:16:00.456\",\n+                \"1950-06-28\");\n+\n+        String startOfSelect = \"SELECT _string, _bigint, _integer, _real, _double, _boolean\";\n+        QueryResult sparkSelect = onSpark().executeQuery(format(\"%s, CAST(_timestamp AS STRING), CAST(_date AS STRING) FROM %s\", startOfSelect, sparkTableName));\n+        assertThat(sparkSelect).containsOnly(row);\n+\n+        QueryResult prestoSelect = onPresto().executeQuery(format(\"%s, REPLACE(CAST(_timestamp AS VARCHAR), ' UTC'), CAST(_date AS VARCHAR) FROM %s\", startOfSelect, prestoTableName(baseTableName)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUwNDU5MA==", "bodyText": "If we use java.sql types in the expected value, we could drop the casts to STRING.", "url": "https://github.com/trinodb/trino/pull/4671#discussion_r464504590", "createdAt": "2020-08-03T15:55:11Z", "author": {"login": "aalbu"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "diffHunk": "@@ -30,30 +29,177 @@\n public class TestIcebergBasic\n         extends ProductTest\n {\n+    // TODO: Spark SQL doesn't yet support decimal.  When it does add it to the test.\n+    // TODO: Spark SQL only stores TIMESTAMP WITH TIME ZONE, and Iceberg only supports\n+    // TIMESTAMP with no time zone.  The Spark writes/Presto reads test can pass by\n+    // stripping off the UTC.  However, I haven't been able to get the\n+    // Presto writes/Spark reads test TIMESTAMPs to match.\n+\n     // see spark-defaults.conf\n     private static final String SPARK_CATALOG = \"iceberg_test\";\n     private static final String PRESTO_CATALOG = \"iceberg\";\n-    private static final String TABLE_NAME = \"test_iceberg_basic\";\n-    private static final String SPARK_TABLE_NAME = format(\"%s.default.%s\", SPARK_CATALOG, TABLE_NAME);\n-    private static final String PRESTO_TABLE_NAME = format(\"%s.default.%s\", PRESTO_CATALOG, TABLE_NAME);\n \n-    @BeforeTestWithContext\n-    @AfterTestWithContext\n-    public void dropTestTables()\n+    @Test(groups = {ICEBERG, PROFILE_SPECIFIC_TESTS})\n+    public void testPrestoReadingSparkData()\n+    {\n+        String baseTableName = \"test_presto_reading_primitive_types\";\n+        String sparkTableName = sparkTableName(baseTableName);\n+\n+        String sparkTableDefinition =\n+                \"CREATE TABLE %s (\" +\n+                        \"  _string STRING\" +\n+                        \", _bigint BIGINT\" +\n+                        \", _integer INTEGER\" +\n+                        \", _real REAL\" +\n+                        \", _double DOUBLE\" +\n+                        \", _boolean BOOLEAN\" +\n+                        \", _timestamp TIMESTAMP\" +\n+                        \", _date DATE\" +\n+                        \") USING ICEBERG\";\n+        onSpark().executeQuery(format(sparkTableDefinition, sparkTableName));\n+\n+        String values = \"VALUES (\" +\n+                \"'a_string'\" +\n+                \", 1000000000000000\" +\n+                \", 1000000000\" +\n+                \", 10000000.123\" +\n+                \", 100000000000.123\" +\n+                \", true\" +\n+                \", TIMESTAMP '2020-06-28 14:16:00.456'\" +\n+                \", DATE '1950-06-28'\" +\n+                \")\";\n+        String insert = format(\"INSERT INTO %s %s\", sparkTableName, values);\n+        onSpark().executeQuery(insert);\n+\n+        Row row = row(\n+                \"a_string\",\n+                1000000000000000L,\n+                1000000000,\n+                10000000.123F,\n+                100000000000.123,\n+                true,\n+                \"2020-06-28 14:16:00.456\",\n+                \"1950-06-28\");\n+\n+        String startOfSelect = \"SELECT _string, _bigint, _integer, _real, _double, _boolean\";\n+        QueryResult sparkSelect = onSpark().executeQuery(format(\"%s, CAST(_timestamp AS STRING), CAST(_date AS STRING) FROM %s\", startOfSelect, sparkTableName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUxMDI0OQ==", "bodyText": "We could use java.sql types:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            \"2020-06-28 14:16:00.456\",\n          \n          \n            \n                            \"1950-06-28\");\n          \n          \n            \n                            Timestamp.valueOf(\"2020-06-28 14:16:00.456\"),\n          \n          \n            \n                            Date.valueOf(\"1950-06-28\"));", "url": "https://github.com/trinodb/trino/pull/4671#discussion_r464510249", "createdAt": "2020-08-03T16:04:47Z", "author": {"login": "aalbu"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/iceberg/TestIcebergBasic.java", "diffHunk": "@@ -30,30 +29,177 @@\n public class TestIcebergBasic\n         extends ProductTest\n {\n+    // TODO: Spark SQL doesn't yet support decimal.  When it does add it to the test.\n+    // TODO: Spark SQL only stores TIMESTAMP WITH TIME ZONE, and Iceberg only supports\n+    // TIMESTAMP with no time zone.  The Spark writes/Presto reads test can pass by\n+    // stripping off the UTC.  However, I haven't been able to get the\n+    // Presto writes/Spark reads test TIMESTAMPs to match.\n+\n     // see spark-defaults.conf\n     private static final String SPARK_CATALOG = \"iceberg_test\";\n     private static final String PRESTO_CATALOG = \"iceberg\";\n-    private static final String TABLE_NAME = \"test_iceberg_basic\";\n-    private static final String SPARK_TABLE_NAME = format(\"%s.default.%s\", SPARK_CATALOG, TABLE_NAME);\n-    private static final String PRESTO_TABLE_NAME = format(\"%s.default.%s\", PRESTO_CATALOG, TABLE_NAME);\n \n-    @BeforeTestWithContext\n-    @AfterTestWithContext\n-    public void dropTestTables()\n+    @Test(groups = {ICEBERG, PROFILE_SPECIFIC_TESTS})\n+    public void testPrestoReadingSparkData()\n+    {\n+        String baseTableName = \"test_presto_reading_primitive_types\";\n+        String sparkTableName = sparkTableName(baseTableName);\n+\n+        String sparkTableDefinition =\n+                \"CREATE TABLE %s (\" +\n+                        \"  _string STRING\" +\n+                        \", _bigint BIGINT\" +\n+                        \", _integer INTEGER\" +\n+                        \", _real REAL\" +\n+                        \", _double DOUBLE\" +\n+                        \", _boolean BOOLEAN\" +\n+                        \", _timestamp TIMESTAMP\" +\n+                        \", _date DATE\" +\n+                        \") USING ICEBERG\";\n+        onSpark().executeQuery(format(sparkTableDefinition, sparkTableName));\n+\n+        String values = \"VALUES (\" +\n+                \"'a_string'\" +\n+                \", 1000000000000000\" +\n+                \", 1000000000\" +\n+                \", 10000000.123\" +\n+                \", 100000000000.123\" +\n+                \", true\" +\n+                \", TIMESTAMP '2020-06-28 14:16:00.456'\" +\n+                \", DATE '1950-06-28'\" +\n+                \")\";\n+        String insert = format(\"INSERT INTO %s %s\", sparkTableName, values);\n+        onSpark().executeQuery(insert);\n+\n+        Row row = row(\n+                \"a_string\",\n+                1000000000000000L,\n+                1000000000,\n+                10000000.123F,\n+                100000000000.123,\n+                true,\n+                \"2020-06-28 14:16:00.456\",\n+                \"1950-06-28\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 74}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ec089f0ba6c5428d29b9429d947f32f3ccba78a", "author": {"user": {"login": "djsstarburst", "name": "David Stryker"}}, "url": "https://github.com/trinodb/trino/commit/8ec089f0ba6c5428d29b9429d947f32f3ccba78a", "committedDate": "2020-08-17T20:10:53Z", "message": "Demonstrate Spark/Iceberg and Presto/Iceberg compatibility\n\nThis commit adds a number of tests to ensure that Iceberg data\nwritten by Spark/SQL can be read by the Presto Iceberg connector,\nand vice versa.  Everything worked correctly except DECIMAL, which\nisn't yet supported in the Iceberg connector, and TIMESTAMP, because\nIceberg doesn't yet support TIMESTAMP WITH TIME ZONE and Spark\nonly supports TIMESTAMP WITH TIME ZONE."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "8ec089f0ba6c5428d29b9429d947f32f3ccba78a", "author": {"user": {"login": "djsstarburst", "name": "David Stryker"}}, "url": "https://github.com/trinodb/trino/commit/8ec089f0ba6c5428d29b9429d947f32f3ccba78a", "committedDate": "2020-08-17T20:10:53Z", "message": "Demonstrate Spark/Iceberg and Presto/Iceberg compatibility\n\nThis commit adds a number of tests to ensure that Iceberg data\nwritten by Spark/SQL can be read by the Presto Iceberg connector,\nand vice versa.  Everything worked correctly except DECIMAL, which\nisn't yet supported in the Iceberg connector, and TIMESTAMP, because\nIceberg doesn't yet support TIMESTAMP WITH TIME ZONE and Spark\nonly supports TIMESTAMP WITH TIME ZONE."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcwNjg4OTc5", "url": "https://github.com/trinodb/trino/pull/4671#pullrequestreview-470688979", "createdAt": "2020-08-19T17:28:34Z", "commit": {"oid": "8ec089f0ba6c5428d29b9429d947f32f3ccba78a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4606, "cost": 1, "resetAt": "2021-10-28T20:13:43Z"}}}