{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAxNTE1OTgz", "number": 3396, "title": "Add support for dereference pushdown to parquet reader", "bodyText": "Prunes requestedSchema of ParquetReader to only included referenced nested columns. See #3116 for more detail. Note that this patch also requires #2672 to see the performance gain.", "createdAt": "2020-04-09T16:14:05Z", "url": "https://github.com/trinodb/trino/pull/3396", "merged": true, "mergeCommit": {"oid": "237313b11f6e27356b48c014bb9ee53cb1e7af6c"}, "closed": true, "closedAt": "2020-04-22T06:38:56Z", "author": {"login": "JamesRTaylor"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcWTQsngFqTM5MTUxODkzNg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcZ-SBJgBqjMyNTg0OTMzODU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxNTE4OTM2", "url": "https://github.com/trinodb/trino/pull/3396#pullrequestreview-391518936", "createdAt": "2020-04-10T15:58:50Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNTo1ODo1MVrOGD-dLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNTo1ODo1MVrOGD-dLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgyMjE5MA==", "bodyText": "Nit: Optional.ofNullable(type)?", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r406822190", "createdAt": "2020-04-10T15:58:51Z", "author": {"login": "lhofhansl"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -264,6 +278,50 @@ public static ParquetPageSource createParquetPageSource(\n         }\n     }\n \n+    public static Optional<org.apache.parquet.schema.Type> getParquetType(GroupType groupType, boolean useParquetColumnNames, HiveColumnHandle column)\n+    {\n+        org.apache.parquet.schema.Type type = null;\n+        if (useParquetColumnNames) {\n+            type = getParquetTypeByName(column.getBaseColumnName(), groupType);\n+        }\n+        else if (column.getBaseHiveColumnIndex() < groupType.getFieldCount()) {\n+            type = groupType.getType(column.getBaseHiveColumnIndex());\n+        }\n+\n+        if (type == null) {\n+            return Optional.empty();\n+        }\n+        return Optional.of(type);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 166}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxNjg3NTcw", "url": "https://github.com/trinodb/trino/pull/3396#pullrequestreview-391687570", "createdAt": "2020-04-10T22:14:32Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQyMjoxNDozMlrOGEHMCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQyMjoyNzoxMlrOGEHYGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2NTI1OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    .map(projection -> projection.getReaderColumns())\n          \n          \n            \n                                    .map(ReaderProjections::getReaderColumns)", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r406965258", "createdAt": "2020-04-10T22:14:32Z", "author": {"login": "martint"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -131,31 +132,60 @@ public ParquetPageSourceFactory(HdfsEnvironment hdfsEnvironment, FileFormatDataS\n \n         // Ignore predicates on partial columns for now.\n         effectivePredicate = effectivePredicate.transform(column -> column.isBaseColumn() ? column : null);\n+        boolean useParquetColumnNames = isUseParquetColumnNames(session);\n \n-        Optional<ReaderProjections> projectedReaderColumns = projectBaseColumns(columns);\n-\n-        ConnectorPageSource parquetPageSource = createParquetPageSource(\n+        ParquetReader parquetReader = createParquetReader(\n                 hdfsEnvironment,\n                 session.getUser(),\n                 configuration,\n                 path,\n                 start,\n                 length,\n                 fileSize,\n-                projectedReaderColumns\n-                        .map(ReaderProjections::getReaderColumns)\n+                projectSufficientColumns(columns) // TODO: method that returns only list of columns\n+                        .map(projection -> projection.getReaderColumns())", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2NTM2Mg==", "bodyText": "Not sure I understand what the TODO is saying", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r406965362", "createdAt": "2020-04-10T22:14:59Z", "author": {"login": "martint"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -131,31 +132,60 @@ public ParquetPageSourceFactory(HdfsEnvironment hdfsEnvironment, FileFormatDataS\n \n         // Ignore predicates on partial columns for now.\n         effectivePredicate = effectivePredicate.transform(column -> column.isBaseColumn() ? column : null);\n+        boolean useParquetColumnNames = isUseParquetColumnNames(session);\n \n-        Optional<ReaderProjections> projectedReaderColumns = projectBaseColumns(columns);\n-\n-        ConnectorPageSource parquetPageSource = createParquetPageSource(\n+        ParquetReader parquetReader = createParquetReader(\n                 hdfsEnvironment,\n                 session.getUser(),\n                 configuration,\n                 path,\n                 start,\n                 length,\n                 fileSize,\n-                projectedReaderColumns\n-                        .map(ReaderProjections::getReaderColumns)\n+                projectSufficientColumns(columns) // TODO: method that returns only list of columns", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2NTU1OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            projectedReaderColumns.map(projection -> projection.getReaderColumns()).orElse(columns),\n          \n          \n            \n                            projectedReaderColumns.map(ReaderProjections::getReaderColumns).orElse(columns),", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r406965559", "createdAt": "2020-04-10T22:15:43Z", "author": {"login": "martint"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -131,31 +132,60 @@ public ParquetPageSourceFactory(HdfsEnvironment hdfsEnvironment, FileFormatDataS\n \n         // Ignore predicates on partial columns for now.\n         effectivePredicate = effectivePredicate.transform(column -> column.isBaseColumn() ? column : null);\n+        boolean useParquetColumnNames = isUseParquetColumnNames(session);\n \n-        Optional<ReaderProjections> projectedReaderColumns = projectBaseColumns(columns);\n-\n-        ConnectorPageSource parquetPageSource = createParquetPageSource(\n+        ParquetReader parquetReader = createParquetReader(\n                 hdfsEnvironment,\n                 session.getUser(),\n                 configuration,\n                 path,\n                 start,\n                 length,\n                 fileSize,\n-                projectedReaderColumns\n-                        .map(ReaderProjections::getReaderColumns)\n+                projectSufficientColumns(columns) // TODO: method that returns only list of columns\n+                        .map(projection -> projection.getReaderColumns())\n                         .orElse(columns),\n-                isUseParquetColumnNames(session),\n+                useParquetColumnNames,\n                 options\n                         .withFailOnCorruptedStatistics(isFailOnCorruptedParquetStatistics(session))\n                         .withMaxReadBlockSize(getParquetMaxReadBlockSize(session)),\n                 effectivePredicate,\n                 stats);\n \n+        Optional<ReaderProjections> projectedReaderColumns = projectBaseColumns(columns);\n+        ConnectorPageSource parquetPageSource = createPageSource(\n+                parquetReader,\n+                projectedReaderColumns.map(projection -> projection.getReaderColumns()).orElse(columns),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2NzI5NA==", "bodyText": "This is no longer used", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r406967294", "createdAt": "2020-04-10T22:23:04Z", "author": {"login": "martint"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -215,28 +244,13 @@ public static ParquetPageSource createParquetPageSource(\n             }\n             MessageColumnIO messageColumnIO = getColumnIO(fileSchema, requestedSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2ODM0Nw==", "bodyText": "These are unnecessary. They are there only so that createPageSource can get them out of the reader. If, instead, you inlined the createPageSource method or structured it differently, you could use them directly in the caller.", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r406968347", "createdAt": "2020-04-10T22:27:12Z", "author": {"login": "martint"}, "path": "presto-parquet/src/main/java/io/prestosql/parquet/reader/ParquetReader.java", "diffHunk": "@@ -122,6 +129,16 @@ public ParquetReader(\n         chunkReaders = dataSource.planRead(ranges);\n     }\n \n+    public MessageType getFileSchema()\n+    {\n+        return fileSchema;\n+    }\n+\n+    public MessageColumnIO getMessageColumn()\n+    {\n+        return this.messageColumn;\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 72}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3NDUyNzU1", "url": "https://github.com/trinodb/trino/pull/3396#pullrequestreview-397452755", "createdAt": "2020-04-21T15:41:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxNTo0MTo0MVrOGJMT8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxNzowODoyOFrOGJPATQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjI5MjA4Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        type = getParquetTypeByName(column.getBaseColumnName(), groupType);\n          \n          \n            \n                        return Optional.of(getParquetTypeByName(column.getBaseColumnName(), groupType));", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r412292082", "createdAt": "2020-04-21T15:41:41Z", "author": {"login": "martint"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -262,6 +209,74 @@ public static ParquetPageSource createParquetPageSource(\n             }\n             throw new PrestoException(HIVE_CANNOT_OPEN_SPLIT, message, e);\n         }\n+\n+        Optional<ReaderProjections> readerProjections = projectBaseColumns(columns);\n+        List<HiveColumnHandle> baseColumns = readerProjections.map(ReaderProjections::getReaderColumns).orElse(columns);\n+        for (HiveColumnHandle column : baseColumns) {\n+            checkArgument(column.getColumnType() == REGULAR, \"column type must be REGULAR: %s\", column);\n+        }\n+\n+        List<Optional<org.apache.parquet.schema.Type>> parquetFields = baseColumns.stream()\n+                .map(column -> getParquetType(column, fileSchema, useParquetColumnNames))\n+                .map(Optional::ofNullable)\n+                .collect(toImmutableList());\n+        ImmutableList.Builder<Type> prestoTypes = ImmutableList.builder();\n+        ImmutableList.Builder<Optional<Field>> internalFields = ImmutableList.builder();\n+        for (int columnIndex = 0; columnIndex < baseColumns.size(); columnIndex++) {\n+            HiveColumnHandle column = baseColumns.get(columnIndex);\n+            Optional<org.apache.parquet.schema.Type> parquetField = parquetFields.get(columnIndex);\n+\n+            prestoTypes.add(column.getBaseType());\n+\n+            internalFields.add(parquetField.flatMap(field -> {\n+                String columnName = useParquetColumnNames ? column.getBaseColumnName() : fileSchema.getFields().get(column.getBaseHiveColumnIndex()).getName();\n+                return constructField(column.getBaseType(), lookupColumnByName(messageColumn, columnName));\n+            }));\n+        }\n+\n+        ConnectorPageSource parquetPageSource = new ParquetPageSource(parquetReader, prestoTypes.build(), internalFields.build());\n+        return Optional.of(new ReaderPageSourceWithProjections(parquetPageSource, readerProjections));\n+    }\n+\n+    public static Optional<org.apache.parquet.schema.Type> getParquetType(GroupType groupType, boolean useParquetColumnNames, HiveColumnHandle column)\n+    {\n+        org.apache.parquet.schema.Type type = null;\n+        if (useParquetColumnNames) {\n+            type = getParquetTypeByName(column.getBaseColumnName(), groupType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 193}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjMzNTU2NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        type = groupType.getType(column.getBaseHiveColumnIndex());\n          \n          \n            \n                        Optional.of(groupType.getType(column.getBaseHiveColumnIndex()));", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r412335564", "createdAt": "2020-04-21T17:07:35Z", "author": {"login": "martint"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -262,6 +209,74 @@ public static ParquetPageSource createParquetPageSource(\n             }\n             throw new PrestoException(HIVE_CANNOT_OPEN_SPLIT, message, e);\n         }\n+\n+        Optional<ReaderProjections> readerProjections = projectBaseColumns(columns);\n+        List<HiveColumnHandle> baseColumns = readerProjections.map(ReaderProjections::getReaderColumns).orElse(columns);\n+        for (HiveColumnHandle column : baseColumns) {\n+            checkArgument(column.getColumnType() == REGULAR, \"column type must be REGULAR: %s\", column);\n+        }\n+\n+        List<Optional<org.apache.parquet.schema.Type>> parquetFields = baseColumns.stream()\n+                .map(column -> getParquetType(column, fileSchema, useParquetColumnNames))\n+                .map(Optional::ofNullable)\n+                .collect(toImmutableList());\n+        ImmutableList.Builder<Type> prestoTypes = ImmutableList.builder();\n+        ImmutableList.Builder<Optional<Field>> internalFields = ImmutableList.builder();\n+        for (int columnIndex = 0; columnIndex < baseColumns.size(); columnIndex++) {\n+            HiveColumnHandle column = baseColumns.get(columnIndex);\n+            Optional<org.apache.parquet.schema.Type> parquetField = parquetFields.get(columnIndex);\n+\n+            prestoTypes.add(column.getBaseType());\n+\n+            internalFields.add(parquetField.flatMap(field -> {\n+                String columnName = useParquetColumnNames ? column.getBaseColumnName() : fileSchema.getFields().get(column.getBaseHiveColumnIndex()).getName();\n+                return constructField(column.getBaseType(), lookupColumnByName(messageColumn, columnName));\n+            }));\n+        }\n+\n+        ConnectorPageSource parquetPageSource = new ParquetPageSource(parquetReader, prestoTypes.build(), internalFields.build());\n+        return Optional.of(new ReaderPageSourceWithProjections(parquetPageSource, readerProjections));\n+    }\n+\n+    public static Optional<org.apache.parquet.schema.Type> getParquetType(GroupType groupType, boolean useParquetColumnNames, HiveColumnHandle column)\n+    {\n+        org.apache.parquet.schema.Type type = null;\n+        if (useParquetColumnNames) {\n+            type = getParquetTypeByName(column.getBaseColumnName(), groupType);\n+        }\n+        else if (column.getBaseHiveColumnIndex() < groupType.getFieldCount()) {\n+            type = groupType.getType(column.getBaseHiveColumnIndex());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 196}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjMzNTc4Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return Optional.ofNullable(type);\n          \n          \n            \n                    return Optional.empty();", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r412335782", "createdAt": "2020-04-21T17:07:53Z", "author": {"login": "martint"}, "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/parquet/ParquetPageSourceFactory.java", "diffHunk": "@@ -262,6 +209,74 @@ public static ParquetPageSource createParquetPageSource(\n             }\n             throw new PrestoException(HIVE_CANNOT_OPEN_SPLIT, message, e);\n         }\n+\n+        Optional<ReaderProjections> readerProjections = projectBaseColumns(columns);\n+        List<HiveColumnHandle> baseColumns = readerProjections.map(ReaderProjections::getReaderColumns).orElse(columns);\n+        for (HiveColumnHandle column : baseColumns) {\n+            checkArgument(column.getColumnType() == REGULAR, \"column type must be REGULAR: %s\", column);\n+        }\n+\n+        List<Optional<org.apache.parquet.schema.Type>> parquetFields = baseColumns.stream()\n+                .map(column -> getParquetType(column, fileSchema, useParquetColumnNames))\n+                .map(Optional::ofNullable)\n+                .collect(toImmutableList());\n+        ImmutableList.Builder<Type> prestoTypes = ImmutableList.builder();\n+        ImmutableList.Builder<Optional<Field>> internalFields = ImmutableList.builder();\n+        for (int columnIndex = 0; columnIndex < baseColumns.size(); columnIndex++) {\n+            HiveColumnHandle column = baseColumns.get(columnIndex);\n+            Optional<org.apache.parquet.schema.Type> parquetField = parquetFields.get(columnIndex);\n+\n+            prestoTypes.add(column.getBaseType());\n+\n+            internalFields.add(parquetField.flatMap(field -> {\n+                String columnName = useParquetColumnNames ? column.getBaseColumnName() : fileSchema.getFields().get(column.getBaseHiveColumnIndex()).getName();\n+                return constructField(column.getBaseType(), lookupColumnByName(messageColumn, columnName));\n+            }));\n+        }\n+\n+        ConnectorPageSource parquetPageSource = new ParquetPageSource(parquetReader, prestoTypes.build(), internalFields.build());\n+        return Optional.of(new ReaderPageSourceWithProjections(parquetPageSource, readerProjections));\n+    }\n+\n+    public static Optional<org.apache.parquet.schema.Type> getParquetType(GroupType groupType, boolean useParquetColumnNames, HiveColumnHandle column)\n+    {\n+        org.apache.parquet.schema.Type type = null;\n+        if (useParquetColumnNames) {\n+            type = getParquetTypeByName(column.getBaseColumnName(), groupType);\n+        }\n+        else if (column.getBaseHiveColumnIndex() < groupType.getFieldCount()) {\n+            type = groupType.getType(column.getBaseHiveColumnIndex());\n+        }\n+\n+        return Optional.ofNullable(type);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjMzNjIwNQ==", "bodyText": "Why is this commented out?", "url": "https://github.com/trinodb/trino/pull/3396#discussion_r412336205", "createdAt": "2020-04-21T17:08:28Z", "author": {"login": "martint"}, "path": "presto-hive/src/test/java/io/prestosql/plugin/hive/TestHiveIntegrationSmokeTest.java", "diffHunk": "@@ -6303,11 +6321,11 @@ private void testSelectWithNoColumns(Session session, HiveStorageFormat storageF\n     public void testColumnPruning()\n     {\n         Session session = Session.builder(getSession())\n-                .setCatalogSessionProperty(\"hive\", \"orc_use_column_names\", \"true\")\n-                .setCatalogSessionProperty(\"hive\", \"parquet_use_column_names\", \"true\")\n+                .setCatalogSessionProperty(catalog, \"orc_use_column_names\", \"true\")\n+                .setCatalogSessionProperty(catalog, \"parquet_use_column_names\", \"true\")\n                 .build();\n \n-        testWithStorageFormat(new TestingHiveStorageFormat(session, HiveStorageFormat.ORC), this::testColumnPruning);\n+        //testWithStorageFormat(new TestingHiveStorageFormat(session, HiveStorageFormat.ORC), this::testColumnPruning);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "6e272ed5f865e50dc8b1675c191fe67f49b09dfe", "author": {"user": {"login": "JamesRTaylor", "name": "James Taylor"}}, "url": "https://github.com/trinodb/trino/commit/6e272ed5f865e50dc8b1675c191fe67f49b09dfe", "committedDate": "2020-04-22T01:47:27Z", "message": "Add support for dereference pushdown to parquet reader"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "6e272ed5f865e50dc8b1675c191fe67f49b09dfe", "author": {"user": {"login": "JamesRTaylor", "name": "James Taylor"}}, "url": "https://github.com/trinodb/trino/commit/6e272ed5f865e50dc8b1675c191fe67f49b09dfe", "committedDate": "2020-04-22T01:47:27Z", "message": "Add support for dereference pushdown to parquet reader"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1869, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}