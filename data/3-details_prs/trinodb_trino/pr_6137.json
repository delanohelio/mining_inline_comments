{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI5MDg3Njkz", "number": 6137, "title": "Confluent schema registry", "bodyText": "Supersedes: #2499", "createdAt": "2020-11-29T10:21:31Z", "url": "https://github.com/trinodb/trino/pull/6137", "merged": true, "mergeCommit": {"oid": "bbda25e8f6fa41274bac64f2f8b483aec5f75608"}, "closed": true, "closedAt": "2020-12-18T21:54:12Z", "author": {"login": "elonazoulay"}, "timelineItems": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdhOYCqABqjQwNDg5MDA3Mzg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdnd9tgABqjQxMzEzMjI3ODE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba", "author": {"user": {"login": "elonazoulay", "name": "Elon Azoulay"}}, "url": "https://github.com/trinodb/trino/commit/6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba", "committedDate": "2020-11-29T10:41:27Z", "message": "Add ConfluentSchemaRegistryTableDescriptionSupplier"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwNjM4MDg1", "url": "https://github.com/trinodb/trino/pull/6137#pullrequestreview-540638085", "createdAt": "2020-11-30T07:00:08Z", "commit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwNzowMDowOVrOH7t_hQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwNzoxMjoxNVrOH7uPWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4MTU3Mw==", "bodyText": "Can we have something like a SessionPropertyProvider ? So we can inject a Set<SessionPropertyProvider> and finally flatten when we create the Connector ? Set<List<PropertyMetadata>> looks a bit verbose ?\n@kokosing your insights ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532381573", "createdAt": "2020-11-30T07:00:09Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSessionProperties.java", "diffHunk": "@@ -20,19 +20,24 @@\n import javax.inject.Inject;\n \n import java.util.List;\n+import java.util.Set;\n \n public final class KafkaSessionProperties\n {\n     private static final String TIMESTAMP_UPPER_BOUND_FORCE_PUSH_DOWN_ENABLED = \"timestamp_upper_bound_force_push_down_enabled\";\n     private final List<PropertyMetadata<?>> sessionProperties;\n \n     @Inject\n-    public KafkaSessionProperties(KafkaConfig kafkaConfig)\n+    public KafkaSessionProperties(KafkaConfig kafkaConfig, @ForKafka Set<List<PropertyMetadata<?>>> extraProperties)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4MjEyMw==", "bodyText": "Can we have it as a Properties is subject is used only in SchemaRegistry or ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532382123", "createdAt": "2020-11-30T07:01:57Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaTableHandle.java", "diffHunk": "@@ -64,6 +66,8 @@ public KafkaTableHandle(\n             @JsonProperty(\"messageDataFormat\") String messageDataFormat,\n             @JsonProperty(\"keyDataSchemaLocation\") Optional<String> keyDataSchemaLocation,\n             @JsonProperty(\"messageDataSchemaLocation\") Optional<String> messageDataSchemaLocation,\n+            @JsonProperty(\"keySubject\") Optional<String> keySubject,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4MjU2Ng==", "bodyText": "Can we could them as a Map<String, Object> ? So in future other related properties can be passed without modifying these classes", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532382566", "createdAt": "2020-11-30T07:03:30Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaTopicFieldGroup.java", "diffHunk": "@@ -30,16 +30,19 @@\n {\n     private final String dataFormat;\n     private final Optional<String> dataSchema;\n+    private final Optional<String> subject;\n     private final List<KafkaTopicFieldDescription> fields;\n \n     @JsonCreator\n     public KafkaTopicFieldGroup(\n             @JsonProperty(\"dataFormat\") String dataFormat,\n             @JsonProperty(\"dataSchema\") Optional<String> dataSchema,\n+            @JsonProperty(\"decoderParams\") Optional<String> subject,\n             @JsonProperty(\"fields\") List<KafkaTopicFieldDescription> fields)\n     {\n         this.dataFormat = requireNonNull(dataFormat, \"dataFormat is null\");\n         this.dataSchema = requireNonNull(dataSchema, \"dataSchema is null\");\n+        this.subject = requireNonNull(subject, \"subject is null\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4Mjg0Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public interface ContentSchemaReader\n          \n          \n            \n            public interface SchemaReader", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532382843", "createdAt": "2020-11-30T07:04:22Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/ContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,27 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema;\n+\n+import io.prestosql.plugin.kafka.KafkaTableHandle;\n+\n+import java.util.Optional;\n+\n+public interface ContentSchemaReader", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4NDEyNQ==", "bodyText": "Any min/max restriction for it ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532384125", "createdAt": "2020-11-30T07:08:09Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryConfig.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import io.airlift.configuration.Config;\n+import io.airlift.configuration.ConfigDescription;\n+import io.airlift.units.Duration;\n+import io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy;\n+\n+import javax.validation.constraints.NotNull;\n+\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.IGNORE;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+public class ConfluentSchemaRegistryConfig\n+{\n+    private String confluentSchemaRegistryUrl;\n+    private int confluentSchemaRegistryClientCacheSize = 1000;\n+    private EmptyFieldStrategy emptyFieldStrategy = IGNORE;\n+    private Duration confluentSubjectsCacheRefreshInterval = new Duration(1, SECONDS);\n+\n+    @NotNull\n+    public String getConfluentSchemaRegistryUrl()\n+    {\n+        return confluentSchemaRegistryUrl;\n+    }\n+\n+    @Config(\"kafka.confluent-schema-registry-url\")\n+    @ConfigDescription(\"The url of the Confluent Schema Registry\")\n+    public ConfluentSchemaRegistryConfig setConfluentSchemaRegistryUrl(String confluentSchemaRegistryUrl)\n+    {\n+        this.confluentSchemaRegistryUrl = confluentSchemaRegistryUrl;\n+        return this;\n+    }\n+\n+    public int getConfluentSchemaRegistryClientCacheSize()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4NTA4MQ==", "bodyText": "What if we could pass the KafkaTableHandle to the underlying implementation ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532385081", "createdAt": "2020-11-30T07:10:54Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/AbstractContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema;\n+\n+import io.prestosql.plugin.kafka.KafkaTableHandle;\n+\n+import java.util.Optional;\n+\n+public abstract class AbstractContentSchemaReader\n+        implements ContentSchemaReader\n+{\n+    @Override\n+    public Optional<String> readKeyContentSchema(KafkaTableHandle tableHandle)\n+    {\n+        return readSchema(tableHandle.getKeyDataSchemaLocation(), tableHandle.getKeySubject());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4NTYyNQ==", "bodyText": "Do we really need this ForKafka ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532385625", "createdAt": "2020-11-30T07:12:15Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentModule.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.inject.Binder;\n+import com.google.inject.Provides;\n+import com.google.inject.Scopes;\n+import com.google.inject.TypeLiteral;\n+import io.airlift.configuration.AbstractConfigurationAwareModule;\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.RowDecoderFactory;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroDeserializer;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.ForKafka;\n+import io.prestosql.plugin.kafka.schema.ContentSchemaReader;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.session.PropertyMetadata;\n+\n+import javax.inject.Singleton;\n+\n+import java.util.List;\n+\n+import static com.google.inject.multibindings.MapBinder.newMapBinder;\n+import static com.google.inject.multibindings.Multibinder.newSetBinder;\n+import static io.airlift.configuration.ConfigBinder.configBinder;\n+\n+public class ConfluentModule\n+        extends AbstractConfigurationAwareModule\n+{\n+    @Override\n+    protected void setup(Binder binder)\n+    {\n+        configBinder(binder).bindConfig(ConfluentSchemaRegistryConfig.class);\n+        binder.bind(AvroReaderSupplier.Factory.class).to(ConfluentAvroReaderSupplier.Factory.class).in(Scopes.SINGLETON);\n+        binder.bind(AvroDeserializer.Factory.class).to(AvroBytesDeserializer.Factory.class).in(Scopes.SINGLETON);\n+        binder.bind(ContentSchemaReader.class).to(AvroConfluentContentSchemaReader.class).in(Scopes.SINGLETON);\n+        newSetBinder(binder, new TypeLiteral<List<PropertyMetadata<?>>>() {}, ForKafka.class).addBinding().toProvider(ConfluentSessionProperties.class).in(Scopes.SINGLETON);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwODkyNDk3", "url": "https://github.com/trinodb/trino/pull/6137#pullrequestreview-540892497", "createdAt": "2020-11-30T13:10:36Z", "commit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "state": "COMMENTED", "comments": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxMzoxMDozNlrOH76ZzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMjo1OTo0MlrOH8reNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjU4NDkwOQ==", "bodyText": "I don't see this used.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532584909", "createdAt": "2020-11-30T13:10:36Z", "author": {"login": "kokosing"}, "path": "pom.xml", "diffHunk": "@@ -1311,6 +1347,12 @@\n                 <artifactId>snappy-java</artifactId>\n                 <version>1.1.7.3</version>\n             </dependency>\n+\n+            <dependency>\n+                <groupId>org.yaml</groupId>\n+                <artifactId>snakeyaml</artifactId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjU4NzA2OA==", "bodyText": "combine?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532587068", "createdAt": "2020-11-30T13:14:18Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -166,8 +168,9 @@ private static DistributedQueryRunner createKafkaQueryRunner(\n                     installModuleIf(\n                             KafkaConfig.class,\n                             kafkaConfig -> kafkaConfig.getTableDescriptionSupplier().equalsIgnoreCase(TEST),\n-                            binder -> binder.bind(TableDescriptionSupplier.class)\n-                                    .toInstance(new MapBasedTableDescriptionSupplier(topicDescriptions)))));\n+                            installModules(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjU5MzE0NQ==", "bodyText": "See JdbcConnector to see how it can be implemented.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r532593145", "createdAt": "2020-11-30T13:24:19Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSessionProperties.java", "diffHunk": "@@ -20,19 +20,24 @@\n import javax.inject.Inject;\n \n import java.util.List;\n+import java.util.Set;\n \n public final class KafkaSessionProperties\n {\n     private static final String TIMESTAMP_UPPER_BOUND_FORCE_PUSH_DOWN_ENABLED = \"timestamp_upper_bound_force_push_down_enabled\";\n     private final List<PropertyMetadata<?>> sessionProperties;\n \n     @Inject\n-    public KafkaSessionProperties(KafkaConfig kafkaConfig)\n+    public KafkaSessionProperties(KafkaConfig kafkaConfig, @ForKafka Set<List<PropertyMetadata<?>>> extraProperties)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4MTU3Mw=="}, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMTc0Ng==", "bodyText": "Is it possible to find a confluent version that would match kafka version that we are using to avoid doing exclusions. Possibly we could update current kafka version if needed.\nCan you please share details of how it fails without exclussions?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533331746", "createdAt": "2020-12-01T11:18:10Z", "author": {"login": "kokosing"}, "path": "pom.xml", "diffHunk": "@@ -1070,6 +1078,34 @@\n                 <version>4.5.1</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-avro-serializer</artifactId>\n+                <version>${dep.confluent.version}</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMjAzMg==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533332032", "createdAt": "2020-12-01T11:18:41Z", "author": {"login": "kokosing"}, "path": "presto-testing-kafka/src/main/java/io/prestosql/testing/kafka/TestingKafkaWithSchemaRegistry.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.testing.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.io.Closer;\n+import io.confluent.kafka.serializers.KafkaAvroSerializer;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.testcontainers.containers.GenericContainer;\n+import org.testcontainers.containers.Network;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class TestingKafkaWithSchemaRegistry\n+        implements Closeable\n+{\n+    public static final int SCHEMA_REGISTRY_PORT = 8081;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMjE1Nw==", "bodyText": "it is not used", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533332157", "createdAt": "2020-12-01T11:18:56Z", "author": {"login": "kokosing"}, "path": "presto-testing-kafka/src/main/java/io/prestosql/testing/kafka/TestingKafkaWithSchemaRegistry.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.testing.kafka;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.io.Closer;\n+import io.confluent.kafka.serializers.KafkaAvroSerializer;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.testcontainers.containers.GenericContainer;\n+import org.testcontainers.containers.Network;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class TestingKafkaWithSchemaRegistry\n+        implements Closeable\n+{\n+    public static final int SCHEMA_REGISTRY_PORT = 8081;\n+    private final TestingKafka testingKafka;\n+    private final GenericContainer<?> schemaRegistryContainer;\n+\n+    @SuppressWarnings(\"resource\")\n+    private final Closer closer = Closer.create();\n+\n+    public TestingKafkaWithSchemaRegistry(TestingKafka testingKafka)\n+    {\n+        this.testingKafka = requireNonNull(testingKafka, \"testingKafka is null\");\n+        schemaRegistryContainer = new GenericContainer<>(\"confluentinc/cp-schema-registry:5.4.1\")\n+                .withNetwork(Network.SHARED)\n+                .withEnv(\"SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS\", \"PLAINTEXT://kafka:9092\")\n+                .withEnv(\"SCHEMA_REGISTRY_HOST_NAME\", \"0.0.0.0\")\n+                .withEnv(\"SCHEMA_REGISTRY_LISTENERS\", format(\"http://0.0.0.0:%s\", SCHEMA_REGISTRY_PORT))\n+                .withExposedPorts(SCHEMA_REGISTRY_PORT);\n+        closer.register(testingKafka);\n+        closer.register(schemaRegistryContainer::stop);\n+    }\n+\n+    public void start()\n+    {\n+        testingKafka.start();\n+        try {\n+            schemaRegistryContainer.start();\n+        }\n+        catch (Throwable e) {\n+            testingKafka.close();\n+            throw e;\n+        }\n+    }\n+\n+    @Override\n+    public void close()\n+            throws IOException\n+    {\n+        closer.close();\n+    }\n+\n+    public void createTopic(String topic)\n+    {\n+        testingKafka.createTopic(topic);\n+    }\n+\n+    public String getConnectString()\n+    {\n+        return testingKafka.getConnectString();\n+    }\n+\n+    public String getSchemaRegistryConnectString()\n+    {\n+        return \"http://\" + schemaRegistryContainer.getContainerIpAddress() + \":\" + schemaRegistryContainer.getMappedPort(SCHEMA_REGISTRY_PORT);\n+    }\n+\n+    public KafkaProducer<Long, Object> createProducer()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMjY4Ng==", "bodyText": "Same comment as in previous commit.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533332686", "createdAt": "2020-12-01T11:19:49Z", "author": {"login": "kokosing"}, "path": "pom.xml", "diffHunk": "@@ -1106,6 +1106,22 @@\n                 </exclusions>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-schema-registry-client</artifactId>\n+                <version>${dep.confluent.version}</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzNjU5MQ==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533336591", "createdAt": "2020-12-01T11:26:43Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentAvroReaderSupplier.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.io.DatumReader;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentAvroReaderSupplier<T>\n+        implements AvroReaderSupplier<T>\n+{\n+    private final Schema targetSchema;\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final LoadingCache<Integer, GenericDatumReader<T>> avroRecordReaderCache;\n+\n+    public ConfluentAvroReaderSupplier(Schema targetSchema, SchemaRegistryClient schemaRegistryClient)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzNzMxMw==", "bodyText": "Can sourceSchema be null? Maybe use requireNonNull?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533337313", "createdAt": "2020-12-01T11:27:59Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentAvroReaderSupplier.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.io.DatumReader;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentAvroReaderSupplier<T>\n+        implements AvroReaderSupplier<T>\n+{\n+    private final Schema targetSchema;\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final LoadingCache<Integer, GenericDatumReader<T>> avroRecordReaderCache;\n+\n+    public ConfluentAvroReaderSupplier(Schema targetSchema, SchemaRegistryClient schemaRegistryClient)\n+    {\n+        this.targetSchema = requireNonNull(targetSchema, \"targetSchema is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        avroRecordReaderCache = CacheBuilder.newBuilder()\n+                .build(CacheLoader.from(this::lookupReader));\n+    }\n+\n+    private GenericDatumReader<T> lookupReader(int id)\n+    {\n+        try {\n+            Schema sourceSchema = schemaRegistryClient.getById(id);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzODgxNw==", "bodyText": "So schema id is going to change every time it is changed (like new field added)? And so we can indefinitely cache all schemas?\nI think it would make sense to make it possible to configure maximum size of this cache. Maybe confluentConfig.getConfluentSchemaRegistryClientCacheSize()? WDYT?\nAlso do you need this caching at all if you are using CachedSchemaRegistryClient?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533338817", "createdAt": "2020-12-01T11:30:30Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentAvroReaderSupplier.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.spi.PrestoException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.io.DatumReader;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentAvroReaderSupplier<T>\n+        implements AvroReaderSupplier<T>\n+{\n+    private final Schema targetSchema;\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final LoadingCache<Integer, GenericDatumReader<T>> avroRecordReaderCache;\n+\n+    public ConfluentAvroReaderSupplier(Schema targetSchema, SchemaRegistryClient schemaRegistryClient)\n+    {\n+        this.targetSchema = requireNonNull(targetSchema, \"targetSchema is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        avroRecordReaderCache = CacheBuilder.newBuilder()\n+                .build(CacheLoader.from(this::lookupReader));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM0ODU4Nw==", "bodyText": "Why do you need it? I would prefer to avoid hacking guice if it is not needed. Best if we would be able to reuse production guice binding, by using actual schema registry.\nIf it is really needed, then please pass this module as parameter to createKafkaQueryRunner so it is clear what tests need that, and we could allow tests to modify kafka connector according to their needs. That way we would avoid hacking all things together when kafkaConfig.getTableDescriptionSupplier().equalsIgnoreCase(TEST).\nIn other words can we simply use io.prestosql.plugin.kafka.KafkaQueryRunner.Builder#setExtension for that module?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533348587", "createdAt": "2020-12-01T11:47:51Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -166,8 +168,9 @@ private static DistributedQueryRunner createKafkaQueryRunner(\n                     installModuleIf(\n                             KafkaConfig.class,\n                             kafkaConfig -> kafkaConfig.getTableDescriptionSupplier().equalsIgnoreCase(TEST),\n-                            binder -> binder.bind(TableDescriptionSupplier.class)\n-                                    .toInstance(new MapBasedTableDescriptionSupplier(topicDescriptions)))));\n+                            installModules(\n+                                    binder -> binder.bind(TableDescriptionSupplier.class).toInstance(new MapBasedTableDescriptionSupplier(topicDescriptions)),\n+                                    new AvroDecoderModule()))));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1MDM5MQ==", "bodyText": "use simple concatenation. format makes it just obscure. Same goes to below places like that:\n        testRow(rowDecoder, generateRecord(initialSchema, Arrays.asList(3, format(\"string-%s\", 3))), 1);\n        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(4, format(\"string-%s\", 4), 4L)), 2);\n        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(5, format(\"string-%s\", 5), null)), 2);", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533350391", "createdAt": "2020-12-01T11:51:20Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .name(\"col3\").type().optional().longType()\n+                .endRecord();\n+\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), initialSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1MTQ0MA==", "bodyText": "How does noDefault works, if there is no value then is it going to return null or fail? Can you please add few tests with org.apache.avro.SchemaBuilder.FieldDefault#usingDefault being used?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533351440", "createdAt": "2020-12-01T11:53:12Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1MjgzOA==", "bodyText": "Can you please tests how non optional fields work with null values:", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533352838", "createdAt": "2020-12-01T11:55:28Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .name(\"col3\").type().optional().longType()\n+                .endRecord();\n+\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), initialSchema);\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), evolvedSchema);\n+\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", INTEGER, \"col1\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col2\", VARCHAR, \"col2\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, evolvedSchema);\n+        testRow(rowDecoder, generateRecord(initialSchema, Arrays.asList(3, format(\"string-%s\", 3))), 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1MzEyNw==", "bodyText": "Can you please test empty string?\nCan you please test UTF8 characters?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533353127", "createdAt": "2020-12-01T11:55:59Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .name(\"col3\").type().optional().longType()\n+                .endRecord();\n+\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), initialSchema);\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), evolvedSchema);\n+\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", INTEGER, \"col1\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col2\", VARCHAR, \"col2\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, evolvedSchema);\n+        testRow(rowDecoder, generateRecord(initialSchema, Arrays.asList(3, format(\"string-%s\", 3))), 1);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(4, format(\"string-%s\", 4), 4L)), 2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1NDAwOQ==", "bodyText": "remove final", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533354009", "createdAt": "2020-12-01T11:57:26Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .name(\"col3\").type().optional().longType()\n+                .endRecord();\n+\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), initialSchema);\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), evolvedSchema);\n+\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", INTEGER, \"col1\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col2\", VARCHAR, \"col2\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, evolvedSchema);\n+        testRow(rowDecoder, generateRecord(initialSchema, Arrays.asList(3, format(\"string-%s\", 3))), 1);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(4, format(\"string-%s\", 4), 4L)), 2);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(5, format(\"string-%s\", 5), null)), 2);\n+    }\n+\n+    @Test\n+    public void testSingleValueRow()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema schema = Schema.create(Schema.Type.LONG);\n+        mockSchemaRegistryClient.register(format(\"%s-key\", TOPIC), schema);\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, schema);\n+        testSingleValueRow(rowDecoder, 3L, schema, 1);\n+    }\n+\n+    private void testRow(RowDecoder rowDecoder, GenericRecord record, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(record, record.getSchema(), schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        assertRowsAreEqual(decodedRow, record);\n+    }\n+\n+    private void testSingleValueRow(RowDecoder rowDecoder, Object value, Schema schema, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(value, schema, schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        checkState(decodedRow.isPresent(), \"decodedRow is not present\");\n+        Map.Entry<DecoderColumnHandle, FieldValueProvider> entry = getOnlyElement(decodedRow.get().entrySet());\n+        assertValuesAreEqual(entry.getValue(), value, schema.getType());\n+    }\n+\n+    private byte[] serializeRecord(Object record, Schema schema, int schemaId)\n+    {\n+        try {\n+            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n+            outputStream.write(0);\n+            outputStream.write(ByteBuffer.allocate(4).putInt(schemaId).array());\n+            BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(outputStream, null);\n+            GenericDatumWriter<Object> avroRecordWriter = new GenericDatumWriter<>(schema);\n+            avroRecordWriter.write(record, encoder);\n+            encoder.flush();\n+            byte[] serializedRecord = outputStream.toByteArray();\n+            outputStream.close();\n+            return serializedRecord;\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private RowDecoder getRowDecoder(SchemaRegistryClient schemaRegistryClient, Set<DecoderColumnHandle> columnHandles, Schema schema)\n+    {\n+        ImmutableMap<String, String> decoderParams = ImmutableMap.<String, String>builder()\n+                .put(DATA_SCHEMA, schema.toString())\n+                .build();\n+        return getAvroRowDecoderyFactory(schemaRegistryClient).create(decoderParams, columnHandles);\n+    }\n+\n+    public static final AvroRowDecoderFactory getAvroRowDecoderyFactory(SchemaRegistryClient schemaRegistryClient)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1NDIxNQ==", "bodyText": "static", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533354215", "createdAt": "2020-12-01T11:57:48Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .name(\"col3\").type().optional().longType()\n+                .endRecord();\n+\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), initialSchema);\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), evolvedSchema);\n+\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", INTEGER, \"col1\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col2\", VARCHAR, \"col2\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, evolvedSchema);\n+        testRow(rowDecoder, generateRecord(initialSchema, Arrays.asList(3, format(\"string-%s\", 3))), 1);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(4, format(\"string-%s\", 4), 4L)), 2);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(5, format(\"string-%s\", 5), null)), 2);\n+    }\n+\n+    @Test\n+    public void testSingleValueRow()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema schema = Schema.create(Schema.Type.LONG);\n+        mockSchemaRegistryClient.register(format(\"%s-key\", TOPIC), schema);\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, schema);\n+        testSingleValueRow(rowDecoder, 3L, schema, 1);\n+    }\n+\n+    private void testRow(RowDecoder rowDecoder, GenericRecord record, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(record, record.getSchema(), schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        assertRowsAreEqual(decodedRow, record);\n+    }\n+\n+    private void testSingleValueRow(RowDecoder rowDecoder, Object value, Schema schema, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(value, schema, schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        checkState(decodedRow.isPresent(), \"decodedRow is not present\");\n+        Map.Entry<DecoderColumnHandle, FieldValueProvider> entry = getOnlyElement(decodedRow.get().entrySet());\n+        assertValuesAreEqual(entry.getValue(), value, schema.getType());\n+    }\n+\n+    private byte[] serializeRecord(Object record, Schema schema, int schemaId)\n+    {\n+        try {\n+            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n+            outputStream.write(0);\n+            outputStream.write(ByteBuffer.allocate(4).putInt(schemaId).array());\n+            BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(outputStream, null);\n+            GenericDatumWriter<Object> avroRecordWriter = new GenericDatumWriter<>(schema);\n+            avroRecordWriter.write(record, encoder);\n+            encoder.flush();\n+            byte[] serializedRecord = outputStream.toByteArray();\n+            outputStream.close();\n+            return serializedRecord;\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private RowDecoder getRowDecoder(SchemaRegistryClient schemaRegistryClient, Set<DecoderColumnHandle> columnHandles, Schema schema)\n+    {\n+        ImmutableMap<String, String> decoderParams = ImmutableMap.<String, String>builder()\n+                .put(DATA_SCHEMA, schema.toString())\n+                .build();\n+        return getAvroRowDecoderyFactory(schemaRegistryClient).create(decoderParams, columnHandles);\n+    }\n+\n+    public static final AvroRowDecoderFactory getAvroRowDecoderyFactory(SchemaRegistryClient schemaRegistryClient)\n+    {\n+        return new AvroRowDecoderFactory(new ConfluentAvroReaderSupplier.Factory(schemaRegistryClient), new AvroBytesDeserializer.Factory());\n+    }\n+\n+    private void assertRowsAreEqual(Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow, GenericRecord expected)\n+    {\n+        checkState(decodedRow.isPresent(), \"decoded row is not present\");\n+        for (Map.Entry<DecoderColumnHandle, FieldValueProvider> entry : decodedRow.get().entrySet()) {\n+            String columnName = entry.getKey().getName();\n+            assertValuesAreEqual(entry.getValue(), expected.get(columnName), expected.getSchema().getField(columnName).schema().getType());\n+        }\n+    }\n+\n+    private void assertValuesAreEqual(FieldValueProvider actual, Object expected, Schema.Type avroType)\n+    {\n+        if (actual.isNull()) {\n+            assertNull(expected);\n+        }\n+        else {\n+            switch (avroType) {\n+                case INT:\n+                case LONG:\n+                    assertEquals(actual.getLong(), ((Number) expected).longValue());\n+                    break;\n+                case STRING:\n+                    assertEquals(actual.getSlice().toStringUtf8(), expected);\n+                    break;\n+                default:\n+                    throw new IllegalStateException(\"Unexpected type\");\n+            }\n+        }\n+    }\n+\n+    private GenericRecord generateRecord(Schema schema, List<Object> values)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1NDI1MA==", "bodyText": "static", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533354250", "createdAt": "2020-12-01T11:57:53Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .name(\"col3\").type().optional().longType()\n+                .endRecord();\n+\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), initialSchema);\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), evolvedSchema);\n+\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", INTEGER, \"col1\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col2\", VARCHAR, \"col2\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, evolvedSchema);\n+        testRow(rowDecoder, generateRecord(initialSchema, Arrays.asList(3, format(\"string-%s\", 3))), 1);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(4, format(\"string-%s\", 4), 4L)), 2);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(5, format(\"string-%s\", 5), null)), 2);\n+    }\n+\n+    @Test\n+    public void testSingleValueRow()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema schema = Schema.create(Schema.Type.LONG);\n+        mockSchemaRegistryClient.register(format(\"%s-key\", TOPIC), schema);\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, schema);\n+        testSingleValueRow(rowDecoder, 3L, schema, 1);\n+    }\n+\n+    private void testRow(RowDecoder rowDecoder, GenericRecord record, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(record, record.getSchema(), schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        assertRowsAreEqual(decodedRow, record);\n+    }\n+\n+    private void testSingleValueRow(RowDecoder rowDecoder, Object value, Schema schema, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(value, schema, schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        checkState(decodedRow.isPresent(), \"decodedRow is not present\");\n+        Map.Entry<DecoderColumnHandle, FieldValueProvider> entry = getOnlyElement(decodedRow.get().entrySet());\n+        assertValuesAreEqual(entry.getValue(), value, schema.getType());\n+    }\n+\n+    private byte[] serializeRecord(Object record, Schema schema, int schemaId)\n+    {\n+        try {\n+            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n+            outputStream.write(0);\n+            outputStream.write(ByteBuffer.allocate(4).putInt(schemaId).array());\n+            BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(outputStream, null);\n+            GenericDatumWriter<Object> avroRecordWriter = new GenericDatumWriter<>(schema);\n+            avroRecordWriter.write(record, encoder);\n+            encoder.flush();\n+            byte[] serializedRecord = outputStream.toByteArray();\n+            outputStream.close();\n+            return serializedRecord;\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private RowDecoder getRowDecoder(SchemaRegistryClient schemaRegistryClient, Set<DecoderColumnHandle> columnHandles, Schema schema)\n+    {\n+        ImmutableMap<String, String> decoderParams = ImmutableMap.<String, String>builder()\n+                .put(DATA_SCHEMA, schema.toString())\n+                .build();\n+        return getAvroRowDecoderyFactory(schemaRegistryClient).create(decoderParams, columnHandles);\n+    }\n+\n+    public static final AvroRowDecoderFactory getAvroRowDecoderyFactory(SchemaRegistryClient schemaRegistryClient)\n+    {\n+        return new AvroRowDecoderFactory(new ConfluentAvroReaderSupplier.Factory(schemaRegistryClient), new AvroBytesDeserializer.Factory());\n+    }\n+\n+    private void assertRowsAreEqual(Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow, GenericRecord expected)\n+    {\n+        checkState(decodedRow.isPresent(), \"decoded row is not present\");\n+        for (Map.Entry<DecoderColumnHandle, FieldValueProvider> entry : decodedRow.get().entrySet()) {\n+            String columnName = entry.getKey().getName();\n+            assertValuesAreEqual(entry.getValue(), expected.get(columnName), expected.getSchema().getField(columnName).schema().getType());\n+        }\n+    }\n+\n+    private void assertValuesAreEqual(FieldValueProvider actual, Object expected, Schema.Type avroType)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 163}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1NDM2Ng==", "bodyText": "static", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533354366", "createdAt": "2020-12-01T11:58:05Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentRowDecoder.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderColumnHandle;\n+import io.prestosql.decoder.FieldValueProvider;\n+import io.prestosql.decoder.RowDecoder;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaColumnHandle;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.EncoderFactory;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.decoder.avro.AvroRowDecoderFactory.DATA_SCHEMA;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNull;\n+\n+public class TestAvroConfluentRowDecoder\n+{\n+    private static final String TOPIC = \"test\";\n+\n+    @Test\n+    public void testDecodingRows()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema initialSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .endRecord();\n+\n+        Schema evolvedSchema = SchemaBuilder.record(TOPIC)\n+                .fields()\n+                .name(\"col1\").type().intType().noDefault()\n+                .name(\"col2\").type().stringType().noDefault()\n+                .name(\"col3\").type().optional().longType()\n+                .endRecord();\n+\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), initialSchema);\n+        mockSchemaRegistryClient.register(format(\"%s-value\", TOPIC), evolvedSchema);\n+\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", INTEGER, \"col1\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col2\", VARCHAR, \"col2\", null, null, false, false, false))\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, evolvedSchema);\n+        testRow(rowDecoder, generateRecord(initialSchema, Arrays.asList(3, format(\"string-%s\", 3))), 1);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(4, format(\"string-%s\", 4), 4L)), 2);\n+        testRow(rowDecoder, generateRecord(evolvedSchema, Arrays.asList(5, format(\"string-%s\", 5), null)), 2);\n+    }\n+\n+    @Test\n+    public void testSingleValueRow()\n+            throws Exception\n+    {\n+        MockSchemaRegistryClient mockSchemaRegistryClient = new MockSchemaRegistryClient();\n+        Schema schema = Schema.create(Schema.Type.LONG);\n+        mockSchemaRegistryClient.register(format(\"%s-key\", TOPIC), schema);\n+        Set<DecoderColumnHandle> columnHandles = ImmutableSet.<DecoderColumnHandle>builder()\n+                .add(new KafkaColumnHandle(\"col1\", BIGINT, \"col1\", null, null, false, false, false))\n+                .build();\n+        RowDecoder rowDecoder = getRowDecoder(mockSchemaRegistryClient, columnHandles, schema);\n+        testSingleValueRow(rowDecoder, 3L, schema, 1);\n+    }\n+\n+    private void testRow(RowDecoder rowDecoder, GenericRecord record, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(record, record.getSchema(), schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        assertRowsAreEqual(decodedRow, record);\n+    }\n+\n+    private void testSingleValueRow(RowDecoder rowDecoder, Object value, Schema schema, int schemaId)\n+    {\n+        byte[] serializedRecord = serializeRecord(value, schema, schemaId);\n+        Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow = rowDecoder.decodeRow(serializedRecord, null);\n+        checkState(decodedRow.isPresent(), \"decodedRow is not present\");\n+        Map.Entry<DecoderColumnHandle, FieldValueProvider> entry = getOnlyElement(decodedRow.get().entrySet());\n+        assertValuesAreEqual(entry.getValue(), value, schema.getType());\n+    }\n+\n+    private byte[] serializeRecord(Object record, Schema schema, int schemaId)\n+    {\n+        try {\n+            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n+            outputStream.write(0);\n+            outputStream.write(ByteBuffer.allocate(4).putInt(schemaId).array());\n+            BinaryEncoder encoder = EncoderFactory.get().directBinaryEncoder(outputStream, null);\n+            GenericDatumWriter<Object> avroRecordWriter = new GenericDatumWriter<>(schema);\n+            avroRecordWriter.write(record, encoder);\n+            encoder.flush();\n+            byte[] serializedRecord = outputStream.toByteArray();\n+            outputStream.close();\n+            return serializedRecord;\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private RowDecoder getRowDecoder(SchemaRegistryClient schemaRegistryClient, Set<DecoderColumnHandle> columnHandles, Schema schema)\n+    {\n+        ImmutableMap<String, String> decoderParams = ImmutableMap.<String, String>builder()\n+                .put(DATA_SCHEMA, schema.toString())\n+                .build();\n+        return getAvroRowDecoderyFactory(schemaRegistryClient).create(decoderParams, columnHandles);\n+    }\n+\n+    public static final AvroRowDecoderFactory getAvroRowDecoderyFactory(SchemaRegistryClient schemaRegistryClient)\n+    {\n+        return new AvroRowDecoderFactory(new ConfluentAvroReaderSupplier.Factory(schemaRegistryClient), new AvroBytesDeserializer.Factory());\n+    }\n+\n+    private void assertRowsAreEqual(Optional<Map<DecoderColumnHandle, FieldValueProvider>> decodedRow, GenericRecord expected)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM1NTAxNg==", "bodyText": "Extracting AvroDecoderModule should go in separate commit. If you like you can do this as separate pull request so we can merge it sooner.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533355016", "createdAt": "2020-12-01T11:59:12Z", "author": {"login": "kokosing"}, "path": "presto-kinesis/src/main/java/io/prestosql/plugin/kinesis/KinesisModule.java", "diffHunk": "@@ -54,6 +55,7 @@ public void configure(Binder binder)\n         jsonCodecBinder(binder).bindJsonCodec(KinesisStreamDescription.class);\n \n         binder.install(new DecoderModule());\n+        binder.install(new AvroDecoderModule());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4MDAxMw==", "bodyText": "It would be nice to separate refactor of this class to separate commit, so it is known which class it was turned into.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533380013", "createdAt": "2020-12-01T12:44:06Z", "author": {"login": "kokosing"}, "path": "presto-record-decoder/src/main/java/io/prestosql/decoder/avro/AvroRowDecoder.java", "diffHunk": "@@ -1,95 +0,0 @@\n-/*\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package io.prestosql.decoder.avro;\n-\n-import io.prestosql.decoder.DecoderColumnHandle;\n-import io.prestosql.decoder.FieldValueProvider;\n-import io.prestosql.decoder.RowDecoder;\n-import io.prestosql.spi.PrestoException;\n-import org.apache.avro.file.DataFileStream;\n-import org.apache.avro.generic.GenericRecord;\n-import org.apache.avro.io.DatumReader;\n-\n-import java.io.ByteArrayInputStream;\n-import java.io.IOException;\n-import java.util.Map;\n-import java.util.Optional;\n-import java.util.Set;\n-\n-import static com.google.common.base.Functions.identity;\n-import static com.google.common.collect.ImmutableMap.toImmutableMap;\n-import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n-import static java.util.Objects.requireNonNull;\n-\n-public class AvroRowDecoder", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54aec219cefe53bd3f30df08be43cb2c5330098c"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4MTAxMQ==", "bodyText": "What is FIXED?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533381011", "createdAt": "2020-12-01T12:45:53Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/AvroSchemaConverter.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.spi.type.ArrayType;\n+import io.prestosql.spi.type.BigintType;\n+import io.prestosql.spi.type.BooleanType;\n+import io.prestosql.spi.type.DoubleType;\n+import io.prestosql.spi.type.IntegerType;\n+import io.prestosql.spi.type.MapType;\n+import io.prestosql.spi.type.RealType;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import io.prestosql.spi.type.VarbinaryType;\n+import io.prestosql.spi.type.VarcharType;\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+import static org.apache.avro.Schema.Type.ARRAY;\n+import static org.apache.avro.Schema.Type.BYTES;\n+import static org.apache.avro.Schema.Type.DOUBLE;\n+import static org.apache.avro.Schema.Type.ENUM;\n+import static org.apache.avro.Schema.Type.FIXED;\n+import static org.apache.avro.Schema.Type.FLOAT;\n+import static org.apache.avro.Schema.Type.INT;\n+import static org.apache.avro.Schema.Type.LONG;\n+import static org.apache.avro.Schema.Type.MAP;\n+import static org.apache.avro.Schema.Type.NULL;\n+import static org.apache.avro.Schema.Type.RECORD;\n+import static org.apache.avro.Schema.Type.STRING;\n+import static org.apache.avro.Schema.Type.UNION;\n+\n+public class AvroSchemaConverter\n+{\n+    public static final String DUMMY_FIELD_NAME = \"dummy\";\n+\n+    public enum EmptyFieldStrategy\n+    {\n+        IGNORE,\n+        ADD_DUMMY,\n+        FAIL,\n+    }\n+\n+    private static final Set<Schema.Type> INTEGRAL_TYPES = ImmutableSet.of(INT, LONG);\n+    private static final Set<Schema.Type> DECIMAL_TYPES = ImmutableSet.of(FLOAT, DOUBLE);\n+    private static final Set<Schema.Type> STRING_TYPES = ImmutableSet.of(STRING, ENUM);\n+    private static final Set<Schema.Type> BINARY_TYPES = ImmutableSet.of(BYTES, FIXED);\n+\n+    private final TypeManager typeManager;\n+    private final EmptyFieldStrategy emptyFieldStrategy;\n+\n+    public AvroSchemaConverter(TypeManager typeManager, EmptyFieldStrategy emptyFieldStrategy)\n+    {\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.emptyFieldStrategy = requireNonNull(emptyFieldStrategy, \"emptyFieldStrategy is null\");\n+    }\n+\n+    public List<Type> convertAvroSchema(Schema schema)\n+    {\n+        requireNonNull(schema, \"schema is null\");\n+        List<Type> types;\n+        if (schema.getType().equals(RECORD)) {\n+            types = convertRecordSchema(schema);\n+        }\n+        else {\n+            types = convertSimpleSchema(schema);\n+        }\n+        checkState(!types.isEmpty(), \"Schema has no valid fields: '%s'\", schema);\n+        return types;\n+    }\n+\n+    private List<Type> convertRecordSchema(Schema schema)\n+    {\n+        checkState(schema.getType().equals(RECORD), \"schema is not an avro record\");\n+        ImmutableList.Builder<Type> builder = ImmutableList.builder();\n+        for (Field field : schema.getFields()) {\n+            convert(field.schema()).ifPresent(builder::add);\n+        }\n+        return builder.build();\n+    }\n+\n+    private List<Type> convertSimpleSchema(Schema schema)\n+    {\n+        checkState(!schema.getType().equals(RECORD), \"Unexpected type for simple schema, cannot be a record\");\n+        return convert(schema).stream()\n+                .collect(toImmutableList());\n+    }\n+\n+    private Optional<Type> convert(Schema schema)\n+    {\n+        switch (schema.getType()) {\n+            case INT:\n+                return Optional.of(IntegerType.INTEGER);\n+            case LONG:\n+                return Optional.of(BigintType.BIGINT);\n+            case BOOLEAN:\n+                return Optional.of(BooleanType.BOOLEAN);\n+            case FLOAT:\n+                return Optional.of(RealType.REAL);\n+            case DOUBLE:\n+                return Optional.of(DoubleType.DOUBLE);\n+            case ENUM:\n+            case STRING:\n+                return Optional.of(VarcharType.VARCHAR);\n+            case BYTES:\n+            case FIXED:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "157a72680de83c8673c229ef17e6ac271d9600a1"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4MjIzNg==", "bodyText": "can you please add tests for usingDefault as well?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533382236", "createdAt": "2020-12-01T12:47:56Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroSchemaConverter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.type.ArrayType;\n+import io.prestosql.spi.type.MapType;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.TestingTypeManager;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.DUMMY_FIELD_NAME;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.ADD_DUMMY;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.FAIL;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.IGNORE;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestAvroSchemaConverter\n+{\n+    private static final String RECORD_NAME = \"test\";\n+\n+    private static final TypeManager TYPE_MANAGER = new TestingTypeManager();\n+\n+    @Test\n+    public void testConvertSchema()\n+    {\n+        Schema schema = SchemaBuilder.record(RECORD_NAME)\n+                .fields()\n+                .name(\"bool_col\").type().booleanType().noDefault()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "157a72680de83c8673c229ef17e6ac271d9600a1"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4Mjc0Nw==", "bodyText": "Instead of testing all fields at once, maybe we could have a parametric tests where each test could verify single type?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533382747", "createdAt": "2020-12-01T12:48:50Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroSchemaConverter.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.spi.type.ArrayType;\n+import io.prestosql.spi.type.MapType;\n+import io.prestosql.spi.type.RowType;\n+import io.prestosql.spi.type.TestingTypeManager;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.DUMMY_FIELD_NAME;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.ADD_DUMMY;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.FAIL;\n+import static io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy.IGNORE;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestAvroSchemaConverter\n+{\n+    private static final String RECORD_NAME = \"test\";\n+\n+    private static final TypeManager TYPE_MANAGER = new TestingTypeManager();\n+\n+    @Test\n+    public void testConvertSchema()\n+    {\n+        Schema schema = SchemaBuilder.record(RECORD_NAME)\n+                .fields()\n+                .name(\"bool_col\").type().booleanType().noDefault()\n+                .name(\"int_col\").type().intType().noDefault()\n+                .name(\"long_col\").type().longType().noDefault()\n+                .name(\"float_col\").type().floatType().noDefault()\n+                .name(\"double_col\").type().doubleType().noDefault()\n+                .name(\"string_col\").type().stringType().noDefault()\n+                .name(\"enum_col\").type().enumeration(\"colors\").symbols(\"blue\", \"red\", \"yellow\").noDefault()\n+                .name(\"bytes_col\").type().bytesType().noDefault()\n+                .name(\"fixed_col\").type().fixed(\"fixed\").size(5).noDefault()\n+                .name(\"union_col\").type().unionOf().nullType().and().floatType().and().doubleType().endUnion().noDefault()\n+                .name(\"union_col2\").type().unionOf().nullType().and().intType().and().longType().endUnion().noDefault()\n+                .name(\"union_col3\").type().unionOf().nullType().and().bytesType().and().type(\"fixed\").endUnion().noDefault()\n+                .name(\"union_col4\").type().unionOf().nullType().and().type(\"colors\").and().stringType().endUnion().noDefault()\n+                .name(\"list_col\").type().array().items().intType().noDefault()\n+                .name(\"map_col\").type().map().values().intType().noDefault()\n+                .name(\"record_col\").type().record(\"record_col\")\n+                .fields()\n+                .name(\"nested_list\").type().array().items().map().values().stringType().noDefault()\n+                .name(\"nested_map\").type().map().values().array().items().stringType().noDefault()\n+                .endRecord()\n+                .noDefault()\n+                .endRecord();\n+        AvroSchemaConverter avroSchemaConverter = new AvroSchemaConverter(new TestingTypeManager(), IGNORE);\n+        List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+        List<Type> expected = ImmutableList.<Type>builder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "157a72680de83c8673c229ef17e6ac271d9600a1"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NDcwMQ==", "bodyText": "I don't like using Map<String, Object>, it is easy to lose a control what it stored there. Let's try to avoid pythonization of java ;)\nTo me it is ok how it is now.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533384701", "createdAt": "2020-12-01T12:52:14Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaTopicFieldGroup.java", "diffHunk": "@@ -30,16 +30,19 @@\n {\n     private final String dataFormat;\n     private final Optional<String> dataSchema;\n+    private final Optional<String> subject;\n     private final List<KafkaTopicFieldDescription> fields;\n \n     @JsonCreator\n     public KafkaTopicFieldGroup(\n             @JsonProperty(\"dataFormat\") String dataFormat,\n             @JsonProperty(\"dataSchema\") Optional<String> dataSchema,\n+            @JsonProperty(\"decoderParams\") Optional<String> subject,\n             @JsonProperty(\"fields\") List<KafkaTopicFieldDescription> fields)\n     {\n         this.dataFormat = requireNonNull(dataFormat, \"dataFormat is null\");\n         this.dataSchema = requireNonNull(dataSchema, \"dataSchema is null\");\n+        this.subject = requireNonNull(subject, \"subject is null\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4MjU2Ng=="}, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NTMyOQ==", "bodyText": "I am not sure if I understand, but if you mean Map<String, Object> by term Properties, then:\n\nI don't like using Map<String, Object>, it is easy to lose a control what it stored there. Let's try to avoid pythonization of java ;)", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533385329", "createdAt": "2020-12-01T12:53:27Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaTableHandle.java", "diffHunk": "@@ -64,6 +66,8 @@ public KafkaTableHandle(\n             @JsonProperty(\"messageDataFormat\") String messageDataFormat,\n             @JsonProperty(\"keyDataSchemaLocation\") Optional<String> keyDataSchemaLocation,\n             @JsonProperty(\"messageDataSchemaLocation\") Optional<String> messageDataSchemaLocation,\n+            @JsonProperty(\"keySubject\") Optional<String> keySubject,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4MjEyMw=="}, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NjI4MA==", "bodyText": "final", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533386280", "createdAt": "2020-12-01T12:55:09Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/AbstractContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema;\n+\n+import io.prestosql.plugin.kafka.KafkaTableHandle;\n+\n+import java.util.Optional;\n+\n+public abstract class AbstractContentSchemaReader\n+        implements ContentSchemaReader\n+{\n+    @Override\n+    public Optional<String> readKeyContentSchema(KafkaTableHandle tableHandle)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NjMzMA==", "bodyText": "final", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533386330", "createdAt": "2020-12-01T12:55:14Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/AbstractContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema;\n+\n+import io.prestosql.plugin.kafka.KafkaTableHandle;\n+\n+import java.util.Optional;\n+\n+public abstract class AbstractContentSchemaReader\n+        implements ContentSchemaReader\n+{\n+    @Override\n+    public Optional<String> readKeyContentSchema(KafkaTableHandle tableHandle)\n+    {\n+        return readSchema(tableHandle.getKeyDataSchemaLocation(), tableHandle.getKeySubject());\n+    }\n+\n+    @Override\n+    public Optional<String> readValueContentSchema(KafkaTableHandle tableHandle)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NjU3MQ==", "bodyText": "it is not used", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533386571", "createdAt": "2020-12-01T12:55:41Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/ContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,27 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema;\n+\n+import io.prestosql.plugin.kafka.KafkaTableHandle;\n+\n+import java.util.Optional;\n+\n+public interface ContentSchemaReader\n+{\n+    String SCHEMA_READER = \"schemaReader\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4Njk4NQ==", "bodyText": "So what is going to happen then? Why not to fail? Do you have tests coverage for that?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533386985", "createdAt": "2020-12-01T12:56:24Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/AvroConfluentContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.plugin.kafka.schema.AbstractContentSchemaReader;\n+import io.prestosql.spi.PrestoException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroConfluentContentSchemaReader\n+        extends AbstractContentSchemaReader\n+{\n+    private final SchemaRegistryClient schemaRegistryClient;\n+\n+    @Inject\n+    public AvroConfluentContentSchemaReader(SchemaRegistryClient schemaRegistryClient)\n+    {\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+    }\n+\n+    @Override\n+    protected Optional<String> readSchema(Optional<String> dataSchemaLocation, Optional<String> subject)\n+    {\n+        if (subject.isPresent()) {\n+            try {\n+                return Optional.of(schemaRegistryClient.getLatestSchemaMetadata(subject.get()).getSchema());\n+            }\n+            catch (IOException | RestClientException e) {\n+                throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Could not resolve schema for the '%s' subject\", subject.get()), e);\n+            }\n+        }\n+        return Optional.empty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NzA4OA==", "bodyText": "invert condition", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533387088", "createdAt": "2020-12-01T12:56:35Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/AvroConfluentContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.plugin.kafka.schema.AbstractContentSchemaReader;\n+import io.prestosql.spi.PrestoException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroConfluentContentSchemaReader\n+        extends AbstractContentSchemaReader\n+{\n+    private final SchemaRegistryClient schemaRegistryClient;\n+\n+    @Inject\n+    public AvroConfluentContentSchemaReader(SchemaRegistryClient schemaRegistryClient)\n+    {\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+    }\n+\n+    @Override\n+    protected Optional<String> readSchema(Optional<String> dataSchemaLocation, Optional<String> subject)\n+    {\n+        if (subject.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NzU4OA==", "bodyText": "what if dataSchemaLocation is set? Should we ignore it? Should we fails as this is unexpected?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533387588", "createdAt": "2020-12-01T12:57:26Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/AvroConfluentContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.plugin.kafka.schema.AbstractContentSchemaReader;\n+import io.prestosql.spi.PrestoException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroConfluentContentSchemaReader\n+        extends AbstractContentSchemaReader\n+{\n+    private final SchemaRegistryClient schemaRegistryClient;\n+\n+    @Inject\n+    public AvroConfluentContentSchemaReader(SchemaRegistryClient schemaRegistryClient)\n+    {\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+    }\n+\n+    @Override\n+    protected Optional<String> readSchema(Optional<String> dataSchemaLocation, Optional<String> subject)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4NzgxOQ==", "bodyText": "verify that subject is not set?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533387819", "createdAt": "2020-12-01T12:57:52Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/file/FileContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.file;\n+\n+import com.google.common.io.CharStreams;\n+import io.prestosql.plugin.kafka.schema.AbstractContentSchemaReader;\n+import io.prestosql.spi.PrestoException;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.Optional;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.util.Locale.ENGLISH;\n+\n+public class FileContentSchemaReader\n+        extends AbstractContentSchemaReader\n+{\n+    @Override\n+    protected Optional<String> readSchema(Optional<String> dataSchemaLocation, Optional<String> subject)\n+    {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4Nzk3NA==", "bodyText": "do we have tests for that case?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533387974", "createdAt": "2020-12-01T12:58:08Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/file/FileContentSchemaReader.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.file;\n+\n+import com.google.common.io.CharStreams;\n+import io.prestosql.plugin.kafka.schema.AbstractContentSchemaReader;\n+import io.prestosql.spi.PrestoException;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URL;\n+import java.util.Optional;\n+\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.util.Locale.ENGLISH;\n+\n+public class FileContentSchemaReader\n+        extends AbstractContentSchemaReader\n+{\n+    @Override\n+    protected Optional<String> readSchema(Optional<String> dataSchemaLocation, Optional<String> subject)\n+    {\n+        if (!dataSchemaLocation.isPresent()) {\n+            return Optional.empty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM4ODg1Mg==", "bodyText": "update test name to TestAvroConfluentContentSchemaReader", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r533388852", "createdAt": "2020-12-01T12:59:42Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestAvroConfluentSchemaReader.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaTableHandle;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.predicate.TupleDomain;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.testng.annotations.Test;\n+\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestAvroConfluentSchemaReader", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94717cf00c6221e005a03d9eb794b9b3acb7fdf3"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyNzU5NDQz", "url": "https://github.com/trinodb/trino/pull/6137#pullrequestreview-542759443", "createdAt": "2020-12-02T11:51:47Z", "commit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "state": "COMMENTED", "comments": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMTo1MTo0N1rOH9XYiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjoxOTo0N1rOH9YWjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEwODI5Nw==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534108297", "createdAt": "2020-12-02T11:51:47Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEwODM0NA==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534108344", "createdAt": "2020-12-02T11:51:53Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEwODkyNQ==", "bodyText": "it is not used", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534108925", "createdAt": "2020-12-02T11:52:59Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEwOTMxMQ==", "bodyText": "It is Kafka error, not Presto generic internal error", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534109311", "createdAt": "2020-12-02T11:53:43Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExMDc4MQ==", "bodyText": "SetMulitmap?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534110781", "createdAt": "2020-12-02T11:56:05Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExMTM1Mw==", "bodyText": "move topic to the next line", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534111353", "createdAt": "2020-12-02T11:57:04Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNDc3OA==", "bodyText": "isEmpty", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534114778", "createdAt": "2020-12-02T12:03:02Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 175}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNDgzOQ==", "bodyText": "isEmpty", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534114839", "createdAt": "2020-12-02T12:03:10Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNDkyMw==", "bodyText": "isEmpty", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534114923", "createdAt": "2020-12-02T12:03:19Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNjIyOA==", "bodyText": "Why messageSubject and not valueSubject?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534116228", "createdAt": "2020-12-02T12:05:28Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);\n+        }\n+    }\n+\n+    // If the default subject naming strategy is not used, the key and message subjects\n+    // can be specified by adding them to the table name as follows:\n+    //  <tablename>&key-subject=<key subject>&message-subject=<value subject>\n+    // ex. kafka.default.\"mytable&key-subject=foo&message-subject=bar\"\n+    @VisibleForTesting\n+    static TopicAndSubjects parseTopicAndSubjects(SchemaTableName encodedSchemaTableName)\n+    {\n+        String encodedTableName = encodedSchemaTableName.getTableName();\n+        List<String> parts = Splitter.on(DELIMITER).trimResults().splitToList(encodedTableName);\n+        checkState(!parts.isEmpty() && parts.size() <= 3, \"Unexpected format for encodedTableName. Expected format is <tableName>[&keySubject=<key subject>][&messageSubject=<message subject>]\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 240}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNzI5MQ==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534117291", "createdAt": "2020-12-02T12:07:25Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);\n+        }\n+    }\n+\n+    // If the default subject naming strategy is not used, the key and message subjects\n+    // can be specified by adding them to the table name as follows:\n+    //  <tablename>&key-subject=<key subject>&message-subject=<value subject>\n+    // ex. kafka.default.\"mytable&key-subject=foo&message-subject=bar\"\n+    @VisibleForTesting\n+    static TopicAndSubjects parseTopicAndSubjects(SchemaTableName encodedSchemaTableName)\n+    {\n+        String encodedTableName = encodedSchemaTableName.getTableName();\n+        List<String> parts = Splitter.on(DELIMITER).trimResults().splitToList(encodedTableName);\n+        checkState(!parts.isEmpty() && parts.size() <= 3, \"Unexpected format for encodedTableName. Expected format is <tableName>[&keySubject=<key subject>][&messageSubject=<message subject>]\");\n+        String tableName = parts.get(0);\n+        Optional<String> keySubject = Optional.empty();\n+        Optional<String> valueSubject = Optional.empty();\n+        for (int part = 1; part < parts.size(); part++) {\n+            List<String> subjectKeyValue = Splitter.on(SEPARATOR).trimResults().splitToList(parts.get(part));\n+            checkState(subjectKeyValue.size() == 2 && (subjectKeyValue.get(0).equals(KEY_SUBJECT) || subjectKeyValue.get(0).equals(MESSAGE_SUBJECT)), \"Unexpected parameter '%s', should be %s=<key subject>' or %s=<message subject>\", parts.get(part), KEY_SUBJECT, MESSAGE_SUBJECT);\n+            if (subjectKeyValue.get(0).equals(KEY_SUBJECT)) {\n+                checkState(!keySubject.isPresent(), \"Key subject already defined\");\n+                keySubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+            else {\n+                checkState(!valueSubject.isPresent(), \"Value subject already defined\");\n+                valueSubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+        }\n+        return new TopicAndSubjects(tableName, keySubject, valueSubject);\n+    }\n+\n+    @Override\n+    public Set<SchemaTableName> listTables()\n+    {\n+        try {\n+            ImmutableSet.Builder<SchemaTableName> schemaTableNameBuilder = ImmutableSet.builder();\n+            schemaRegistryClient.getAllSubjects().stream()\n+                    .filter(ConfluentSchemaRegistryTableDescriptionSupplier::isValidSubject)\n+                    .forEach(subject -> schemaTableNameBuilder.add(new SchemaTableName(defaultSchema, extractTopicFromSubject(subject))));\n+            return schemaTableNameBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Unable to list tables\", e);\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    static boolean isValidSubject(String subject)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 275}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNzM3OA==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534117378", "createdAt": "2020-12-02T12:07:36Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);\n+        }\n+    }\n+\n+    // If the default subject naming strategy is not used, the key and message subjects\n+    // can be specified by adding them to the table name as follows:\n+    //  <tablename>&key-subject=<key subject>&message-subject=<value subject>\n+    // ex. kafka.default.\"mytable&key-subject=foo&message-subject=bar\"\n+    @VisibleForTesting\n+    static TopicAndSubjects parseTopicAndSubjects(SchemaTableName encodedSchemaTableName)\n+    {\n+        String encodedTableName = encodedSchemaTableName.getTableName();\n+        List<String> parts = Splitter.on(DELIMITER).trimResults().splitToList(encodedTableName);\n+        checkState(!parts.isEmpty() && parts.size() <= 3, \"Unexpected format for encodedTableName. Expected format is <tableName>[&keySubject=<key subject>][&messageSubject=<message subject>]\");\n+        String tableName = parts.get(0);\n+        Optional<String> keySubject = Optional.empty();\n+        Optional<String> valueSubject = Optional.empty();\n+        for (int part = 1; part < parts.size(); part++) {\n+            List<String> subjectKeyValue = Splitter.on(SEPARATOR).trimResults().splitToList(parts.get(part));\n+            checkState(subjectKeyValue.size() == 2 && (subjectKeyValue.get(0).equals(KEY_SUBJECT) || subjectKeyValue.get(0).equals(MESSAGE_SUBJECT)), \"Unexpected parameter '%s', should be %s=<key subject>' or %s=<message subject>\", parts.get(part), KEY_SUBJECT, MESSAGE_SUBJECT);\n+            if (subjectKeyValue.get(0).equals(KEY_SUBJECT)) {\n+                checkState(!keySubject.isPresent(), \"Key subject already defined\");\n+                keySubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+            else {\n+                checkState(!valueSubject.isPresent(), \"Value subject already defined\");\n+                valueSubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+        }\n+        return new TopicAndSubjects(tableName, keySubject, valueSubject);\n+    }\n+\n+    @Override\n+    public Set<SchemaTableName> listTables()\n+    {\n+        try {\n+            ImmutableSet.Builder<SchemaTableName> schemaTableNameBuilder = ImmutableSet.builder();\n+            schemaRegistryClient.getAllSubjects().stream()\n+                    .filter(ConfluentSchemaRegistryTableDescriptionSupplier::isValidSubject)\n+                    .forEach(subject -> schemaTableNameBuilder.add(new SchemaTableName(defaultSchema, extractTopicFromSubject(subject))));\n+            return schemaTableNameBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Unable to list tables\", e);\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    static boolean isValidSubject(String subject)\n+    {\n+        requireNonNull(subject, \"subject is null\");\n+        return subject.endsWith(VALUE_SUFFIX) || subject.endsWith(KEY_SUFFIX);\n+    }\n+\n+    @VisibleForTesting\n+    static String extractTopicFromSubject(String subject)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 282}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNzQ3Mg==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534117472", "createdAt": "2020-12-02T12:07:49Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);\n+        }\n+    }\n+\n+    // If the default subject naming strategy is not used, the key and message subjects\n+    // can be specified by adding them to the table name as follows:\n+    //  <tablename>&key-subject=<key subject>&message-subject=<value subject>\n+    // ex. kafka.default.\"mytable&key-subject=foo&message-subject=bar\"\n+    @VisibleForTesting\n+    static TopicAndSubjects parseTopicAndSubjects(SchemaTableName encodedSchemaTableName)\n+    {\n+        String encodedTableName = encodedSchemaTableName.getTableName();\n+        List<String> parts = Splitter.on(DELIMITER).trimResults().splitToList(encodedTableName);\n+        checkState(!parts.isEmpty() && parts.size() <= 3, \"Unexpected format for encodedTableName. Expected format is <tableName>[&keySubject=<key subject>][&messageSubject=<message subject>]\");\n+        String tableName = parts.get(0);\n+        Optional<String> keySubject = Optional.empty();\n+        Optional<String> valueSubject = Optional.empty();\n+        for (int part = 1; part < parts.size(); part++) {\n+            List<String> subjectKeyValue = Splitter.on(SEPARATOR).trimResults().splitToList(parts.get(part));\n+            checkState(subjectKeyValue.size() == 2 && (subjectKeyValue.get(0).equals(KEY_SUBJECT) || subjectKeyValue.get(0).equals(MESSAGE_SUBJECT)), \"Unexpected parameter '%s', should be %s=<key subject>' or %s=<message subject>\", parts.get(part), KEY_SUBJECT, MESSAGE_SUBJECT);\n+            if (subjectKeyValue.get(0).equals(KEY_SUBJECT)) {\n+                checkState(!keySubject.isPresent(), \"Key subject already defined\");\n+                keySubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+            else {\n+                checkState(!valueSubject.isPresent(), \"Value subject already defined\");\n+                valueSubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+        }\n+        return new TopicAndSubjects(tableName, keySubject, valueSubject);\n+    }\n+\n+    @Override\n+    public Set<SchemaTableName> listTables()\n+    {\n+        try {\n+            ImmutableSet.Builder<SchemaTableName> schemaTableNameBuilder = ImmutableSet.builder();\n+            schemaRegistryClient.getAllSubjects().stream()\n+                    .filter(ConfluentSchemaRegistryTableDescriptionSupplier::isValidSubject)\n+                    .forEach(subject -> schemaTableNameBuilder.add(new SchemaTableName(defaultSchema, extractTopicFromSubject(subject))));\n+            return schemaTableNameBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Unable to list tables\", e);\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    static boolean isValidSubject(String subject)\n+    {\n+        requireNonNull(subject, \"subject is null\");\n+        return subject.endsWith(VALUE_SUFFIX) || subject.endsWith(KEY_SUFFIX);\n+    }\n+\n+    @VisibleForTesting\n+    static String extractTopicFromSubject(String subject)\n+    {\n+        requireNonNull(subject, \"subject is null\");\n+        if (subject.endsWith(VALUE_SUFFIX)) {\n+            return subject.substring(0, subject.length() - VALUE_SUFFIX.length());\n+        }\n+        checkState(subject.endsWith(KEY_SUFFIX), \"Unexpected subject name %s\", subject);\n+        return subject.substring(0, subject.length() - KEY_SUFFIX.length());\n+    }\n+\n+    @VisibleForTesting\n+    static Optional<String> getKeySubjectFromTopic(String topic, Set<String> subjectsForTopic)\n+    {\n+        String keySubject = topic + KEY_SUFFIX;\n+        if (subjectsForTopic.contains(keySubject)) {\n+            return Optional.of(keySubject);\n+        }\n+        return Optional.empty();\n+    }\n+\n+    @VisibleForTesting\n+    static Optional<String> getValueSubjectFromTopic(String topic, Set<String> subjectsForTopic)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 303}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNzQ5Ng==", "bodyText": "private", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534117496", "createdAt": "2020-12-02T12:07:53Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);\n+        }\n+    }\n+\n+    // If the default subject naming strategy is not used, the key and message subjects\n+    // can be specified by adding them to the table name as follows:\n+    //  <tablename>&key-subject=<key subject>&message-subject=<value subject>\n+    // ex. kafka.default.\"mytable&key-subject=foo&message-subject=bar\"\n+    @VisibleForTesting\n+    static TopicAndSubjects parseTopicAndSubjects(SchemaTableName encodedSchemaTableName)\n+    {\n+        String encodedTableName = encodedSchemaTableName.getTableName();\n+        List<String> parts = Splitter.on(DELIMITER).trimResults().splitToList(encodedTableName);\n+        checkState(!parts.isEmpty() && parts.size() <= 3, \"Unexpected format for encodedTableName. Expected format is <tableName>[&keySubject=<key subject>][&messageSubject=<message subject>]\");\n+        String tableName = parts.get(0);\n+        Optional<String> keySubject = Optional.empty();\n+        Optional<String> valueSubject = Optional.empty();\n+        for (int part = 1; part < parts.size(); part++) {\n+            List<String> subjectKeyValue = Splitter.on(SEPARATOR).trimResults().splitToList(parts.get(part));\n+            checkState(subjectKeyValue.size() == 2 && (subjectKeyValue.get(0).equals(KEY_SUBJECT) || subjectKeyValue.get(0).equals(MESSAGE_SUBJECT)), \"Unexpected parameter '%s', should be %s=<key subject>' or %s=<message subject>\", parts.get(part), KEY_SUBJECT, MESSAGE_SUBJECT);\n+            if (subjectKeyValue.get(0).equals(KEY_SUBJECT)) {\n+                checkState(!keySubject.isPresent(), \"Key subject already defined\");\n+                keySubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+            else {\n+                checkState(!valueSubject.isPresent(), \"Value subject already defined\");\n+                valueSubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+        }\n+        return new TopicAndSubjects(tableName, keySubject, valueSubject);\n+    }\n+\n+    @Override\n+    public Set<SchemaTableName> listTables()\n+    {\n+        try {\n+            ImmutableSet.Builder<SchemaTableName> schemaTableNameBuilder = ImmutableSet.builder();\n+            schemaRegistryClient.getAllSubjects().stream()\n+                    .filter(ConfluentSchemaRegistryTableDescriptionSupplier::isValidSubject)\n+                    .forEach(subject -> schemaTableNameBuilder.add(new SchemaTableName(defaultSchema, extractTopicFromSubject(subject))));\n+            return schemaTableNameBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Unable to list tables\", e);\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    static boolean isValidSubject(String subject)\n+    {\n+        requireNonNull(subject, \"subject is null\");\n+        return subject.endsWith(VALUE_SUFFIX) || subject.endsWith(KEY_SUFFIX);\n+    }\n+\n+    @VisibleForTesting\n+    static String extractTopicFromSubject(String subject)\n+    {\n+        requireNonNull(subject, \"subject is null\");\n+        if (subject.endsWith(VALUE_SUFFIX)) {\n+            return subject.substring(0, subject.length() - VALUE_SUFFIX.length());\n+        }\n+        checkState(subject.endsWith(KEY_SUFFIX), \"Unexpected subject name %s\", subject);\n+        return subject.substring(0, subject.length() - KEY_SUFFIX.length());\n+    }\n+\n+    @VisibleForTesting\n+    static Optional<String> getKeySubjectFromTopic(String topic, Set<String> subjectsForTopic)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 293}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExODY4Ng==", "bodyText": "Move this up, then you can:\n        Optional<String> keySubject = topicAndSubjects.getKeySubject()\n                .or(topicAndSubjectsFromCache::getKeySubject);\n\nSame couldbe applied to value. Mind that topicAndSubjectsFromCache is nullable.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534118686", "createdAt": "2020-12-02T12:10:07Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExOTYxOA==", "bodyText": "It is kafka error, not Presto generic internal error.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534119618", "createdAt": "2020-12-02T12:11:46Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 227}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExOTk4Nw==", "bodyText": "use topicAndSubjectsSupplier to list tables.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534119987", "createdAt": "2020-12-02T12:12:27Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,367 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.PrestoException;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.annotation.PreDestroy;\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static io.prestosql.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    @VisibleForTesting\n+    static final String KEY_SUBJECT = \"key-subject\";\n+\n+    @VisibleForTesting\n+    static final String MESSAGE_SUBJECT = \"message-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final ExecutorService subjectsCacheExecutor;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        subjectsCacheExecutor = newCachedThreadPool(daemonThreadsNamed(\"kafka-confluent-subjects-cache-refresher\"));\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::refreshSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    @PreDestroy\n+    public void stop()\n+    {\n+        subjectsCacheExecutor.shutdownNow();\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // Note that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and message subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&message-subject=<message subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&message-subject=bar\"\n+    private Map<String, TopicAndSubjects> refreshSubjects()\n+    {\n+        try {\n+            Map<String, Set<String>> topicToSubjects = new HashMap<>();\n+            for (String subject : schemaRegistryClient.getAllSubjects()) {\n+                if (isValidSubject(subject)) {\n+                    topicToSubjects.computeIfAbsent(extractTopicFromSubject(subject), k -> new HashSet<>()).add(subject);\n+                }\n+            }\n+            ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+            for (Map.Entry<String, Set<String>> entry : topicToSubjects.entrySet()) {\n+                String topic = entry.getKey();\n+                TopicAndSubjects topicAndSubjects = new TopicAndSubjects(topic,\n+                        getKeySubjectFromTopic(topic, entry.getValue()),\n+                        getValueSubjectFromTopic(topic, entry.getValue()));\n+                topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+            }\n+            return topicSubjectsCacheBuilder.build();\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        String topic = topicAndSubjects.getTopic();\n+        Optional<String> keySubject = topicAndSubjects.getKeySubject();\n+        Optional<String> valueSubject = topicAndSubjects.getValueSubject();\n+\n+        TopicAndSubjects topicAndSubjectsFromCache = topicAndSubjectsSupplier.get().get(tableName);\n+\n+        if (topicAndSubjectsFromCache != null) {\n+            // Always use the topic from cache in case the topic is mixed case\n+            topic = topicAndSubjectsFromCache.getTopic();\n+            if (!keySubject.isPresent()) {\n+                keySubject = topicAndSubjectsFromCache.getKeySubject();\n+            }\n+            if (!valueSubject.isPresent()) {\n+                valueSubject = topicAndSubjectsFromCache.getValueSubject();\n+            }\n+        }\n+        if (!keySubject.isPresent() && !valueSubject.isPresent()) {\n+            return Optional.empty();\n+        }\n+        AvroSchemaConverter schemaConverter = new AvroSchemaConverter(typeManager, getEmptyFieldStrategy(session));\n+        Optional<KafkaTopicFieldGroup> key = keySubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        Optional<KafkaTopicFieldGroup> message = valueSubject.map(subject -> getFieldGroup(schemaConverter, subject));\n+        return Optional.of(new KafkaTopicDescription(tableName, Optional.of(schemaTableName.getSchemaName()), topic, key, message));\n+    }\n+\n+    private KafkaTopicFieldGroup getFieldGroup(AvroSchemaConverter avroSchemaConverter, String subject)\n+    {\n+        try {\n+            Schema schema = new Schema.Parser().parse(schemaRegistryClient.getLatestSchemaMetadata(subject).getSchema());\n+            List<Type> types = avroSchemaConverter.convertAvroSchema(schema);\n+            ImmutableList.Builder<KafkaTopicFieldDescription> fieldsBuilder = ImmutableList.builder();\n+            if (schema.getType() != Schema.Type.RECORD) {\n+                checkState(types.size() == 1, \"incompatible schema\");\n+                fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                        subject,\n+                        getOnlyElement(types),\n+                        subject,\n+                        null,\n+                        null,\n+                        null,\n+                        false));\n+            }\n+            else {\n+                List<Schema.Field> avroFields = schema.getFields();\n+                checkState(avroFields.size() == types.size(), \"incompatible schema\");\n+\n+                for (int i = 0; i < types.size(); i++) {\n+                    Schema.Field field = avroFields.get(i);\n+                    fieldsBuilder.add(new KafkaTopicFieldDescription(\n+                            field.name(),\n+                            types.get(i),\n+                            field.name(),\n+                            null,\n+                            null,\n+                            null,\n+                            false));\n+                }\n+            }\n+            return new KafkaTopicFieldGroup(AvroRowDecoderFactory.NAME, Optional.empty(), Optional.of(subject), fieldsBuilder.build());\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, format(\"Unable to get field group for '%s' subject\", subject), e);\n+        }\n+    }\n+\n+    // If the default subject naming strategy is not used, the key and message subjects\n+    // can be specified by adding them to the table name as follows:\n+    //  <tablename>&key-subject=<key subject>&message-subject=<value subject>\n+    // ex. kafka.default.\"mytable&key-subject=foo&message-subject=bar\"\n+    @VisibleForTesting\n+    static TopicAndSubjects parseTopicAndSubjects(SchemaTableName encodedSchemaTableName)\n+    {\n+        String encodedTableName = encodedSchemaTableName.getTableName();\n+        List<String> parts = Splitter.on(DELIMITER).trimResults().splitToList(encodedTableName);\n+        checkState(!parts.isEmpty() && parts.size() <= 3, \"Unexpected format for encodedTableName. Expected format is <tableName>[&keySubject=<key subject>][&messageSubject=<message subject>]\");\n+        String tableName = parts.get(0);\n+        Optional<String> keySubject = Optional.empty();\n+        Optional<String> valueSubject = Optional.empty();\n+        for (int part = 1; part < parts.size(); part++) {\n+            List<String> subjectKeyValue = Splitter.on(SEPARATOR).trimResults().splitToList(parts.get(part));\n+            checkState(subjectKeyValue.size() == 2 && (subjectKeyValue.get(0).equals(KEY_SUBJECT) || subjectKeyValue.get(0).equals(MESSAGE_SUBJECT)), \"Unexpected parameter '%s', should be %s=<key subject>' or %s=<message subject>\", parts.get(part), KEY_SUBJECT, MESSAGE_SUBJECT);\n+            if (subjectKeyValue.get(0).equals(KEY_SUBJECT)) {\n+                checkState(!keySubject.isPresent(), \"Key subject already defined\");\n+                keySubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+            else {\n+                checkState(!valueSubject.isPresent(), \"Value subject already defined\");\n+                valueSubject = Optional.of(subjectKeyValue.get(1));\n+            }\n+        }\n+        return new TopicAndSubjects(tableName, keySubject, valueSubject);\n+    }\n+\n+    @Override\n+    public Set<SchemaTableName> listTables()\n+    {\n+        try {\n+            ImmutableSet.Builder<SchemaTableName> schemaTableNameBuilder = ImmutableSet.builder();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyMDI5OQ==", "bodyText": "private final?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534120299", "createdAt": "2020-12-02T12:13:00Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSessionProperties.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.session.PropertyMetadata;\n+\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.util.List;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentSessionProperties\n+        implements Provider<List<PropertyMetadata<?>>>\n+{\n+    public static final String EMPTY_FIELD_STRATEGY = \"empty_field_strategy\";\n+\n+    List<PropertyMetadata<?>> sessionProperties;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyMjk0MQ==", "bodyText": "Why can't you use simply KafkaQueryRunner? Let's make TestingKafkaWithSchemaRegistry  and TestingKafka to share common interface (maybe interface could be called TestingKafka and implementation BasicTestingKafka, wdyt?), are there any other issues?\nYou can keep this class for main method and some utils if you like. BTW I like the main method here ;)", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534122941", "createdAt": "2020-12-02T12:17:31Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/KafkaWithConfluentSchemaRegistryQueryRunner.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Module;\n+import io.airlift.log.Level;\n+import io.airlift.log.Logger;\n+import io.airlift.log.Logging;\n+import io.prestosql.plugin.kafka.KafkaPlugin;\n+import io.prestosql.plugin.kafka.KafkaQueryRunner;\n+import io.prestosql.plugin.tpch.TpchPlugin;\n+import io.prestosql.testing.DistributedQueryRunner;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import io.prestosql.testing.kafka.TestingKafkaWithSchemaRegistry;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.airlift.testing.Closeables.closeAllSuppress;\n+import static io.prestosql.plugin.kafka.KafkaPlugin.DEFAULT_EXTENSION;\n+import static io.prestosql.testing.TestingSession.testSessionBuilder;\n+import static java.util.Objects.requireNonNull;\n+\n+public class KafkaWithConfluentSchemaRegistryQueryRunner", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyNDA5MA==", "bodyText": "Use failsafe.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534124090", "createdAt": "2020-12-02T12:19:38Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestKafkaWithConfluentSchemaRegistryMinimalFunctionality.java", "diffHunk": "@@ -0,0 +1,278 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.confluent.kafka.serializers.subject.RecordNameStrategy;\n+import io.confluent.kafka.serializers.subject.TopicRecordNameStrategy;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import io.prestosql.testing.kafka.TestingKafkaWithSchemaRegistry;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+import java.util.function.Supplier;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.VALUE_SUBJECT_NAME_STRATEGY;\n+import static java.lang.Math.multiplyExact;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static org.testng.Assert.assertTrue;\n+\n+@Test(singleThreaded = true)\n+public class TestKafkaWithConfluentSchemaRegistryMinimalFunctionality\n+        extends AbstractTestQueryFramework\n+{\n+    private static final String RECORD_NAME = \"test_record\";\n+    private static final int MESSAGE_COUNT = 100;\n+    private static final Schema INITIAL_SCHEMA = SchemaBuilder.record(RECORD_NAME)\n+            .fields()\n+            .name(\"col_1\").type().longType().noDefault()\n+            .name(\"col_2\").type().stringType().noDefault()\n+            .endRecord();\n+    private static final Schema EVOLVED_SCHEMA = SchemaBuilder.record(RECORD_NAME)\n+            .fields()\n+            .name(\"col_1\").type().longType().noDefault()\n+            .name(\"col_2\").type().stringType().noDefault()\n+            .name(\"col_3\").type().optional().doubleType()\n+            .endRecord();\n+\n+    private TestingKafkaWithSchemaRegistry testingKafkaWithSchemaRegistry;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        testingKafkaWithSchemaRegistry = new TestingKafkaWithSchemaRegistry(new TestingKafka());\n+        return KafkaWithConfluentSchemaRegistryQueryRunner.builder(testingKafkaWithSchemaRegistry)\n+                .setExtraKafkaProperties(ImmutableMap.<String, String>builder()\n+                        .put(\"kafka.confluent-subjects-cache-refresh-interval\", \"1ms\")\n+                        .build())\n+                .build();\n+    }\n+\n+    @Test\n+    public void testBasicTopic()\n+    {\n+        String topic = \"topic-basic-MixedCase\";\n+        assertTopic(topic,\n+                format(\"SELECT col_1, col_2 FROM %s\", toDoubleQuoted(topic)),\n+                format(\"SELECT col_1, col_2, col_3 FROM %s\", toDoubleQuoted(topic)),\n+                false,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducerWithLongKeys());\n+    }\n+\n+    @Test\n+    public void testTopicWithKeySubject()\n+    {\n+        String topic = \"topic-key-subject\";\n+        assertTopic(topic,\n+                format(\"SELECT \\\"%s-key\\\", col_1, col_2 FROM %s\", topic, toDoubleQuoted(topic)),\n+                format(\"SELECT \\\"%s-key\\\", col_1, col_2, col_3 FROM %s\", topic, toDoubleQuoted(topic)),\n+                true,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducer());\n+    }\n+\n+    @Test\n+    public void testTopicWithRecordNameStrategy()\n+    {\n+        String topic = \"topic-record-name-strategy\";\n+        assertTopic(topic,\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2 FROM \\\"%1$s&message-subject=%2$s\\\"\", topic, RECORD_NAME),\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2, col_3 FROM \\\"%1$s&message-subject=%2$s\\\"\", topic, RECORD_NAME),\n+                true,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducer(ImmutableMap.<String, String>builder()\n+                        .put(VALUE_SUBJECT_NAME_STRATEGY, RecordNameStrategy.class.getName())\n+                        .build()));\n+    }\n+\n+    @Test\n+    public void testTopicWithTopicRecordNameStrategy()\n+    {\n+        String topic = \"topic-topic-record-name-strategy\";\n+        assertTopic(topic,\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2 FROM \\\"%1$s&message-subject=%1$s-%2$s\\\"\", topic, RECORD_NAME),\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2, col_3 FROM \\\"%1$s&message-subject=%1$s-%2$s\\\"\", topic, RECORD_NAME),\n+                true,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducer(ImmutableMap.<String, String>builder()\n+                        .put(VALUE_SUBJECT_NAME_STRATEGY, TopicRecordNameStrategy.class.getName())\n+                        .build()));\n+    }\n+\n+    private void assertTopic(String topicName, String initialQuery, String evolvedQuery, boolean isKeyIncluded, Supplier<KafkaProducer<Long, GenericRecord>> producerSupplier)\n+    {\n+        testingKafkaWithSchemaRegistry.createTopic(topicName);\n+\n+        assertNotExists(topicName);\n+\n+        List<ProducerRecord<Long, GenericRecord>> messages = createMessages(topicName, MESSAGE_COUNT, true);\n+        sendMessages(messages, producerSupplier);\n+\n+        waitUntilTableExists(topicName);\n+        assertCount(topicName, MESSAGE_COUNT);\n+\n+        assertQuery(initialQuery, getExpectedValues(messages, INITIAL_SCHEMA, isKeyIncluded));\n+\n+        List<ProducerRecord<Long, GenericRecord>> newMessages = createMessages(topicName, MESSAGE_COUNT, false);\n+        sendMessages(newMessages, producerSupplier);\n+\n+        List<ProducerRecord<Long, GenericRecord>> allMessages = ImmutableList.<ProducerRecord<Long, GenericRecord>>builder()\n+                .addAll(messages)\n+                .addAll(newMessages)\n+                .build();\n+        assertCount(topicName, allMessages.size());\n+        assertQuery(evolvedQuery, getExpectedValues(allMessages, EVOLVED_SCHEMA, isKeyIncluded));\n+    }\n+\n+    private String getExpectedValues(List<ProducerRecord<Long, GenericRecord>> messages, Schema schema, boolean isKeyIncluded)\n+    {\n+        StringBuilder valuesBuilder = new StringBuilder(\"VALUES \");\n+        ImmutableList.Builder<String> rowsBuilder = ImmutableList.builder();\n+        for (ProducerRecord<Long, GenericRecord> message : messages) {\n+            ImmutableList.Builder<String> columnsBuilder = ImmutableList.builder();\n+\n+            if (isKeyIncluded) {\n+                columnsBuilder.add(String.valueOf(message.key()));\n+            }\n+\n+            addExpectedColumns(schema, message.value(), columnsBuilder);\n+\n+            rowsBuilder.add(format(\"(%s)\", String.join(\", \", columnsBuilder.build())));\n+        }\n+        valuesBuilder.append(String.join(\", \", rowsBuilder.build()));\n+        return valuesBuilder.toString();\n+    }\n+\n+    private void addExpectedColumns(Schema schema, GenericRecord record, ImmutableList.Builder<String> columnsBuilder)\n+    {\n+        for (Schema.Field field : schema.getFields()) {\n+            Object value = record.get(field.name());\n+            if (value == null) {\n+                columnsBuilder.add(\"null\");\n+            }\n+            else if (field.schema().getType().equals(Schema.Type.STRING)) {\n+                columnsBuilder.add(toSingleQuoted(value));\n+            }\n+            else {\n+                columnsBuilder.add(String.valueOf(value));\n+            }\n+        }\n+    }\n+\n+    private void assertNotExists(String tableName)\n+    {\n+        if (schemaExists()) {\n+            assertQueryReturnsEmptyResult(format(\"SHOW TABLES LIKE '%s'\", tableName));\n+        }\n+    }\n+\n+    private void waitUntilTableExists(String tableName)\n+    {\n+        int attempts = 10;\n+        int waitMs = 100;\n+        for (int attempt = 0; !schemaExists() && attempt < attempts; attempt++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 193}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyNDE3NA==", "bodyText": "failsafe", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r534124174", "createdAt": "2020-12-02T12:19:47Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/schema/confluent/TestKafkaWithConfluentSchemaRegistryMinimalFunctionality.java", "diffHunk": "@@ -0,0 +1,278 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.confluent.kafka.serializers.subject.RecordNameStrategy;\n+import io.confluent.kafka.serializers.subject.TopicRecordNameStrategy;\n+import io.prestosql.testing.AbstractTestQueryFramework;\n+import io.prestosql.testing.QueryRunner;\n+import io.prestosql.testing.kafka.TestingKafka;\n+import io.prestosql.testing.kafka.TestingKafkaWithSchemaRegistry;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+import java.util.function.Supplier;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.VALUE_SUBJECT_NAME_STRATEGY;\n+import static java.lang.Math.multiplyExact;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static org.testng.Assert.assertTrue;\n+\n+@Test(singleThreaded = true)\n+public class TestKafkaWithConfluentSchemaRegistryMinimalFunctionality\n+        extends AbstractTestQueryFramework\n+{\n+    private static final String RECORD_NAME = \"test_record\";\n+    private static final int MESSAGE_COUNT = 100;\n+    private static final Schema INITIAL_SCHEMA = SchemaBuilder.record(RECORD_NAME)\n+            .fields()\n+            .name(\"col_1\").type().longType().noDefault()\n+            .name(\"col_2\").type().stringType().noDefault()\n+            .endRecord();\n+    private static final Schema EVOLVED_SCHEMA = SchemaBuilder.record(RECORD_NAME)\n+            .fields()\n+            .name(\"col_1\").type().longType().noDefault()\n+            .name(\"col_2\").type().stringType().noDefault()\n+            .name(\"col_3\").type().optional().doubleType()\n+            .endRecord();\n+\n+    private TestingKafkaWithSchemaRegistry testingKafkaWithSchemaRegistry;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        testingKafkaWithSchemaRegistry = new TestingKafkaWithSchemaRegistry(new TestingKafka());\n+        return KafkaWithConfluentSchemaRegistryQueryRunner.builder(testingKafkaWithSchemaRegistry)\n+                .setExtraKafkaProperties(ImmutableMap.<String, String>builder()\n+                        .put(\"kafka.confluent-subjects-cache-refresh-interval\", \"1ms\")\n+                        .build())\n+                .build();\n+    }\n+\n+    @Test\n+    public void testBasicTopic()\n+    {\n+        String topic = \"topic-basic-MixedCase\";\n+        assertTopic(topic,\n+                format(\"SELECT col_1, col_2 FROM %s\", toDoubleQuoted(topic)),\n+                format(\"SELECT col_1, col_2, col_3 FROM %s\", toDoubleQuoted(topic)),\n+                false,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducerWithLongKeys());\n+    }\n+\n+    @Test\n+    public void testTopicWithKeySubject()\n+    {\n+        String topic = \"topic-key-subject\";\n+        assertTopic(topic,\n+                format(\"SELECT \\\"%s-key\\\", col_1, col_2 FROM %s\", topic, toDoubleQuoted(topic)),\n+                format(\"SELECT \\\"%s-key\\\", col_1, col_2, col_3 FROM %s\", topic, toDoubleQuoted(topic)),\n+                true,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducer());\n+    }\n+\n+    @Test\n+    public void testTopicWithRecordNameStrategy()\n+    {\n+        String topic = \"topic-record-name-strategy\";\n+        assertTopic(topic,\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2 FROM \\\"%1$s&message-subject=%2$s\\\"\", topic, RECORD_NAME),\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2, col_3 FROM \\\"%1$s&message-subject=%2$s\\\"\", topic, RECORD_NAME),\n+                true,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducer(ImmutableMap.<String, String>builder()\n+                        .put(VALUE_SUBJECT_NAME_STRATEGY, RecordNameStrategy.class.getName())\n+                        .build()));\n+    }\n+\n+    @Test\n+    public void testTopicWithTopicRecordNameStrategy()\n+    {\n+        String topic = \"topic-topic-record-name-strategy\";\n+        assertTopic(topic,\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2 FROM \\\"%1$s&message-subject=%1$s-%2$s\\\"\", topic, RECORD_NAME),\n+                format(\"SELECT \\\"%1$s-key\\\", col_1, col_2, col_3 FROM \\\"%1$s&message-subject=%1$s-%2$s\\\"\", topic, RECORD_NAME),\n+                true,\n+                () -> testingKafkaWithSchemaRegistry.createConfluentProducer(ImmutableMap.<String, String>builder()\n+                        .put(VALUE_SUBJECT_NAME_STRATEGY, TopicRecordNameStrategy.class.getName())\n+                        .build()));\n+    }\n+\n+    private void assertTopic(String topicName, String initialQuery, String evolvedQuery, boolean isKeyIncluded, Supplier<KafkaProducer<Long, GenericRecord>> producerSupplier)\n+    {\n+        testingKafkaWithSchemaRegistry.createTopic(topicName);\n+\n+        assertNotExists(topicName);\n+\n+        List<ProducerRecord<Long, GenericRecord>> messages = createMessages(topicName, MESSAGE_COUNT, true);\n+        sendMessages(messages, producerSupplier);\n+\n+        waitUntilTableExists(topicName);\n+        assertCount(topicName, MESSAGE_COUNT);\n+\n+        assertQuery(initialQuery, getExpectedValues(messages, INITIAL_SCHEMA, isKeyIncluded));\n+\n+        List<ProducerRecord<Long, GenericRecord>> newMessages = createMessages(topicName, MESSAGE_COUNT, false);\n+        sendMessages(newMessages, producerSupplier);\n+\n+        List<ProducerRecord<Long, GenericRecord>> allMessages = ImmutableList.<ProducerRecord<Long, GenericRecord>>builder()\n+                .addAll(messages)\n+                .addAll(newMessages)\n+                .build();\n+        assertCount(topicName, allMessages.size());\n+        assertQuery(evolvedQuery, getExpectedValues(allMessages, EVOLVED_SCHEMA, isKeyIncluded));\n+    }\n+\n+    private String getExpectedValues(List<ProducerRecord<Long, GenericRecord>> messages, Schema schema, boolean isKeyIncluded)\n+    {\n+        StringBuilder valuesBuilder = new StringBuilder(\"VALUES \");\n+        ImmutableList.Builder<String> rowsBuilder = ImmutableList.builder();\n+        for (ProducerRecord<Long, GenericRecord> message : messages) {\n+            ImmutableList.Builder<String> columnsBuilder = ImmutableList.builder();\n+\n+            if (isKeyIncluded) {\n+                columnsBuilder.add(String.valueOf(message.key()));\n+            }\n+\n+            addExpectedColumns(schema, message.value(), columnsBuilder);\n+\n+            rowsBuilder.add(format(\"(%s)\", String.join(\", \", columnsBuilder.build())));\n+        }\n+        valuesBuilder.append(String.join(\", \", rowsBuilder.build()));\n+        return valuesBuilder.toString();\n+    }\n+\n+    private void addExpectedColumns(Schema schema, GenericRecord record, ImmutableList.Builder<String> columnsBuilder)\n+    {\n+        for (Schema.Field field : schema.getFields()) {\n+            Object value = record.get(field.name());\n+            if (value == null) {\n+                columnsBuilder.add(\"null\");\n+            }\n+            else if (field.schema().getType().equals(Schema.Type.STRING)) {\n+                columnsBuilder.add(toSingleQuoted(value));\n+            }\n+            else {\n+                columnsBuilder.add(String.valueOf(value));\n+            }\n+        }\n+    }\n+\n+    private void assertNotExists(String tableName)\n+    {\n+        if (schemaExists()) {\n+            assertQueryReturnsEmptyResult(format(\"SHOW TABLES LIKE '%s'\", tableName));\n+        }\n+    }\n+\n+    private void waitUntilTableExists(String tableName)\n+    {\n+        int attempts = 10;\n+        int waitMs = 100;\n+        for (int attempt = 0; !schemaExists() && attempt < attempts; attempt++) {\n+            try {\n+                Thread.sleep(waitMs);\n+            }\n+            catch (InterruptedException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        assertTrue(schemaExists());\n+        for (int attempt = 0; !tableExists(tableName) && attempt < attempts; attempt++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba"}, "originalPosition": 202}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba", "author": {"user": {"login": "elonazoulay", "name": "Elon Azoulay"}}, "url": "https://github.com/trinodb/trino/commit/6f1411fae1d857c07c8cb8d10f3b3f2afbf8adba", "committedDate": "2020-11-29T10:41:27Z", "message": "Add ConfluentSchemaRegistryTableDescriptionSupplier"}, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyNDAxNTYz", "url": "https://github.com/trinodb/trino/pull/6137#pullrequestreview-552401563", "createdAt": "2020-12-15T12:31:18Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjozMToxOFrOIGIjAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjo0NTowMVrOIGJE6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwMjQwMA==", "bodyText": "All of these fail with: RequireUpperBoundDeps. That means that your kafka has a conflict in transitive dependencies. In Presto in such case use the newest version used, for example if you declare com.fasterxml.jackson.core:jackson-databind:2.11.1 in kafka the issue will be fixed. Please apply whenever possible and we will see what will be left.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r543302400", "createdAt": "2020-12-15T12:31:18Z", "author": {"login": "kokosing"}, "path": "pom.xml", "diffHunk": "@@ -1070,6 +1078,34 @@\n                 <version>4.5.1</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-avro-serializer</artifactId>\n+                <version>${dep.confluent.version}</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMTc0Ng=="}, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwMzU3Ng==", "bodyText": "extact static inner class for this module", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r543303576", "createdAt": "2020-12-15T12:33:13Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentModule.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.inject.Binder;\n+import com.google.inject.Provides;\n+import com.google.inject.Scopes;\n+import io.airlift.configuration.AbstractConfigurationAwareModule;\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderModule;\n+import io.prestosql.decoder.RowDecoderFactory;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroDeserializer;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+\n+import javax.inject.Singleton;\n+\n+import static com.google.inject.multibindings.MapBinder.newMapBinder;\n+import static io.airlift.configuration.ConfigBinder.configBinder;\n+\n+public class ConfluentModule\n+        extends AbstractConfigurationAwareModule\n+{\n+    @Override\n+    protected void setup(Binder binder)\n+    {\n+        configBinder(binder).bindConfig(ConfluentSchemaRegistryConfig.class);\n+        install(new DecoderModule(decoderBinder -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwNTQ0Mg==", "bodyText": "undo this refactor or separate commit", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r543305442", "createdAt": "2020-12-15T12:36:23Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaSessionProperties.java", "diffHunk": "@@ -22,19 +22,23 @@\n import java.util.List;\n \n public final class KafkaSessionProperties\n+        implements SessionPropertiesProvider\n {\n     private static final String TIMESTAMP_UPPER_BOUND_FORCE_PUSH_DOWN_ENABLED = \"timestamp_upper_bound_force_push_down_enabled\";\n     private final List<PropertyMetadata<?>> sessionProperties;\n \n     @Inject\n     public KafkaSessionProperties(KafkaConfig kafkaConfig)\n     {\n-        sessionProperties = ImmutableList.of(PropertyMetadata.booleanProperty(\n-                TIMESTAMP_UPPER_BOUND_FORCE_PUSH_DOWN_ENABLED,\n-                \"Enable or disable timestamp upper bound push down for topic createTime mode\",\n-                kafkaConfig.isTimestampUpperBoundPushDownEnabled(), false));\n+        sessionProperties = ImmutableList.<PropertyMetadata<?>>builder()\n+                .add(PropertyMetadata.booleanProperty(\n+                        TIMESTAMP_UPPER_BOUND_FORCE_PUSH_DOWN_ENABLED,\n+                        \"Enable or disable timestamp upper bound push down for topic createTime mode\",\n+                        kafkaConfig.isTimestampUpperBoundPushDownEnabled(), false))\n+                .build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwNzgwMA==", "bodyText": "I think user custom data is more important that cached. So let's fallback to cached data if it is missing in schemaTableName. What do you think?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r543307800", "createdAt": "2020-12-15T12:40:03Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,350 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSetMultimap;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.function.UnaryOperator.identity;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";\n+\n+    private static final String KEY_SUBJECT = \"key-subject\";\n+\n+    private static final String VALUE_SUBJECT = \"value-subject\";\n+\n+    private static final String KEY_SUFFIX = \"-key\";\n+    private static final String VALUE_SUFFIX = \"-value\";\n+\n+    private final SchemaRegistryClient schemaRegistryClient;\n+    private final String defaultSchema;\n+    private final Supplier<Map<String, TopicAndSubjects>> topicAndSubjectsSupplier;\n+    private final Supplier<Map<String, String>> subjectsSupplier;\n+    private final TypeManager typeManager;\n+\n+    public ConfluentSchemaRegistryTableDescriptionSupplier(SchemaRegistryClient schemaRegistryClient, String defaultSchema, Duration subjectsCacheRefreshInterval, TypeManager typeManager)\n+    {\n+        requireNonNull(typeManager, \"typeManager is null\");\n+        this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.defaultSchema = requireNonNull(defaultSchema, \"defaultSchema is null\");\n+        topicAndSubjectsSupplier = memoizeWithExpiration(this::getTopicAndSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+        subjectsSupplier = memoizeWithExpiration(this::getAllSubjects, subjectsCacheRefreshInterval.toMillis(), MILLISECONDS);\n+    }\n+\n+    public static class Factory\n+            implements Provider<TableDescriptionSupplier>\n+    {\n+        private final String defaultSchema;\n+        private final Duration subjectsCacheRefreshInterval;\n+        private final TypeManager typeManager;\n+        private final SchemaRegistryClient schemaRegistryClient;\n+\n+        @Inject\n+        public Factory(KafkaConfig kafkaConfig, ConfluentSchemaRegistryConfig confluentConfig, TypeManager typeManager, SchemaRegistryClient schemaRegistryClient)\n+        {\n+            requireNonNull(kafkaConfig, \"kafkaConfig is null\");\n+            requireNonNull(typeManager, \"typeManager is null\");\n+            this.schemaRegistryClient = requireNonNull(schemaRegistryClient, \"schemaRegistryClient is null\");\n+            this.defaultSchema = kafkaConfig.getDefaultSchema();\n+            this.subjectsCacheRefreshInterval = confluentConfig.getConfluentSubjectsCacheRefreshInterval();\n+            this.typeManager = typeManager;\n+        }\n+\n+        @Override\n+        public TableDescriptionSupplier get()\n+        {\n+            return new ConfluentSchemaRegistryTableDescriptionSupplier(schemaRegistryClient, defaultSchema, subjectsCacheRefreshInterval, typeManager);\n+        }\n+    }\n+\n+    private String resolveSubject(String candidate)\n+    {\n+        String subject = subjectsSupplier.get().get(candidate);\n+        checkState(subject != null, \"Subject '%s' not found\", candidate);\n+        return subject;\n+    }\n+\n+    private Map<String, String> getAllSubjects()\n+    {\n+        try {\n+            return schemaRegistryClient.getAllSubjects().stream()\n+                    .collect(toImmutableMap(subject -> subject.toLowerCase(ENGLISH), identity()));\n+        }\n+        catch (IOException | RestClientException e) {\n+            throw new RuntimeException(\"Failed to retrieve subjects from schema registry\", e);\n+        }\n+    }\n+\n+    // Refresh mapping of topic to subjects.\n+    // *Note: that this mapping only supports subjects that use the SubjectNameStrategy of TopicName.\n+    // If another SubjectNameStrategy is used then the key and value subject must be encoded\n+    // into the table name as follows:\n+    // <table name>[&key-subject=<key subject>][&value-subject=<value subject]\n+    // ex. kafka.default.\"my-topic&key-subject=foo&value-subject=bar\"\n+    private Map<String, TopicAndSubjects> getTopicAndSubjects()\n+    {\n+        ImmutableSetMultimap.Builder<String, String> topicToSubjectsBuilder = ImmutableSetMultimap.builder();\n+        for (String subject : subjectsSupplier.get().values()) {\n+            if (isValidSubject(subject)) {\n+                topicToSubjectsBuilder.put(extractTopicFromSubject(subject), subject);\n+            }\n+        }\n+        ImmutableMap.Builder<String, TopicAndSubjects> topicSubjectsCacheBuilder = ImmutableMap.builder();\n+        for (Map.Entry<String, Collection<String>> entry : topicToSubjectsBuilder.build().asMap().entrySet()) {\n+            String topic = entry.getKey();\n+            TopicAndSubjects topicAndSubjects = new TopicAndSubjects(\n+                    topic,\n+                    getKeySubjectFromTopic(topic, entry.getValue()),\n+                    getValueSubjectFromTopic(topic, entry.getValue()));\n+            topicSubjectsCacheBuilder.put(topicAndSubjects.getTableName(), topicAndSubjects);\n+        }\n+        return topicSubjectsCacheBuilder.build();\n+    }\n+\n+    @Override\n+    public Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName)\n+    {\n+        requireNonNull(schemaTableName, \"schemaTableName is null\");\n+        TopicAndSubjects topicAndSubjects = parseTopicAndSubjects(schemaTableName);\n+\n+        String tableName = topicAndSubjects.getTableName();\n+        Optional<TopicAndSubjects> topicAndSubjectsFromCache = Optional.ofNullable(topicAndSubjectsSupplier.get().get(tableName));\n+\n+        // Use the topic from cache, if present, in case the topic is mixed case\n+        String topic = topicAndSubjectsFromCache.map(TopicAndSubjects::getTopic).orElse(topicAndSubjects.getTopic());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwOTQ3NQ==", "bodyText": "DELIMITER and SEPARATOR are synonyms. Maybe simply inline them? Otherwise I suggest KEY_VALUE_PAIR_DELIMITER and KEY_VALUE_DELIMITER", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r543309475", "createdAt": "2020-12-15T12:42:37Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryTableDescriptionSupplier.java", "diffHunk": "@@ -0,0 +1,350 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSetMultimap;\n+import io.airlift.units.Duration;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.KafkaConfig;\n+import io.prestosql.plugin.kafka.KafkaTopicDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldDescription;\n+import io.prestosql.plugin.kafka.KafkaTopicFieldGroup;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.connector.SchemaTableName;\n+import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n+import org.apache.avro.Schema;\n+\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Supplier;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Suppliers.memoizeWithExpiration;\n+import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static io.prestosql.plugin.kafka.schema.confluent.ConfluentSessionProperties.getEmptyFieldStrategy;\n+import static java.lang.String.format;\n+import static java.util.Locale.ENGLISH;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.function.UnaryOperator.identity;\n+\n+public class ConfluentSchemaRegistryTableDescriptionSupplier\n+        implements TableDescriptionSupplier\n+{\n+    public static final String NAME = \"confluent\";\n+\n+    private static final String DELIMITER = \"&\";\n+    private static final String SEPARATOR = \"=\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwOTg1Mw==", "bodyText": "static import", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r543309853", "createdAt": "2020-12-15T12:43:13Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSessionProperties.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.kafka.SessionPropertiesProvider;\n+import io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.session.PropertyMetadata;\n+\n+import javax.inject.Inject;\n+\n+import java.util.List;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentSessionProperties\n+        implements SessionPropertiesProvider\n+{\n+    public static final String EMPTY_FIELD_STRATEGY = \"empty_field_strategy\";\n+\n+    private final List<PropertyMetadata<?>> sessionProperties;\n+\n+    @Inject\n+    public ConfluentSessionProperties(ConfluentSchemaRegistryConfig config)\n+    {\n+        requireNonNull(config, \"config is null\");\n+        sessionProperties = ImmutableList.<PropertyMetadata<?>>builder()\n+                .add(PropertyMetadata.enumProperty(EMPTY_FIELD_STRATEGY,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMxMTA4Mw==", "bodyText": "Instead of these methods you can inject Consumer<DistributedQueryRunner> objects that could be called in certain points.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r543311083", "createdAt": "2020-12-15T12:45:01Z", "author": {"login": "kokosing"}, "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/KafkaQueryRunner.java", "diffHunk": "@@ -115,36 +97,14 @@ public Builder setExtension(Module extension)\n         }\n \n         @Override\n-        public DistributedQueryRunner build()\n+        public void preInit(DistributedQueryRunner queryRunner)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 65}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyNTgzNTQ0", "url": "https://github.com/trinodb/trino/pull/6137#pullrequestreview-552583544", "createdAt": "2020-12-15T15:33:22Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0MzIwMDM0", "url": "https://github.com/trinodb/trino/pull/6137#pullrequestreview-554320034", "createdAt": "2020-12-17T06:50:58Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNjo1MDo1OVrOIHnG8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNjo1ODowNVrOIHnQ6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1MTY5OA==", "bodyText": "Can we capture this as a separate commit and apply Confluent schema registry on top of it ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544851698", "createdAt": "2020-12-17T06:50:59Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/TableDescriptionSupplier.java", "diffHunk": "@@ -24,5 +25,5 @@\n {\n     Set<SchemaTableName> listTables();\n \n-    Optional<KafkaTopicDescription> getTopicDescription(SchemaTableName schemaTableName);\n+    Optional<KafkaTopicDescription> getTopicDescription(ConnectorSession session, SchemaTableName schemaTableName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1MTg3NA==", "bodyText": "Any validation for Min/Max value ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544851874", "createdAt": "2020-12-17T06:51:32Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSchemaRegistryConfig.java", "diffHunk": "@@ -70,4 +73,17 @@ public ConfluentSchemaRegistryConfig setEmptyFieldStrategy(EmptyFieldStrategy em\n         this.emptyFieldStrategy = emptyFieldStrategy;\n         return this;\n     }\n+\n+    public Duration getConfluentSubjectsCacheRefreshInterval()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1MjMxMA==", "bodyText": "Do we use it outside test scope ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544852310", "createdAt": "2020-12-17T06:52:46Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/pom.xml", "diffHunk": "@@ -92,6 +92,11 @@\n             <artifactId>joda-time</artifactId>\n         </dependency>\n \n+        <dependency>\n+            <groupId>net.jodah</groupId>\n+            <artifactId>failsafe</artifactId>\n+        </dependency>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1MjkyNQ==", "bodyText": "How about having them as a separate commit", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544852925", "createdAt": "2020-12-17T06:54:30Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/SessionPropertiesProvider.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka;\n+\n+import io.prestosql.spi.session.PropertyMetadata;\n+\n+import java.util.List;\n+\n+public interface SessionPropertiesProvider", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1NDI0OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            .add(enumProperty(EMPTY_FIELD_STRATEGY,\n          \n          \n            \n                                    \"Strategy for handling struct types with no fields: IGNORE (default), FAIL, and ADD_DUMMY to add a boolean field named 'dummy'\",\n          \n          \n            \n                                    EmptyFieldStrategy.class, config.getEmptyFieldStrategy(), false))\n          \n          \n            \n                            .add(enumProperty(\n          \n          \n            \n                                    EMPTY_FIELD_STRATEGY,\n          \n          \n            \n                                    \"Strategy for handling struct types with no fields: IGNORE (default), FAIL, and ADD_DUMMY to add a boolean field named 'dummy'\",\n          \n          \n            \n                                    EmptyFieldStrategy.class, \n          \n          \n            \n                                    config.getEmptyFieldStrategy(), \n          \n          \n            \n                                    false))", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544854248", "createdAt": "2020-12-17T06:58:05Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentSessionProperties.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.plugin.kafka.SessionPropertiesProvider;\n+import io.prestosql.plugin.kafka.schema.confluent.AvroSchemaConverter.EmptyFieldStrategy;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.session.PropertyMetadata;\n+\n+import javax.inject.Inject;\n+\n+import java.util.List;\n+\n+import static io.prestosql.spi.session.PropertyMetadata.enumProperty;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ConfluentSessionProperties\n+        implements SessionPropertiesProvider\n+{\n+    public static final String EMPTY_FIELD_STRATEGY = \"empty_field_strategy\";\n+\n+    private final List<PropertyMetadata<?>> sessionProperties;\n+\n+    @Inject\n+    public ConfluentSessionProperties(ConfluentSchemaRegistryConfig config)\n+    {\n+        requireNonNull(config, \"config is null\");\n+        sessionProperties = ImmutableList.<PropertyMetadata<?>>builder()\n+                .add(enumProperty(EMPTY_FIELD_STRATEGY,\n+                        \"Strategy for handling struct types with no fields: IGNORE (default), FAIL, and ADD_DUMMY to add a boolean field named 'dummy'\",\n+                        EmptyFieldStrategy.class, config.getEmptyFieldStrategy(), false))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0NDA0NTg5", "url": "https://github.com/trinodb/trino/pull/6137#pullrequestreview-554404589", "createdAt": "2020-12-17T09:09:24Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOTowOToyNFrOIHrl-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOToyNzoxNVrOIHsUgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkyNTE3Ng==", "bodyText": "Let's move this comment to presto-kafka/pom.xml. Same for belows.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544925176", "createdAt": "2020-12-17T09:09:24Z", "author": {"login": "kokosing"}, "path": "pom.xml", "diffHunk": "@@ -1077,12 +1086,39 @@\n                 <version>4.5.1</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-avro-serializer</artifactId>\n+                <version>${dep.confluent.version}</version>\n+                <exclusions>\n+                    <exclusion>\n+                        <groupId>org.apache.kafka</groupId>\n+                        <artifactId>kafka-clients</artifactId>\n+                    </exclusion>\n+                </exclusions>\n+            </dependency>\n+\n             <dependency>\n                 <groupId>io.jsonwebtoken</groupId>\n                 <artifactId>jjwt</artifactId>\n                 <version>0.9.0</version>\n             </dependency>\n \n+\n+            <!-- io.confluent:kafka-avro-serializer uses multiple versions of this transitive dependency -->", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkzNzA5MA==", "bodyText": "Please squash this change with the commit that broke this test.", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r544937090", "createdAt": "2020-12-17T09:27:15Z", "author": {"login": "kokosing"}, "path": "presto-plugin-toolkit/src/test/java/io/prestosql/plugin/base/util/TestJsonUtils.java", "diffHunk": "@@ -34,7 +34,7 @@\n     public static class TestObject\n     {\n         @JsonProperty\n-        private TestEnum testEnum;\n+        public TestEnum testEnum;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MTgwNTQ4", "url": "https://github.com/trinodb/trino/pull/6137#pullrequestreview-555180548", "createdAt": "2020-12-18T05:26:07Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwNToyNjowN1rOIITmiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwNTozMjoyN1rOIITtpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4MDY4MQ==", "bodyText": "Is Kafka Avro Serializer is used only in test scope ?", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r545580681", "createdAt": "2020-12-18T05:26:07Z", "author": {"login": "Praveen2112"}, "path": "pom.xml", "diffHunk": "@@ -1070,6 +1078,34 @@\n                 <version>4.5.1</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>io.confluent</groupId>\n+                <artifactId>kafka-avro-serializer</artifactId>\n+                <version>${dep.confluent.version}</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMzMTc0Ng=="}, "originalCommit": {"oid": "9d94d5d713ef6331e35e3e5b314a144e03d5f96c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4MjUwMw==", "bodyText": "Can we provide a way to inject custom List to the SchemaRegistry", "url": "https://github.com/trinodb/trino/pull/6137#discussion_r545582503", "createdAt": "2020-12-18T05:32:27Z", "author": {"login": "Praveen2112"}, "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/schema/confluent/ConfluentModule.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.schema.confluent;\n+\n+import com.google.inject.Binder;\n+import com.google.inject.Module;\n+import com.google.inject.Provides;\n+import com.google.inject.Scopes;\n+import io.airlift.configuration.AbstractConfigurationAwareModule;\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.prestosql.decoder.DecoderModule;\n+import io.prestosql.decoder.RowDecoderFactory;\n+import io.prestosql.decoder.avro.AvroBytesDeserializer;\n+import io.prestosql.decoder.avro.AvroDeserializer;\n+import io.prestosql.decoder.avro.AvroReaderSupplier;\n+import io.prestosql.decoder.avro.AvroRowDecoderFactory;\n+import io.prestosql.plugin.kafka.SessionPropertiesProvider;\n+import io.prestosql.plugin.kafka.schema.ContentSchemaReader;\n+import io.prestosql.plugin.kafka.schema.TableDescriptionSupplier;\n+\n+import javax.inject.Singleton;\n+\n+import static com.google.inject.multibindings.MapBinder.newMapBinder;\n+import static com.google.inject.multibindings.Multibinder.newSetBinder;\n+import static io.airlift.configuration.ConfigBinder.configBinder;\n+\n+public class ConfluentModule\n+        extends AbstractConfigurationAwareModule\n+{\n+    @Override\n+    protected void setup(Binder binder)\n+    {\n+        configBinder(binder).bindConfig(ConfluentSchemaRegistryConfig.class);\n+        install(new DecoderModule(new ConfluentAvroModule()));\n+        binder.bind(ContentSchemaReader.class).to(AvroConfluentContentSchemaReader.class).in(Scopes.SINGLETON);\n+        newSetBinder(binder, SessionPropertiesProvider.class).addBinding().to(ConfluentSessionProperties.class).in(Scopes.SINGLETON);\n+        binder.bind(TableDescriptionSupplier.class).toProvider(ConfluentSchemaRegistryTableDescriptionSupplier.Factory.class).in(Scopes.SINGLETON);\n+    }\n+\n+    @Provides\n+    @Singleton\n+    public SchemaRegistryClient getSchemaRegistryClient(ConfluentSchemaRegistryConfig confluentConfig)\n+    {\n+        return new CachedSchemaRegistryClient(confluentConfig.getConfluentSchemaRegistryUrl(), confluentConfig.getConfluentSchemaRegistryClientCacheSize());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "23c78646d54e29a790d5091623678e5e8d93b027", "author": {"user": {"login": "elonazoulay", "name": "Elon Azoulay"}}, "url": "https://github.com/trinodb/trino/commit/23c78646d54e29a790d5091623678e5e8d93b027", "committedDate": "2020-12-18T08:29:06Z", "message": "Add TestingKafkaWithSchemaRegistry"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "fd0746002c0b1fc3bd938b95c4a39d30a8c1e924", "author": {"user": {"login": "elonazoulay", "name": "Elon Azoulay"}}, "url": "https://github.com/trinodb/trino/commit/fd0746002c0b1fc3bd938b95c4a39d30a8c1e924", "committedDate": "2020-12-18T20:14:13Z", "message": "Add AvroConfluentRowDecoder"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "13f713a8ceb36f988e974734772fe789de11ace0", "author": {"user": {"login": "elonazoulay", "name": "Elon Azoulay"}}, "url": "https://github.com/trinodb/trino/commit/13f713a8ceb36f988e974734772fe789de11ace0", "committedDate": "2020-12-18T20:14:13Z", "message": "Add AvroSchemaConverter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac134bf7ff66c2afdc1308f7e4aebd362f1289f4", "author": {"user": {"login": "elonazoulay", "name": "Elon Azoulay"}}, "url": "https://github.com/trinodb/trino/commit/ac134bf7ff66c2afdc1308f7e4aebd362f1289f4", "committedDate": "2020-12-18T20:14:13Z", "message": "Add content schema readers\n\nCreate an abstraction to allow different methods for\nreading kafka message schemas."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c4e774c64ab0870e7076bfd3df7933806e37048", "author": {"user": {"login": "elonazoulay", "name": "Elon Azoulay"}}, "url": "https://github.com/trinodb/trino/commit/9c4e774c64ab0870e7076bfd3df7933806e37048", "committedDate": "2020-12-18T20:14:13Z", "message": "Add SessionPropertiesProvider\n\nAllow multiple session property providers."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "da42c919777c5a62ded372c66fc2b31bc35a8bdb", "author": {"user": {"login": "elonazoulay", "name": "Elon Azoulay"}}, "url": "https://github.com/trinodb/trino/commit/da42c919777c5a62ded372c66fc2b31bc35a8bdb", "committedDate": "2020-12-18T20:14:13Z", "message": "Add ConfluentSchemaRegistryTableDescriptionSupplier"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "da42c919777c5a62ded372c66fc2b31bc35a8bdb", "author": {"user": {"login": "elonazoulay", "name": "Elon Azoulay"}}, "url": "https://github.com/trinodb/trino/commit/da42c919777c5a62ded372c66fc2b31bc35a8bdb", "committedDate": "2020-12-18T20:14:13Z", "message": "Add ConfluentSchemaRegistryTableDescriptionSupplier"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2420, "cost": 1, "resetAt": "2021-10-28T20:13:43Z"}}}