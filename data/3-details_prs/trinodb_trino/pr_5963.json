{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIxMDU3NzQ1", "number": 5963, "title": "Add pushdown smoke tests and refactor producer factory for Kafka connector", "bodyText": "This builds on top of #5838.", "createdAt": "2020-11-14T20:41:00Z", "url": "https://github.com/trinodb/trino/pull/5963", "merged": true, "mergeCommit": {"oid": "f60a220c38b0ba8689b2f9f5b00dac1e13511b93"}, "closed": true, "closedAt": "2020-11-21T22:50:08Z", "author": {"login": "hashhar"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABddFNO0gFqTUzMTMzMTM0OQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdeyPfOAH2gAyNTIxMDU3NzQ1OjBkN2ExYTY5ZTAyYjJmZjE4MThiODdmYTQzOGRiNTBjMjM2ZTA3M2Y=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMxMzMxMzQ5", "url": "https://github.com/trinodb/trino/pull/5963#pullrequestreview-531331349", "createdAt": "2020-11-16T13:45:01Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxMzo0NTowMVrOHz_UtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxMzo0NTowMVrOHz_UtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI3NjkxNg==", "bodyText": "I would prefer non-UTC timezone here. Or was it already discussed and we cannot for some reason?", "url": "https://github.com/trinodb/trino/pull/5963#discussion_r524276916", "createdAt": "2020-11-16T13:45:01Z", "author": {"login": "losipiuk"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/kafka/TestKafkaWritesSmokeTest.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.tests.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.tempto.ProductTest;\n+import io.prestosql.tempto.Requirement;\n+import io.prestosql.tempto.RequirementsProvider;\n+import io.prestosql.tempto.Requires;\n+import io.prestosql.tempto.configuration.Configuration;\n+import io.prestosql.tempto.fulfillment.table.kafka.KafkaTableDefinition;\n+import io.prestosql.tempto.fulfillment.table.kafka.ListKafkaDataSource;\n+import org.testng.annotations.Test;\n+\n+import java.sql.Date;\n+import java.sql.Time;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+\n+import static io.prestosql.tempto.assertions.QueryAssert.Row.row;\n+import static io.prestosql.tempto.assertions.QueryAssert.assertThat;\n+import static io.prestosql.tempto.fulfillment.table.TableRequirements.immutableTable;\n+import static io.prestosql.tempto.query.QueryExecutor.query;\n+import static io.prestosql.tests.TestGroups.KAFKA;\n+import static io.prestosql.tests.TestGroups.PROFILE_SPECIFIC_TESTS;\n+import static java.lang.String.format;\n+\n+public class TestKafkaWritesSmokeTest\n+        extends ProductTest\n+{\n+    private static final String KAFKA_CATALOG = \"kafka\";\n+    private static final String SCHEMA_NAME = \"product_tests\";\n+\n+    private static final String SIMPLE_KEY_AND_VALUE_TABLE_NAME = \"write_simple_key_and_value\";\n+    private static final String SIMPLE_KEY_AND_VALUE_TOPIC_NAME = \"write_simple_key_and_value\";\n+\n+    // Kafka connector requires tables to be predefined in Presto configuration\n+    // the requirements here will be used to verify that table actually exists and to\n+    // create topics\n+\n+    private static class SimpleKeyAndValueTable\n+            implements RequirementsProvider\n+    {\n+        @Override\n+        public Requirement getRequirements(Configuration configuration)\n+        {\n+            return immutableTable(new KafkaTableDefinition(\n+                    SCHEMA_NAME + \".\" + SIMPLE_KEY_AND_VALUE_TABLE_NAME,\n+                    SIMPLE_KEY_AND_VALUE_TOPIC_NAME,\n+                    new ListKafkaDataSource(ImmutableList.of()),\n+                    1,\n+                    1));\n+        }\n+    }\n+\n+    @Test(groups = {KAFKA, PROFILE_SPECIFIC_TESTS})\n+    @Requires(SimpleKeyAndValueTable.class)\n+    public void testInsertSimpleKeyAndValue()\n+    {\n+        assertThat(query(format(\n+                \"INSERT INTO %s.%s.%s VALUES \" +\n+                        \"('jasio', 1, 'ania', 2), \" +\n+                        \"('piotr', 3, 'kasia', 4)\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                SIMPLE_KEY_AND_VALUE_TABLE_NAME)))\n+                .updatedRowsCountIsEqualTo(2);\n+\n+        assertThat(query(format(\n+                \"SELECT * FROM %s.%s.%s\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                SIMPLE_KEY_AND_VALUE_TABLE_NAME)))\n+                .containsOnly(\n+                        row(\"jasio\", 1, \"ania\", 2),\n+                        row(\"piotr\", 3, \"kasia\", 4));\n+    }\n+\n+    private static final String ALL_DATATYPES_RAW_TABLE_NAME = \"write_all_datatypes_raw\";\n+    private static final String ALL_DATATYPES_RAW_TOPIC_NAME = \"write_all_datatypes_raw\";\n+\n+    private static class AllDataTypesRawTable\n+            implements RequirementsProvider\n+    {\n+        @Override\n+        public Requirement getRequirements(Configuration configuration)\n+        {\n+            return immutableTable(new KafkaTableDefinition(\n+                    SCHEMA_NAME + \".\" + ALL_DATATYPES_RAW_TABLE_NAME,\n+                    ALL_DATATYPES_RAW_TOPIC_NAME,\n+                    new ListKafkaDataSource(ImmutableList.of()),\n+                    1,\n+                    1));\n+        }\n+    }\n+\n+    @Test(groups = {KAFKA, PROFILE_SPECIFIC_TESTS})\n+    @Requires(AllDataTypesRawTable.class)\n+    public void testInsertRawTable()\n+    {\n+        // TODO RawRowEncoder doesn't take mapping length into considertion while writing so a\n+        //  BIGINT with dataFormat = BYTE takes up 8 bytes during write (as opposed to 1 byte\n+        //  during read) and hence a buffer overflow is possible before we are able to reach\n+        //  the end of row.\n+        assertThat(query(format(\n+                \"INSERT INTO %s.%s.%s VALUES \" +\n+                        \"('jasio', 9223372036854775807, 2147483647, 32767, 127, 1234567890.123456789, true), \" +\n+                        \"('piotr', -9223372036854775808, -2147483648, -32768, -128, -1234567890.123456789, false), \" +\n+                        \"('hasan', 9223372036854775807, 2147483647, 32767, 127, 1234567890.123456789, true), \" +\n+                        \"('kasia', -9223372036854775808, -2147483648, -32768, -128, -1234567890.123456789, false)\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                ALL_DATATYPES_RAW_TABLE_NAME)))\n+                .updatedRowsCountIsEqualTo(4);\n+\n+        assertThat(query(format(\n+                \"SELECT * FROM %s.%s.%s\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                ALL_DATATYPES_RAW_TABLE_NAME)))\n+                .containsOnly(\n+                        row(\"jasio\", 9223372036854775807L, 2147483647, 32767, 127, 1234567890.123456789, true),\n+                        row(\"piotr\", -9223372036854775808L, -2147483648, -32768, -128, -1234567890.123456789, false),\n+                        row(\"hasan\", 9223372036854775807L, 2147483647, 32767, 127, 1234567890.123456789, true),\n+                        row(\"kasia\", -9223372036854775808L, -2147483648, -32768, -128, -1234567890.123456789, false));\n+    }\n+\n+    private static final String ALL_DATATYPES_CSV_TABLE_NAME = \"write_all_datatypes_csv\";\n+    private static final String ALL_DATATYPES_CSV_TOPIC_NAME = \"write_all_datatypes_csv\";\n+\n+    private static class AllDataTypesCsvTable\n+            implements RequirementsProvider\n+    {\n+        @Override\n+        public Requirement getRequirements(Configuration configuration)\n+        {\n+            return immutableTable(new KafkaTableDefinition(\n+                    SCHEMA_NAME + \".\" + ALL_DATATYPES_CSV_TABLE_NAME,\n+                    ALL_DATATYPES_CSV_TOPIC_NAME,\n+                    new ListKafkaDataSource(ImmutableList.of()),\n+                    1,\n+                    1));\n+        }\n+    }\n+\n+    @Test(groups = {KAFKA, PROFILE_SPECIFIC_TESTS})\n+    @Requires(AllDataTypesCsvTable.class)\n+    public void testInsertCsvTable()\n+    {\n+        assertThat(query(format(\n+                \"INSERT INTO %s.%s.%s VALUES \" +\n+                        \"('jasio', 9223372036854775807, 2147483647, 32767, 127, 1234567890.123456789, true), \" +\n+                        \"('stasio', -9223372036854775808, -2147483648, -32768, -128, -1234567890.123456789, false), \" +\n+                        \"(null, null, null, null, null, null, null), \" +\n+                        \"('krzysio', 9223372036854775807, 2147483647, 32767, 127, 1234567890.123456789, false), \" +\n+                        \"('kasia', 9223372036854775807, 2147483647, 32767, null, null, null)\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                ALL_DATATYPES_CSV_TABLE_NAME)))\n+                .updatedRowsCountIsEqualTo(5);\n+\n+        assertThat(query(format(\n+                \"SELECT * FROM %s.%s.%s\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                ALL_DATATYPES_CSV_TABLE_NAME)))\n+                .containsOnly(\n+                        row(\"jasio\", 9223372036854775807L, 2147483647, 32767, 127, 1234567890.123456789, true),\n+                        row(\"stasio\", -9223372036854775808L, -2147483648, -32768, -128, -1234567890.123456789, false),\n+                        row(null, null, null, null, null, null, null),\n+                        row(\"krzysio\", 9223372036854775807L, 2147483647, 32767, 127, 1234567890.123456789, false),\n+                        row(\"kasia\", 9223372036854775807L, 2147483647, 32767, null, null, null));\n+    }\n+\n+    private static final String ALL_DATATYPES_JSON_TABLE_NAME = \"write_all_datatypes_json\";\n+    private static final String ALL_DATATYPES_JSON_TOPIC_NAME = \"write_all_datatypes_json\";\n+\n+    private static class AllDataTypesJsonTable\n+            implements RequirementsProvider\n+    {\n+        @Override\n+        public Requirement getRequirements(Configuration configuration)\n+        {\n+            return immutableTable(new KafkaTableDefinition(\n+                    SCHEMA_NAME + \".\" + ALL_DATATYPES_JSON_TABLE_NAME,\n+                    ALL_DATATYPES_JSON_TOPIC_NAME,\n+                    new ListKafkaDataSource(ImmutableList.of()),\n+                    1,\n+                    1));\n+        }\n+    }\n+\n+    @Test(groups = {KAFKA, PROFILE_SPECIFIC_TESTS})\n+    @Requires(AllDataTypesJsonTable.class)\n+    public void testInsertJsonTable()\n+    {\n+        assertThat(query(format(\n+                \"INSERT INTO %s.%s.%s VALUES (\" +\n+                        \"'ala ma kota',\" +\n+                        \"9223372036854775807,\" +\n+                        \"2147483647,\" +\n+                        \"32767,\" +\n+                        \"127,\" +\n+                        \"1234567890.123456789,\" +\n+                        \"true,\" +\n+                        \"TIMESTAMP '2018-02-09 13:15:16',\" +\n+                        \"TIMESTAMP '2018-02-09 13:15:17',\" +\n+                        \"TIMESTAMP '2018-02-09 13:15:18',\" +\n+                        \"TIMESTAMP '2018-02-09 13:15:19',\" +\n+                        \"TIMESTAMP '2018-02-09 13:15:20',\" +\n+                        \"DATE '2018-02-11',\" +\n+                        \"DATE '2018-02-13',\" +\n+                        \"TIME '13:15:16',\" +\n+                        \"TIME '13:15:17',\" +\n+                        \"TIME '13:15:18',\" +\n+                        \"TIME '13:15:20',\" +\n+                        \"TIMESTAMP '2018-02-09 13:15:18 UTC',\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 230}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMxMzM3MDcy", "url": "https://github.com/trinodb/trino/pull/5963#pullrequestreview-531337072", "createdAt": "2020-11-16T13:51:52Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxMzo1MTo1MlrOHz_nCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxMzo1MTo1MlrOHz_nCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDI4MTYwOQ==", "bodyText": "I think we decided to skip that. If we want a followup please create an issue for that and add as //todo parameter.\nDrop whole comment otherwise.", "url": "https://github.com/trinodb/trino/pull/5963#discussion_r524281609", "createdAt": "2020-11-16T13:51:52Z", "author": {"login": "losipiuk"}, "path": "presto-product-tests/src/main/java/io/prestosql/tests/kafka/TestKafkaPushdownSmokeTest.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.tests.kafka;\n+\n+import com.google.common.collect.ImmutableList;\n+import io.prestosql.tempto.ProductTest;\n+import io.prestosql.tempto.Requirement;\n+import io.prestosql.tempto.RequirementsProvider;\n+import io.prestosql.tempto.Requires;\n+import io.prestosql.tempto.configuration.Configuration;\n+import io.prestosql.tempto.fulfillment.table.kafka.KafkaMessage;\n+import io.prestosql.tempto.fulfillment.table.kafka.KafkaTableDefinition;\n+import io.prestosql.tempto.fulfillment.table.kafka.ListKafkaDataSource;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.LongStream;\n+\n+import static io.prestosql.tempto.assertions.QueryAssert.Row.row;\n+import static io.prestosql.tempto.assertions.QueryAssert.assertThat;\n+import static io.prestosql.tempto.fulfillment.table.TableRequirements.immutableTable;\n+import static io.prestosql.tempto.fulfillment.table.kafka.KafkaMessageContentsBuilder.contentsBuilder;\n+import static io.prestosql.tempto.query.QueryExecutor.query;\n+import static io.prestosql.tests.TestGroups.KAFKA;\n+import static io.prestosql.tests.TestGroups.PROFILE_SPECIFIC_TESTS;\n+import static java.lang.String.format;\n+\n+public class TestKafkaPushdownSmokeTest\n+        extends ProductTest\n+{\n+    private static final String KAFKA_CATALOG = \"kafka\";\n+    private static final String SCHEMA_NAME = \"product_tests\";\n+\n+    private static final long NUM_MESSAGES = 1000;\n+    private static final long TIMESTAMP_NUM_MESSAGES = 10;\n+\n+    private static final String PUSHDOWN_PARTITION_TABLE_NAME = \"pushdown_partition\";\n+    private static final String PUSHDOWN_PARTITION_TOPIC_NAME = \"pushdown_partition\";\n+\n+    private static final String PUSHDOWN_OFFSET_TABLE_NAME = \"pushdown_offset\";\n+    private static final String PUSHDOWN_OFFSET_TOPIC_NAME = \"pushdown_offset\";\n+\n+    private static final String PUSHDOWN_CREATE_TIME_TABLE_NAME = \"pushdown_create_time\";\n+    private static final String PUSHDOWN_CREATE_TIME_TOPIC_NAME = \"pushdown_create_time\";\n+\n+    // Kafka connector requires tables to be predefined in Presto configuration\n+    // the code here will be used to verify that table actually exists and to\n+    // create topics and insert test data\n+\n+    private static class PushdownPartitionTable\n+            implements RequirementsProvider\n+    {\n+        @Override\n+        public Requirement getRequirements(Configuration configuration)\n+        {\n+            List<KafkaMessage> records = LongStream.rangeClosed(1, NUM_MESSAGES)\n+                    .boxed()\n+                    .map(i -> new KafkaMessage(\n+                            // only two possible keys to ensure each partition has NUM_MESSAGES/2 messages\n+                            contentsBuilder().appendUTF8(format(\"%s\", i % 2)).build(),\n+                            contentsBuilder().appendUTF8(format(\"%s\", i)).build()))\n+                    .collect(Collectors.toList());\n+\n+            return immutableTable(new KafkaTableDefinition(\n+                    SCHEMA_NAME + \".\" + PUSHDOWN_PARTITION_TABLE_NAME,\n+                    PUSHDOWN_PARTITION_TOPIC_NAME,\n+                    new ListKafkaDataSource(records),\n+                    2,\n+                    1));\n+        }\n+    }\n+\n+    @Test(groups = {KAFKA, PROFILE_SPECIFIC_TESTS})\n+    @Requires(PushdownPartitionTable.class)\n+    public void testPartitionPushdown()\n+    {\n+        // TODO Assert from the query stats that only NUM_MESSAGES / 2 records were read\n+        assertThat(query(format(\n+                \"SELECT COUNT(*) FROM %s.%s.%s WHERE _partition_id = 1\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_PARTITION_TABLE_NAME)))\n+                .hasAnyRows();\n+    }\n+\n+    private static class PushdownOffsetTable\n+            implements RequirementsProvider\n+    {\n+        @Override\n+        public Requirement getRequirements(Configuration configuration)\n+        {\n+            List<KafkaMessage> records = LongStream.rangeClosed(1, NUM_MESSAGES)\n+                    .boxed()\n+                    .map(i -> new KafkaMessage(\n+                            // only two possible keys to ensure each partition has NUM_MESSAGES/2 messages\n+                            contentsBuilder().appendUTF8(format(\"%s\", i % 2)).build(),\n+                            contentsBuilder().appendUTF8(format(\"%s\", i)).build()))\n+                    .collect(Collectors.toList());\n+\n+            return immutableTable(new KafkaTableDefinition(\n+                    SCHEMA_NAME + \".\" + PUSHDOWN_OFFSET_TABLE_NAME,\n+                    PUSHDOWN_OFFSET_TOPIC_NAME,\n+                    new ListKafkaDataSource(records),\n+                    2,\n+                    1));\n+        }\n+    }\n+\n+    @Test(groups = {KAFKA, PROFILE_SPECIFIC_TESTS})\n+    @Requires(PushdownOffsetTable.class)\n+    public void testOffsetPushdown()\n+    {\n+        // TODO Assert from the query stats that only 10 messages were read\n+        assertThat(query(format(\n+                \"SELECT COUNT(*) FROM %s.%s.%s WHERE _partition_offset BETWEEN 6 AND 10\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_OFFSET_TABLE_NAME)))\n+                .containsExactly(row(10));\n+\n+        // TODO Assert from the query stats that only 8 messages were read\n+        assertThat(query(format(\n+                \"SELECT COUNT(*) FROM %s.%s.%s WHERE _partition_offset > 5 AND _partition_offset < 10\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_OFFSET_TABLE_NAME)))\n+                .containsExactly(row(8));\n+\n+        // TODO Assert from the query stats that only 12 messages were read\n+        assertThat(query(format(\n+                \"SELECT COUNT(*) FROM %s.%s.%s WHERE _partition_offset >= 5 AND _partition_offset <= 10\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_OFFSET_TABLE_NAME)))\n+                .containsExactly(row(12));\n+\n+        // TODO Assert from the query stats that only 10 messages were read\n+        assertThat(query(format(\n+                \"SELECT COUNT(*) FROM %s.%s.%s WHERE _partition_offset >= 5 AND _partition_offset < 10\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_OFFSET_TABLE_NAME)))\n+                .containsExactly(row(10));\n+\n+        // TODO Assert from the query stats that only 10 messages were read\n+        assertThat(query(format(\n+                \"SELECT COUNT(*) FROM %s.%s.%s WHERE _partition_offset > 5 AND _partition_offset <= 10\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_OFFSET_TABLE_NAME)))\n+                .containsExactly(row(10));\n+\n+        // TODO Assert from the query stats that only 2 messages were read\n+        assertThat(query(format(\n+                \"SELECT COUNT(*) FROM %s.%s.%s WHERE _partition_offset = 5\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_OFFSET_TABLE_NAME)))\n+                .containsExactly(row(2));\n+    }\n+\n+    private static class PushdownCreateTimeTable\n+            implements RequirementsProvider\n+    {\n+        @Override\n+        public Requirement getRequirements(Configuration configuration)\n+        {\n+            return immutableTable(new KafkaTableDefinition(\n+                    SCHEMA_NAME + \".\" + PUSHDOWN_CREATE_TIME_TABLE_NAME,\n+                    PUSHDOWN_CREATE_TIME_TOPIC_NAME,\n+                    new ListKafkaDataSource(ImmutableList.of()),\n+                    1,\n+                    1));\n+        }\n+    }\n+\n+    @Test(groups = {KAFKA, PROFILE_SPECIFIC_TESTS})\n+    @Requires(PushdownCreateTimeTable.class)\n+    public void testCreateTimePushdown()\n+            throws InterruptedException\n+    {\n+        // Ensure a spread of at-least TIMESTAMP_NUM_MESSAGES * 100 milliseconds\n+        for (int i = 1; i <= TIMESTAMP_NUM_MESSAGES; i++) {\n+            query(format(\"INSERT INTO %s.%s.%s (bigint_key, bigint_value) VALUES (%s, %s)\",\n+                    KAFKA_CATALOG, SCHEMA_NAME, PUSHDOWN_CREATE_TIME_TABLE_NAME, i, i));\n+            Thread.sleep(100);\n+        }\n+\n+        long startKey = 4;\n+        long endKey = 6;\n+        List<List<?>> rows = query(format(\n+                \"SELECT CAST(_timestamp AS VARCHAR) FROM %s.%s.%s WHERE bigint_key IN (\" + startKey + \", \" + endKey + \") ORDER BY bigint_key\",\n+                KAFKA_CATALOG,\n+                SCHEMA_NAME,\n+                PUSHDOWN_CREATE_TIME_TABLE_NAME))\n+                .rows();\n+        String startTime = (String) rows.get(0).get(0);\n+        String endTime = (String) rows.get(1).get(0);\n+\n+        // TODO Assert from the query stats that only TIMESTAMP_NUM_MESSAGES - startKey messages were read because upper bound is not pushed down", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 212}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMxMzM5OTYw", "url": "https://github.com/trinodb/trino/pull/5963#pullrequestreview-531339960", "createdAt": "2020-11-16T13:55:15Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff4a2e65a536da812f960069f4d9f5a42fbe19f5", "author": {"user": {"login": "hashhar", "name": "Ashhar Hasan"}}, "url": "https://github.com/trinodb/trino/commit/ff4a2e65a536da812f960069f4d9f5a42fbe19f5", "committedDate": "2020-11-21T20:42:25Z", "message": "Make naming regarding Kafka read tests more explicit"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "001a4f2333e2112285618f8aea36395b62e312da", "author": {"user": {"login": "hashhar", "name": "Ashhar Hasan"}}, "url": "https://github.com/trinodb/trino/commit/001a4f2333e2112285618f8aea36395b62e312da", "committedDate": "2020-11-21T20:42:25Z", "message": "Fix Kafka Avro encoder to use mappings\n\nFixes #5791."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c41498b51391b8f17367b6eb6e69081185349a90", "author": {"user": {"login": "hashhar", "name": "Ashhar Hasan"}}, "url": "https://github.com/trinodb/trino/commit/c41498b51391b8f17367b6eb6e69081185349a90", "committedDate": "2020-11-21T20:42:25Z", "message": "Fix Kafka CSV, JSON and RAW encoders to use mappings\n\nSee https://github.com/prestosql/presto/issues/5791."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ce275e9b4da376e4dd9b8fafe0ae808940cc6226", "author": {"user": {"login": "hashhar", "name": "Ashhar Hasan"}}, "url": "https://github.com/trinodb/trino/commit/ce275e9b4da376e4dd9b8fafe0ae808940cc6226", "committedDate": "2020-11-21T20:47:08Z", "message": "Add Kafka pushdown smoke tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b4a8ba5306fde220b505c269942f022df8ee9ba", "author": {"user": {"login": "hashhar", "name": "Ashhar Hasan"}}, "url": "https://github.com/trinodb/trino/commit/4b4a8ba5306fde220b505c269942f022df8ee9ba", "committedDate": "2020-11-21T20:47:08Z", "message": "Refactor Kafka Producer factory"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0d7a1a69e02b2ff1818b87fa438db50c236e073f", "author": {"user": {"login": "hashhar", "name": "Ashhar Hasan"}}, "url": "https://github.com/trinodb/trino/commit/0d7a1a69e02b2ff1818b87fa438db50c236e073f", "committedDate": "2020-11-21T20:47:08Z", "message": "Refactor Kafka Admin factory"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2567, "cost": 1, "resetAt": "2021-10-28T20:13:43Z"}}}