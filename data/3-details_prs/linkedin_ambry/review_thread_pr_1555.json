{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMxMzYxMTMy", "number": 1555, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNTozOToxOFrOEIOXgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNTowODowNVrOEKv0Ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MDU5NDU4OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNTozOToxOVrOGoDnmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMjowMzo1NlrOGokswA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1NTUxNQ==", "bodyText": "Do we need to check if replicasStr is null in this situation?", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r444655515", "createdAt": "2020-06-24T05:39:19Z", "author": {"login": "SophieGuo410"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "diffHunk": "@@ -33,82 +40,146 @@\n public class InstanceConfigToDataNodeConfigAdapter implements DataNodeConfigSource {\n   private static final Logger LOGGER = LoggerFactory.getLogger(InstanceConfigToDataNodeConfigAdapter.class);\n   private final HelixManager helixManager;\n-  private final ClusterMapConfig clusterMapConfig;\n+  private final Converter converter;\n+  private final String clusterName;\n+  private volatile HelixAdmin helixAdmin = null;\n \n   /**\n    * @param helixManager the {@link HelixManager} to use as the source of truth for {@link InstanceConfig}s.\n    * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n    */\n   public InstanceConfigToDataNodeConfigAdapter(HelixManager helixManager, ClusterMapConfig clusterMapConfig) {\n     this.helixManager = helixManager;\n-    this.clusterMapConfig = clusterMapConfig;\n+    this.converter = new Converter(clusterMapConfig);\n+    clusterName = clusterMapConfig.clusterMapClusterName;\n   }\n \n   @Override\n   public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n     helixManager.addInstanceConfigChangeListener((InstanceConfigChangeListener) (instanceConfigs, context) -> {\n       Iterable<DataNodeConfig> dataNodeConfigs =\n-          () -> instanceConfigs.stream().map(this::convert).filter(Objects::nonNull).iterator();\n+          () -> instanceConfigs.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n       listener.onDataNodeConfigChange(dataNodeConfigs);\n     });\n   }\n \n-  /**\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  private DataNodeConfig convert(InstanceConfig instanceConfig) {\n-    return convert(instanceConfig, clusterMapConfig);\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    InstanceConfig instanceConfig = converter.convert(config);\n+    return getHelixAdmin().setInstanceConfig(clusterName, instanceConfig.getInstanceName(), instanceConfig);\n   }\n \n-  /**\n-   * Exposed for testing.\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @param clusterMapConfig the {@link ClusterMapConfig} containing any default values that may be needed.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  static DataNodeConfig convert(InstanceConfig instanceConfig, ClusterMapConfig clusterMapConfig) {\n-    int schemaVersion = getSchemaVersion(instanceConfig);\n-    if (schemaVersion != 0) {\n-      LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n-      return null;\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    InstanceConfig instanceConfig = getHelixAdmin().getInstanceConfig(clusterName, instanceName);\n+    return instanceConfig != null ? converter.convert(instanceConfig) : null;\n+  }\n+\n+  void setHelixAdmin(HelixAdmin helixAdmin) {\n+    this.helixAdmin = helixAdmin;\n+  }\n+\n+  private HelixAdmin getHelixAdmin() {\n+    return Objects.requireNonNull(helixAdmin, \"helixAdmin not set\");\n+  }\n+\n+  static class Converter {\n+    private final ClusterMapConfig clusterMapConfig;\n+\n+    Converter(ClusterMapConfig clusterMapConfig) {\n+      this.clusterMapConfig = clusterMapConfig;\n     }\n-    DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n-        Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n-        getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n-    dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n-    dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n-    // TODO uncomment this line once 1534 is merged\n-    // dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n-    instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n-      if (diskProps.get(DISK_STATE) == null) {\n-        // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n-        // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n-        // state to InstanceConfig.\n-        LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n-            instanceConfig.getInstanceName());\n-      } else {\n-        DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n-            diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n-            Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n-        String replicasStr = diskProps.get(REPLICAS_STR);\n-        if (!replicasStr.isEmpty()) {\n-          for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n-            String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n-            // partition name and replica name are the same.\n-            String partitionName = replicaStrParts[0];\n-            long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n-            String partitionClass =\n-                replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n-            disk.getReplicaConfigs()\n-                .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+\n+    /**\n+     * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported\n+     *         schema version.\n+     */\n+    DataNodeConfig convert(InstanceConfig instanceConfig) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n+        return null;\n+      }\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n+          Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n+          getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n+      instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n+        if (diskProps.get(DISK_STATE) == null) {\n+          // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n+          // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n+          // state to InstanceConfig.\n+          LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n+              instanceConfig.getInstanceName());\n+        } else {\n+          DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n+              diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n+              Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n+          String replicasStr = diskProps.get(REPLICAS_STR);\n+          if (!replicasStr.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9dddd29c9c1d77dcdb40814f290aa0aad25523f5"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE5NzUwNA==", "bodyText": "That is a good idea. Updated to do that and added more unit tests.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r445197504", "createdAt": "2020-06-24T22:03:56Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "diffHunk": "@@ -33,82 +40,146 @@\n public class InstanceConfigToDataNodeConfigAdapter implements DataNodeConfigSource {\n   private static final Logger LOGGER = LoggerFactory.getLogger(InstanceConfigToDataNodeConfigAdapter.class);\n   private final HelixManager helixManager;\n-  private final ClusterMapConfig clusterMapConfig;\n+  private final Converter converter;\n+  private final String clusterName;\n+  private volatile HelixAdmin helixAdmin = null;\n \n   /**\n    * @param helixManager the {@link HelixManager} to use as the source of truth for {@link InstanceConfig}s.\n    * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n    */\n   public InstanceConfigToDataNodeConfigAdapter(HelixManager helixManager, ClusterMapConfig clusterMapConfig) {\n     this.helixManager = helixManager;\n-    this.clusterMapConfig = clusterMapConfig;\n+    this.converter = new Converter(clusterMapConfig);\n+    clusterName = clusterMapConfig.clusterMapClusterName;\n   }\n \n   @Override\n   public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n     helixManager.addInstanceConfigChangeListener((InstanceConfigChangeListener) (instanceConfigs, context) -> {\n       Iterable<DataNodeConfig> dataNodeConfigs =\n-          () -> instanceConfigs.stream().map(this::convert).filter(Objects::nonNull).iterator();\n+          () -> instanceConfigs.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n       listener.onDataNodeConfigChange(dataNodeConfigs);\n     });\n   }\n \n-  /**\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  private DataNodeConfig convert(InstanceConfig instanceConfig) {\n-    return convert(instanceConfig, clusterMapConfig);\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    InstanceConfig instanceConfig = converter.convert(config);\n+    return getHelixAdmin().setInstanceConfig(clusterName, instanceConfig.getInstanceName(), instanceConfig);\n   }\n \n-  /**\n-   * Exposed for testing.\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @param clusterMapConfig the {@link ClusterMapConfig} containing any default values that may be needed.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  static DataNodeConfig convert(InstanceConfig instanceConfig, ClusterMapConfig clusterMapConfig) {\n-    int schemaVersion = getSchemaVersion(instanceConfig);\n-    if (schemaVersion != 0) {\n-      LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n-      return null;\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    InstanceConfig instanceConfig = getHelixAdmin().getInstanceConfig(clusterName, instanceName);\n+    return instanceConfig != null ? converter.convert(instanceConfig) : null;\n+  }\n+\n+  void setHelixAdmin(HelixAdmin helixAdmin) {\n+    this.helixAdmin = helixAdmin;\n+  }\n+\n+  private HelixAdmin getHelixAdmin() {\n+    return Objects.requireNonNull(helixAdmin, \"helixAdmin not set\");\n+  }\n+\n+  static class Converter {\n+    private final ClusterMapConfig clusterMapConfig;\n+\n+    Converter(ClusterMapConfig clusterMapConfig) {\n+      this.clusterMapConfig = clusterMapConfig;\n     }\n-    DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n-        Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n-        getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n-    dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n-    dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n-    // TODO uncomment this line once 1534 is merged\n-    // dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n-    instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n-      if (diskProps.get(DISK_STATE) == null) {\n-        // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n-        // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n-        // state to InstanceConfig.\n-        LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n-            instanceConfig.getInstanceName());\n-      } else {\n-        DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n-            diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n-            Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n-        String replicasStr = diskProps.get(REPLICAS_STR);\n-        if (!replicasStr.isEmpty()) {\n-          for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n-            String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n-            // partition name and replica name are the same.\n-            String partitionName = replicaStrParts[0];\n-            long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n-            String partitionClass =\n-                replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n-            disk.getReplicaConfigs()\n-                .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+\n+    /**\n+     * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported\n+     *         schema version.\n+     */\n+    DataNodeConfig convert(InstanceConfig instanceConfig) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n+        return null;\n+      }\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n+          Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n+          getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n+      instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n+        if (diskProps.get(DISK_STATE) == null) {\n+          // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n+          // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n+          // state to InstanceConfig.\n+          LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n+              instanceConfig.getInstanceName());\n+        } else {\n+          DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n+              diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n+              Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n+          String replicasStr = diskProps.get(REPLICAS_STR);\n+          if (!replicasStr.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1NTUxNQ=="}, "originalCommit": {"oid": "9dddd29c9c1d77dcdb40814f290aa0aad25523f5"}, "originalPosition": 149}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3ODcwMDgwOnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/DataNodeConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwNDoxOTozOFrOGpTLqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwNDoxOTozOFrOGpTLqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk1OTA4Mw==", "bodyText": "minor: can be removed", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r445959083", "createdAt": "2020-06-26T04:19:38Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/DataNodeConfig.java", "diffHunk": "@@ -15,9 +15,11 @@\n \n package com.github.ambry.clustermap;\n \n+import java.util.Comparator;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3ODc2Njc5OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwNTowNDo0NFrOGpTyiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwMzo0ODowN1rOGr8cZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2OTAzMw==", "bodyText": "Looks like we will change the format of mount path in PropertyStore record.  Unlike InstanceConfig (which is a pure mount path), here we add a prefix mount- ahead of each mount path. I just wonder why not use the same way in InstanceConfigToDataNodeConfigAdapter to determine if this is a valid mount path.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r445969033", "createdAt": "2020-06-26T05:04:44Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *\n+ */\n+\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import com.github.ambry.utils.Utils;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.helix.AccessOption;\n+import org.apache.helix.store.HelixPropertyListener;\n+import org.apache.helix.store.HelixPropertyStore;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+import org.apache.zookeeper.data.Stat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * An implementation of {@link DataNodeConfigSource} that reads {@link ZNRecord}s stored under the\n+ * \"/DataNodeConfigs\" path in a {@link HelixPropertyStore}.\n+ */\n+public class PropertyStoreToDataNodeConfigAdapter implements DataNodeConfigSource {\n+  static final String CONFIG_PATH = \"/DataNodeConfigs\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(PropertyStoreToDataNodeConfigAdapter.class);\n+  private final HelixPropertyStore<ZNRecord> propertyStore;\n+  private final Converter converter;\n+  private final Executor eventExecutor;\n+\n+  /**\n+   * @param propertyStore the {@link HelixPropertyStore} instance to use to interact with zookeeper.\n+   * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n+   * @param dcName the datacenter name for for the property store.\n+   */\n+  public PropertyStoreToDataNodeConfigAdapter(HelixPropertyStore<ZNRecord> propertyStore,\n+      ClusterMapConfig clusterMapConfig, String dcName) {\n+    this.propertyStore = propertyStore;\n+    this.converter = new Converter(clusterMapConfig.clusterMapDefaultPartitionClass, dcName);\n+    this.eventExecutor = Executors.newSingleThreadExecutor();\n+  }\n+\n+  @Override\n+  public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) {\n+    Subscription subscription = new Subscription(listener);\n+    propertyStore.subscribe(CONFIG_PATH, subscription);\n+    subscription.start();\n+  }\n+\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    ZNRecord record = converter.convert(config);\n+    String path = CONFIG_PATH + \"/\" + record.getId();\n+    return propertyStore.set(path, record, AccessOption.PERSISTENT);\n+  }\n+\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    String path = CONFIG_PATH + \"/\" + instanceName;\n+    ZNRecord record = propertyStore.get(path, new Stat(), AccessOption.PERSISTENT);\n+    return record != null ? converter.convert(record) : null;\n+  }\n+\n+  private class Subscription implements HelixPropertyListener {\n+    private final DataNodeConfigChangeListener listener;\n+    private final ReentrantLock lock = new ReentrantLock();\n+    // reading/writing this variable should be protected by the lock.\n+    private boolean initialized = false;\n+\n+    Subscription(DataNodeConfigChangeListener listener) {\n+      this.listener = listener;\n+    }\n+\n+    /**\n+     * Tell the background executor to perform any initialization needed. This should be called after registering this\n+     * listener using {@link HelixPropertyStore#subscribe}. This will ensure that the first event sent to\n+     * {@link #listener} will contain the entire set of current configs.\n+     */\n+    void start() {\n+      eventExecutor.execute(this::initializeIfNeeded);\n+    }\n+\n+    @Override\n+    public void onDataChange(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} changed\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataCreate(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} created\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataDelete(String path) {\n+      // TODO handle node deletions dynamically. Doing so requires further work in the ClusterChangeHandler impl\n+      LOGGER.info(\"DataNodeConfig path {} deleted. This requires a restart to handle\", path);\n+    }\n+\n+    private void onPathChange(String path) {\n+      eventExecutor.execute(() -> updateOrInitialize(path));\n+    }\n+\n+    private Iterable<DataNodeConfig> lazyIterable(Collection<ZNRecord> records) {\n+      return () -> records.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n+    }\n+\n+    /**\n+     * Either perform a full initialization or notify the listener about an incremental update to a path. This should be\n+     * called from the event executor thread.\n+     * @param changedPath the path that changed.\n+     */\n+    private void updateOrInitialize(String changedPath) {\n+      lock.lock();\n+      try {\n+        if (!initializeIfNeeded()) {\n+          ZNRecord record = propertyStore.get(changedPath, null, AccessOption.PERSISTENT);\n+          if (record != null) {\n+            listener.onDataNodeConfigChange(lazyIterable(Collections.singleton(record)));\n+          } else {\n+            LOGGER.info(\"DataNodeConfig at path {} not found\", changedPath);\n+          }\n+        }\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig change\", e);\n+      } finally {\n+        lock.unlock();\n+      }\n+    }\n+\n+    /**\n+     * If initialization has not yet been completed succesfully, attempt an initialization. This should be called from\n+     * the event executor thread.\n+     * @return {@code true} if a successful full initialization (whether successful or not) was attempted in this call.\n+     */\n+    private boolean initializeIfNeeded() {\n+      lock.lock();\n+      try {\n+        if (initialized) {\n+          return false;\n+        }\n+        List<ZNRecord> records = propertyStore.getChildren(CONFIG_PATH, null, AccessOption.PERSISTENT);\n+        listener.onDataNodeConfigChange(lazyIterable(records));\n+        initialized = true;\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig initialization\", e);\n+      } finally {\n+        lock.unlock();\n+      }\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Convert between {@link DataNodeConfig}s and {@link ZNRecord}s.\n+   */\n+  static class Converter {\n+    private static final int VERSION_0 = 0;\n+    private static final String MOUNT_PREFIX = \"mount-\";\n+    private static final String HOSTNAME_FIELD = \"hostname\";\n+    private static final String PORT_FIELD = \"port\";\n+    private final String defaultPartitionClass;\n+    private final String dcName;\n+\n+    /**\n+     * @param defaultPartitionClass the default partition class for these nodes.\n+     * @param dcName the datacenter name for these nodes.\n+     */\n+    Converter(String defaultPartitionClass, String dcName) {\n+      this.defaultPartitionClass = defaultPartitionClass;\n+      this.dcName = dcName;\n+    }\n+\n+    /**\n+     * @param record the {@link ZNRecord} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link ZNRecord} provided has an unsupported schema\n+     *         version.\n+     */\n+    DataNodeConfig convert(ZNRecord record) {\n+      int schemaVersion = getSchemaVersion(record);\n+      if (schemaVersion != VERSION_0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, record);\n+        return null;\n+      }\n+\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(record.getId(), record.getSimpleField(HOSTNAME_FIELD),\n+          record.getIntField(PORT_FIELD, DataNodeId.UNKNOWN_PORT), dcName, getSslPortStr(record),\n+          getHttp2PortStr(record), getRackId(record), DEFAULT_XID);\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(record));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(record));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(record));\n+      record.getMapFields().forEach((key, diskProps) -> {\n+        if (key.startsWith(MOUNT_PREFIX)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIwNzIxNw==", "bodyText": "The way that we had it would work correctly too, but I felt that was a little bit of a hacky approach since it checked the content inside of the map field. My thought around adding a prefix was that it would give us more flexibility to add other types of map fields in the future.\nSince there are a few other changes to the schema, like the removal of datacenter name and xid, I thought this was a good time to make changes. However, if you prefer the shorter name, I could go back to that too.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r447207217", "createdAt": "2020-06-29T19:38:08Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *\n+ */\n+\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import com.github.ambry.utils.Utils;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.helix.AccessOption;\n+import org.apache.helix.store.HelixPropertyListener;\n+import org.apache.helix.store.HelixPropertyStore;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+import org.apache.zookeeper.data.Stat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * An implementation of {@link DataNodeConfigSource} that reads {@link ZNRecord}s stored under the\n+ * \"/DataNodeConfigs\" path in a {@link HelixPropertyStore}.\n+ */\n+public class PropertyStoreToDataNodeConfigAdapter implements DataNodeConfigSource {\n+  static final String CONFIG_PATH = \"/DataNodeConfigs\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(PropertyStoreToDataNodeConfigAdapter.class);\n+  private final HelixPropertyStore<ZNRecord> propertyStore;\n+  private final Converter converter;\n+  private final Executor eventExecutor;\n+\n+  /**\n+   * @param propertyStore the {@link HelixPropertyStore} instance to use to interact with zookeeper.\n+   * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n+   * @param dcName the datacenter name for for the property store.\n+   */\n+  public PropertyStoreToDataNodeConfigAdapter(HelixPropertyStore<ZNRecord> propertyStore,\n+      ClusterMapConfig clusterMapConfig, String dcName) {\n+    this.propertyStore = propertyStore;\n+    this.converter = new Converter(clusterMapConfig.clusterMapDefaultPartitionClass, dcName);\n+    this.eventExecutor = Executors.newSingleThreadExecutor();\n+  }\n+\n+  @Override\n+  public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) {\n+    Subscription subscription = new Subscription(listener);\n+    propertyStore.subscribe(CONFIG_PATH, subscription);\n+    subscription.start();\n+  }\n+\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    ZNRecord record = converter.convert(config);\n+    String path = CONFIG_PATH + \"/\" + record.getId();\n+    return propertyStore.set(path, record, AccessOption.PERSISTENT);\n+  }\n+\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    String path = CONFIG_PATH + \"/\" + instanceName;\n+    ZNRecord record = propertyStore.get(path, new Stat(), AccessOption.PERSISTENT);\n+    return record != null ? converter.convert(record) : null;\n+  }\n+\n+  private class Subscription implements HelixPropertyListener {\n+    private final DataNodeConfigChangeListener listener;\n+    private final ReentrantLock lock = new ReentrantLock();\n+    // reading/writing this variable should be protected by the lock.\n+    private boolean initialized = false;\n+\n+    Subscription(DataNodeConfigChangeListener listener) {\n+      this.listener = listener;\n+    }\n+\n+    /**\n+     * Tell the background executor to perform any initialization needed. This should be called after registering this\n+     * listener using {@link HelixPropertyStore#subscribe}. This will ensure that the first event sent to\n+     * {@link #listener} will contain the entire set of current configs.\n+     */\n+    void start() {\n+      eventExecutor.execute(this::initializeIfNeeded);\n+    }\n+\n+    @Override\n+    public void onDataChange(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} changed\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataCreate(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} created\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataDelete(String path) {\n+      // TODO handle node deletions dynamically. Doing so requires further work in the ClusterChangeHandler impl\n+      LOGGER.info(\"DataNodeConfig path {} deleted. This requires a restart to handle\", path);\n+    }\n+\n+    private void onPathChange(String path) {\n+      eventExecutor.execute(() -> updateOrInitialize(path));\n+    }\n+\n+    private Iterable<DataNodeConfig> lazyIterable(Collection<ZNRecord> records) {\n+      return () -> records.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n+    }\n+\n+    /**\n+     * Either perform a full initialization or notify the listener about an incremental update to a path. This should be\n+     * called from the event executor thread.\n+     * @param changedPath the path that changed.\n+     */\n+    private void updateOrInitialize(String changedPath) {\n+      lock.lock();\n+      try {\n+        if (!initializeIfNeeded()) {\n+          ZNRecord record = propertyStore.get(changedPath, null, AccessOption.PERSISTENT);\n+          if (record != null) {\n+            listener.onDataNodeConfigChange(lazyIterable(Collections.singleton(record)));\n+          } else {\n+            LOGGER.info(\"DataNodeConfig at path {} not found\", changedPath);\n+          }\n+        }\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig change\", e);\n+      } finally {\n+        lock.unlock();\n+      }\n+    }\n+\n+    /**\n+     * If initialization has not yet been completed succesfully, attempt an initialization. This should be called from\n+     * the event executor thread.\n+     * @return {@code true} if a successful full initialization (whether successful or not) was attempted in this call.\n+     */\n+    private boolean initializeIfNeeded() {\n+      lock.lock();\n+      try {\n+        if (initialized) {\n+          return false;\n+        }\n+        List<ZNRecord> records = propertyStore.getChildren(CONFIG_PATH, null, AccessOption.PERSISTENT);\n+        listener.onDataNodeConfigChange(lazyIterable(records));\n+        initialized = true;\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig initialization\", e);\n+      } finally {\n+        lock.unlock();\n+      }\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Convert between {@link DataNodeConfig}s and {@link ZNRecord}s.\n+   */\n+  static class Converter {\n+    private static final int VERSION_0 = 0;\n+    private static final String MOUNT_PREFIX = \"mount-\";\n+    private static final String HOSTNAME_FIELD = \"hostname\";\n+    private static final String PORT_FIELD = \"port\";\n+    private final String defaultPartitionClass;\n+    private final String dcName;\n+\n+    /**\n+     * @param defaultPartitionClass the default partition class for these nodes.\n+     * @param dcName the datacenter name for these nodes.\n+     */\n+    Converter(String defaultPartitionClass, String dcName) {\n+      this.defaultPartitionClass = defaultPartitionClass;\n+      this.dcName = dcName;\n+    }\n+\n+    /**\n+     * @param record the {@link ZNRecord} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link ZNRecord} provided has an unsupported schema\n+     *         version.\n+     */\n+    DataNodeConfig convert(ZNRecord record) {\n+      int schemaVersion = getSchemaVersion(record);\n+      if (schemaVersion != VERSION_0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, record);\n+        return null;\n+      }\n+\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(record.getId(), record.getSimpleField(HOSTNAME_FIELD),\n+          record.getIntField(PORT_FIELD, DataNodeId.UNKNOWN_PORT), dcName, getSslPortStr(record),\n+          getHttp2PortStr(record), getRackId(record), DEFAULT_XID);\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(record));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(record));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(record));\n+      record.getMapFields().forEach((key, diskProps) -> {\n+        if (key.startsWith(MOUNT_PREFIX)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2OTAzMw=="}, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODczMjI2MQ==", "bodyText": "Make sense, let's keep the prefix there.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r448732261", "createdAt": "2020-07-02T03:48:07Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *\n+ */\n+\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import com.github.ambry.utils.Utils;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.helix.AccessOption;\n+import org.apache.helix.store.HelixPropertyListener;\n+import org.apache.helix.store.HelixPropertyStore;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+import org.apache.zookeeper.data.Stat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * An implementation of {@link DataNodeConfigSource} that reads {@link ZNRecord}s stored under the\n+ * \"/DataNodeConfigs\" path in a {@link HelixPropertyStore}.\n+ */\n+public class PropertyStoreToDataNodeConfigAdapter implements DataNodeConfigSource {\n+  static final String CONFIG_PATH = \"/DataNodeConfigs\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(PropertyStoreToDataNodeConfigAdapter.class);\n+  private final HelixPropertyStore<ZNRecord> propertyStore;\n+  private final Converter converter;\n+  private final Executor eventExecutor;\n+\n+  /**\n+   * @param propertyStore the {@link HelixPropertyStore} instance to use to interact with zookeeper.\n+   * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n+   * @param dcName the datacenter name for for the property store.\n+   */\n+  public PropertyStoreToDataNodeConfigAdapter(HelixPropertyStore<ZNRecord> propertyStore,\n+      ClusterMapConfig clusterMapConfig, String dcName) {\n+    this.propertyStore = propertyStore;\n+    this.converter = new Converter(clusterMapConfig.clusterMapDefaultPartitionClass, dcName);\n+    this.eventExecutor = Executors.newSingleThreadExecutor();\n+  }\n+\n+  @Override\n+  public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) {\n+    Subscription subscription = new Subscription(listener);\n+    propertyStore.subscribe(CONFIG_PATH, subscription);\n+    subscription.start();\n+  }\n+\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    ZNRecord record = converter.convert(config);\n+    String path = CONFIG_PATH + \"/\" + record.getId();\n+    return propertyStore.set(path, record, AccessOption.PERSISTENT);\n+  }\n+\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    String path = CONFIG_PATH + \"/\" + instanceName;\n+    ZNRecord record = propertyStore.get(path, new Stat(), AccessOption.PERSISTENT);\n+    return record != null ? converter.convert(record) : null;\n+  }\n+\n+  private class Subscription implements HelixPropertyListener {\n+    private final DataNodeConfigChangeListener listener;\n+    private final ReentrantLock lock = new ReentrantLock();\n+    // reading/writing this variable should be protected by the lock.\n+    private boolean initialized = false;\n+\n+    Subscription(DataNodeConfigChangeListener listener) {\n+      this.listener = listener;\n+    }\n+\n+    /**\n+     * Tell the background executor to perform any initialization needed. This should be called after registering this\n+     * listener using {@link HelixPropertyStore#subscribe}. This will ensure that the first event sent to\n+     * {@link #listener} will contain the entire set of current configs.\n+     */\n+    void start() {\n+      eventExecutor.execute(this::initializeIfNeeded);\n+    }\n+\n+    @Override\n+    public void onDataChange(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} changed\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataCreate(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} created\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataDelete(String path) {\n+      // TODO handle node deletions dynamically. Doing so requires further work in the ClusterChangeHandler impl\n+      LOGGER.info(\"DataNodeConfig path {} deleted. This requires a restart to handle\", path);\n+    }\n+\n+    private void onPathChange(String path) {\n+      eventExecutor.execute(() -> updateOrInitialize(path));\n+    }\n+\n+    private Iterable<DataNodeConfig> lazyIterable(Collection<ZNRecord> records) {\n+      return () -> records.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n+    }\n+\n+    /**\n+     * Either perform a full initialization or notify the listener about an incremental update to a path. This should be\n+     * called from the event executor thread.\n+     * @param changedPath the path that changed.\n+     */\n+    private void updateOrInitialize(String changedPath) {\n+      lock.lock();\n+      try {\n+        if (!initializeIfNeeded()) {\n+          ZNRecord record = propertyStore.get(changedPath, null, AccessOption.PERSISTENT);\n+          if (record != null) {\n+            listener.onDataNodeConfigChange(lazyIterable(Collections.singleton(record)));\n+          } else {\n+            LOGGER.info(\"DataNodeConfig at path {} not found\", changedPath);\n+          }\n+        }\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig change\", e);\n+      } finally {\n+        lock.unlock();\n+      }\n+    }\n+\n+    /**\n+     * If initialization has not yet been completed succesfully, attempt an initialization. This should be called from\n+     * the event executor thread.\n+     * @return {@code true} if a successful full initialization (whether successful or not) was attempted in this call.\n+     */\n+    private boolean initializeIfNeeded() {\n+      lock.lock();\n+      try {\n+        if (initialized) {\n+          return false;\n+        }\n+        List<ZNRecord> records = propertyStore.getChildren(CONFIG_PATH, null, AccessOption.PERSISTENT);\n+        listener.onDataNodeConfigChange(lazyIterable(records));\n+        initialized = true;\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig initialization\", e);\n+      } finally {\n+        lock.unlock();\n+      }\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Convert between {@link DataNodeConfig}s and {@link ZNRecord}s.\n+   */\n+  static class Converter {\n+    private static final int VERSION_0 = 0;\n+    private static final String MOUNT_PREFIX = \"mount-\";\n+    private static final String HOSTNAME_FIELD = \"hostname\";\n+    private static final String PORT_FIELD = \"port\";\n+    private final String defaultPartitionClass;\n+    private final String dcName;\n+\n+    /**\n+     * @param defaultPartitionClass the default partition class for these nodes.\n+     * @param dcName the datacenter name for these nodes.\n+     */\n+    Converter(String defaultPartitionClass, String dcName) {\n+      this.defaultPartitionClass = defaultPartitionClass;\n+      this.dcName = dcName;\n+    }\n+\n+    /**\n+     * @param record the {@link ZNRecord} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link ZNRecord} provided has an unsupported schema\n+     *         version.\n+     */\n+    DataNodeConfig convert(ZNRecord record) {\n+      int schemaVersion = getSchemaVersion(record);\n+      if (schemaVersion != VERSION_0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, record);\n+        return null;\n+      }\n+\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(record.getId(), record.getSimpleField(HOSTNAME_FIELD),\n+          record.getIntField(PORT_FIELD, DataNodeId.UNKNOWN_PORT), dcName, getSslPortStr(record),\n+          getHttp2PortStr(record), getRackId(record), DEFAULT_XID);\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(record));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(record));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(record));\n+      record.getMapFields().forEach((key, diskProps) -> {\n+        if (key.startsWith(MOUNT_PREFIX)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2OTAzMw=="}, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 215}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3ODc3MjM0OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwNTowODoyNFrOGpT1yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwNTowODoyNFrOGpT1yA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2OTg2NA==", "bodyText": "typo: successfully", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r445969864", "createdAt": "2020-06-26T05:08:24Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *\n+ */\n+\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import com.github.ambry.utils.Utils;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.helix.AccessOption;\n+import org.apache.helix.store.HelixPropertyListener;\n+import org.apache.helix.store.HelixPropertyStore;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+import org.apache.zookeeper.data.Stat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * An implementation of {@link DataNodeConfigSource} that reads {@link ZNRecord}s stored under the\n+ * \"/DataNodeConfigs\" path in a {@link HelixPropertyStore}.\n+ */\n+public class PropertyStoreToDataNodeConfigAdapter implements DataNodeConfigSource {\n+  static final String CONFIG_PATH = \"/DataNodeConfigs\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(PropertyStoreToDataNodeConfigAdapter.class);\n+  private final HelixPropertyStore<ZNRecord> propertyStore;\n+  private final Converter converter;\n+  private final Executor eventExecutor;\n+\n+  /**\n+   * @param propertyStore the {@link HelixPropertyStore} instance to use to interact with zookeeper.\n+   * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n+   * @param dcName the datacenter name for for the property store.\n+   */\n+  public PropertyStoreToDataNodeConfigAdapter(HelixPropertyStore<ZNRecord> propertyStore,\n+      ClusterMapConfig clusterMapConfig, String dcName) {\n+    this.propertyStore = propertyStore;\n+    this.converter = new Converter(clusterMapConfig.clusterMapDefaultPartitionClass, dcName);\n+    this.eventExecutor = Executors.newSingleThreadExecutor();\n+  }\n+\n+  @Override\n+  public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) {\n+    Subscription subscription = new Subscription(listener);\n+    propertyStore.subscribe(CONFIG_PATH, subscription);\n+    subscription.start();\n+  }\n+\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    ZNRecord record = converter.convert(config);\n+    String path = CONFIG_PATH + \"/\" + record.getId();\n+    return propertyStore.set(path, record, AccessOption.PERSISTENT);\n+  }\n+\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    String path = CONFIG_PATH + \"/\" + instanceName;\n+    ZNRecord record = propertyStore.get(path, new Stat(), AccessOption.PERSISTENT);\n+    return record != null ? converter.convert(record) : null;\n+  }\n+\n+  private class Subscription implements HelixPropertyListener {\n+    private final DataNodeConfigChangeListener listener;\n+    private final ReentrantLock lock = new ReentrantLock();\n+    // reading/writing this variable should be protected by the lock.\n+    private boolean initialized = false;\n+\n+    Subscription(DataNodeConfigChangeListener listener) {\n+      this.listener = listener;\n+    }\n+\n+    /**\n+     * Tell the background executor to perform any initialization needed. This should be called after registering this\n+     * listener using {@link HelixPropertyStore#subscribe}. This will ensure that the first event sent to\n+     * {@link #listener} will contain the entire set of current configs.\n+     */\n+    void start() {\n+      eventExecutor.execute(this::initializeIfNeeded);\n+    }\n+\n+    @Override\n+    public void onDataChange(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} changed\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataCreate(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} created\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataDelete(String path) {\n+      // TODO handle node deletions dynamically. Doing so requires further work in the ClusterChangeHandler impl\n+      LOGGER.info(\"DataNodeConfig path {} deleted. This requires a restart to handle\", path);\n+    }\n+\n+    private void onPathChange(String path) {\n+      eventExecutor.execute(() -> updateOrInitialize(path));\n+    }\n+\n+    private Iterable<DataNodeConfig> lazyIterable(Collection<ZNRecord> records) {\n+      return () -> records.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n+    }\n+\n+    /**\n+     * Either perform a full initialization or notify the listener about an incremental update to a path. This should be\n+     * called from the event executor thread.\n+     * @param changedPath the path that changed.\n+     */\n+    private void updateOrInitialize(String changedPath) {\n+      lock.lock();\n+      try {\n+        if (!initializeIfNeeded()) {\n+          ZNRecord record = propertyStore.get(changedPath, null, AccessOption.PERSISTENT);\n+          if (record != null) {\n+            listener.onDataNodeConfigChange(lazyIterable(Collections.singleton(record)));\n+          } else {\n+            LOGGER.info(\"DataNodeConfig at path {} not found\", changedPath);\n+          }\n+        }\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig change\", e);\n+      } finally {\n+        lock.unlock();\n+      }\n+    }\n+\n+    /**\n+     * If initialization has not yet been completed succesfully, attempt an initialization. This should be called from", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3ODc5MTU1OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwNToyMDo0OVrOGpUBVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwMDozNTo1OFrOGrS-vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk3MjgyMQ==", "bodyText": "Just curious, if exception occurs, how can we block the startup of current node?", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r445972821", "createdAt": "2020-06-26T05:20:49Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *\n+ */\n+\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import com.github.ambry.utils.Utils;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.helix.AccessOption;\n+import org.apache.helix.store.HelixPropertyListener;\n+import org.apache.helix.store.HelixPropertyStore;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+import org.apache.zookeeper.data.Stat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * An implementation of {@link DataNodeConfigSource} that reads {@link ZNRecord}s stored under the\n+ * \"/DataNodeConfigs\" path in a {@link HelixPropertyStore}.\n+ */\n+public class PropertyStoreToDataNodeConfigAdapter implements DataNodeConfigSource {\n+  static final String CONFIG_PATH = \"/DataNodeConfigs\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(PropertyStoreToDataNodeConfigAdapter.class);\n+  private final HelixPropertyStore<ZNRecord> propertyStore;\n+  private final Converter converter;\n+  private final Executor eventExecutor;\n+\n+  /**\n+   * @param propertyStore the {@link HelixPropertyStore} instance to use to interact with zookeeper.\n+   * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n+   * @param dcName the datacenter name for for the property store.\n+   */\n+  public PropertyStoreToDataNodeConfigAdapter(HelixPropertyStore<ZNRecord> propertyStore,\n+      ClusterMapConfig clusterMapConfig, String dcName) {\n+    this.propertyStore = propertyStore;\n+    this.converter = new Converter(clusterMapConfig.clusterMapDefaultPartitionClass, dcName);\n+    this.eventExecutor = Executors.newSingleThreadExecutor();\n+  }\n+\n+  @Override\n+  public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) {\n+    Subscription subscription = new Subscription(listener);\n+    propertyStore.subscribe(CONFIG_PATH, subscription);\n+    subscription.start();\n+  }\n+\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    ZNRecord record = converter.convert(config);\n+    String path = CONFIG_PATH + \"/\" + record.getId();\n+    return propertyStore.set(path, record, AccessOption.PERSISTENT);\n+  }\n+\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    String path = CONFIG_PATH + \"/\" + instanceName;\n+    ZNRecord record = propertyStore.get(path, new Stat(), AccessOption.PERSISTENT);\n+    return record != null ? converter.convert(record) : null;\n+  }\n+\n+  private class Subscription implements HelixPropertyListener {\n+    private final DataNodeConfigChangeListener listener;\n+    private final ReentrantLock lock = new ReentrantLock();\n+    // reading/writing this variable should be protected by the lock.\n+    private boolean initialized = false;\n+\n+    Subscription(DataNodeConfigChangeListener listener) {\n+      this.listener = listener;\n+    }\n+\n+    /**\n+     * Tell the background executor to perform any initialization needed. This should be called after registering this\n+     * listener using {@link HelixPropertyStore#subscribe}. This will ensure that the first event sent to\n+     * {@link #listener} will contain the entire set of current configs.\n+     */\n+    void start() {\n+      eventExecutor.execute(this::initializeIfNeeded);\n+    }\n+\n+    @Override\n+    public void onDataChange(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} changed\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataCreate(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} created\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataDelete(String path) {\n+      // TODO handle node deletions dynamically. Doing so requires further work in the ClusterChangeHandler impl\n+      LOGGER.info(\"DataNodeConfig path {} deleted. This requires a restart to handle\", path);\n+    }\n+\n+    private void onPathChange(String path) {\n+      eventExecutor.execute(() -> updateOrInitialize(path));\n+    }\n+\n+    private Iterable<DataNodeConfig> lazyIterable(Collection<ZNRecord> records) {\n+      return () -> records.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n+    }\n+\n+    /**\n+     * Either perform a full initialization or notify the listener about an incremental update to a path. This should be\n+     * called from the event executor thread.\n+     * @param changedPath the path that changed.\n+     */\n+    private void updateOrInitialize(String changedPath) {\n+      lock.lock();\n+      try {\n+        if (!initializeIfNeeded()) {\n+          ZNRecord record = propertyStore.get(changedPath, null, AccessOption.PERSISTENT);\n+          if (record != null) {\n+            listener.onDataNodeConfigChange(lazyIterable(Collections.singleton(record)));\n+          } else {\n+            LOGGER.info(\"DataNodeConfig at path {} not found\", changedPath);\n+          }\n+        }\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig change\", e);\n+      } finally {\n+        lock.unlock();\n+      }\n+    }\n+\n+    /**\n+     * If initialization has not yet been completed succesfully, attempt an initialization. This should be called from\n+     * the event executor thread.\n+     * @return {@code true} if a successful full initialization (whether successful or not) was attempted in this call.\n+     */\n+    private boolean initializeIfNeeded() {\n+      lock.lock();\n+      try {\n+        if (initialized) {\n+          return false;\n+        }\n+        List<ZNRecord> records = propertyStore.getChildren(CONFIG_PATH, null, AccessOption.PERSISTENT);\n+        listener.onDataNodeConfigChange(lazyIterable(records));\n+        initialized = true;\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig initialization\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI1MjcwNw==", "bodyText": "Hmm, that is a good point. So, with InstanceConfig listeners, does the exception thrown in the addListener call bubble up to DatacenterInitializer? If so, I will have to address this. Also, I realize that this method does not currently block until initialization is finished. I may have to make changes to support this.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r447252707", "createdAt": "2020-06-29T21:03:46Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *\n+ */\n+\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import com.github.ambry.utils.Utils;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.helix.AccessOption;\n+import org.apache.helix.store.HelixPropertyListener;\n+import org.apache.helix.store.HelixPropertyStore;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+import org.apache.zookeeper.data.Stat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * An implementation of {@link DataNodeConfigSource} that reads {@link ZNRecord}s stored under the\n+ * \"/DataNodeConfigs\" path in a {@link HelixPropertyStore}.\n+ */\n+public class PropertyStoreToDataNodeConfigAdapter implements DataNodeConfigSource {\n+  static final String CONFIG_PATH = \"/DataNodeConfigs\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(PropertyStoreToDataNodeConfigAdapter.class);\n+  private final HelixPropertyStore<ZNRecord> propertyStore;\n+  private final Converter converter;\n+  private final Executor eventExecutor;\n+\n+  /**\n+   * @param propertyStore the {@link HelixPropertyStore} instance to use to interact with zookeeper.\n+   * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n+   * @param dcName the datacenter name for for the property store.\n+   */\n+  public PropertyStoreToDataNodeConfigAdapter(HelixPropertyStore<ZNRecord> propertyStore,\n+      ClusterMapConfig clusterMapConfig, String dcName) {\n+    this.propertyStore = propertyStore;\n+    this.converter = new Converter(clusterMapConfig.clusterMapDefaultPartitionClass, dcName);\n+    this.eventExecutor = Executors.newSingleThreadExecutor();\n+  }\n+\n+  @Override\n+  public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) {\n+    Subscription subscription = new Subscription(listener);\n+    propertyStore.subscribe(CONFIG_PATH, subscription);\n+    subscription.start();\n+  }\n+\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    ZNRecord record = converter.convert(config);\n+    String path = CONFIG_PATH + \"/\" + record.getId();\n+    return propertyStore.set(path, record, AccessOption.PERSISTENT);\n+  }\n+\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    String path = CONFIG_PATH + \"/\" + instanceName;\n+    ZNRecord record = propertyStore.get(path, new Stat(), AccessOption.PERSISTENT);\n+    return record != null ? converter.convert(record) : null;\n+  }\n+\n+  private class Subscription implements HelixPropertyListener {\n+    private final DataNodeConfigChangeListener listener;\n+    private final ReentrantLock lock = new ReentrantLock();\n+    // reading/writing this variable should be protected by the lock.\n+    private boolean initialized = false;\n+\n+    Subscription(DataNodeConfigChangeListener listener) {\n+      this.listener = listener;\n+    }\n+\n+    /**\n+     * Tell the background executor to perform any initialization needed. This should be called after registering this\n+     * listener using {@link HelixPropertyStore#subscribe}. This will ensure that the first event sent to\n+     * {@link #listener} will contain the entire set of current configs.\n+     */\n+    void start() {\n+      eventExecutor.execute(this::initializeIfNeeded);\n+    }\n+\n+    @Override\n+    public void onDataChange(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} changed\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataCreate(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} created\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataDelete(String path) {\n+      // TODO handle node deletions dynamically. Doing so requires further work in the ClusterChangeHandler impl\n+      LOGGER.info(\"DataNodeConfig path {} deleted. This requires a restart to handle\", path);\n+    }\n+\n+    private void onPathChange(String path) {\n+      eventExecutor.execute(() -> updateOrInitialize(path));\n+    }\n+\n+    private Iterable<DataNodeConfig> lazyIterable(Collection<ZNRecord> records) {\n+      return () -> records.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n+    }\n+\n+    /**\n+     * Either perform a full initialization or notify the listener about an incremental update to a path. This should be\n+     * called from the event executor thread.\n+     * @param changedPath the path that changed.\n+     */\n+    private void updateOrInitialize(String changedPath) {\n+      lock.lock();\n+      try {\n+        if (!initializeIfNeeded()) {\n+          ZNRecord record = propertyStore.get(changedPath, null, AccessOption.PERSISTENT);\n+          if (record != null) {\n+            listener.onDataNodeConfigChange(lazyIterable(Collections.singleton(record)));\n+          } else {\n+            LOGGER.info(\"DataNodeConfig at path {} not found\", changedPath);\n+          }\n+        }\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig change\", e);\n+      } finally {\n+        lock.unlock();\n+      }\n+    }\n+\n+    /**\n+     * If initialization has not yet been completed succesfully, attempt an initialization. This should be called from\n+     * the event executor thread.\n+     * @return {@code true} if a successful full initialization (whether successful or not) was attempted in this call.\n+     */\n+    private boolean initializeIfNeeded() {\n+      lock.lock();\n+      try {\n+        if (initialized) {\n+          return false;\n+        }\n+        List<ZNRecord> records = propertyStore.getChildren(CONFIG_PATH, null, AccessOption.PERSISTENT);\n+        listener.onDataNodeConfigChange(lazyIterable(records));\n+        initialized = true;\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig initialization\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk3MjgyMQ=="}, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA1MjkyNw==", "bodyText": "Changed the add listener call to wait for initialization to finish in the background thread to match how the other DataNodeConfigSource works. The code will now rethrow an exception thrown by the listener during initialization.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r448052927", "createdAt": "2020-07-01T00:35:58Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *\n+ */\n+\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import com.github.ambry.utils.Utils;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.helix.AccessOption;\n+import org.apache.helix.store.HelixPropertyListener;\n+import org.apache.helix.store.HelixPropertyStore;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+import org.apache.zookeeper.data.Stat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * An implementation of {@link DataNodeConfigSource} that reads {@link ZNRecord}s stored under the\n+ * \"/DataNodeConfigs\" path in a {@link HelixPropertyStore}.\n+ */\n+public class PropertyStoreToDataNodeConfigAdapter implements DataNodeConfigSource {\n+  static final String CONFIG_PATH = \"/DataNodeConfigs\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(PropertyStoreToDataNodeConfigAdapter.class);\n+  private final HelixPropertyStore<ZNRecord> propertyStore;\n+  private final Converter converter;\n+  private final Executor eventExecutor;\n+\n+  /**\n+   * @param propertyStore the {@link HelixPropertyStore} instance to use to interact with zookeeper.\n+   * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n+   * @param dcName the datacenter name for for the property store.\n+   */\n+  public PropertyStoreToDataNodeConfigAdapter(HelixPropertyStore<ZNRecord> propertyStore,\n+      ClusterMapConfig clusterMapConfig, String dcName) {\n+    this.propertyStore = propertyStore;\n+    this.converter = new Converter(clusterMapConfig.clusterMapDefaultPartitionClass, dcName);\n+    this.eventExecutor = Executors.newSingleThreadExecutor();\n+  }\n+\n+  @Override\n+  public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) {\n+    Subscription subscription = new Subscription(listener);\n+    propertyStore.subscribe(CONFIG_PATH, subscription);\n+    subscription.start();\n+  }\n+\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    ZNRecord record = converter.convert(config);\n+    String path = CONFIG_PATH + \"/\" + record.getId();\n+    return propertyStore.set(path, record, AccessOption.PERSISTENT);\n+  }\n+\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    String path = CONFIG_PATH + \"/\" + instanceName;\n+    ZNRecord record = propertyStore.get(path, new Stat(), AccessOption.PERSISTENT);\n+    return record != null ? converter.convert(record) : null;\n+  }\n+\n+  private class Subscription implements HelixPropertyListener {\n+    private final DataNodeConfigChangeListener listener;\n+    private final ReentrantLock lock = new ReentrantLock();\n+    // reading/writing this variable should be protected by the lock.\n+    private boolean initialized = false;\n+\n+    Subscription(DataNodeConfigChangeListener listener) {\n+      this.listener = listener;\n+    }\n+\n+    /**\n+     * Tell the background executor to perform any initialization needed. This should be called after registering this\n+     * listener using {@link HelixPropertyStore#subscribe}. This will ensure that the first event sent to\n+     * {@link #listener} will contain the entire set of current configs.\n+     */\n+    void start() {\n+      eventExecutor.execute(this::initializeIfNeeded);\n+    }\n+\n+    @Override\n+    public void onDataChange(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} changed\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataCreate(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} created\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataDelete(String path) {\n+      // TODO handle node deletions dynamically. Doing so requires further work in the ClusterChangeHandler impl\n+      LOGGER.info(\"DataNodeConfig path {} deleted. This requires a restart to handle\", path);\n+    }\n+\n+    private void onPathChange(String path) {\n+      eventExecutor.execute(() -> updateOrInitialize(path));\n+    }\n+\n+    private Iterable<DataNodeConfig> lazyIterable(Collection<ZNRecord> records) {\n+      return () -> records.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n+    }\n+\n+    /**\n+     * Either perform a full initialization or notify the listener about an incremental update to a path. This should be\n+     * called from the event executor thread.\n+     * @param changedPath the path that changed.\n+     */\n+    private void updateOrInitialize(String changedPath) {\n+      lock.lock();\n+      try {\n+        if (!initializeIfNeeded()) {\n+          ZNRecord record = propertyStore.get(changedPath, null, AccessOption.PERSISTENT);\n+          if (record != null) {\n+            listener.onDataNodeConfigChange(lazyIterable(Collections.singleton(record)));\n+          } else {\n+            LOGGER.info(\"DataNodeConfig at path {} not found\", changedPath);\n+          }\n+        }\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig change\", e);\n+      } finally {\n+        lock.unlock();\n+      }\n+    }\n+\n+    /**\n+     * If initialization has not yet been completed succesfully, attempt an initialization. This should be called from\n+     * the event executor thread.\n+     * @return {@code true} if a successful full initialization (whether successful or not) was attempted in this call.\n+     */\n+    private boolean initializeIfNeeded() {\n+      lock.lock();\n+      try {\n+        if (initialized) {\n+          return false;\n+        }\n+        List<ZNRecord> records = propertyStore.getChildren(CONFIG_PATH, null, AccessOption.PERSISTENT);\n+        listener.onDataNodeConfigChange(lazyIterable(records));\n+        initialized = true;\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig initialization\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk3MjgyMQ=="}, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 168}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3OTAyNzc1OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwNzoxNjozNVrOGpWQyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwNDo1Nzo0M1rOGqrtnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjAwOTU0NQ==", "bodyText": "A quick check, do we have any test cases cover this method?", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r446009545", "createdAt": "2020-06-26T07:16:35Z", "author": {"login": "SophieGuo410"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "diffHunk": "@@ -33,82 +38,146 @@\n public class InstanceConfigToDataNodeConfigAdapter implements DataNodeConfigSource {\n   private static final Logger LOGGER = LoggerFactory.getLogger(InstanceConfigToDataNodeConfigAdapter.class);\n   private final HelixManager helixManager;\n-  private final ClusterMapConfig clusterMapConfig;\n+  private final Converter converter;\n+  private final String clusterName;\n+  private volatile HelixAdmin helixAdmin = null;\n \n   /**\n    * @param helixManager the {@link HelixManager} to use as the source of truth for {@link InstanceConfig}s.\n    * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n    */\n   public InstanceConfigToDataNodeConfigAdapter(HelixManager helixManager, ClusterMapConfig clusterMapConfig) {\n     this.helixManager = helixManager;\n-    this.clusterMapConfig = clusterMapConfig;\n+    this.converter = new Converter(clusterMapConfig);\n+    clusterName = clusterMapConfig.clusterMapClusterName;\n   }\n \n   @Override\n   public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n     helixManager.addInstanceConfigChangeListener((InstanceConfigChangeListener) (instanceConfigs, context) -> {\n       Iterable<DataNodeConfig> dataNodeConfigs =\n-          () -> instanceConfigs.stream().map(this::convert).filter(Objects::nonNull).iterator();\n+          () -> instanceConfigs.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n       listener.onDataNodeConfigChange(dataNodeConfigs);\n     });\n   }\n \n-  /**\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  private DataNodeConfig convert(InstanceConfig instanceConfig) {\n-    return convert(instanceConfig, clusterMapConfig);\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    InstanceConfig instanceConfig = converter.convert(config);\n+    return getHelixAdmin().setInstanceConfig(clusterName, instanceConfig.getInstanceName(), instanceConfig);\n   }\n \n-  /**\n-   * Exposed for testing.\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @param clusterMapConfig the {@link ClusterMapConfig} containing any default values that may be needed.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  static DataNodeConfig convert(InstanceConfig instanceConfig, ClusterMapConfig clusterMapConfig) {\n-    int schemaVersion = getSchemaVersion(instanceConfig);\n-    if (schemaVersion != 0) {\n-      LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n-      return null;\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    InstanceConfig instanceConfig = getHelixAdmin().getInstanceConfig(clusterName, instanceName);\n+    return instanceConfig != null ? converter.convert(instanceConfig) : null;\n+  }\n+\n+  void setHelixAdmin(HelixAdmin helixAdmin) {\n+    this.helixAdmin = helixAdmin;\n+  }\n+\n+  private HelixAdmin getHelixAdmin() {\n+    return Objects.requireNonNull(helixAdmin, \"helixAdmin not set\");\n+  }\n+\n+  static class Converter {\n+    private final ClusterMapConfig clusterMapConfig;\n+\n+    Converter(ClusterMapConfig clusterMapConfig) {\n+      this.clusterMapConfig = clusterMapConfig;\n     }\n-    DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n-        Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n-        getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n-    dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n-    dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n-    // TODO uncomment this line once 1534 is merged\n-    // dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n-    instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n-      if (diskProps.get(DISK_STATE) == null) {\n-        // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n-        // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n-        // state to InstanceConfig.\n-        LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n-            instanceConfig.getInstanceName());\n-      } else {\n-        DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n-            diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n-            Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n-        String replicasStr = diskProps.get(REPLICAS_STR);\n-        if (!replicasStr.isEmpty()) {\n-          for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n-            String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n-            // partition name and replica name are the same.\n-            String partitionName = replicaStrParts[0];\n-            long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n-            String partitionClass =\n-                replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n-            disk.getReplicaConfigs()\n-                .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+\n+    /**\n+     * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported\n+     *         schema version.\n+     */\n+    DataNodeConfig convert(InstanceConfig instanceConfig) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n+        return null;\n+      }\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n+          Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n+          getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n+      instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n+        if (diskProps.get(DISK_STATE) == null) {\n+          // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n+          // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n+          // state to InstanceConfig.\n+          LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n+              instanceConfig.getInstanceName());\n+        } else {\n+          DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n+              diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n+              Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n+          String replicasStr = diskProps.get(REPLICAS_STR);\n+          if (!Utils.isNullOrEmpty(replicasStr)) {\n+            for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n+              String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n+              // partition name and replica name are the same.\n+              String partitionName = replicaStrParts[0];\n+              long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n+              String partitionClass =\n+                  replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+              disk.getReplicaConfigs()\n+                  .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+            }\n           }\n+          dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n         }\n-        dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n+      });\n+      return dataNodeConfig;\n+    }\n+\n+    /**\n+     * @param dataNodeConfig the {@link DataNodeConfig} to convert into an {@link InstanceConfig}.\n+     * @return the {@link InstanceConfig}.\n+     */\n+    InstanceConfig convert(DataNodeConfig dataNodeConfig) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI1MDcyMw==", "bodyText": "If you would like, I can make unit tests specifically for the converter. Currently they are tested using the DataNodeConfigSource.set and get methods, which call these functions.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r447250723", "createdAt": "2020-06-29T20:59:59Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "diffHunk": "@@ -33,82 +38,146 @@\n public class InstanceConfigToDataNodeConfigAdapter implements DataNodeConfigSource {\n   private static final Logger LOGGER = LoggerFactory.getLogger(InstanceConfigToDataNodeConfigAdapter.class);\n   private final HelixManager helixManager;\n-  private final ClusterMapConfig clusterMapConfig;\n+  private final Converter converter;\n+  private final String clusterName;\n+  private volatile HelixAdmin helixAdmin = null;\n \n   /**\n    * @param helixManager the {@link HelixManager} to use as the source of truth for {@link InstanceConfig}s.\n    * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n    */\n   public InstanceConfigToDataNodeConfigAdapter(HelixManager helixManager, ClusterMapConfig clusterMapConfig) {\n     this.helixManager = helixManager;\n-    this.clusterMapConfig = clusterMapConfig;\n+    this.converter = new Converter(clusterMapConfig);\n+    clusterName = clusterMapConfig.clusterMapClusterName;\n   }\n \n   @Override\n   public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n     helixManager.addInstanceConfigChangeListener((InstanceConfigChangeListener) (instanceConfigs, context) -> {\n       Iterable<DataNodeConfig> dataNodeConfigs =\n-          () -> instanceConfigs.stream().map(this::convert).filter(Objects::nonNull).iterator();\n+          () -> instanceConfigs.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n       listener.onDataNodeConfigChange(dataNodeConfigs);\n     });\n   }\n \n-  /**\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  private DataNodeConfig convert(InstanceConfig instanceConfig) {\n-    return convert(instanceConfig, clusterMapConfig);\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    InstanceConfig instanceConfig = converter.convert(config);\n+    return getHelixAdmin().setInstanceConfig(clusterName, instanceConfig.getInstanceName(), instanceConfig);\n   }\n \n-  /**\n-   * Exposed for testing.\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @param clusterMapConfig the {@link ClusterMapConfig} containing any default values that may be needed.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  static DataNodeConfig convert(InstanceConfig instanceConfig, ClusterMapConfig clusterMapConfig) {\n-    int schemaVersion = getSchemaVersion(instanceConfig);\n-    if (schemaVersion != 0) {\n-      LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n-      return null;\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    InstanceConfig instanceConfig = getHelixAdmin().getInstanceConfig(clusterName, instanceName);\n+    return instanceConfig != null ? converter.convert(instanceConfig) : null;\n+  }\n+\n+  void setHelixAdmin(HelixAdmin helixAdmin) {\n+    this.helixAdmin = helixAdmin;\n+  }\n+\n+  private HelixAdmin getHelixAdmin() {\n+    return Objects.requireNonNull(helixAdmin, \"helixAdmin not set\");\n+  }\n+\n+  static class Converter {\n+    private final ClusterMapConfig clusterMapConfig;\n+\n+    Converter(ClusterMapConfig clusterMapConfig) {\n+      this.clusterMapConfig = clusterMapConfig;\n     }\n-    DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n-        Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n-        getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n-    dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n-    dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n-    // TODO uncomment this line once 1534 is merged\n-    // dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n-    instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n-      if (diskProps.get(DISK_STATE) == null) {\n-        // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n-        // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n-        // state to InstanceConfig.\n-        LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n-            instanceConfig.getInstanceName());\n-      } else {\n-        DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n-            diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n-            Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n-        String replicasStr = diskProps.get(REPLICAS_STR);\n-        if (!replicasStr.isEmpty()) {\n-          for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n-            String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n-            // partition name and replica name are the same.\n-            String partitionName = replicaStrParts[0];\n-            long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n-            String partitionClass =\n-                replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n-            disk.getReplicaConfigs()\n-                .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+\n+    /**\n+     * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported\n+     *         schema version.\n+     */\n+    DataNodeConfig convert(InstanceConfig instanceConfig) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n+        return null;\n+      }\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n+          Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n+          getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n+      instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n+        if (diskProps.get(DISK_STATE) == null) {\n+          // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n+          // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n+          // state to InstanceConfig.\n+          LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n+              instanceConfig.getInstanceName());\n+        } else {\n+          DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n+              diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n+              Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n+          String replicasStr = diskProps.get(REPLICAS_STR);\n+          if (!Utils.isNullOrEmpty(replicasStr)) {\n+            for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n+              String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n+              // partition name and replica name are the same.\n+              String partitionName = replicaStrParts[0];\n+              long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n+              String partitionClass =\n+                  replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+              disk.getReplicaConfigs()\n+                  .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+            }\n           }\n+          dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n         }\n-        dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n+      });\n+      return dataNodeConfig;\n+    }\n+\n+    /**\n+     * @param dataNodeConfig the {@link DataNodeConfig} to convert into an {@link InstanceConfig}.\n+     * @return the {@link InstanceConfig}.\n+     */\n+    InstanceConfig convert(DataNodeConfig dataNodeConfig) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjAwOTU0NQ=="}, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQwOTU2Nw==", "bodyText": "As long as we have test call these functions, I'm fine with it. Thanks for clarify.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r447409567", "createdAt": "2020-06-30T04:57:43Z", "author": {"login": "SophieGuo410"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "diffHunk": "@@ -33,82 +38,146 @@\n public class InstanceConfigToDataNodeConfigAdapter implements DataNodeConfigSource {\n   private static final Logger LOGGER = LoggerFactory.getLogger(InstanceConfigToDataNodeConfigAdapter.class);\n   private final HelixManager helixManager;\n-  private final ClusterMapConfig clusterMapConfig;\n+  private final Converter converter;\n+  private final String clusterName;\n+  private volatile HelixAdmin helixAdmin = null;\n \n   /**\n    * @param helixManager the {@link HelixManager} to use as the source of truth for {@link InstanceConfig}s.\n    * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n    */\n   public InstanceConfigToDataNodeConfigAdapter(HelixManager helixManager, ClusterMapConfig clusterMapConfig) {\n     this.helixManager = helixManager;\n-    this.clusterMapConfig = clusterMapConfig;\n+    this.converter = new Converter(clusterMapConfig);\n+    clusterName = clusterMapConfig.clusterMapClusterName;\n   }\n \n   @Override\n   public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n     helixManager.addInstanceConfigChangeListener((InstanceConfigChangeListener) (instanceConfigs, context) -> {\n       Iterable<DataNodeConfig> dataNodeConfigs =\n-          () -> instanceConfigs.stream().map(this::convert).filter(Objects::nonNull).iterator();\n+          () -> instanceConfigs.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n       listener.onDataNodeConfigChange(dataNodeConfigs);\n     });\n   }\n \n-  /**\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  private DataNodeConfig convert(InstanceConfig instanceConfig) {\n-    return convert(instanceConfig, clusterMapConfig);\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    InstanceConfig instanceConfig = converter.convert(config);\n+    return getHelixAdmin().setInstanceConfig(clusterName, instanceConfig.getInstanceName(), instanceConfig);\n   }\n \n-  /**\n-   * Exposed for testing.\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @param clusterMapConfig the {@link ClusterMapConfig} containing any default values that may be needed.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  static DataNodeConfig convert(InstanceConfig instanceConfig, ClusterMapConfig clusterMapConfig) {\n-    int schemaVersion = getSchemaVersion(instanceConfig);\n-    if (schemaVersion != 0) {\n-      LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n-      return null;\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    InstanceConfig instanceConfig = getHelixAdmin().getInstanceConfig(clusterName, instanceName);\n+    return instanceConfig != null ? converter.convert(instanceConfig) : null;\n+  }\n+\n+  void setHelixAdmin(HelixAdmin helixAdmin) {\n+    this.helixAdmin = helixAdmin;\n+  }\n+\n+  private HelixAdmin getHelixAdmin() {\n+    return Objects.requireNonNull(helixAdmin, \"helixAdmin not set\");\n+  }\n+\n+  static class Converter {\n+    private final ClusterMapConfig clusterMapConfig;\n+\n+    Converter(ClusterMapConfig clusterMapConfig) {\n+      this.clusterMapConfig = clusterMapConfig;\n     }\n-    DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n-        Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n-        getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n-    dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n-    dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n-    // TODO uncomment this line once 1534 is merged\n-    // dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n-    instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n-      if (diskProps.get(DISK_STATE) == null) {\n-        // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n-        // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n-        // state to InstanceConfig.\n-        LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n-            instanceConfig.getInstanceName());\n-      } else {\n-        DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n-            diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n-            Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n-        String replicasStr = diskProps.get(REPLICAS_STR);\n-        if (!replicasStr.isEmpty()) {\n-          for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n-            String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n-            // partition name and replica name are the same.\n-            String partitionName = replicaStrParts[0];\n-            long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n-            String partitionClass =\n-                replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n-            disk.getReplicaConfigs()\n-                .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+\n+    /**\n+     * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported\n+     *         schema version.\n+     */\n+    DataNodeConfig convert(InstanceConfig instanceConfig) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n+        return null;\n+      }\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n+          Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n+          getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n+      instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n+        if (diskProps.get(DISK_STATE) == null) {\n+          // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n+          // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n+          // state to InstanceConfig.\n+          LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n+              instanceConfig.getInstanceName());\n+        } else {\n+          DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n+              diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n+              Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n+          String replicasStr = diskProps.get(REPLICAS_STR);\n+          if (!Utils.isNullOrEmpty(replicasStr)) {\n+            for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n+              String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n+              // partition name and replica name are the same.\n+              String partitionName = replicaStrParts[0];\n+              long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n+              String partitionClass =\n+                  replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+              disk.getReplicaConfigs()\n+                  .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+            }\n           }\n+          dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n         }\n-        dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n+      });\n+      return dataNodeConfig;\n+    }\n+\n+    /**\n+     * @param dataNodeConfig the {@link DataNodeConfig} to convert into an {@link InstanceConfig}.\n+     * @return the {@link InstanceConfig}.\n+     */\n+    InstanceConfig convert(DataNodeConfig dataNodeConfig) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjAwOTU0NQ=="}, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 170}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5NjkzODY1OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/DataNodeConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNDowMzoxOVrOGr8qRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNTozMjozMlrOGwt6yA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODczNTgxNA==", "bodyText": "Minor: suggest moving port == that.port && xid == that.xid to the position close to Objects.equals(sslPort, that.sslPort). We can compare instanceName first. The port and xid are usually same.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r448735814", "createdAt": "2020-07-02T04:03:19Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/DataNodeConfig.java", "diffHunk": "@@ -152,6 +154,27 @@ public String toString() {\n         + stoppedReplicas + \", disabledReplicas=\" + disabledReplicas + \", diskConfigs=\" + diskConfigs + '}';\n   }\n \n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    DataNodeConfig that = (DataNodeConfig) o;\n+    return port == that.port && xid == that.xid && Objects.equals(instanceName, that.instanceName) && Objects.equals(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzczNzE2MA==", "bodyText": "good idea", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r453737160", "createdAt": "2020-07-13T15:32:32Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/DataNodeConfig.java", "diffHunk": "@@ -152,6 +154,27 @@ public String toString() {\n         + stoppedReplicas + \", disabledReplicas=\" + disabledReplicas + \", diskConfigs=\" + diskConfigs + '}';\n   }\n \n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    DataNodeConfig that = (DataNodeConfig) o;\n+    return port == that.port && xid == that.xid && Objects.equals(instanceName, that.instanceName) && Objects.equals(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODczNTgxNA=="}, "originalCommit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5Njk3ODM1OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNDoyNzo0NVrOGr9A9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNTozMDozMVrOGwt1hA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODc0MTYyMw==", "bodyText": "It seems this method is only called in test. I guess the invocation in production code will be in next PR?", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r448741623", "createdAt": "2020-07-02T04:27:45Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "diffHunk": "@@ -33,82 +38,146 @@\n public class InstanceConfigToDataNodeConfigAdapter implements DataNodeConfigSource {\n   private static final Logger LOGGER = LoggerFactory.getLogger(InstanceConfigToDataNodeConfigAdapter.class);\n   private final HelixManager helixManager;\n-  private final ClusterMapConfig clusterMapConfig;\n+  private final Converter converter;\n+  private final String clusterName;\n+  private volatile HelixAdmin helixAdmin = null;\n \n   /**\n    * @param helixManager the {@link HelixManager} to use as the source of truth for {@link InstanceConfig}s.\n    * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n    */\n   public InstanceConfigToDataNodeConfigAdapter(HelixManager helixManager, ClusterMapConfig clusterMapConfig) {\n     this.helixManager = helixManager;\n-    this.clusterMapConfig = clusterMapConfig;\n+    this.converter = new Converter(clusterMapConfig);\n+    clusterName = clusterMapConfig.clusterMapClusterName;\n   }\n \n   @Override\n   public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n     helixManager.addInstanceConfigChangeListener((InstanceConfigChangeListener) (instanceConfigs, context) -> {\n       Iterable<DataNodeConfig> dataNodeConfigs =\n-          () -> instanceConfigs.stream().map(this::convert).filter(Objects::nonNull).iterator();\n+          () -> instanceConfigs.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n       listener.onDataNodeConfigChange(dataNodeConfigs);\n     });\n   }\n \n-  /**\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  private DataNodeConfig convert(InstanceConfig instanceConfig) {\n-    return convert(instanceConfig, clusterMapConfig);\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    InstanceConfig instanceConfig = converter.convert(config);\n+    return getHelixAdmin().setInstanceConfig(clusterName, instanceConfig.getInstanceName(), instanceConfig);\n   }\n \n-  /**\n-   * Exposed for testing.\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @param clusterMapConfig the {@link ClusterMapConfig} containing any default values that may be needed.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  static DataNodeConfig convert(InstanceConfig instanceConfig, ClusterMapConfig clusterMapConfig) {\n-    int schemaVersion = getSchemaVersion(instanceConfig);\n-    if (schemaVersion != 0) {\n-      LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n-      return null;\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    InstanceConfig instanceConfig = getHelixAdmin().getInstanceConfig(clusterName, instanceName);\n+    return instanceConfig != null ? converter.convert(instanceConfig) : null;\n+  }\n+\n+  void setHelixAdmin(HelixAdmin helixAdmin) {\n+    this.helixAdmin = helixAdmin;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzczNTgxMg==", "bodyText": "Yes, this will be used in the next PR, so I will keep it. I was originally working on these two PRs on the same branch so they weren't fully isolated.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r453735812", "createdAt": "2020-07-13T15:30:31Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "diffHunk": "@@ -33,82 +38,146 @@\n public class InstanceConfigToDataNodeConfigAdapter implements DataNodeConfigSource {\n   private static final Logger LOGGER = LoggerFactory.getLogger(InstanceConfigToDataNodeConfigAdapter.class);\n   private final HelixManager helixManager;\n-  private final ClusterMapConfig clusterMapConfig;\n+  private final Converter converter;\n+  private final String clusterName;\n+  private volatile HelixAdmin helixAdmin = null;\n \n   /**\n    * @param helixManager the {@link HelixManager} to use as the source of truth for {@link InstanceConfig}s.\n    * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n    */\n   public InstanceConfigToDataNodeConfigAdapter(HelixManager helixManager, ClusterMapConfig clusterMapConfig) {\n     this.helixManager = helixManager;\n-    this.clusterMapConfig = clusterMapConfig;\n+    this.converter = new Converter(clusterMapConfig);\n+    clusterName = clusterMapConfig.clusterMapClusterName;\n   }\n \n   @Override\n   public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n     helixManager.addInstanceConfigChangeListener((InstanceConfigChangeListener) (instanceConfigs, context) -> {\n       Iterable<DataNodeConfig> dataNodeConfigs =\n-          () -> instanceConfigs.stream().map(this::convert).filter(Objects::nonNull).iterator();\n+          () -> instanceConfigs.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n       listener.onDataNodeConfigChange(dataNodeConfigs);\n     });\n   }\n \n-  /**\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  private DataNodeConfig convert(InstanceConfig instanceConfig) {\n-    return convert(instanceConfig, clusterMapConfig);\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    InstanceConfig instanceConfig = converter.convert(config);\n+    return getHelixAdmin().setInstanceConfig(clusterName, instanceConfig.getInstanceName(), instanceConfig);\n   }\n \n-  /**\n-   * Exposed for testing.\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @param clusterMapConfig the {@link ClusterMapConfig} containing any default values that may be needed.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  static DataNodeConfig convert(InstanceConfig instanceConfig, ClusterMapConfig clusterMapConfig) {\n-    int schemaVersion = getSchemaVersion(instanceConfig);\n-    if (schemaVersion != 0) {\n-      LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n-      return null;\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    InstanceConfig instanceConfig = getHelixAdmin().getInstanceConfig(clusterName, instanceName);\n+    return instanceConfig != null ? converter.convert(instanceConfig) : null;\n+  }\n+\n+  void setHelixAdmin(HelixAdmin helixAdmin) {\n+    this.helixAdmin = helixAdmin;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODc0MTYyMw=="}, "originalCommit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5Njk5NDUyOnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNDozODowNlrOGr9KSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNjowNzoyN1rOGwvdDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODc0NDAwOA==", "bodyText": "This reminds me that we probably can use setMapField instead of setMapFields to avoid race condition that Helix concurrently writes to HELIX_DISABLED_PARTITION field.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r448744008", "createdAt": "2020-07-02T04:38:06Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "diffHunk": "@@ -33,82 +38,146 @@\n public class InstanceConfigToDataNodeConfigAdapter implements DataNodeConfigSource {\n   private static final Logger LOGGER = LoggerFactory.getLogger(InstanceConfigToDataNodeConfigAdapter.class);\n   private final HelixManager helixManager;\n-  private final ClusterMapConfig clusterMapConfig;\n+  private final Converter converter;\n+  private final String clusterName;\n+  private volatile HelixAdmin helixAdmin = null;\n \n   /**\n    * @param helixManager the {@link HelixManager} to use as the source of truth for {@link InstanceConfig}s.\n    * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n    */\n   public InstanceConfigToDataNodeConfigAdapter(HelixManager helixManager, ClusterMapConfig clusterMapConfig) {\n     this.helixManager = helixManager;\n-    this.clusterMapConfig = clusterMapConfig;\n+    this.converter = new Converter(clusterMapConfig);\n+    clusterName = clusterMapConfig.clusterMapClusterName;\n   }\n \n   @Override\n   public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n     helixManager.addInstanceConfigChangeListener((InstanceConfigChangeListener) (instanceConfigs, context) -> {\n       Iterable<DataNodeConfig> dataNodeConfigs =\n-          () -> instanceConfigs.stream().map(this::convert).filter(Objects::nonNull).iterator();\n+          () -> instanceConfigs.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n       listener.onDataNodeConfigChange(dataNodeConfigs);\n     });\n   }\n \n-  /**\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  private DataNodeConfig convert(InstanceConfig instanceConfig) {\n-    return convert(instanceConfig, clusterMapConfig);\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    InstanceConfig instanceConfig = converter.convert(config);\n+    return getHelixAdmin().setInstanceConfig(clusterName, instanceConfig.getInstanceName(), instanceConfig);\n   }\n \n-  /**\n-   * Exposed for testing.\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @param clusterMapConfig the {@link ClusterMapConfig} containing any default values that may be needed.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  static DataNodeConfig convert(InstanceConfig instanceConfig, ClusterMapConfig clusterMapConfig) {\n-    int schemaVersion = getSchemaVersion(instanceConfig);\n-    if (schemaVersion != 0) {\n-      LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n-      return null;\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    InstanceConfig instanceConfig = getHelixAdmin().getInstanceConfig(clusterName, instanceName);\n+    return instanceConfig != null ? converter.convert(instanceConfig) : null;\n+  }\n+\n+  void setHelixAdmin(HelixAdmin helixAdmin) {\n+    this.helixAdmin = helixAdmin;\n+  }\n+\n+  private HelixAdmin getHelixAdmin() {\n+    return Objects.requireNonNull(helixAdmin, \"helixAdmin not set\");\n+  }\n+\n+  static class Converter {\n+    private final ClusterMapConfig clusterMapConfig;\n+\n+    Converter(ClusterMapConfig clusterMapConfig) {\n+      this.clusterMapConfig = clusterMapConfig;\n     }\n-    DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n-        Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n-        getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n-    dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n-    dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n-    // TODO uncomment this line once 1534 is merged\n-    // dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n-    instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n-      if (diskProps.get(DISK_STATE) == null) {\n-        // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n-        // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n-        // state to InstanceConfig.\n-        LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n-            instanceConfig.getInstanceName());\n-      } else {\n-        DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n-            diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n-            Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n-        String replicasStr = diskProps.get(REPLICAS_STR);\n-        if (!replicasStr.isEmpty()) {\n-          for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n-            String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n-            // partition name and replica name are the same.\n-            String partitionName = replicaStrParts[0];\n-            long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n-            String partitionClass =\n-                replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n-            disk.getReplicaConfigs()\n-                .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+\n+    /**\n+     * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported\n+     *         schema version.\n+     */\n+    DataNodeConfig convert(InstanceConfig instanceConfig) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n+        return null;\n+      }\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n+          Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n+          getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n+      instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n+        if (diskProps.get(DISK_STATE) == null) {\n+          // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n+          // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n+          // state to InstanceConfig.\n+          LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n+              instanceConfig.getInstanceName());\n+        } else {\n+          DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n+              diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n+              Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n+          String replicasStr = diskProps.get(REPLICAS_STR);\n+          if (!Utils.isNullOrEmpty(replicasStr)) {\n+            for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n+              String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n+              // partition name and replica name are the same.\n+              String partitionName = replicaStrParts[0];\n+              long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n+              String partitionClass =\n+                  replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+              disk.getReplicaConfigs()\n+                  .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+            }\n           }\n+          dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n         }\n-        dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n+      });\n+      return dataNodeConfig;\n+    }\n+\n+    /**\n+     * @param dataNodeConfig the {@link DataNodeConfig} to convert into an {@link InstanceConfig}.\n+     * @return the {@link InstanceConfig}.\n+     */\n+    InstanceConfig convert(DataNodeConfig dataNodeConfig) {\n+      InstanceConfig instanceConfig = new InstanceConfig(dataNodeConfig.getInstanceName());\n+      instanceConfig.setHostName(dataNodeConfig.getHostName());\n+      instanceConfig.setPort(Integer.toString(dataNodeConfig.getPort()));\n+      if (dataNodeConfig.getSslPort() != null) {\n+        instanceConfig.getRecord().setIntField(SSL_PORT_STR, dataNodeConfig.getSslPort());\n       }\n-    });\n-    return dataNodeConfig;\n+      if (dataNodeConfig.getHttp2Port() != null) {\n+        instanceConfig.getRecord().setIntField(HTTP2_PORT_STR, dataNodeConfig.getHttp2Port());\n+      }\n+      instanceConfig.getRecord().setSimpleField(DATACENTER_STR, dataNodeConfig.getDatacenterName());\n+      instanceConfig.getRecord().setSimpleField(RACKID_STR, dataNodeConfig.getRackId());\n+      long xid = dataNodeConfig.getXid();\n+      if (xid != DEFAULT_XID) {\n+        // Set the XID only if it is not the default, in order to avoid unnecessary updates.\n+        instanceConfig.getRecord().setLongField(XID_STR, xid);\n+      }\n+      instanceConfig.getRecord().setIntField(SCHEMA_VERSION_STR, CURRENT_SCHEMA_VERSION);\n+      instanceConfig.getRecord().setListField(SEALED_STR, new ArrayList<>(dataNodeConfig.getSealedReplicas()));\n+      instanceConfig.getRecord()\n+          .setListField(STOPPED_REPLICAS_STR, new ArrayList<>(dataNodeConfig.getStoppedReplicas()));\n+      instanceConfig.getRecord()\n+          .setListField(DISABLED_REPLICAS_STR, new ArrayList<>(dataNodeConfig.getDisabledReplicas()));\n+      dataNodeConfig.getDiskConfigs().forEach((mountPath, diskConfig) -> {\n+        Map<String, String> diskProps = new HashMap<>();\n+        diskProps.put(DISK_STATE, diskConfig.getState() == HardwareState.AVAILABLE ? AVAILABLE_STR : UNAVAILABLE_STR);\n+        diskProps.put(DISK_CAPACITY_STR, Long.toString(diskConfig.getDiskCapacityInBytes()));\n+        StringBuilder replicasStrBuilder = new StringBuilder();\n+        diskConfig.getReplicaConfigs()\n+            .forEach((partitionName, replicaConfig) -> replicasStrBuilder.append(partitionName)\n+                .append(REPLICAS_STR_SEPARATOR)\n+                .append(replicaConfig.getReplicaCapacityInBytes())\n+                .append(REPLICAS_STR_SEPARATOR)\n+                .append(replicaConfig.getPartitionClass())\n+                .append(REPLICAS_DELIM_STR));\n+        diskProps.put(REPLICAS_STR, replicasStrBuilder.toString());\n+        instanceConfig.getRecord().setMapField(mountPath, diskProps);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "originalPosition": 208}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzczNjk3Nw==", "bodyText": "Are you referring to pr #1584 here?", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r453736977", "createdAt": "2020-07-13T15:32:15Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "diffHunk": "@@ -33,82 +38,146 @@\n public class InstanceConfigToDataNodeConfigAdapter implements DataNodeConfigSource {\n   private static final Logger LOGGER = LoggerFactory.getLogger(InstanceConfigToDataNodeConfigAdapter.class);\n   private final HelixManager helixManager;\n-  private final ClusterMapConfig clusterMapConfig;\n+  private final Converter converter;\n+  private final String clusterName;\n+  private volatile HelixAdmin helixAdmin = null;\n \n   /**\n    * @param helixManager the {@link HelixManager} to use as the source of truth for {@link InstanceConfig}s.\n    * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n    */\n   public InstanceConfigToDataNodeConfigAdapter(HelixManager helixManager, ClusterMapConfig clusterMapConfig) {\n     this.helixManager = helixManager;\n-    this.clusterMapConfig = clusterMapConfig;\n+    this.converter = new Converter(clusterMapConfig);\n+    clusterName = clusterMapConfig.clusterMapClusterName;\n   }\n \n   @Override\n   public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n     helixManager.addInstanceConfigChangeListener((InstanceConfigChangeListener) (instanceConfigs, context) -> {\n       Iterable<DataNodeConfig> dataNodeConfigs =\n-          () -> instanceConfigs.stream().map(this::convert).filter(Objects::nonNull).iterator();\n+          () -> instanceConfigs.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n       listener.onDataNodeConfigChange(dataNodeConfigs);\n     });\n   }\n \n-  /**\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  private DataNodeConfig convert(InstanceConfig instanceConfig) {\n-    return convert(instanceConfig, clusterMapConfig);\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    InstanceConfig instanceConfig = converter.convert(config);\n+    return getHelixAdmin().setInstanceConfig(clusterName, instanceConfig.getInstanceName(), instanceConfig);\n   }\n \n-  /**\n-   * Exposed for testing.\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @param clusterMapConfig the {@link ClusterMapConfig} containing any default values that may be needed.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  static DataNodeConfig convert(InstanceConfig instanceConfig, ClusterMapConfig clusterMapConfig) {\n-    int schemaVersion = getSchemaVersion(instanceConfig);\n-    if (schemaVersion != 0) {\n-      LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n-      return null;\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    InstanceConfig instanceConfig = getHelixAdmin().getInstanceConfig(clusterName, instanceName);\n+    return instanceConfig != null ? converter.convert(instanceConfig) : null;\n+  }\n+\n+  void setHelixAdmin(HelixAdmin helixAdmin) {\n+    this.helixAdmin = helixAdmin;\n+  }\n+\n+  private HelixAdmin getHelixAdmin() {\n+    return Objects.requireNonNull(helixAdmin, \"helixAdmin not set\");\n+  }\n+\n+  static class Converter {\n+    private final ClusterMapConfig clusterMapConfig;\n+\n+    Converter(ClusterMapConfig clusterMapConfig) {\n+      this.clusterMapConfig = clusterMapConfig;\n     }\n-    DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n-        Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n-        getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n-    dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n-    dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n-    // TODO uncomment this line once 1534 is merged\n-    // dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n-    instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n-      if (diskProps.get(DISK_STATE) == null) {\n-        // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n-        // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n-        // state to InstanceConfig.\n-        LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n-            instanceConfig.getInstanceName());\n-      } else {\n-        DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n-            diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n-            Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n-        String replicasStr = diskProps.get(REPLICAS_STR);\n-        if (!replicasStr.isEmpty()) {\n-          for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n-            String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n-            // partition name and replica name are the same.\n-            String partitionName = replicaStrParts[0];\n-            long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n-            String partitionClass =\n-                replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n-            disk.getReplicaConfigs()\n-                .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+\n+    /**\n+     * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported\n+     *         schema version.\n+     */\n+    DataNodeConfig convert(InstanceConfig instanceConfig) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n+        return null;\n+      }\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n+          Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n+          getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n+      instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n+        if (diskProps.get(DISK_STATE) == null) {\n+          // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n+          // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n+          // state to InstanceConfig.\n+          LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n+              instanceConfig.getInstanceName());\n+        } else {\n+          DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n+              diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n+              Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n+          String replicasStr = diskProps.get(REPLICAS_STR);\n+          if (!Utils.isNullOrEmpty(replicasStr)) {\n+            for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n+              String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n+              // partition name and replica name are the same.\n+              String partitionName = replicaStrParts[0];\n+              long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n+              String partitionClass =\n+                  replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+              disk.getReplicaConfigs()\n+                  .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+            }\n           }\n+          dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n         }\n-        dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n+      });\n+      return dataNodeConfig;\n+    }\n+\n+    /**\n+     * @param dataNodeConfig the {@link DataNodeConfig} to convert into an {@link InstanceConfig}.\n+     * @return the {@link InstanceConfig}.\n+     */\n+    InstanceConfig convert(DataNodeConfig dataNodeConfig) {\n+      InstanceConfig instanceConfig = new InstanceConfig(dataNodeConfig.getInstanceName());\n+      instanceConfig.setHostName(dataNodeConfig.getHostName());\n+      instanceConfig.setPort(Integer.toString(dataNodeConfig.getPort()));\n+      if (dataNodeConfig.getSslPort() != null) {\n+        instanceConfig.getRecord().setIntField(SSL_PORT_STR, dataNodeConfig.getSslPort());\n       }\n-    });\n-    return dataNodeConfig;\n+      if (dataNodeConfig.getHttp2Port() != null) {\n+        instanceConfig.getRecord().setIntField(HTTP2_PORT_STR, dataNodeConfig.getHttp2Port());\n+      }\n+      instanceConfig.getRecord().setSimpleField(DATACENTER_STR, dataNodeConfig.getDatacenterName());\n+      instanceConfig.getRecord().setSimpleField(RACKID_STR, dataNodeConfig.getRackId());\n+      long xid = dataNodeConfig.getXid();\n+      if (xid != DEFAULT_XID) {\n+        // Set the XID only if it is not the default, in order to avoid unnecessary updates.\n+        instanceConfig.getRecord().setLongField(XID_STR, xid);\n+      }\n+      instanceConfig.getRecord().setIntField(SCHEMA_VERSION_STR, CURRENT_SCHEMA_VERSION);\n+      instanceConfig.getRecord().setListField(SEALED_STR, new ArrayList<>(dataNodeConfig.getSealedReplicas()));\n+      instanceConfig.getRecord()\n+          .setListField(STOPPED_REPLICAS_STR, new ArrayList<>(dataNodeConfig.getStoppedReplicas()));\n+      instanceConfig.getRecord()\n+          .setListField(DISABLED_REPLICAS_STR, new ArrayList<>(dataNodeConfig.getDisabledReplicas()));\n+      dataNodeConfig.getDiskConfigs().forEach((mountPath, diskConfig) -> {\n+        Map<String, String> diskProps = new HashMap<>();\n+        diskProps.put(DISK_STATE, diskConfig.getState() == HardwareState.AVAILABLE ? AVAILABLE_STR : UNAVAILABLE_STR);\n+        diskProps.put(DISK_CAPACITY_STR, Long.toString(diskConfig.getDiskCapacityInBytes()));\n+        StringBuilder replicasStrBuilder = new StringBuilder();\n+        diskConfig.getReplicaConfigs()\n+            .forEach((partitionName, replicaConfig) -> replicasStrBuilder.append(partitionName)\n+                .append(REPLICAS_STR_SEPARATOR)\n+                .append(replicaConfig.getReplicaCapacityInBytes())\n+                .append(REPLICAS_STR_SEPARATOR)\n+                .append(replicaConfig.getPartitionClass())\n+                .append(REPLICAS_DELIM_STR));\n+        diskProps.put(REPLICAS_STR, replicasStrBuilder.toString());\n+        instanceConfig.getRecord().setMapField(mountPath, diskProps);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODc0NDAwOA=="}, "originalCommit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "originalPosition": 208}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc2MjMxNw==", "bodyText": "Correct, but later I found setMapField still has potential race condition as whole InstanceConfig will be overridden either by ambry node or by Helix. I added some await mechanism in that PR to fix it.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r453762317", "createdAt": "2020-07-13T16:07:27Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "diffHunk": "@@ -33,82 +38,146 @@\n public class InstanceConfigToDataNodeConfigAdapter implements DataNodeConfigSource {\n   private static final Logger LOGGER = LoggerFactory.getLogger(InstanceConfigToDataNodeConfigAdapter.class);\n   private final HelixManager helixManager;\n-  private final ClusterMapConfig clusterMapConfig;\n+  private final Converter converter;\n+  private final String clusterName;\n+  private volatile HelixAdmin helixAdmin = null;\n \n   /**\n    * @param helixManager the {@link HelixManager} to use as the source of truth for {@link InstanceConfig}s.\n    * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n    */\n   public InstanceConfigToDataNodeConfigAdapter(HelixManager helixManager, ClusterMapConfig clusterMapConfig) {\n     this.helixManager = helixManager;\n-    this.clusterMapConfig = clusterMapConfig;\n+    this.converter = new Converter(clusterMapConfig);\n+    clusterName = clusterMapConfig.clusterMapClusterName;\n   }\n \n   @Override\n   public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n     helixManager.addInstanceConfigChangeListener((InstanceConfigChangeListener) (instanceConfigs, context) -> {\n       Iterable<DataNodeConfig> dataNodeConfigs =\n-          () -> instanceConfigs.stream().map(this::convert).filter(Objects::nonNull).iterator();\n+          () -> instanceConfigs.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n       listener.onDataNodeConfigChange(dataNodeConfigs);\n     });\n   }\n \n-  /**\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  private DataNodeConfig convert(InstanceConfig instanceConfig) {\n-    return convert(instanceConfig, clusterMapConfig);\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    InstanceConfig instanceConfig = converter.convert(config);\n+    return getHelixAdmin().setInstanceConfig(clusterName, instanceConfig.getInstanceName(), instanceConfig);\n   }\n \n-  /**\n-   * Exposed for testing.\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @param clusterMapConfig the {@link ClusterMapConfig} containing any default values that may be needed.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  static DataNodeConfig convert(InstanceConfig instanceConfig, ClusterMapConfig clusterMapConfig) {\n-    int schemaVersion = getSchemaVersion(instanceConfig);\n-    if (schemaVersion != 0) {\n-      LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n-      return null;\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    InstanceConfig instanceConfig = getHelixAdmin().getInstanceConfig(clusterName, instanceName);\n+    return instanceConfig != null ? converter.convert(instanceConfig) : null;\n+  }\n+\n+  void setHelixAdmin(HelixAdmin helixAdmin) {\n+    this.helixAdmin = helixAdmin;\n+  }\n+\n+  private HelixAdmin getHelixAdmin() {\n+    return Objects.requireNonNull(helixAdmin, \"helixAdmin not set\");\n+  }\n+\n+  static class Converter {\n+    private final ClusterMapConfig clusterMapConfig;\n+\n+    Converter(ClusterMapConfig clusterMapConfig) {\n+      this.clusterMapConfig = clusterMapConfig;\n     }\n-    DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n-        Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n-        getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n-    dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n-    dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n-    // TODO uncomment this line once 1534 is merged\n-    // dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n-    instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n-      if (diskProps.get(DISK_STATE) == null) {\n-        // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n-        // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n-        // state to InstanceConfig.\n-        LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n-            instanceConfig.getInstanceName());\n-      } else {\n-        DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n-            diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n-            Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n-        String replicasStr = diskProps.get(REPLICAS_STR);\n-        if (!replicasStr.isEmpty()) {\n-          for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n-            String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n-            // partition name and replica name are the same.\n-            String partitionName = replicaStrParts[0];\n-            long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n-            String partitionClass =\n-                replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n-            disk.getReplicaConfigs()\n-                .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+\n+    /**\n+     * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported\n+     *         schema version.\n+     */\n+    DataNodeConfig convert(InstanceConfig instanceConfig) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n+        return null;\n+      }\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n+          Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n+          getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n+      instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n+        if (diskProps.get(DISK_STATE) == null) {\n+          // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n+          // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n+          // state to InstanceConfig.\n+          LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n+              instanceConfig.getInstanceName());\n+        } else {\n+          DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n+              diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n+              Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n+          String replicasStr = diskProps.get(REPLICAS_STR);\n+          if (!Utils.isNullOrEmpty(replicasStr)) {\n+            for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n+              String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n+              // partition name and replica name are the same.\n+              String partitionName = replicaStrParts[0];\n+              long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n+              String partitionClass =\n+                  replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+              disk.getReplicaConfigs()\n+                  .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+            }\n           }\n+          dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n         }\n-        dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n+      });\n+      return dataNodeConfig;\n+    }\n+\n+    /**\n+     * @param dataNodeConfig the {@link DataNodeConfig} to convert into an {@link InstanceConfig}.\n+     * @return the {@link InstanceConfig}.\n+     */\n+    InstanceConfig convert(DataNodeConfig dataNodeConfig) {\n+      InstanceConfig instanceConfig = new InstanceConfig(dataNodeConfig.getInstanceName());\n+      instanceConfig.setHostName(dataNodeConfig.getHostName());\n+      instanceConfig.setPort(Integer.toString(dataNodeConfig.getPort()));\n+      if (dataNodeConfig.getSslPort() != null) {\n+        instanceConfig.getRecord().setIntField(SSL_PORT_STR, dataNodeConfig.getSslPort());\n       }\n-    });\n-    return dataNodeConfig;\n+      if (dataNodeConfig.getHttp2Port() != null) {\n+        instanceConfig.getRecord().setIntField(HTTP2_PORT_STR, dataNodeConfig.getHttp2Port());\n+      }\n+      instanceConfig.getRecord().setSimpleField(DATACENTER_STR, dataNodeConfig.getDatacenterName());\n+      instanceConfig.getRecord().setSimpleField(RACKID_STR, dataNodeConfig.getRackId());\n+      long xid = dataNodeConfig.getXid();\n+      if (xid != DEFAULT_XID) {\n+        // Set the XID only if it is not the default, in order to avoid unnecessary updates.\n+        instanceConfig.getRecord().setLongField(XID_STR, xid);\n+      }\n+      instanceConfig.getRecord().setIntField(SCHEMA_VERSION_STR, CURRENT_SCHEMA_VERSION);\n+      instanceConfig.getRecord().setListField(SEALED_STR, new ArrayList<>(dataNodeConfig.getSealedReplicas()));\n+      instanceConfig.getRecord()\n+          .setListField(STOPPED_REPLICAS_STR, new ArrayList<>(dataNodeConfig.getStoppedReplicas()));\n+      instanceConfig.getRecord()\n+          .setListField(DISABLED_REPLICAS_STR, new ArrayList<>(dataNodeConfig.getDisabledReplicas()));\n+      dataNodeConfig.getDiskConfigs().forEach((mountPath, diskConfig) -> {\n+        Map<String, String> diskProps = new HashMap<>();\n+        diskProps.put(DISK_STATE, diskConfig.getState() == HardwareState.AVAILABLE ? AVAILABLE_STR : UNAVAILABLE_STR);\n+        diskProps.put(DISK_CAPACITY_STR, Long.toString(diskConfig.getDiskCapacityInBytes()));\n+        StringBuilder replicasStrBuilder = new StringBuilder();\n+        diskConfig.getReplicaConfigs()\n+            .forEach((partitionName, replicaConfig) -> replicasStrBuilder.append(partitionName)\n+                .append(REPLICAS_STR_SEPARATOR)\n+                .append(replicaConfig.getReplicaCapacityInBytes())\n+                .append(REPLICAS_STR_SEPARATOR)\n+                .append(replicaConfig.getPartitionClass())\n+                .append(REPLICAS_DELIM_STR));\n+        diskProps.put(REPLICAS_STR, replicasStrBuilder.toString());\n+        instanceConfig.getRecord().setMapField(mountPath, diskProps);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODc0NDAwOA=="}, "originalCommit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "originalPosition": 208}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5NzA0NjEwOnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNTowODowNVrOGr9n-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNTozNTozM1rOGwuDUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODc1MTYxMA==", "bodyText": "This comment might be out of scope but I wonder if node (frontend/server) lost connection to ZK and got reconnected, is there a mechanism to re-read all datanode configs in PropertyStore (and re-populate in-mem clustermap)?\nAs far as I know, InstanceConfig current has an async way to read all configs again (not ideal) but in next release of Helix, it's changed to a sync way, which guarantees all configs will be read again after reconnection.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r448751610", "createdAt": "2020-07-02T05:08:05Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "diffHunk": "@@ -0,0 +1,278 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *\n+ */\n+\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import com.github.ambry.utils.Utils;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.Executors;\n+import org.apache.helix.AccessOption;\n+import org.apache.helix.store.HelixPropertyListener;\n+import org.apache.helix.store.HelixPropertyStore;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+import org.apache.zookeeper.data.Stat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * An implementation of {@link DataNodeConfigSource} that reads {@link ZNRecord}s stored under the\n+ * \"/DataNodeConfigs\" path in a {@link HelixPropertyStore}.\n+ */\n+public class PropertyStoreToDataNodeConfigAdapter implements DataNodeConfigSource {\n+  static final String CONFIG_PATH = \"/DataNodeConfigs\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(PropertyStoreToDataNodeConfigAdapter.class);\n+  private final HelixPropertyStore<ZNRecord> propertyStore;\n+  private final Converter converter;\n+  private final Executor eventExecutor;\n+\n+  /**\n+   * @param propertyStore the {@link HelixPropertyStore} instance to use to interact with zookeeper.\n+   * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n+   * @param dcName the datacenter name for for the property store.\n+   */\n+  public PropertyStoreToDataNodeConfigAdapter(HelixPropertyStore<ZNRecord> propertyStore,\n+      ClusterMapConfig clusterMapConfig, String dcName) {\n+    this.propertyStore = propertyStore;\n+    this.converter = new Converter(clusterMapConfig.clusterMapDefaultPartitionClass, dcName);\n+    this.eventExecutor = Executors.newSingleThreadExecutor();\n+  }\n+\n+  @Override\n+  public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n+    Subscription subscription = new Subscription(listener);\n+    propertyStore.subscribe(CONFIG_PATH, subscription);\n+    subscription.start();\n+  }\n+\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    ZNRecord record = converter.convert(config);\n+    String path = CONFIG_PATH + \"/\" + record.getId();\n+    return propertyStore.set(path, record, AccessOption.PERSISTENT);\n+  }\n+\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    String path = CONFIG_PATH + \"/\" + instanceName;\n+    ZNRecord record = propertyStore.get(path, new Stat(), AccessOption.PERSISTENT);\n+    return record != null ? converter.convert(record) : null;\n+  }\n+\n+  private class Subscription implements HelixPropertyListener {\n+    private final DataNodeConfigChangeListener listener;\n+    private final CompletableFuture<Void> initFuture = new CompletableFuture<>();\n+\n+    Subscription(DataNodeConfigChangeListener listener) {\n+      this.listener = listener;\n+    }\n+\n+    /**\n+     * Tell the background executor to perform any initialization needed. This should be called after registering this\n+     * listener using {@link HelixPropertyStore#subscribe}. This will ensure that the first event sent to\n+     * {@link #listener} will contain the entire set of current configs.\n+     * @throws Exception if there was an exception while making the initialization call.\n+     */\n+    void start() throws Exception {\n+      eventExecutor.execute(this::initializeIfNeeded);\n+      try {\n+        initFuture.get();\n+      } catch (ExecutionException e) {\n+        throw Utils.extractExecutionExceptionCause(e);\n+      }\n+    }\n+\n+    @Override\n+    public void onDataChange(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} changed\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataCreate(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} created\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataDelete(String path) {\n+      // TODO handle node deletions dynamically. Doing so requires further work in the ClusterChangeHandler impl\n+      LOGGER.info(\"DataNodeConfig path {} deleted. This requires a restart to handle\", path);\n+    }\n+\n+    private void onPathChange(String path) {\n+      eventExecutor.execute(() -> updateOrInitialize(path));\n+    }\n+\n+    private Iterable<DataNodeConfig> lazyIterable(Collection<ZNRecord> records) {\n+      return () -> records.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n+    }\n+\n+    /**\n+     * Either perform a full initialization or notify the listener about an incremental update to a path. This should be\n+     * called from the event executor thread.\n+     * @param changedPath the path that changed.\n+     */\n+    private synchronized void updateOrInitialize(String changedPath) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzczOTM0Nw==", "bodyText": "That is a good point, since you could potentially miss updates while zk was disconnected. I will follow up with Helix regarding this and make changes in a future PR if needed. If it is not possible for this to happen with the PropertyStore impl, we could consider using a background polling thread that periodically fetches all configs.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r453739347", "createdAt": "2020-07-13T15:35:33Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "diffHunk": "@@ -0,0 +1,278 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *\n+ */\n+\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import com.github.ambry.utils.Utils;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.Executors;\n+import org.apache.helix.AccessOption;\n+import org.apache.helix.store.HelixPropertyListener;\n+import org.apache.helix.store.HelixPropertyStore;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+import org.apache.zookeeper.data.Stat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * An implementation of {@link DataNodeConfigSource} that reads {@link ZNRecord}s stored under the\n+ * \"/DataNodeConfigs\" path in a {@link HelixPropertyStore}.\n+ */\n+public class PropertyStoreToDataNodeConfigAdapter implements DataNodeConfigSource {\n+  static final String CONFIG_PATH = \"/DataNodeConfigs\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(PropertyStoreToDataNodeConfigAdapter.class);\n+  private final HelixPropertyStore<ZNRecord> propertyStore;\n+  private final Converter converter;\n+  private final Executor eventExecutor;\n+\n+  /**\n+   * @param propertyStore the {@link HelixPropertyStore} instance to use to interact with zookeeper.\n+   * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n+   * @param dcName the datacenter name for for the property store.\n+   */\n+  public PropertyStoreToDataNodeConfigAdapter(HelixPropertyStore<ZNRecord> propertyStore,\n+      ClusterMapConfig clusterMapConfig, String dcName) {\n+    this.propertyStore = propertyStore;\n+    this.converter = new Converter(clusterMapConfig.clusterMapDefaultPartitionClass, dcName);\n+    this.eventExecutor = Executors.newSingleThreadExecutor();\n+  }\n+\n+  @Override\n+  public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n+    Subscription subscription = new Subscription(listener);\n+    propertyStore.subscribe(CONFIG_PATH, subscription);\n+    subscription.start();\n+  }\n+\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    ZNRecord record = converter.convert(config);\n+    String path = CONFIG_PATH + \"/\" + record.getId();\n+    return propertyStore.set(path, record, AccessOption.PERSISTENT);\n+  }\n+\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    String path = CONFIG_PATH + \"/\" + instanceName;\n+    ZNRecord record = propertyStore.get(path, new Stat(), AccessOption.PERSISTENT);\n+    return record != null ? converter.convert(record) : null;\n+  }\n+\n+  private class Subscription implements HelixPropertyListener {\n+    private final DataNodeConfigChangeListener listener;\n+    private final CompletableFuture<Void> initFuture = new CompletableFuture<>();\n+\n+    Subscription(DataNodeConfigChangeListener listener) {\n+      this.listener = listener;\n+    }\n+\n+    /**\n+     * Tell the background executor to perform any initialization needed. This should be called after registering this\n+     * listener using {@link HelixPropertyStore#subscribe}. This will ensure that the first event sent to\n+     * {@link #listener} will contain the entire set of current configs.\n+     * @throws Exception if there was an exception while making the initialization call.\n+     */\n+    void start() throws Exception {\n+      eventExecutor.execute(this::initializeIfNeeded);\n+      try {\n+        initFuture.get();\n+      } catch (ExecutionException e) {\n+        throw Utils.extractExecutionExceptionCause(e);\n+      }\n+    }\n+\n+    @Override\n+    public void onDataChange(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} changed\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataCreate(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} created\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataDelete(String path) {\n+      // TODO handle node deletions dynamically. Doing so requires further work in the ClusterChangeHandler impl\n+      LOGGER.info(\"DataNodeConfig path {} deleted. This requires a restart to handle\", path);\n+    }\n+\n+    private void onPathChange(String path) {\n+      eventExecutor.execute(() -> updateOrInitialize(path));\n+    }\n+\n+    private Iterable<DataNodeConfig> lazyIterable(Collection<ZNRecord> records) {\n+      return () -> records.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n+    }\n+\n+    /**\n+     * Either perform a full initialization or notify the listener about an incremental update to a path. This should be\n+     * called from the event executor thread.\n+     * @param changedPath the path that changed.\n+     */\n+    private synchronized void updateOrInitialize(String changedPath) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODc1MTYxMA=="}, "originalCommit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "originalPosition": 140}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1244, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}