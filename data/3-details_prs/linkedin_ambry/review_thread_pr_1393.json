{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc3NTQyNDQ3", "number": 1393, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMjoxNzozMFrODiB25g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMjoyNDo1OVrODiB9OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDA4NjE0OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosDataAccessor.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMjoxNzozMFrOFtGQYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMzo0MDo0N1rOFtHpIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzMjczOA==", "bodyText": "what is the format of results here? Instead of findFirst could you get the relevant entry by looking up its key?", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382832738", "createdAt": "2020-02-21T22:17:30Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -155,6 +157,28 @@ void testConnectivity() {\n     }\n   }\n \n+  /**\n+   * Get the number of blobs in the specified partition matching the specified DocumentDB query.\n+   * @param partitionPath the partition to query.\n+   * @param querySpec the DocumentDB query to execute.\n+   * @param timer the {@link Timer} to use to record query time (excluding waiting).\n+   * @return the number of matching blobs.\n+   */\n+  int countMetadata(String partitionPath, SqlQuerySpec querySpec, Timer timer)\n+      throws DocumentClientException {\n+    FeedOptions feedOptions = new FeedOptions();\n+    feedOptions.setPartitionKey(new PartitionKey(partitionPath));\n+    try {\n+      FeedResponse<Document> response = executeCosmosQuery(querySpec, feedOptions, timer).single();\n+      return ((Number) response.getResults().get(0).getHashMap().values().stream().findFirst().get()).intValue();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg1NDE1Mw==", "bodyText": "The returned document is: {\"_aggregate\":20}\nI can change it to lookup by the key, it just involves ugly hardcoding and I could break if they decide to change the key name.", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382854153", "createdAt": "2020-02-21T23:34:30Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -155,6 +157,28 @@ void testConnectivity() {\n     }\n   }\n \n+  /**\n+   * Get the number of blobs in the specified partition matching the specified DocumentDB query.\n+   * @param partitionPath the partition to query.\n+   * @param querySpec the DocumentDB query to execute.\n+   * @param timer the {@link Timer} to use to record query time (excluding waiting).\n+   * @return the number of matching blobs.\n+   */\n+  int countMetadata(String partitionPath, SqlQuerySpec querySpec, Timer timer)\n+      throws DocumentClientException {\n+    FeedOptions feedOptions = new FeedOptions();\n+    feedOptions.setPartitionKey(new PartitionKey(partitionPath));\n+    try {\n+      FeedResponse<Document> response = executeCosmosQuery(querySpec, feedOptions, timer).single();\n+      return ((Number) response.getResults().get(0).getHashMap().values().stream().findFirst().get()).intValue();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzMjczOA=="}, "originalCommit": {"oid": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg1NTQ1OQ==", "bodyText": "Never mind, Cosmos does have a constant for it: Constants.Properties.AGGREGATE, so I'll make the change.", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382855459", "createdAt": "2020-02-21T23:40:47Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -155,6 +157,28 @@ void testConnectivity() {\n     }\n   }\n \n+  /**\n+   * Get the number of blobs in the specified partition matching the specified DocumentDB query.\n+   * @param partitionPath the partition to query.\n+   * @param querySpec the DocumentDB query to execute.\n+   * @param timer the {@link Timer} to use to record query time (excluding waiting).\n+   * @return the number of matching blobs.\n+   */\n+  int countMetadata(String partitionPath, SqlQuerySpec querySpec, Timer timer)\n+      throws DocumentClientException {\n+    FeedOptions feedOptions = new FeedOptions();\n+    feedOptions.setPartitionKey(new PartitionKey(partitionPath));\n+    try {\n+      FeedResponse<Document> response = executeCosmosQuery(querySpec, feedOptions, timer).single();\n+      return ((Number) response.getResults().get(0).getHashMap().values().stream().findFirst().get()).intValue();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzMjczOA=="}, "originalCommit": {"oid": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDA5MTgzOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/azure/AzureIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMjoyMDoxMFrOFtGT7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMzo1Mjo0NlrOFtHzJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzMzY0NQ==", "bodyText": "let's make a github issue to track that this test needs a fix", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382833645", "createdAt": "2020-02-21T22:20:10Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/azure/AzureIntegrationTest.java", "diffHunk": "@@ -298,23 +269,58 @@ public void testPurgeDeadBlobs() throws Exception {\n           azureDest.uploadBlob(blobId, blobSize, cloudBlobMetadata, inputStream));\n     }\n \n-    // run getDeadBlobs query, should return 20\n+    // run getDeadBlobs query, should return 2 * bucketCount\n     String partitionPath = String.valueOf(testPartition);\n-    List<CloudBlobMetadata> deadBlobs = azureDest.getDeadBlobs(partitionPath);\n-    assertEquals(\"Unexpected number of dead blobs\", expectedDeadBlobs, deadBlobs.size());\n-\n-    logger.info(\"Running purge\");\n-    int numPurged = azureDest.purgeBlobs(deadBlobs);\n-    assertEquals(\"Not all blobs were purged\", expectedDeadBlobs, numPurged);\n+    assertEquals(\"Unexpected number of dead blobs\", expectedDeadBlobs, azureDest.getDeadBlobCount(partitionPath, now));\n+    logger.info(\"First call to getDeadBlobs\");\n+    List<CloudBlobMetadata> deadBlobs = azureDest.getDeadBlobs(partitionPath, now, bucketCount);\n+    assertEquals(\"Unexpected number returned\", bucketCount, deadBlobs.size());\n+    logger.info(\"First call to purge\");\n+    assertEquals(\"Not all blobs were purged\", bucketCount, azureDest.purgeBlobs(deadBlobs));\n+    logger.info(\"Second call to getDeadBlobs\");\n+    deadBlobs = azureDest.getDeadBlobs(partitionPath, now, bucketCount);\n+    assertEquals(\"Unexpected number returned\", bucketCount, deadBlobs.size());\n+    logger.info(\"Second call to purge\");\n+    assertEquals(\"Not all blobs were purged\", bucketCount, azureDest.purgeBlobs(deadBlobs));\n+    logger.info(\"Final call to getDeadBlobs\");\n+    deadBlobs = azureDest.getDeadBlobs(partitionPath, now, bucketCount);\n+    assertEquals(\"Expected zero\", 0, deadBlobs.size());\n     cleanup();\n   }\n \n   /**\n-   * Test findEntriesSince.\n-   * @throws Exception on error\n+   * Test findEntriesSince with CosmosUpdateTimeFindTokenFactory.\n+   */\n+  @Test\n+  public void testFindEntriesSinceByUpdateTime() throws Exception {\n+    testFindEntriesSince(\"com.github.ambry.cloud.azure.CosmosUpdateTimeFindTokenFactory\");\n+  }\n+\n+  /**\n+   * Test findEntriesSince with CosmosChangeFeedFindTokenFactory.\n    */\n+  @Ignore // Fails with wrong number of queries.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg1ODAyMg==", "bodyText": "Good point.  Filed #1394 to track.", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382858022", "createdAt": "2020-02-21T23:52:46Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/azure/AzureIntegrationTest.java", "diffHunk": "@@ -298,23 +269,58 @@ public void testPurgeDeadBlobs() throws Exception {\n           azureDest.uploadBlob(blobId, blobSize, cloudBlobMetadata, inputStream));\n     }\n \n-    // run getDeadBlobs query, should return 20\n+    // run getDeadBlobs query, should return 2 * bucketCount\n     String partitionPath = String.valueOf(testPartition);\n-    List<CloudBlobMetadata> deadBlobs = azureDest.getDeadBlobs(partitionPath);\n-    assertEquals(\"Unexpected number of dead blobs\", expectedDeadBlobs, deadBlobs.size());\n-\n-    logger.info(\"Running purge\");\n-    int numPurged = azureDest.purgeBlobs(deadBlobs);\n-    assertEquals(\"Not all blobs were purged\", expectedDeadBlobs, numPurged);\n+    assertEquals(\"Unexpected number of dead blobs\", expectedDeadBlobs, azureDest.getDeadBlobCount(partitionPath, now));\n+    logger.info(\"First call to getDeadBlobs\");\n+    List<CloudBlobMetadata> deadBlobs = azureDest.getDeadBlobs(partitionPath, now, bucketCount);\n+    assertEquals(\"Unexpected number returned\", bucketCount, deadBlobs.size());\n+    logger.info(\"First call to purge\");\n+    assertEquals(\"Not all blobs were purged\", bucketCount, azureDest.purgeBlobs(deadBlobs));\n+    logger.info(\"Second call to getDeadBlobs\");\n+    deadBlobs = azureDest.getDeadBlobs(partitionPath, now, bucketCount);\n+    assertEquals(\"Unexpected number returned\", bucketCount, deadBlobs.size());\n+    logger.info(\"Second call to purge\");\n+    assertEquals(\"Not all blobs were purged\", bucketCount, azureDest.purgeBlobs(deadBlobs));\n+    logger.info(\"Final call to getDeadBlobs\");\n+    deadBlobs = azureDest.getDeadBlobs(partitionPath, now, bucketCount);\n+    assertEquals(\"Expected zero\", 0, deadBlobs.size());\n     cleanup();\n   }\n \n   /**\n-   * Test findEntriesSince.\n-   * @throws Exception on error\n+   * Test findEntriesSince with CosmosUpdateTimeFindTokenFactory.\n+   */\n+  @Test\n+  public void testFindEntriesSinceByUpdateTime() throws Exception {\n+    testFindEntriesSince(\"com.github.ambry.cloud.azure.CosmosUpdateTimeFindTokenFactory\");\n+  }\n+\n+  /**\n+   * Test findEntriesSince with CosmosChangeFeedFindTokenFactory.\n    */\n+  @Ignore // Fails with wrong number of queries.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzMzY0NQ=="}, "originalCommit": {"oid": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017"}, "originalPosition": 144}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDEwMjMzOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudStorageCompactor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMjoyNDo1OVrOFtGaMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMzo0Mjo1N1rOFtHrAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzNTI1MQ==", "bodyText": "do we need to sleep in between successive calls to getDeadBlobs, or do you feel that the cosmo's retry-with-backoff will be good enough for rate limiting this query?", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382835251", "createdAt": "2020-02-21T22:24:59Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudStorageCompactor.java", "diffHunk": "@@ -91,14 +99,25 @@ public int compactPartitions() {\n   /**\n    * Purge the inactive blobs in the specified partition.\n    * @param partitionPath the partition to compact.\n-   * @return the number of blobs purged.\n+   * @param cutoffTime the time at which a blob's active status should be evaluated.\n+   * @return the number of blobs purged or found.\n    */\n-  public int compactPartition(String partitionPath) throws CloudStorageException {\n-    List<CloudBlobMetadata> deadBlobs = cloudDestination.getDeadBlobs(partitionPath);\n-    if (!testMode) {\n-      return cloudDestination.purgeBlobs(deadBlobs);\n-    } else {\n-      return deadBlobs.size();\n+  public int compactPartition(String partitionPath, long cutoffTime) throws CloudStorageException {\n+    if (testMode) {\n+      return cloudDestination.getDeadBlobCount(partitionPath, cutoffTime);\n     }\n+\n+    int numDeadBlobs = 0;\n+    // Iterate until returned list size < limit or time runs out\n+    while (System.currentTimeMillis() < cutoffTime + compactionTimeLimitMs) {\n+      List<CloudBlobMetadata> deadBlobs =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg1NTkzNw==", "bodyText": "There is an expensive call to cloudDestination.purgeBlobs() between calls to getDeadBlobs, and that should provide more than enough throttling between queries.", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382855937", "createdAt": "2020-02-21T23:42:57Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudStorageCompactor.java", "diffHunk": "@@ -91,14 +99,25 @@ public int compactPartitions() {\n   /**\n    * Purge the inactive blobs in the specified partition.\n    * @param partitionPath the partition to compact.\n-   * @return the number of blobs purged.\n+   * @param cutoffTime the time at which a blob's active status should be evaluated.\n+   * @return the number of blobs purged or found.\n    */\n-  public int compactPartition(String partitionPath) throws CloudStorageException {\n-    List<CloudBlobMetadata> deadBlobs = cloudDestination.getDeadBlobs(partitionPath);\n-    if (!testMode) {\n-      return cloudDestination.purgeBlobs(deadBlobs);\n-    } else {\n-      return deadBlobs.size();\n+  public int compactPartition(String partitionPath, long cutoffTime) throws CloudStorageException {\n+    if (testMode) {\n+      return cloudDestination.getDeadBlobCount(partitionPath, cutoffTime);\n     }\n+\n+    int numDeadBlobs = 0;\n+    // Iterate until returned list size < limit or time runs out\n+    while (System.currentTimeMillis() < cutoffTime + compactionTimeLimitMs) {\n+      List<CloudBlobMetadata> deadBlobs =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzNTI1MQ=="}, "originalCommit": {"oid": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017"}, "originalPosition": 73}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1727, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}