{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk4MzI5NjYy", "number": 1455, "title": "Start supporting undelete in compaction", "bodyText": "Please reference it to https://docs.google.com/document/d/1rm6NBOUeZRMbuF26116lerytiW3tde38I23cZVr3SG0/edit?pli=1#heading=h.hdj8rn37deoq", "createdAt": "2020-04-03T18:45:45Z", "url": "https://github.com/linkedin/ambry/pull/1455", "merged": true, "mergeCommit": {"oid": "040577e90fe067e9b792309082fa99fb6df1fe92"}, "closed": true, "closedAt": "2020-04-22T16:24:30Z", "author": {"login": "justinlin-linkedin"}, "timelineItems": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcVffragBqjMyMTIzOTk4NTk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcaKzhDAFqTM5ODM2MTE0Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "79fe8287e375853499b93c44fffe66cf64375b85", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/79fe8287e375853499b93c44fffe66cf64375b85", "committedDate": "2020-04-03T18:44:01Z", "message": "WIP: start supporting undelete in compaction"}, "afterCommit": {"oid": "e70902eec88ce420333b4a896a6522b08c21fe74", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/e70902eec88ce420333b4a896a6522b08c21fe74", "committedDate": "2020-04-08T03:36:04Z", "message": "Fix compactor test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxMjg0NjYx", "url": "https://github.com/linkedin/ambry/pull/1455#pullrequestreview-391284661", "createdAt": "2020-04-10T06:19:56Z", "commit": {"oid": "590ee784f0700f82f335a060ece1494f33198e1e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQwNjoxOTo1NlrOGDyHgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQwNjoxOTo1NlrOGDyHgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjYyMDAzNQ==", "bodyText": "Can you explain why this logic is moved?", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r406620035", "createdAt": "2020-04-10T06:19:56Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -556,27 +530,15 @@ private boolean copyDataByLogSegment(LogSegment logSegmentToCopy, FileSpan dupli\n   private boolean copyDataByIndexSegment(LogSegment logSegmentToCopy, IndexSegment indexSegmentToCopy,\n       FileSpan duplicateSearchSpan) throws IOException, StoreException {\n     logger.debug(\"Copying data from {}\", indexSegmentToCopy.getFile());\n-    List<IndexEntry> allIndexEntries = new ArrayList<>();\n     // call into diskIOScheduler to make sure we can proceed (assuming it won't be 0).\n     diskIOScheduler.getSlice(INDEX_SEGMENT_READ_JOB_NAME, INDEX_SEGMENT_READ_JOB_NAME, 1);\n-    // get all entries. We get one entry per key\n-    indexSegmentToCopy.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n-        new AtomicLong(0), true);\n-\n-    // save a token for restart (the key gets ignored but is required to be non null for construction)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "590ee784f0700f82f335a060ece1494f33198e1e"}, "originalPosition": 53}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "590ee784f0700f82f335a060ece1494f33198e1e", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/590ee784f0700f82f335a060ece1494f33198e1e", "committedDate": "2020-04-09T04:35:25Z", "message": "Change"}, "afterCommit": {"oid": "069572cfa54e9caf48fcfb808b68354714d93701", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/069572cfa54e9caf48fcfb808b68354714d93701", "committedDate": "2020-04-10T19:49:38Z", "message": "More comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkzMDgxNDg3", "url": "https://github.com/linkedin/ambry/pull/1455#pullrequestreview-393081487", "createdAt": "2020-04-14T16:10:11Z", "commit": {"oid": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNjoxMDoxMlrOGFWLiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQwNTo0OTo1OFrOGFqspQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI1OTQ2NA==", "bodyText": "java doc please", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r408259464", "createdAt": "2020-04-14T16:10:12Z", "author": {"login": "jsjtzyy"}, "path": "ambry-api/src/main/java/com/github/ambry/config/StoreConfig.java", "diffHunk": "@@ -110,6 +110,10 @@\n   @Default(\"10*1024*1024\")\n   public final int storeCompactionMinBufferSize;\n \n+  @Config(\"store.compaction.filter\")\n+  @Default(\"IndexSegmentValidEntryFilterWithoutUndelete\")\n+  public final String storeCompactionFilter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU2NjcwOA==", "bodyText": "it seems filter here is unnecessary, can be removed", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r408566708", "createdAt": "2020-04-15T03:59:28Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -125,6 +127,13 @@ public boolean accept(File dir, String name) {\n     this.sessionId = sessionId;\n     this.incarnationId = incarnationId;\n     this.useDirectIO = Utils.isLinux() && config.storeCompactionEnableDirectIO;\n+    IndexSegmentValidEntryFilter filter;\n+    if (config.storeCompactionFilter.equals(IndexSegmentValidEntryFilterWithoutUndelete.class.getSimpleName())) {\n+      filter = new IndexSegmentValidEntryFilterWithoutUndelete();\n+    } else {\n+      filter = new IndexSegmentValidEntryFilterWithUndelete();\n+    }\n+    validEntryFilter = filter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU2ODQ4MA==", "bodyText": "nit: without", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r408568480", "createdAt": "2020-04-15T04:07:00Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with out undelete log records.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1"}, "originalPosition": 457}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU4MDAyMw==", "bodyText": "I know this is from previous code, could you clarify a little bit? In current implementation, does this mean TtlUpdate index entry will be dropped,  PUT entry's ttl is set to -1 and TtlUpdate record won't be copied to log segment?", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r408580023", "createdAt": "2020-04-15T04:54:01Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with out undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1"}, "originalPosition": 508}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5MDgyNw==", "bodyText": "When will this happen?", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r408590827", "createdAt": "2020-04-15T05:34:33Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with out undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1"}, "originalPosition": 831}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5NTEwMA==", "bodyText": "what does the retention mean, within retention time?", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r408595100", "createdAt": "2020-04-15T05:48:22Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with out undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isTtlUpdate()) {\n+          if (currentFinalState.isPut()) {\n+            // If isPut returns true, when the final state doesn't carry ttl_update flag, this is wrong\n+            throw new IllegalStateException(\n+                \"TtlUpdate's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          // This is a TTL_UPDATE record, then the blob can't be expired. Only check if it's deleted or not.\n+          if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else if (isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(currentKey,\n+                indexSegment.getStartOffset())) {\n+              validEntries.add(entry);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        } else {\n+          if (srcIndex.isExpired(currentFinalState)) {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                entry, indexSegment.getStartOffset(), storeId);\n+            continue;\n+          }\n+          if (currentFinalState.isPut()) {\n+            if (currentFinalState.getLifeVersion() != currentValue.getLifeVersion()) {\n+              throw new IllegalStateException(\n+                  \"Two different lifeVersions  for puts key\" + currentKey + \" in store \" + dataDir);\n+            }\n+            validEntries.add(entry);\n+          } else if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else {\n+              logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                  entry, indexSegment.getStartOffset(), storeId);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(StoreKey key,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1"}, "originalPosition": 907}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5NTYyMQ==", "bodyText": "Can you explain the intention of currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()?\nAlso looks like if CurrentFinalState.isDelete(), the if/else if can be combined together.", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r408595621", "createdAt": "2020-04-15T05:49:58Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with out undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isTtlUpdate()) {\n+          if (currentFinalState.isPut()) {\n+            // If isPut returns true, when the final state doesn't carry ttl_update flag, this is wrong\n+            throw new IllegalStateException(\n+                \"TtlUpdate's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          // This is a TTL_UPDATE record, then the blob can't be expired. Only check if it's deleted or not.\n+          if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1"}, "originalPosition": 863}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/c51c4385c4715b5bfb98e932cd3f9bf0a4629df1", "committedDate": "2020-04-13T18:34:40Z", "message": "Use config to turn on new code"}, "afterCommit": {"oid": "f63f846e7ecc866775a4712892c9ae06c1900daa", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/f63f846e7ecc866775a4712892c9ae06c1900daa", "committedDate": "2020-04-15T21:27:58Z", "message": "Comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0MjM4NjEz", "url": "https://github.com/linkedin/ambry/pull/1455#pullrequestreview-394238613", "createdAt": "2020-04-16T01:23:23Z", "commit": {"oid": "f63f846e7ecc866775a4712892c9ae06c1900daa"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwMToyMzoyM1rOGGRK3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwMToyMzoyM1rOGGRK3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTIyNTk0OA==", "bodyText": "This can be simplified to return srcValue.isUndelete(). We have precluded other cases.  (The comment doesn't need change. Leave it here is helpful to understand)", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409225948", "createdAt": "2020-04-16T01:23:23Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isTtlUpdate()) {\n+          if (currentFinalState.isPut()) {\n+            // If isPut returns true, when the final state doesn't carry ttl_update flag, this is wrong\n+            throw new IllegalStateException(\n+                \"TtlUpdate's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          // This is a TTL_UPDATE record, then the blob can't be expired. Only check if it's deleted or not.\n+          if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else if (isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(currentKey,\n+                indexSegment.getStartOffset())) {\n+              validEntries.add(entry);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        } else {\n+          if (srcIndex.isExpired(currentFinalState)) {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                entry, indexSegment.getStartOffset(), storeId);\n+            continue;\n+          }\n+          if (currentFinalState.isPut()) {\n+            if (currentFinalState.getLifeVersion() != currentValue.getLifeVersion()) {\n+              throw new IllegalStateException(\n+                  \"Two different lifeVersions  for puts key\" + currentKey + \" in store \" + dataDir);\n+            }\n+            validEntries.add(entry);\n+          } else if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else {\n+              logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                  entry, indexSegment.getStartOffset(), storeId);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(StoreKey key,\n+        Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - it might be compacted\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // Exists in the source. This can happen either because\n+        // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+        // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+        // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+        //\n+        // For condition one, we will keep TTL_UPDATE, for condition 2, we will remove it.\n+        if (isOffsetUnderCompaction(srcValue.getOffset())) {\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      if (value == null) {\n+        return false;\n+      }\n+      if (value.getLifeVersion() > srcValue.getLifeVersion()) {\n+        // If the IndexValue in the previous index has a lifeVersion higher than source value, then this\n+        // must be a duplicate.\n+        return true;\n+      } else if (value.getLifeVersion() < srcValue.getLifeVersion()) {\n+        // If the IndexValue in the previous index has a lifeVersion lower than source value, then this\n+        // is not a duplicate.\n+        return false;\n+      } else {\n+        // When the lifeVersions are the same, the order of the record are **FIX** as\n+        // P/U -> T -> D\n+        if (value.isDelete()) {\n+          return true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          return !srcValue.isDelete();\n+        } else if (value.isUndelete()) {\n+          if (srcValue.isPut()) {\n+            throw new IllegalStateException(\n+                \"An Undelete[\" + value + \"] and a Put[\" + srcValue + \"] can't be at the same lifeVersion at store \"\n+                    + dataDir);\n+          }\n+          // value is a UNDELETE without a TTL update or a DELETE\n+          return !srcValue.isDelete() && !srcValue.isTtlUpdate();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f63f846e7ecc866775a4712892c9ae06c1900daa"}, "originalPosition": 971}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0MjM5NTI5", "url": "https://github.com/linkedin/ambry/pull/1455#pullrequestreview-394239529", "createdAt": "2020-04-16T01:26:23Z", "commit": {"oid": "f63f846e7ecc866775a4712892c9ae06c1900daa"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwMToyNjoyM1rOGGROTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwMToyNjoyM1rOGGROTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTIyNjgzMQ==", "bodyText": "same here, this can be simplified (as long as comment is here to help us understand the only possible duplicate case is PUT)", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409226831", "createdAt": "2020-04-16T01:26:23Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isTtlUpdate()) {\n+          if (currentFinalState.isPut()) {\n+            // If isPut returns true, when the final state doesn't carry ttl_update flag, this is wrong\n+            throw new IllegalStateException(\n+                \"TtlUpdate's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          // This is a TTL_UPDATE record, then the blob can't be expired. Only check if it's deleted or not.\n+          if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else if (isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(currentKey,\n+                indexSegment.getStartOffset())) {\n+              validEntries.add(entry);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        } else {\n+          if (srcIndex.isExpired(currentFinalState)) {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                entry, indexSegment.getStartOffset(), storeId);\n+            continue;\n+          }\n+          if (currentFinalState.isPut()) {\n+            if (currentFinalState.getLifeVersion() != currentValue.getLifeVersion()) {\n+              throw new IllegalStateException(\n+                  \"Two different lifeVersions  for puts key\" + currentKey + \" in store \" + dataDir);\n+            }\n+            validEntries.add(entry);\n+          } else if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else {\n+              logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                  entry, indexSegment.getStartOffset(), storeId);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(StoreKey key,\n+        Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - it might be compacted\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // Exists in the source. This can happen either because\n+        // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+        // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+        // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+        //\n+        // For condition one, we will keep TTL_UPDATE, for condition 2, we will remove it.\n+        if (isOffsetUnderCompaction(srcValue.getOffset())) {\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      if (value == null) {\n+        return false;\n+      }\n+      if (value.getLifeVersion() > srcValue.getLifeVersion()) {\n+        // If the IndexValue in the previous index has a lifeVersion higher than source value, then this\n+        // must be a duplicate.\n+        return true;\n+      } else if (value.getLifeVersion() < srcValue.getLifeVersion()) {\n+        // If the IndexValue in the previous index has a lifeVersion lower than source value, then this\n+        // is not a duplicate.\n+        return false;\n+      } else {\n+        // When the lifeVersions are the same, the order of the record are **FIX** as\n+        // P/U -> T -> D\n+        if (value.isDelete()) {\n+          return true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          return !srcValue.isDelete();\n+        } else if (value.isUndelete()) {\n+          if (srcValue.isPut()) {\n+            throw new IllegalStateException(\n+                \"An Undelete[\" + value + \"] and a Put[\" + srcValue + \"] can't be at the same lifeVersion at store \"\n+                    + dataDir);\n+          }\n+          // value is a UNDELETE without a TTL update or a DELETE\n+          return !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        } else {\n+          if (srcValue.isUndelete()) {\n+            throw new IllegalStateException(\n+                \"A Put[\" + value + \"] and an Undelete[\" + srcValue + \"] can't be at the same lifeVersion at store \"\n+                    + dataDir);\n+          }\n+          // value is a PUT without a TTL update or a DELETE\n+          return !srcValue.isDelete() && !srcValue.isTtlUpdate();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f63f846e7ecc866775a4712892c9ae06c1900daa"}, "originalPosition": 979}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0MzAyNzI5", "url": "https://github.com/linkedin/ambry/pull/1455#pullrequestreview-394302729", "createdAt": "2020-04-16T05:11:58Z", "commit": {"oid": "f63f846e7ecc866775a4712892c9ae06c1900daa"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwNToxMTo1OFrOGGU0ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwNTo0Nzo0NFrOGGVeng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI4NTczOA==", "bodyText": "better to change Action to Is Valid", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409285738", "createdAt": "2020-04-16T05:11:58Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f63f846e7ecc866775a4712892c9ae06c1900daa"}, "originalPosition": 796}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI4NzI0Nw==", "bodyText": "why we don't check retention time here?", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409287247", "createdAt": "2020-04-16T05:17:07Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f63f846e7ecc866775a4712892c9ae06c1900daa"}, "originalPosition": 852}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5MDQxOQ==", "bodyText": "To make the comment clearer. could you change srcValue to PUT?\n(same for it, if I understand correctly, it is also referring to PUT)", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409290419", "createdAt": "2020-04-16T05:28:02Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isTtlUpdate()) {\n+          if (currentFinalState.isPut()) {\n+            // If isPut returns true, when the final state doesn't carry ttl_update flag, this is wrong\n+            throw new IllegalStateException(\n+                \"TtlUpdate's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          // This is a TTL_UPDATE record, then the blob can't be expired. Only check if it's deleted or not.\n+          if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else if (isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(currentKey,\n+                indexSegment.getStartOffset())) {\n+              validEntries.add(entry);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        } else {\n+          if (srcIndex.isExpired(currentFinalState)) {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                entry, indexSegment.getStartOffset(), storeId);\n+            continue;\n+          }\n+          if (currentFinalState.isPut()) {\n+            if (currentFinalState.getLifeVersion() != currentValue.getLifeVersion()) {\n+              throw new IllegalStateException(\n+                  \"Two different lifeVersions  for puts key\" + currentKey + \" in store \" + dataDir);\n+            }\n+            validEntries.add(entry);\n+          } else if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else {\n+              logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                  entry, indexSegment.getStartOffset(), storeId);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(StoreKey key,\n+        Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - it might be compacted\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // Exists in the source. This can happen either because\n+        // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f63f846e7ecc866775a4712892c9ae06c1900daa"}, "originalPosition": 920}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5MjMxNQ==", "bodyText": "I probably asked this before, here I just want to confirm again: both delete and undelete will carry the TTL from original PUT, right?", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409292315", "createdAt": "2020-04-16T05:34:26Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isTtlUpdate()) {\n+          if (currentFinalState.isPut()) {\n+            // If isPut returns true, when the final state doesn't carry ttl_update flag, this is wrong\n+            throw new IllegalStateException(\n+                \"TtlUpdate's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          // This is a TTL_UPDATE record, then the blob can't be expired. Only check if it's deleted or not.\n+          if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else if (isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(currentKey,\n+                indexSegment.getStartOffset())) {\n+              validEntries.add(entry);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        } else {\n+          if (srcIndex.isExpired(currentFinalState)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f63f846e7ecc866775a4712892c9ae06c1900daa"}, "originalPosition": 873}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5NTg3OA==", "bodyText": "if this is a sealed segment, is numberOfEntries(mmap) a fixed value? If yes, it's worth keeping it somewhere without calculating multiple times.", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409295878", "createdAt": "2020-04-16T05:45:39Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/IndexSegment.java", "diffHunk": "@@ -1170,5 +1183,34 @@ static Offset getIndexSegmentStartOffset(String filename) {\n     }\n     return new Offset(logSegmentName, Long.parseLong(startOffsetValue));\n   }\n+\n+  /**\n+   * An {@link IndexEntry} {@link Iterator} for a sealed {@link IndexSegment}. This {@link Iterator} should\n+   * only be used in the compaction so that the {@link IndexSegment} should be sealed already.\n+   */\n+  class SealedIndexSegmentEntryIterator implements Iterator<IndexEntry> {\n+    private int currentIdx = 0;\n+    private ByteBuffer mmap = serEntries.duplicate();\n+    private byte[] valueBuf = new byte[valueSize];\n+\n+    @Override\n+    public boolean hasNext() {\n+      return currentIdx < numberOfEntries(mmap);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f63f846e7ecc866775a4712892c9ae06c1900daa"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI5NjU0Mg==", "bodyText": "This will cause NullPointException if it failed to read index. Is this intended?", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r409296542", "createdAt": "2020-04-16T05:47:44Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/IndexSegment.java", "diffHunk": "@@ -1170,5 +1183,34 @@ static Offset getIndexSegmentStartOffset(String filename) {\n     }\n     return new Offset(logSegmentName, Long.parseLong(startOffsetValue));\n   }\n+\n+  /**\n+   * An {@link IndexEntry} {@link Iterator} for a sealed {@link IndexSegment}. This {@link Iterator} should\n+   * only be used in the compaction so that the {@link IndexSegment} should be sealed already.\n+   */\n+  class SealedIndexSegmentEntryIterator implements Iterator<IndexEntry> {\n+    private int currentIdx = 0;\n+    private ByteBuffer mmap = serEntries.duplicate();\n+    private byte[] valueBuf = new byte[valueSize];\n+\n+    @Override\n+    public boolean hasNext() {\n+      return currentIdx < numberOfEntries(mmap);\n+    }\n+\n+    @Override\n+    public IndexEntry next() {\n+      try {\n+        StoreKey key = getKeyAt(mmap, currentIdx);\n+        mmap.get(valueBuf);\n+        return new IndexEntry(key, new IndexValue(startOffset.getName(), ByteBuffer.wrap(valueBuf), getVersion()));\n+      } catch (Exception e) {\n+        logger.error(\"Failed to read index entry at \" + currentIdx, e);\n+      } finally {\n+        currentIdx++;\n+      }\n+      return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f63f846e7ecc866775a4712892c9ae06c1900daa"}, "originalPosition": 95}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f63f846e7ecc866775a4712892c9ae06c1900daa", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/f63f846e7ecc866775a4712892c9ae06c1900daa", "committedDate": "2020-04-15T21:27:58Z", "message": "Comments"}, "afterCommit": {"oid": "6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9", "committedDate": "2020-04-16T21:45:18Z", "message": "Comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2NzU5MDg1", "url": "https://github.com/linkedin/ambry/pull/1455#pullrequestreview-396759085", "createdAt": "2020-04-20T19:44:52Z", "commit": {"oid": "6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxOTo0NDo1MlrOGIkvRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxOTo1NDoxOVrOGIlFEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0MzcxOA==", "bodyText": "minor: index", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r411643718", "createdAt": "2020-04-20T19:44:52Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com/github/ambry/store/IndexSegmentValidEntryFilter.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import java.util.List;\n+\n+\n+/**\n+ * Interface to return valid entries from a index segment. Those valid entries will be copied to a new segment\n+ * in compaction process.\n+ */\n+public interface IndexSegmentValidEntryFilter {\n+  /**\n+   * Return the valid {@link IndexEntry} from a given {@link IndexSegment}.\n+   * @param indexSegment The {@link IndexSegment} in which to return all valid {@link IndexEntry}.\n+   * @param duplicateSearchSpan The {@link FileSpan} to search for duplication.\n+   * @param checkAlreadyCopied {@code true} if a check for existence in the swap spaces has to be executed (due to\n+   *                                       crash/shutdown), {@code false} otherwise.\n+   * @return the list of valid {@link IndexEntry} sorted by their offset.\n+   */\n+  List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan, boolean checkAlreadyCopied)\n+      throws StoreException;\n+\n+  /**\n+   * Checks if a record already exists in {@code idx}.\n+   * @param idx the {@link PersistentIndex} to search in", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0NDQ2NA==", "bodyText": "I like it.", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r411644464", "createdAt": "2020-04-20T19:46:09Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com/github/ambry/store/IndexSegmentValidEntryFilter.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import java.util.List;\n+\n+\n+/**\n+ * Interface to return valid entries from a index segment. Those valid entries will be copied to a new segment\n+ * in compaction process.\n+ */\n+public interface IndexSegmentValidEntryFilter {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0NjEzMQ==", "bodyText": "final -> latest   to align with comments.", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r411646131", "createdAt": "2020-04-20T19:48:54Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +996,536 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId, incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Is Valid                                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9"}, "originalPosition": 813}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0NjUzOA==", "bodyText": "+1 , comment to explain why don't need to query latest state: because they share same latest state.", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r411646538", "createdAt": "2020-04-20T19:49:37Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with out undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5MDgyNw=="}, "originalCommit": {"oid": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1"}, "originalPosition": 831}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0OTA4NQ==", "bodyText": "final -> latest", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r411649085", "createdAt": "2020-04-20T19:54:01Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +998,533 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with out undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId,\n+              incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Action                                       |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        if (currentKey.equals(lastKey)) {\n+          currentFinalState = lastFinalState;\n+        } else {\n+          lastKey = currentKey;\n+          lastFinalState = currentFinalState = srcIndex.findKey(currentKey);\n+        }\n+\n+        if (currentValue.isUndelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Undelete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isUndelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()\n+              && !srcIndex.isExpired(currentValue)) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isDelete()) {\n+          if (currentFinalState.isPut()) {\n+            throw new IllegalStateException(\n+                \"Delete's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          if (currentFinalState.isDelete() && currentFinalState.getLifeVersion() == currentValue.getLifeVersion()) {\n+            validEntries.add(entry);\n+          }\n+        } else if (currentValue.isTtlUpdate()) {\n+          if (currentFinalState.isPut()) {\n+            // If isPut returns true, when the final state doesn't carry ttl_update flag, this is wrong\n+            throw new IllegalStateException(\n+                \"TtlUpdate's final state can't be put for key\" + currentKey + \" in store \" + dataDir);\n+          }\n+          // This is a TTL_UPDATE record, then the blob can't be expired. Only check if it's deleted or not.\n+          if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else if (isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(currentKey,\n+                indexSegment.getStartOffset())) {\n+              validEntries.add(entry);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        } else {\n+          if (srcIndex.isExpired(currentFinalState)) {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                entry, indexSegment.getStartOffset(), storeId);\n+            continue;\n+          }\n+          if (currentFinalState.isPut()) {\n+            if (currentFinalState.getLifeVersion() != currentValue.getLifeVersion()) {\n+              throw new IllegalStateException(\n+                  \"Two different lifeVersions  for puts key\" + currentKey + \" in store \" + dataDir);\n+            }\n+            validEntries.add(entry);\n+          } else if (currentFinalState.isDelete()) {\n+            if (currentFinalState.getOperationTimeInMs() >= compactionLog.getCompactionDetails().getReferenceTimeMs()) {\n+              validEntries.add(entry);\n+            } else {\n+              logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                  entry, indexSegment.getStartOffset(), storeId);\n+            }\n+          } else {\n+            validEntries.add(entry);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValidWhenFinalStateIsDeleteAndRetention(StoreKey key,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5NTEwMA=="}, "originalCommit": {"oid": "c51c4385c4715b5bfb98e932cd3f9bf0a4629df1"}, "originalPosition": 907}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0OTI5Nw==", "bodyText": "final -> latest", "url": "https://github.com/linkedin/ambry/pull/1455#discussion_r411649297", "createdAt": "2020-04-20T19:54:19Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1295,4 +996,536 @@ private void endCompaction() {\n     }\n     return indexSegmentStartOffsetToFile;\n   }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter without undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithoutUndelete implements IndexSegmentValidEntryFilter {\n+\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> allIndexEntries = new ArrayList<>();\n+      // get all entries. We get one entry per key\n+      indexSegment.getIndexEntriesSince(null, new FindEntriesCondition(Long.MAX_VALUE), allIndexEntries,\n+          new AtomicLong(0), true);\n+      // save a token for restart (the key gets ignored but is required to be non null for construction)\n+      StoreFindToken safeToken =\n+          new StoreFindToken(allIndexEntries.get(0).getKey(), indexSegment.getStartOffset(), sessionId, incarnationId);\n+      compactionLog.setSafeToken(safeToken);\n+      logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment, allIndexEntries);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of {} entries, {} are valid and {} will be copied in this round\", allIndexEntries.size(),\n+          validEntriesSize, copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Total index entries: {}. Entries to copy: {}\",\n+          indexSegment.getStartOffset(), storeId, allIndexEntries, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @param allIndexEntries the list of {@link IndexEntry} instances from which the valid entries have to be chosen.\n+     *                        There should be only one entry per key where a DELETE is preferred over all other entries\n+     *                        and PUT is preferred over a TTL update entry\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment, List<IndexEntry> allIndexEntries)\n+        throws StoreException {\n+      // Assumed preference order from IndexSegment (current impl)\n+      // (Legend: entry/entries in segment -> output from IndexSegment#getIndexEntriesSince())\n+      // PUT entry only -> PUT entry\n+      // TTL update entry only -> TTL update entry\n+      // DELETE entry only -> DELETE entry\n+      // PUT + DELETE -> DELETE\n+      // TTL update + DELETE -> DELETE\n+      // PUT + TTL update -> PUT (the one relevant to this comment)\n+      // PUT + TTL update + DELETE -> DELETE\n+      // TODO: move this blob store stats\n+      Offset startOffsetOfLastIndexSegmentForDeleteCheck = getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      // deletes are in effect if this index segment does not have any deletes that are less than\n+      // StoreConfig#storeDeletedMessageRetentionDays days old. If there are such deletes, then they are not counted as\n+      // deletes and the PUT records are still valid as far as compaction is concerned\n+      boolean deletesInEffect = startOffsetOfLastIndexSegmentForDeleteCheck != null\n+          && indexSegment.getStartOffset().compareTo(startOffsetOfLastIndexSegmentForDeleteCheck) <= 0;\n+      logger.trace(\"Deletes in effect is {} for index segment with start offset {} in {}\", deletesInEffect,\n+          indexSegment.getStartOffset(), storeId);\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      for (IndexEntry indexEntry : allIndexEntries) {\n+        IndexValue value = indexEntry.getValue();\n+        if (value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromSrc(indexEntry.getKey(), value, indexSegment);\n+          if (putValue != null) {\n+            // In PersistentIndex.markAsDeleted(), the expiry time of the put/ttl update value is copied into the\n+            // delete value. So it is safe to check for isExpired() on the delete value.\n+            if (deletesInEffect || srcIndex.isExpired(value)) {\n+              // still have to evaluate whether the TTL update has to be copied\n+              NavigableSet<IndexValue> values = indexSegment.find(indexEntry.getKey());\n+              IndexValue secondVal = values.lower(values.last());\n+              if (secondVal != null && secondVal.isFlagSet(IndexValue.Flags.Ttl_Update_Index) && isTtlUpdateEntryValid(\n+                  indexEntry.getKey(), indexSegment.getStartOffset())) {\n+                validEntries.add(new IndexEntry(indexEntry.getKey(), secondVal));\n+              }\n+              // DELETE entry. Always valid.\n+              validEntries.add(indexEntry);\n+            } else {\n+              // if this delete cannot be counted and there is a corresponding unexpired PUT/TTL update entry in the same\n+              // index segment, we will need to add it.\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            // DELETE entry. Always valid.\n+            validEntries.add(indexEntry);\n+          }\n+        } else if (value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+          // if IndexSegment::getIndexEntriesSince() returns a TTL update entry, it is because it is the ONLY entry i.e.\n+          // no PUT or DELETE in the same index segment.\n+          if (isTtlUpdateEntryValid(indexEntry.getKey(), indexSegment.getStartOffset())) {\n+            validEntries.add(indexEntry);\n+          }\n+        } else {\n+          IndexValue valueFromIdx = srcIndex.findKey(indexEntry.getKey());\n+          // Doesn't matter whether we get the PUT or DELETE entry for the expiry test\n+          if (!srcIndex.isExpired(valueFromIdx)) {\n+            // unexpired PUT entry.\n+            if (deletesInEffect) {\n+              if (!hasDeleteEntryInSpan(indexEntry.getKey(), indexSegment.getStartOffset(),\n+                  startOffsetOfLastIndexSegmentForDeleteCheck)) {\n+                // PUT entry that has not expired and is not considered deleted.\n+                // Add all values in this index segment (to account for the presence of TTL updates)\n+                addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+              } else {\n+                logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is a deleted PUT\",\n+                    indexEntry, indexSegment.getStartOffset(), storeId);\n+              }\n+            } else {\n+              // valid PUT entry\n+              // Add all values in this index segment (to account for the presence of TTL updates)\n+              addAllEntriesForKeyInSegment(validEntries, indexSegment, indexEntry);\n+            }\n+          } else {\n+            logger.trace(\"{} in index segment with start offset {} in {} is not valid because it is an expired PUT\",\n+                indexEntry, indexSegment.getStartOffset(), storeId);\n+          }\n+        }\n+      }\n+      return validEntries;\n+    }\n+\n+    /**\n+     * @return the start {@link Offset} of the index segment up until which delete records are considered applicable. Any\n+     * delete records of blobs in subsequent index segments do not count and the blob is considered as not deleted.\n+     * <p/>\n+     * Returns {@code null} if none of the delete records are considered applicable.\n+     */\n+    private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n+      // TODO: move this to BlobStoreStats\n+      Offset cutoffOffset = compactionLog.getStartOffsetOfLastIndexSegmentForDeleteCheck();\n+      if (cutoffOffset == null || !srcIndex.getIndexSegments().containsKey(cutoffOffset)) {\n+        long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n+        for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n+          if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n+            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n+            // using this as the end offset for delete includes the whole index segment in the search.\n+            cutoffOffset = indexSegment.getStartOffset();\n+            break;\n+          }\n+        }\n+        if (cutoffOffset != null) {\n+          compactionLog.setStartOffsetOfLastIndexSegmentForDeleteCheck(cutoffOffset);\n+          logger.info(\"Start offset of last index segment for delete check is {} for {}\", cutoffOffset, storeId);\n+        }\n+      }\n+      return cutoffOffset;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT from the {@link #srcIndex} (if it exists)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param updateValue the update (TTL update/delete) {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code updateValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     * @throws StoreException if there are problems with the index\n+     */\n+    private IndexValue getPutValueFromSrc(StoreKey key, IndexValue updateValue, IndexSegment indexSegmentOfUpdateValue)\n+        throws StoreException {\n+      IndexValue putValue = srcIndex.findKey(key, new FileSpan(srcIndex.getStartOffset(), updateValue.getOffset()),\n+          EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      // in a non multi valued segment, if putValue is not found directly from the index, check if the PUT and DELETE\n+      // are the same segment so that the PUT entry can be constructed from the DELETE entry\n+      if (putValue == null && updateValue.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+        putValue = getPutValueFromDeleteEntry(key, updateValue, indexSegmentOfUpdateValue);\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Gets the {@link IndexValue} for the PUT using info in the {@code deleteValue)\n+     * @param key the {@link StoreKey} whose PUT is required\n+     * @param deleteValue the delete {@link IndexValue} associated with the same {@code key}\n+     * @param indexSegmentOfUpdateValue the {@link IndexSegment} that {@code deleteValue} belongs to\n+     * @return the {@link IndexValue} for the PUT in the {@link #srcIndex} (if it exists)\n+     */\n+    private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteValue,\n+        IndexSegment indexSegmentOfUpdateValue) {\n+      // TODO: find a way to test this?\n+      IndexValue putValue = null;\n+      long putRecordOffset = deleteValue.getOriginalMessageOffset();\n+      if (putRecordOffset != IndexValue.UNKNOWN_ORIGINAL_MESSAGE_OFFSET && putRecordOffset != deleteValue.getOffset()\n+          .getOffset() && indexSegmentOfUpdateValue.getStartOffset().getOffset() <= putRecordOffset) {\n+        try (BlobReadOptions options = srcIndex.getBlobReadInfo(key, EnumSet.allOf(StoreGetOptions.class))) {\n+          Offset offset = new Offset(indexSegmentOfUpdateValue.getStartOffset().getName(), options.getOffset());\n+          MessageInfo info = options.getMessageInfo();\n+          putValue = new IndexValue(info.getSize(), offset, info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n+              info.getAccountId(), info.getContainerId());\n+        } catch (StoreException e) {\n+          logger.error(\"Fetching PUT index entry of {} in {} failed\", key, indexSegmentOfUpdateValue.getStartOffset());\n+        }\n+      }\n+      return putValue;\n+    }\n+\n+    /**\n+     * Determines whether a TTL update entry is valid. A TTL update entry is valid as long as the associated PUT record\n+     * is still present in the store (the validity of the PUT record does not matter - only its presence/absence does).\n+     * @param key the {@link StoreKey} being examined\n+     * @param indexSegmentStartOffset the start offset of the {@link IndexSegment} that the TTL update record is in\n+     * @return {@code true} if the TTL update entry is valid\n+     * @throws StoreException if there are problems reading the index\n+     */\n+    private boolean isTtlUpdateEntryValid(StoreKey key, Offset indexSegmentStartOffset) throws StoreException {\n+      boolean valid = false;\n+      //  A TTL update entry is \"valid\" if the corresponding PUT is still alive\n+      // The PUT entry, if it exists, must be \"before\" this TTL update entry.\n+      FileSpan srcSearchSpan = new FileSpan(srcIndex.getStartOffset(), indexSegmentStartOffset);\n+      IndexValue srcValue = srcIndex.findKey(key, srcSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+      if (srcValue == null) {\n+        // PUT is not in the source - therefore can't be in target. This TTL update can be cleaned up\n+        logger.trace(\"TTL update of {} in segment with start offset {} in {} is not valid the corresponding PUT entry \"\n+            + \"does not exist anymore\", key, indexSegmentStartOffset, storeId);\n+      } else {\n+        // exists in source - now we need to check if it exists in the target\n+        IndexValue tgtValue = tgtIndex.findKey(key, null, EnumSet.of(PersistentIndex.IndexEntryType.PUT));\n+        if (tgtValue == null && isOffsetUnderCompaction(srcValue.getOffset())) {\n+          // exists in src but not in tgt. This can happen either because\n+          // 1. The FileSpan to which srcValue belongs is not under compaction (so there is no reason for tgt to have it)\n+          // 2. srcValue will be compacted in this cycle (because it has been determined that the PUT is not valid. Since\n+          // the PUT is going away in this cycle, it is safe to remove the TTL update also)\n+          logger.trace(\n+              \"TTL update of {} in segment with start offset {} in {} is not valid because the corresponding PUT entry\"\n+                  + \" {} will be compacted in this cycle ({} are being compacted in this cycle)\", key,\n+              indexSegmentStartOffset, storeId, srcValue,\n+              compactionLog.getCompactionDetails().getLogSegmentsUnderCompaction());\n+        } else {\n+          // PUT entry exists in both the src and tgt,\n+          // OR PUT entry exists in source and the offset of the source entry is not under compaction in this cycle\n+          // therefore this TTL update entry cannot be compacted\n+          valid = true;\n+        }\n+      }\n+      return valid;\n+    }\n+\n+    /**\n+     * Adds entries related to {@code entry} that are in the same {@code indexSegment} including {@code entry}\n+     * @param entries the list of {@link IndexEntry} to add to.\n+     * @param indexSegment the {@link IndexSegment} to fetch values from.\n+     * @param entry the {@link IndexEntry} that is under processing\n+     * @throws StoreException if there are problems using the index\n+     */\n+    private void addAllEntriesForKeyInSegment(List<IndexEntry> entries, IndexSegment indexSegment, IndexEntry entry)\n+        throws StoreException {\n+      logger.trace(\"Fetching related entries of a blob with entry {} in index segment with start offset {} in {} \"\n+          + \"because they need to be retained\", entry, indexSegment.getStartOffset(), storeId);\n+      NavigableSet<IndexValue> values = indexSegment.find(entry.getKey());\n+      if (values.size() > 1) {\n+        // we are using a multivalued index segment. Any related values will be in this set\n+        values.forEach(valueFromSeg -> entries.add(new IndexEntry(entry.getKey(), valueFromSeg)));\n+      } else {\n+        // in a non multi valued segment, there can only be PUTs and DELETEs in the same segment\n+        entries.add(entry);\n+        if (entry.getValue().isFlagSet(IndexValue.Flags.Delete_Index)) {\n+          IndexValue putValue = getPutValueFromDeleteEntry(entry.getKey(), entry.getValue(), indexSegment);\n+          if (putValue != null) {\n+            entries.add(new IndexEntry(entry.getKey(), putValue));\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * @param key the {@link StoreKey} to check\n+     * @param searchStartOffset the start offset of the search for delete entry\n+     * @param searchEndOffset the end offset of the search for delete entry\n+     * @return {@code true} if the key has a delete entry in the given search span.\n+     * @throws StoreException if there are any problems using the index\n+     */\n+    private boolean hasDeleteEntryInSpan(StoreKey key, Offset searchStartOffset, Offset searchEndOffset)\n+        throws StoreException {\n+      FileSpan deleteSearchSpan = new FileSpan(searchStartOffset, searchEndOffset);\n+      return srcIndex.findKey(key, deleteSearchSpan, EnumSet.of(PersistentIndex.IndexEntryType.DELETE)) != null;\n+    }\n+\n+    @Override\n+    public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey key, IndexValue srcValue)\n+        throws StoreException {\n+      IndexValue value = idx.findKey(key, searchSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+      boolean exists = false;\n+      if (value != null) {\n+        if (value.isDelete()) {\n+          exists = true;\n+        } else if (value.isTtlUpdate()) {\n+          // if srcValue is not a delete, it is a duplicate.\n+          exists = !srcValue.isDelete();\n+        } else {\n+          // value is a PUT without a TTL update or a DELETE\n+          exists = !srcValue.isDelete() && !srcValue.isTtlUpdate();\n+        }\n+      }\n+      return exists;\n+    }\n+  }\n+\n+  /**\n+   * IndexSegmentValidEntryFilter with undelete log records.\n+   */\n+  class IndexSegmentValidEntryFilterWithUndelete implements IndexSegmentValidEntryFilter {\n+    @Override\n+    public List<IndexEntry> getValidEntry(IndexSegment indexSegment, FileSpan duplicateSearchSpan,\n+        boolean checkAlreadyCopied) throws StoreException {\n+      List<IndexEntry> copyCandidates = getValidIndexEntries(indexSegment);\n+      int validEntriesSize = copyCandidates.size();\n+      copyCandidates.removeIf(\n+          copyCandidate -> isDuplicate(copyCandidate, duplicateSearchSpan, indexSegment.getStartOffset(),\n+              checkAlreadyCopied));\n+      // order by offset in log.\n+      copyCandidates.sort(PersistentIndex.INDEX_ENTRIES_OFFSET_COMPARATOR);\n+      logger.debug(\"Out of entries, {} are valid and {} will be copied in this round\", validEntriesSize,\n+          copyCandidates.size());\n+      logger.trace(\"For index segment with start offset {} in {} - Entries to copy: {}\", indexSegment.getStartOffset(),\n+          storeId, copyCandidates);\n+      return copyCandidates;\n+    }\n+\n+    /**\n+     * Gets all the valid index entries in the given list of index entries.\n+     * @param indexSegment the {@link IndexSegment} that {@code allIndexEntries} are from.\n+     * @return the list of valid entries generated from {@code allIndexEntries}. May contain entries not in\n+     * {@code allIndexEntries}.\n+     * @throws StoreException if {@link BlobReadOptions} could not be obtained from the store for deleted blobs.\n+     */\n+    private List<IndexEntry> getValidIndexEntries(IndexSegment indexSegment) throws StoreException {\n+      // Validity of a IndexValue is determined by itself and it's latest state. For example, if the current IndexValue\n+      // is a Put, and the latest IndexValue for this blob is a Delete, and the operation is done out of retention duration,\n+      // then this Put IndexValue is considered as invalid.\n+      //\n+      // There is one exception, TtlUpdate IndexValue. A TtlUpdate IndexValue's validity is not only depends on the latest\n+      // state of the blob, it's also affected by the Put IndexValue. If the latest IndexValue for this blob is a Delete,\n+      // and it's operation time is out of the retention duration, this TtlUpdate record would be considered as invalid.\n+      // However if the Put IndexValue exists for this TtlUpdate and is not under compaction, then we will keep this TtlUpdate\n+      // anyway.\n+      //\n+      // This is a table that shows how the validity of the current IndexValue is determined.\n+      // ----------------------------------------------------------------------------------------\n+      // Current IndexValue  | Latest IndexValue | Is Valid                                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Put(verion c)       | Put(version f)    | isExpired(Pc)?false:true                     |\n+      //                     | Delete(f)         | reachRetention(Df)||isExpired(Df)?false:true |\n+      //                     | Undelete(f)       | isExpired(Uf)?false:true                     |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // TtlUpdate(c)        | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | reachRetention(Df)?false:true                |\n+      //                     | Undelete(f)       | true                                         |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Delete(c)           | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | c==f?true:false                              |\n+      //                     | Undelete(f)       | false                                        |\n+      // --------------------+-------------------+----------------------------------------------|\n+      // Undelete(c)         | Put(f)            | Exception                                    |\n+      //                     | Delete(f)         | false                                        |\n+      //                     | Undelete(f)       | c==f&&!isExpired(Uc)?true:false              |\n+      // ----------------------------------------------------------------------------------------\n+      List<IndexEntry> validEntries = new ArrayList<>();\n+      Iterator<IndexEntry> iterator = indexSegment.getIterator();\n+      StoreKey lastKey = null;\n+      IndexValue lastFinalState = null;\n+      while (iterator.hasNext()) {\n+        IndexEntry entry = iterator.next();\n+        StoreKey currentKey = entry.getKey();\n+        IndexValue currentValue = entry.getValue();\n+        IndexValue currentFinalState;\n+\n+        if (lastKey == null) {\n+          // save a token for restart (the key gets ignored but is required to be non null for construction)\n+          StoreFindToken safeToken =\n+              new StoreFindToken(currentKey, indexSegment.getStartOffset(), sessionId, incarnationId);\n+          compactionLog.setSafeToken(safeToken);\n+          logger.debug(\"Set safe token for compaction in {} to {}\", storeId, safeToken);\n+        }\n+        // If an IndexSegment contains more than one IndexValue for the same StoreKey, then they must follow each other\n+        // since IndexSegment store IndexValues based on StoreKey. If the current key equals to the previous key, then\n+        // we don't have to query the final state again.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9"}, "originalPosition": 830}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b22c38bea7080b54e82e38c4d277a8fe76078499", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/b22c38bea7080b54e82e38c4d277a8fe76078499", "committedDate": "2020-04-20T20:07:43Z", "message": "WIP"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3be91ecd82e4bc6fe1847e90b1642b568c2864c7", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/3be91ecd82e4bc6fe1847e90b1642b568c2864c7", "committedDate": "2020-04-20T20:07:43Z", "message": "WIP: start supporting undelete in compaction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "377623725e10d1fa847c73506e981443cd8f678b", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/377623725e10d1fa847c73506e981443cd8f678b", "committedDate": "2020-04-20T20:07:43Z", "message": "Fix compactor test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ec95e8c2a4b6d56c42c979410da6f4eae5975513", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/ec95e8c2a4b6d56c42c979410da6f4eae5975513", "committedDate": "2020-04-20T20:07:43Z", "message": "Add new tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f1a0036c183253e7a7f82146c22cd5e4dd3cb055", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/f1a0036c183253e7a7f82146c22cd5e4dd3cb055", "committedDate": "2020-04-20T20:07:43Z", "message": "Change"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6fc2a8b3b1f1a5ea16bbcd28868dfdcb0248ad78", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/6fc2a8b3b1f1a5ea16bbcd28868dfdcb0248ad78", "committedDate": "2020-04-20T20:07:43Z", "message": "More comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a24fff27b79b52e6cb17d1d3b3ca2d43ea0cfac", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/1a24fff27b79b52e6cb17d1d3b3ca2d43ea0cfac", "committedDate": "2020-04-20T20:07:43Z", "message": "Use config to turn on new code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b896361e4f7eecbbc0cc58652150cf01c4b1966e", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/b896361e4f7eecbbc0cc58652150cf01c4b1966e", "committedDate": "2020-04-20T20:07:43Z", "message": "Comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bed482ed041cab159fb81d5dbfbb475fcd9ab370", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/bed482ed041cab159fb81d5dbfbb475fcd9ab370", "committedDate": "2020-04-20T20:07:43Z", "message": "Comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8819e4cd00112bfa9995a6e6b4145718fb0533f0", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/8819e4cd00112bfa9995a6e6b4145718fb0533f0", "committedDate": "2020-04-20T20:16:54Z", "message": "Address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/6c7e4e33b7bd2e6a76d9f61d619dfa7817078eb9", "committedDate": "2020-04-16T21:45:18Z", "message": "Comments"}, "afterCommit": {"oid": "8819e4cd00112bfa9995a6e6b4145718fb0533f0", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/8819e4cd00112bfa9995a6e6b4145718fb0533f0", "committedDate": "2020-04-20T20:16:54Z", "message": "Address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3NzU4MDY4", "url": "https://github.com/linkedin/ambry/pull/1455#pullrequestreview-397758068", "createdAt": "2020-04-22T00:10:08Z", "commit": {"oid": "8819e4cd00112bfa9995a6e6b4145718fb0533f0"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk4MzYxMTQ2", "url": "https://github.com/linkedin/ambry/pull/1455#pullrequestreview-398361146", "createdAt": "2020-04-22T16:23:27Z", "commit": {"oid": "8819e4cd00112bfa9995a6e6b4145718fb0533f0"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1354, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}