{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc3NTQyNDQ3", "number": 1393, "title": "Cloud compaction retries and throttling", "bodyText": "Goal is to eliminate the Cosmos 429 errors by breaking query up into smaller ones.", "createdAt": "2020-02-20T04:25:26Z", "url": "https://github.com/linkedin/ambry/pull/1393", "merged": true, "mergeCommit": {"oid": "09501a26aa19e187dfc8609e97aae099bc94d3a5"}, "closed": true, "closedAt": "2020-02-22T01:02:38Z", "author": {"login": "lightningrob"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcGDLpiAH2gAyMzc3NTQyNDQ3OmY5OGU2NzBlYTExYTI2YmMwMDkyNzNiYmNkNGRiMGZjZTYxOWMzYjA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcGolvNgH2gAyMzc3NTQyNDQ3OmYxNmNmMGI2MWUyNmIzMTY5YzRmNjExNzNkMzJhZDhhZDI5N2NhNjY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "f98e670ea11a26bc009273bbcd4db0fce619c3b0", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/f98e670ea11a26bc009273bbcd4db0fce619c3b0", "committedDate": "2020-02-20T04:12:04Z", "message": "Add retries and throttling for cloud blob compaction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3a8603f7c5aa982d22ff549170d173acaa43827c", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/3a8603f7c5aa982d22ff549170d173acaa43827c", "committedDate": "2020-02-20T04:22:25Z", "message": "Move doWithRetries to CloudRequestAgent class"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5281695d8019200c18a4a20c4bf0e69bc2e647a3", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/5281695d8019200c18a4a20c4bf0e69bc2e647a3", "committedDate": "2020-02-21T19:02:26Z", "message": "Fixed AzureIntegrationTest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/9c181d83c21c8a6be0872dfd74d0d87ad7bd0017", "committedDate": "2020-02-21T20:53:45Z", "message": "Make one mock lenient to fix test exceptions"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYyOTUxMTY1", "url": "https://github.com/linkedin/ambry/pull/1393#pullrequestreview-362951165", "createdAt": "2020-02-21T22:17:30Z", "commit": {"oid": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMjoxNzozMFrOFtGQYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMjoyNDo1OVrOFtGaMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzMjczOA==", "bodyText": "what is the format of results here? Instead of findFirst could you get the relevant entry by looking up its key?", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382832738", "createdAt": "2020-02-21T22:17:30Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -155,6 +157,28 @@ void testConnectivity() {\n     }\n   }\n \n+  /**\n+   * Get the number of blobs in the specified partition matching the specified DocumentDB query.\n+   * @param partitionPath the partition to query.\n+   * @param querySpec the DocumentDB query to execute.\n+   * @param timer the {@link Timer} to use to record query time (excluding waiting).\n+   * @return the number of matching blobs.\n+   */\n+  int countMetadata(String partitionPath, SqlQuerySpec querySpec, Timer timer)\n+      throws DocumentClientException {\n+    FeedOptions feedOptions = new FeedOptions();\n+    feedOptions.setPartitionKey(new PartitionKey(partitionPath));\n+    try {\n+      FeedResponse<Document> response = executeCosmosQuery(querySpec, feedOptions, timer).single();\n+      return ((Number) response.getResults().get(0).getHashMap().values().stream().findFirst().get()).intValue();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzMzY0NQ==", "bodyText": "let's make a github issue to track that this test needs a fix", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382833645", "createdAt": "2020-02-21T22:20:10Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/azure/AzureIntegrationTest.java", "diffHunk": "@@ -298,23 +269,58 @@ public void testPurgeDeadBlobs() throws Exception {\n           azureDest.uploadBlob(blobId, blobSize, cloudBlobMetadata, inputStream));\n     }\n \n-    // run getDeadBlobs query, should return 20\n+    // run getDeadBlobs query, should return 2 * bucketCount\n     String partitionPath = String.valueOf(testPartition);\n-    List<CloudBlobMetadata> deadBlobs = azureDest.getDeadBlobs(partitionPath);\n-    assertEquals(\"Unexpected number of dead blobs\", expectedDeadBlobs, deadBlobs.size());\n-\n-    logger.info(\"Running purge\");\n-    int numPurged = azureDest.purgeBlobs(deadBlobs);\n-    assertEquals(\"Not all blobs were purged\", expectedDeadBlobs, numPurged);\n+    assertEquals(\"Unexpected number of dead blobs\", expectedDeadBlobs, azureDest.getDeadBlobCount(partitionPath, now));\n+    logger.info(\"First call to getDeadBlobs\");\n+    List<CloudBlobMetadata> deadBlobs = azureDest.getDeadBlobs(partitionPath, now, bucketCount);\n+    assertEquals(\"Unexpected number returned\", bucketCount, deadBlobs.size());\n+    logger.info(\"First call to purge\");\n+    assertEquals(\"Not all blobs were purged\", bucketCount, azureDest.purgeBlobs(deadBlobs));\n+    logger.info(\"Second call to getDeadBlobs\");\n+    deadBlobs = azureDest.getDeadBlobs(partitionPath, now, bucketCount);\n+    assertEquals(\"Unexpected number returned\", bucketCount, deadBlobs.size());\n+    logger.info(\"Second call to purge\");\n+    assertEquals(\"Not all blobs were purged\", bucketCount, azureDest.purgeBlobs(deadBlobs));\n+    logger.info(\"Final call to getDeadBlobs\");\n+    deadBlobs = azureDest.getDeadBlobs(partitionPath, now, bucketCount);\n+    assertEquals(\"Expected zero\", 0, deadBlobs.size());\n     cleanup();\n   }\n \n   /**\n-   * Test findEntriesSince.\n-   * @throws Exception on error\n+   * Test findEntriesSince with CosmosUpdateTimeFindTokenFactory.\n+   */\n+  @Test\n+  public void testFindEntriesSinceByUpdateTime() throws Exception {\n+    testFindEntriesSince(\"com.github.ambry.cloud.azure.CosmosUpdateTimeFindTokenFactory\");\n+  }\n+\n+  /**\n+   * Test findEntriesSince with CosmosChangeFeedFindTokenFactory.\n    */\n+  @Ignore // Fails with wrong number of queries.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzNTI1MQ==", "bodyText": "do we need to sleep in between successive calls to getDeadBlobs, or do you feel that the cosmo's retry-with-backoff will be good enough for rate limiting this query?", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382835251", "createdAt": "2020-02-21T22:24:59Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudStorageCompactor.java", "diffHunk": "@@ -91,14 +99,25 @@ public int compactPartitions() {\n   /**\n    * Purge the inactive blobs in the specified partition.\n    * @param partitionPath the partition to compact.\n-   * @return the number of blobs purged.\n+   * @param cutoffTime the time at which a blob's active status should be evaluated.\n+   * @return the number of blobs purged or found.\n    */\n-  public int compactPartition(String partitionPath) throws CloudStorageException {\n-    List<CloudBlobMetadata> deadBlobs = cloudDestination.getDeadBlobs(partitionPath);\n-    if (!testMode) {\n-      return cloudDestination.purgeBlobs(deadBlobs);\n-    } else {\n-      return deadBlobs.size();\n+  public int compactPartition(String partitionPath, long cutoffTime) throws CloudStorageException {\n+    if (testMode) {\n+      return cloudDestination.getDeadBlobCount(partitionPath, cutoffTime);\n     }\n+\n+    int numDeadBlobs = 0;\n+    // Iterate until returned list size < limit or time runs out\n+    while (System.currentTimeMillis() < cutoffTime + compactionTimeLimitMs) {\n+      List<CloudBlobMetadata> deadBlobs =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017"}, "originalPosition": 73}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "97a0c512583aee3bd07720ae8f1562dcc43417d8", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/97a0c512583aee3bd07720ae8f1562dcc43417d8", "committedDate": "2020-02-21T23:46:10Z", "message": "Address Casey's comments, use Cosmos property constant."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f16cf0b61e26b3169c4f61173d32ad8ad297ca66", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/f16cf0b61e26b3169c4f61173d32ad8ad297ca66", "committedDate": "2020-02-21T23:47:03Z", "message": "Merge branch 'master' of github.com:linkedin/ambry into cloud-compaction-throttle"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1632, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}