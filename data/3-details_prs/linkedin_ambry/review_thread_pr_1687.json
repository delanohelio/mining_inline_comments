{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE1ODMzNjUw", "number": 1687, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQyMzozOTozOFrOE2RbZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQwMToyOTozMlrOE3BVYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MzQ0MTAzOnYy", "diffSide": "RIGHT", "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQyMzozOTozOFrOHvAtew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0MToxOVrOHv-xfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA1Njc2Mw==", "bodyText": "Can the flag == Blob case on line 203 be removed now?", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r519056763", "createdAt": "2020-11-06T23:39:38Z", "author": {"login": "cgtz"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);\n+            InputStream is = new ByteBufInputStream(blobAll);\n+\n+            MessageHeader_Format headerFormat = parseHeaderAndVerifyStoreKey(is, i);\n+\n+            MessageMetadata messageMetadata = null;\n+            if (headerFormat.hasEncryptionKeyRecord()) {\n+              // If encryption key exists, MessageMetadata with encryption key is needed.\n+              ByteBuf duplicatedByteBuf = blobAll.duplicate();\n+              duplicatedByteBuf.readerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset());\n+              duplicatedByteBuf.writerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset()\n+                  + headerFormat.getBlobEncryptionKeyRecordSize());\n+              messageMetadata =\n+                  new MessageMetadata(deserializeBlobEncryptionKey(new ByteBufInputStream(duplicatedByteBuf)));\n+            }\n+            messageMetadataList.add(messageMetadata);\n+            sendInfoList.add(i,\n+                new SendInfo(headerFormat.getBlobRecordRelativeOffset(), headerFormat.getBlobRecordSize()));\n+            totalSizeToWrite += headerFormat.getBlobRecordSize();\n+\n+            // Adjust underlying ByteBuf reader and writer index.\n+            blobAll.readerIndex(headerFormat.getBlobRecordRelativeOffset());\n+            blobAll.writerIndex((int) (headerFormat.getBlobRecordRelativeOffset() + headerFormat.getBlobRecordSize()));\n+          }\n         } else {\n-          long startTime = SystemTime.getInstance().milliseconds();\n           BufferedInputStream bufferedInputStream =\n               new BufferedInputStream(new MessageReadSetIndexInputStream(readSet, i, 0),\n                   BUFFERED_INPUT_STREAM_BUFFER_SIZE);\n-          // read and verify header version\n-          byte[] headerVersionBytes = new byte[Version_Field_Size_In_Bytes];\n-          bufferedInputStream.read(headerVersionBytes, 0, Version_Field_Size_In_Bytes);\n-          short version = ByteBuffer.wrap(headerVersionBytes).getShort();\n-          if (!isValidHeaderVersion(version)) {\n-            throw new MessageFormatException(\n-                \"Version not known while reading message - version \" + version + \", StoreKey \" + readSet.getKeyAt(i),\n-                MessageFormatErrorCodes.Unknown_Format_Version);\n-          }\n-          logger.trace(\"Calculate offsets, read and verify header version time: {}\",\n-              SystemTime.getInstance().milliseconds() - startTime);\n-\n-          // read and verify header\n-          startTime = SystemTime.getInstance().milliseconds();\n-          byte[] headerBytes = new byte[getHeaderSizeForVersion(version)];\n-          bufferedInputStream.read(headerBytes, Version_Field_Size_In_Bytes,\n-              headerBytes.length - Version_Field_Size_In_Bytes);\n-\n-          ByteBuffer header = ByteBuffer.wrap(headerBytes);\n-          header.putShort(version);\n-          header.rewind();\n-          MessageHeader_Format headerFormat = getMessageHeader(version, header);\n-          headerFormat.verifyHeader();\n-          logger.trace(\"Calculate offsets, read and verify header time: {}\",\n-              SystemTime.getInstance().milliseconds() - startTime);\n-\n-          // read and verify storeKey\n-          startTime = SystemTime.getInstance().milliseconds();\n-          StoreKey storeKey = storeKeyFactory.getStoreKey(new DataInputStream(bufferedInputStream));\n-          if (storeKey.compareTo(readSet.getKeyAt(i)) != 0) {\n-            throw new MessageFormatException(\n-                \"Id mismatch between metadata and store - metadataId \" + readSet.getKeyAt(i) + \" storeId \" + storeKey,\n-                MessageFormatErrorCodes.Store_Key_Id_MisMatch);\n-          }\n-          logger.trace(\"Calculate offsets, read and verify storeKey time: {}\",\n-              SystemTime.getInstance().milliseconds() - startTime);\n \n-          startTime = SystemTime.getInstance().milliseconds();\n+          MessageHeader_Format headerFormat = parseHeaderAndVerifyStoreKey(bufferedInputStream, i);\n+\n+          long startTime = SystemTime.getInstance().milliseconds();\n           if (flag == MessageFormatFlags.BlobProperties) {\n             sendInfoList.add(i, new SendInfo(headerFormat.getBlobPropertiesRecordRelativeOffset(),\n                 headerFormat.getBlobPropertiesRecordSize()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0949f241d1d2531bcd7fe0042479c9f6a8bcf909"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3MzU5Nw==", "bodyText": "removed.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520073597", "createdAt": "2020-11-09T19:41:19Z", "author": {"login": "zzmao"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);\n+            InputStream is = new ByteBufInputStream(blobAll);\n+\n+            MessageHeader_Format headerFormat = parseHeaderAndVerifyStoreKey(is, i);\n+\n+            MessageMetadata messageMetadata = null;\n+            if (headerFormat.hasEncryptionKeyRecord()) {\n+              // If encryption key exists, MessageMetadata with encryption key is needed.\n+              ByteBuf duplicatedByteBuf = blobAll.duplicate();\n+              duplicatedByteBuf.readerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset());\n+              duplicatedByteBuf.writerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset()\n+                  + headerFormat.getBlobEncryptionKeyRecordSize());\n+              messageMetadata =\n+                  new MessageMetadata(deserializeBlobEncryptionKey(new ByteBufInputStream(duplicatedByteBuf)));\n+            }\n+            messageMetadataList.add(messageMetadata);\n+            sendInfoList.add(i,\n+                new SendInfo(headerFormat.getBlobRecordRelativeOffset(), headerFormat.getBlobRecordSize()));\n+            totalSizeToWrite += headerFormat.getBlobRecordSize();\n+\n+            // Adjust underlying ByteBuf reader and writer index.\n+            blobAll.readerIndex(headerFormat.getBlobRecordRelativeOffset());\n+            blobAll.writerIndex((int) (headerFormat.getBlobRecordRelativeOffset() + headerFormat.getBlobRecordSize()));\n+          }\n         } else {\n-          long startTime = SystemTime.getInstance().milliseconds();\n           BufferedInputStream bufferedInputStream =\n               new BufferedInputStream(new MessageReadSetIndexInputStream(readSet, i, 0),\n                   BUFFERED_INPUT_STREAM_BUFFER_SIZE);\n-          // read and verify header version\n-          byte[] headerVersionBytes = new byte[Version_Field_Size_In_Bytes];\n-          bufferedInputStream.read(headerVersionBytes, 0, Version_Field_Size_In_Bytes);\n-          short version = ByteBuffer.wrap(headerVersionBytes).getShort();\n-          if (!isValidHeaderVersion(version)) {\n-            throw new MessageFormatException(\n-                \"Version not known while reading message - version \" + version + \", StoreKey \" + readSet.getKeyAt(i),\n-                MessageFormatErrorCodes.Unknown_Format_Version);\n-          }\n-          logger.trace(\"Calculate offsets, read and verify header version time: {}\",\n-              SystemTime.getInstance().milliseconds() - startTime);\n-\n-          // read and verify header\n-          startTime = SystemTime.getInstance().milliseconds();\n-          byte[] headerBytes = new byte[getHeaderSizeForVersion(version)];\n-          bufferedInputStream.read(headerBytes, Version_Field_Size_In_Bytes,\n-              headerBytes.length - Version_Field_Size_In_Bytes);\n-\n-          ByteBuffer header = ByteBuffer.wrap(headerBytes);\n-          header.putShort(version);\n-          header.rewind();\n-          MessageHeader_Format headerFormat = getMessageHeader(version, header);\n-          headerFormat.verifyHeader();\n-          logger.trace(\"Calculate offsets, read and verify header time: {}\",\n-              SystemTime.getInstance().milliseconds() - startTime);\n-\n-          // read and verify storeKey\n-          startTime = SystemTime.getInstance().milliseconds();\n-          StoreKey storeKey = storeKeyFactory.getStoreKey(new DataInputStream(bufferedInputStream));\n-          if (storeKey.compareTo(readSet.getKeyAt(i)) != 0) {\n-            throw new MessageFormatException(\n-                \"Id mismatch between metadata and store - metadataId \" + readSet.getKeyAt(i) + \" storeId \" + storeKey,\n-                MessageFormatErrorCodes.Store_Key_Id_MisMatch);\n-          }\n-          logger.trace(\"Calculate offsets, read and verify storeKey time: {}\",\n-              SystemTime.getInstance().milliseconds() - startTime);\n \n-          startTime = SystemTime.getInstance().milliseconds();\n+          MessageHeader_Format headerFormat = parseHeaderAndVerifyStoreKey(bufferedInputStream, i);\n+\n+          long startTime = SystemTime.getInstance().milliseconds();\n           if (flag == MessageFormatFlags.BlobProperties) {\n             sendInfoList.add(i, new SendInfo(headerFormat.getBlobPropertiesRecordRelativeOffset(),\n                 headerFormat.getBlobPropertiesRecordSize()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA1Njc2Mw=="}, "originalCommit": {"oid": "0949f241d1d2531bcd7fe0042479c9f6a8bcf909"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MzQ0NjQ3OnYy", "diffSide": "RIGHT", "path": "ambry-server/src/integration-test/java/com/github/ambry/server/ServerTestUtil.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQyMzo0MzowM1rOHvAwig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0MToyNVrOHv-xpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA1NzU0Ng==", "bodyText": "extra println here", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r519057546", "createdAt": "2020-11-06T23:43:03Z", "author": {"login": "cgtz"}, "path": "ambry-server/src/integration-test/java/com/github/ambry/server/ServerTestUtil.java", "diffHunk": "@@ -2066,6 +2066,7 @@ static void endToEndReplicationWithMultiNodeSinglePartitionTest(String routerDat\n       channel3.disconnect();\n     } catch (Exception e) {\n       e.printStackTrace();\n+      System.out.println(e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0949f241d1d2531bcd7fe0042479c9f6a8bcf909"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3MzYzOQ==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520073639", "createdAt": "2020-11-09T19:41:25Z", "author": {"login": "zzmao"}, "path": "ambry-server/src/integration-test/java/com/github/ambry/server/ServerTestUtil.java", "diffHunk": "@@ -2066,6 +2066,7 @@ static void endToEndReplicationWithMultiNodeSinglePartitionTest(String routerDat\n       channel3.disconnect();\n     } catch (Exception e) {\n       e.printStackTrace();\n+      System.out.println(e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA1NzU0Ng=="}, "originalCommit": {"oid": "0949f241d1d2531bcd7fe0042479c9f6a8bcf909"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDI5OTIwOnYy", "diffSide": "RIGHT", "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0Njo0MVrOHv-9JA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMDoyMzoyOVrOHwAODQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NjU4MA==", "bodyText": "nit: we can use a more descriptive name than just is, something like 'blobInputStream.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520076580", "createdAt": "2020-11-09T19:46:41Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);\n+            InputStream is = new ByteBufInputStream(blobAll);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA5NzI5Mw==", "bodyText": "sure. Changed.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520097293", "createdAt": "2020-11-09T20:23:29Z", "author": {"login": "zzmao"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);\n+            InputStream is = new ByteBufInputStream(blobAll);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NjU4MA=="}, "originalCommit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDMwMTM3OnYy", "diffSide": "RIGHT", "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0NzoxNlrOHv--fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMDoyMzo0MVrOHwAOaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NjkyNg==", "bodyText": "you can do duplicatedByteBuf.setIndex(readerIndex, writerIndex) in just one statement.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520076926", "createdAt": "2020-11-09T19:47:16Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);\n+            InputStream is = new ByteBufInputStream(blobAll);\n+\n+            MessageHeader_Format headerFormat = parseHeaderAndVerifyStoreKey(is, i);\n+\n+            MessageMetadata messageMetadata = null;\n+            if (headerFormat.hasEncryptionKeyRecord()) {\n+              // If encryption key exists, MessageMetadata with encryption key is needed.\n+              ByteBuf duplicatedByteBuf = blobAll.duplicate();\n+              duplicatedByteBuf.readerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset());\n+              duplicatedByteBuf.writerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA5NzM4NA==", "bodyText": "done.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520097384", "createdAt": "2020-11-09T20:23:41Z", "author": {"login": "zzmao"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);\n+            InputStream is = new ByteBufInputStream(blobAll);\n+\n+            MessageHeader_Format headerFormat = parseHeaderAndVerifyStoreKey(is, i);\n+\n+            MessageMetadata messageMetadata = null;\n+            if (headerFormat.hasEncryptionKeyRecord()) {\n+              // If encryption key exists, MessageMetadata with encryption key is needed.\n+              ByteBuf duplicatedByteBuf = blobAll.duplicate();\n+              duplicatedByteBuf.readerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset());\n+              duplicatedByteBuf.writerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NjkyNg=="}, "originalCommit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDMwMTg3OnYy", "diffSide": "RIGHT", "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0NzoyNlrOHv--1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0NzoyNlrOHv--1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NzAxNA==", "bodyText": "same above", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520077014", "createdAt": "2020-11-09T19:47:26Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);\n+            InputStream is = new ByteBufInputStream(blobAll);\n+\n+            MessageHeader_Format headerFormat = parseHeaderAndVerifyStoreKey(is, i);\n+\n+            MessageMetadata messageMetadata = null;\n+            if (headerFormat.hasEncryptionKeyRecord()) {\n+              // If encryption key exists, MessageMetadata with encryption key is needed.\n+              ByteBuf duplicatedByteBuf = blobAll.duplicate();\n+              duplicatedByteBuf.readerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset());\n+              duplicatedByteBuf.writerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset()\n+                  + headerFormat.getBlobEncryptionKeyRecordSize());\n+              messageMetadata =\n+                  new MessageMetadata(deserializeBlobEncryptionKey(new ByteBufInputStream(duplicatedByteBuf)));\n+            }\n+            messageMetadataList.add(messageMetadata);\n+            sendInfoList.add(i,\n+                new SendInfo(headerFormat.getBlobRecordRelativeOffset(), headerFormat.getBlobRecordSize()));\n+            totalSizeToWrite += headerFormat.getBlobRecordSize();\n+\n+            // Adjust underlying ByteBuf reader and writer index.\n+            blobAll.readerIndex(headerFormat.getBlobRecordRelativeOffset());\n+            blobAll.writerIndex((int) (headerFormat.getBlobRecordRelativeOffset() + headerFormat.getBlobRecordSize()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDMzNTQwOnYy", "diffSide": "RIGHT", "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo1NjozM1rOHv_Trw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjowNzo1MFrOHwDiCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA4MjM1MQ==", "bodyText": "I am a bit worried about the using prefetchedData here, without adding it to the dataFromReadSet list.\nIf there is any exception while parsing the header, this ByteBuf will be leaked. We should probably add the prefetchedData ByteBuf to the dataFromReadSet list before using it to parse the header.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520082351", "createdAt": "2020-11-09T19:56:33Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA5ODQzNw==", "bodyText": "dataFromReadSet.add(readSet.getPrefetchedData(i)); also covers this case. (It's after the if-else for each message). no?", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520098437", "createdAt": "2020-11-09T20:25:42Z", "author": {"login": "zzmao"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA4MjM1MQ=="}, "originalCommit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDEzMzI1NA==", "bodyText": "it does, but what if at line 136, parseHeaderAndVerifyStoreKey failed with an exception, then this ByteBuf will not be released.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520133254", "createdAt": "2020-11-09T21:31:43Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA4MjM1MQ=="}, "originalCommit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE1MTU2Mw==", "bodyText": "Got it. changes made.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520151563", "createdAt": "2020-11-09T22:07:50Z", "author": {"login": "zzmao"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA4MjM1MQ=="}, "originalCommit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDk0MzY2OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudMessageReadSet.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMzowMTowNVrOHwFB1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMzowMTowNVrOHwFB1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE3NjA4NA==", "bodyText": "why go extra step for this, we can just do\n      ByteBuf outputBuf = blobReadInfo.getBlobContent().duplicate();\n      long sizeToRead = Math.min(maxSize, blobReadInfo.getBlobSize() - relativeOffset);\n      outputBuf.setIndex((int) (relativeOffset), (int)(relativeOffset + sizeToRead));\n      written = channel.write(outputBuf.nioBuffer());", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520176084", "createdAt": "2020-11-09T23:01:05Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudMessageReadSet.java", "diffHunk": "@@ -56,14 +54,15 @@ public long writeTo(int index, WritableByteChannel channel, long relativeOffset,\n     try {\n       // TODO: Need to refactor the code to avoid prefetching blobs for BlobInfo request,\n       // or at least to prefetch only the header (requires CloudDestination enhancement)\n-      if (!blobReadInfo.isPrefetched()) {\n-        blobReadInfo.prefetchBlob(blobStore);\n+      if (!blobReadInfo.isBlobDownloaded()) {\n+        blobReadInfo.downloadBlob(blobStore);\n       }\n-      ByteBuffer outputBuffer = blobReadInfo.getPrefetchedBuffer();\n+      ByteBuf outputBuf = blobReadInfo.getBlobContent().duplicate();\n       long sizeToRead = Math.min(maxSize, blobReadInfo.getBlobSize() - relativeOffset);\n-      outputBuffer.limit((int) (relativeOffset + sizeToRead));\n-      outputBuffer.position((int) (relativeOffset));\n-      written = channel.write(outputBuffer);\n+      byte[] array = new byte[(int) sizeToRead];\n+      outputBuf.readerIndex((int) (relativeOffset));\n+      outputBuf.readBytes(array);\n+      written = channel.write(ByteBuffer.wrap(array));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7116d6d0fc1ae4cc2ad720f641d1426353f03ae3"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MTI3MDMxOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudMessageReadSet.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQwMToyMDozM1rOHwICBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQwMToyMDozM1rOHwICBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIyNTI4NA==", "bodyText": "nit: byteBuf.setIndex.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520225284", "createdAt": "2020-11-10T01:20:33Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudMessageReadSet.java", "diffHunk": "@@ -98,10 +97,12 @@ public StoreKey getKeyAt(int index) {\n   public void doPrefetch(int index, long relativeOffset, long size) throws IOException {\n     BlobReadInfo blobReadInfo = blobReadInfoList.get(index);\n     try {\n-      if (!blobReadInfo.isPrefetched()) {\n-        blobReadInfo.prefetchBlob(blobStore);\n+      if (!blobReadInfo.isBlobDownloaded()) {\n+        blobReadInfo.downloadBlob(blobStore);\n       }\n-      blobReadInfo.setPositionAndSize(relativeOffset, size);\n+      ByteBuf byteBuf = blobReadInfoList.get(index).getBlobContent();\n+      byteBuf.readerIndex((int) (relativeOffset));\n+      byteBuf.writerIndex((int) (relativeOffset + size));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7116d6d0fc1ae4cc2ad720f641d1426353f03ae3"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MTI4OTk1OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudMessageReadSet.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQwMToyOTozMlrOHwINbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxOToxNDowMlrOH1FKrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIyODIwNw==", "bodyText": "is it safe to use netty ByteBuf in CloudMessageReadSet? @ankagrawal  what is the cloud network implementation? does it use http2? socket server?", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520228207", "createdAt": "2020-11-10T01:29:32Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudMessageReadSet.java", "diffHunk": "@@ -140,81 +131,38 @@ private void validateIndex(int index) {\n   static class BlobReadInfo {\n     private final CloudBlobMetadata blobMetadata;\n     private final BlobId blobId;\n-    private ByteBuffer prefetchedBuffer;\n-    private int position = -1;\n-    private int size = -1;\n-    private boolean isPrefetched;\n+    private ByteBuf blobContent = null;\n \n     public BlobReadInfo(CloudBlobMetadata blobMetadata, BlobId blobId) {\n       this.blobMetadata = blobMetadata;\n       this.blobId = blobId;\n-      this.isPrefetched = false;\n     }\n \n     /**\n-     * Prefetch the {@code blob} from {@code CloudDestination} and put it in {@code prefetchedBuffer}\n+     * Download the {@code blob} from {@code CloudDestination} and put it in {@code blobContent}\n      * @param blobStore {@code CloudBlobStore} implementation representing the cloud from which download will happen.\n      * @throws StoreException if blob cloud not be downloaded\n      */\n-    public void prefetchBlob(CloudBlobStore blobStore) throws StoreException {\n+    public void downloadBlob(CloudBlobStore blobStore) throws StoreException {\n       // Casting blobsize to int, as blobs are chunked in Ambry, and chunk size is 4/8MB.\n       // However, if in future, if very large size of blobs are allowed, then prefetching logic should be changed.\n-      prefetchedBuffer = ByteBuffer.allocate((int) blobMetadata.getSize());\n-      ByteBufferOutputStream outputStream = new ByteBufferOutputStream(prefetchedBuffer);\n-      blobStore.downloadBlob(blobMetadata, blobId, outputStream);\n-      prefetchedBuffer.flip();\n-      isPrefetched = true;\n-    }\n-\n-    /**\n-     * Donwload the blob from the {@code CloudDestination} to the {@code OutputStream}\n-     * @param blobStore {@code CloudBlobStore} implementation representing the cloud from which download will happen.\n-     * @param outputStream OutputStream to download the blob to\n-     * @throws StoreException if blob download fails.\n-     */\n-    public void downloadBlob(CloudBlobStore blobStore, OutputStream outputStream) throws StoreException {\n+      blobContent = PooledByteBufAllocator.DEFAULT.ioBuffer((int) blobMetadata.getSize());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7116d6d0fc1ae4cc2ad720f641d1426353f03ae3"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjM3NDM0Ng==", "bodyText": "@justinlin-linkedin you are concerning if it's used in socket server, the ByteBuf can't be released?\nstoreMessageSet had the same problem. I guess we should retire socket server soon.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r522374346", "createdAt": "2020-11-12T19:41:33Z", "author": {"login": "zzmao"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudMessageReadSet.java", "diffHunk": "@@ -140,81 +131,38 @@ private void validateIndex(int index) {\n   static class BlobReadInfo {\n     private final CloudBlobMetadata blobMetadata;\n     private final BlobId blobId;\n-    private ByteBuffer prefetchedBuffer;\n-    private int position = -1;\n-    private int size = -1;\n-    private boolean isPrefetched;\n+    private ByteBuf blobContent = null;\n \n     public BlobReadInfo(CloudBlobMetadata blobMetadata, BlobId blobId) {\n       this.blobMetadata = blobMetadata;\n       this.blobId = blobId;\n-      this.isPrefetched = false;\n     }\n \n     /**\n-     * Prefetch the {@code blob} from {@code CloudDestination} and put it in {@code prefetchedBuffer}\n+     * Download the {@code blob} from {@code CloudDestination} and put it in {@code blobContent}\n      * @param blobStore {@code CloudBlobStore} implementation representing the cloud from which download will happen.\n      * @throws StoreException if blob cloud not be downloaded\n      */\n-    public void prefetchBlob(CloudBlobStore blobStore) throws StoreException {\n+    public void downloadBlob(CloudBlobStore blobStore) throws StoreException {\n       // Casting blobsize to int, as blobs are chunked in Ambry, and chunk size is 4/8MB.\n       // However, if in future, if very large size of blobs are allowed, then prefetching logic should be changed.\n-      prefetchedBuffer = ByteBuffer.allocate((int) blobMetadata.getSize());\n-      ByteBufferOutputStream outputStream = new ByteBufferOutputStream(prefetchedBuffer);\n-      blobStore.downloadBlob(blobMetadata, blobId, outputStream);\n-      prefetchedBuffer.flip();\n-      isPrefetched = true;\n-    }\n-\n-    /**\n-     * Donwload the blob from the {@code CloudDestination} to the {@code OutputStream}\n-     * @param blobStore {@code CloudBlobStore} implementation representing the cloud from which download will happen.\n-     * @param outputStream OutputStream to download the blob to\n-     * @throws StoreException if blob download fails.\n-     */\n-    public void downloadBlob(CloudBlobStore blobStore, OutputStream outputStream) throws StoreException {\n+      blobContent = PooledByteBufAllocator.DEFAULT.ioBuffer((int) blobMetadata.getSize());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIyODIwNw=="}, "originalCommit": {"oid": "7116d6d0fc1ae4cc2ad720f641d1426353f03ae3"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDM4MDQ3Mw==", "bodyText": "For vcr the network implementation currently is socket server based. It doesn't use http2.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r524380473", "createdAt": "2020-11-16T16:03:15Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudMessageReadSet.java", "diffHunk": "@@ -140,81 +131,38 @@ private void validateIndex(int index) {\n   static class BlobReadInfo {\n     private final CloudBlobMetadata blobMetadata;\n     private final BlobId blobId;\n-    private ByteBuffer prefetchedBuffer;\n-    private int position = -1;\n-    private int size = -1;\n-    private boolean isPrefetched;\n+    private ByteBuf blobContent = null;\n \n     public BlobReadInfo(CloudBlobMetadata blobMetadata, BlobId blobId) {\n       this.blobMetadata = blobMetadata;\n       this.blobId = blobId;\n-      this.isPrefetched = false;\n     }\n \n     /**\n-     * Prefetch the {@code blob} from {@code CloudDestination} and put it in {@code prefetchedBuffer}\n+     * Download the {@code blob} from {@code CloudDestination} and put it in {@code blobContent}\n      * @param blobStore {@code CloudBlobStore} implementation representing the cloud from which download will happen.\n      * @throws StoreException if blob cloud not be downloaded\n      */\n-    public void prefetchBlob(CloudBlobStore blobStore) throws StoreException {\n+    public void downloadBlob(CloudBlobStore blobStore) throws StoreException {\n       // Casting blobsize to int, as blobs are chunked in Ambry, and chunk size is 4/8MB.\n       // However, if in future, if very large size of blobs are allowed, then prefetching logic should be changed.\n-      prefetchedBuffer = ByteBuffer.allocate((int) blobMetadata.getSize());\n-      ByteBufferOutputStream outputStream = new ByteBufferOutputStream(prefetchedBuffer);\n-      blobStore.downloadBlob(blobMetadata, blobId, outputStream);\n-      prefetchedBuffer.flip();\n-      isPrefetched = true;\n-    }\n-\n-    /**\n-     * Donwload the blob from the {@code CloudDestination} to the {@code OutputStream}\n-     * @param blobStore {@code CloudBlobStore} implementation representing the cloud from which download will happen.\n-     * @param outputStream OutputStream to download the blob to\n-     * @throws StoreException if blob download fails.\n-     */\n-    public void downloadBlob(CloudBlobStore blobStore, OutputStream outputStream) throws StoreException {\n+      blobContent = PooledByteBufAllocator.DEFAULT.ioBuffer((int) blobMetadata.getSize());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIyODIwNw=="}, "originalCommit": {"oid": "7116d6d0fc1ae4cc2ad720f641d1426353f03ae3"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDM4Njk2OQ==", "bodyText": "For now, until we have netty based http2 implementation in cloud, is there an issue with using ByteBuffer.allocate for cloud?", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r524386969", "createdAt": "2020-11-16T16:12:20Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudMessageReadSet.java", "diffHunk": "@@ -140,81 +131,38 @@ private void validateIndex(int index) {\n   static class BlobReadInfo {\n     private final CloudBlobMetadata blobMetadata;\n     private final BlobId blobId;\n-    private ByteBuffer prefetchedBuffer;\n-    private int position = -1;\n-    private int size = -1;\n-    private boolean isPrefetched;\n+    private ByteBuf blobContent = null;\n \n     public BlobReadInfo(CloudBlobMetadata blobMetadata, BlobId blobId) {\n       this.blobMetadata = blobMetadata;\n       this.blobId = blobId;\n-      this.isPrefetched = false;\n     }\n \n     /**\n-     * Prefetch the {@code blob} from {@code CloudDestination} and put it in {@code prefetchedBuffer}\n+     * Download the {@code blob} from {@code CloudDestination} and put it in {@code blobContent}\n      * @param blobStore {@code CloudBlobStore} implementation representing the cloud from which download will happen.\n      * @throws StoreException if blob cloud not be downloaded\n      */\n-    public void prefetchBlob(CloudBlobStore blobStore) throws StoreException {\n+    public void downloadBlob(CloudBlobStore blobStore) throws StoreException {\n       // Casting blobsize to int, as blobs are chunked in Ambry, and chunk size is 4/8MB.\n       // However, if in future, if very large size of blobs are allowed, then prefetching logic should be changed.\n-      prefetchedBuffer = ByteBuffer.allocate((int) blobMetadata.getSize());\n-      ByteBufferOutputStream outputStream = new ByteBufferOutputStream(prefetchedBuffer);\n-      blobStore.downloadBlob(blobMetadata, blobId, outputStream);\n-      prefetchedBuffer.flip();\n-      isPrefetched = true;\n-    }\n-\n-    /**\n-     * Donwload the blob from the {@code CloudDestination} to the {@code OutputStream}\n-     * @param blobStore {@code CloudBlobStore} implementation representing the cloud from which download will happen.\n-     * @param outputStream OutputStream to download the blob to\n-     * @throws StoreException if blob download fails.\n-     */\n-    public void downloadBlob(CloudBlobStore blobStore, OutputStream outputStream) throws StoreException {\n+      blobContent = PooledByteBufAllocator.DEFAULT.ioBuffer((int) blobMetadata.getSize());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIyODIwNw=="}, "originalCommit": {"oid": "7116d6d0fc1ae4cc2ad720f641d1426353f03ae3"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTQyMTIzMQ==", "bodyText": "Both socket and http network stack can take care of allocated ByteBuf. As long as we have no cloud based network implementation, it's good. (Or release ByteBuf correctly in new implementations)", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r525421231", "createdAt": "2020-11-17T19:14:02Z", "author": {"login": "zzmao"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudMessageReadSet.java", "diffHunk": "@@ -140,81 +131,38 @@ private void validateIndex(int index) {\n   static class BlobReadInfo {\n     private final CloudBlobMetadata blobMetadata;\n     private final BlobId blobId;\n-    private ByteBuffer prefetchedBuffer;\n-    private int position = -1;\n-    private int size = -1;\n-    private boolean isPrefetched;\n+    private ByteBuf blobContent = null;\n \n     public BlobReadInfo(CloudBlobMetadata blobMetadata, BlobId blobId) {\n       this.blobMetadata = blobMetadata;\n       this.blobId = blobId;\n-      this.isPrefetched = false;\n     }\n \n     /**\n-     * Prefetch the {@code blob} from {@code CloudDestination} and put it in {@code prefetchedBuffer}\n+     * Download the {@code blob} from {@code CloudDestination} and put it in {@code blobContent}\n      * @param blobStore {@code CloudBlobStore} implementation representing the cloud from which download will happen.\n      * @throws StoreException if blob cloud not be downloaded\n      */\n-    public void prefetchBlob(CloudBlobStore blobStore) throws StoreException {\n+    public void downloadBlob(CloudBlobStore blobStore) throws StoreException {\n       // Casting blobsize to int, as blobs are chunked in Ambry, and chunk size is 4/8MB.\n       // However, if in future, if very large size of blobs are allowed, then prefetching logic should be changed.\n-      prefetchedBuffer = ByteBuffer.allocate((int) blobMetadata.getSize());\n-      ByteBufferOutputStream outputStream = new ByteBufferOutputStream(prefetchedBuffer);\n-      blobStore.downloadBlob(blobMetadata, blobId, outputStream);\n-      prefetchedBuffer.flip();\n-      isPrefetched = true;\n-    }\n-\n-    /**\n-     * Donwload the blob from the {@code CloudDestination} to the {@code OutputStream}\n-     * @param blobStore {@code CloudBlobStore} implementation representing the cloud from which download will happen.\n-     * @param outputStream OutputStream to download the blob to\n-     * @throws StoreException if blob download fails.\n-     */\n-    public void downloadBlob(CloudBlobStore blobStore, OutputStream outputStream) throws StoreException {\n+      blobContent = PooledByteBufAllocator.DEFAULT.ioBuffer((int) blobMetadata.getSize());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIyODIwNw=="}, "originalCommit": {"oid": "7116d6d0fc1ae4cc2ad720f641d1426353f03ae3"}, "originalPosition": 115}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1116, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}