{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI2ODQ2NTc4", "number": 1705, "title": "Fix null replica in HelixClusterMap when decommissioning replicas.", "bodyText": "Ambry frontend now allows GET operation to try on OFFLINE replicas. During replica decommission, the first step is to\ndisable replica. This will eventually bring replica to OFFLINE state and remove its entry from DataNodeConfig. All listeners\n(both frontends and servers) will receive the DataNodeConfig change and remove this replica from in-mem clustermap. However, at this point of time, the IdealState in Helix hasn't been changed and this replica still exists in external view. So when router calls getReplicaIdByState(), the RoutingTableProvider still returns instance(server) associated with this replica. This causes a null replica when router queries in-mem clustermap by this instance and our code doesn't invalidate the null value in the return result, which triggers NPEs in the end.\nThis PR makes following changes:\n\nAdd a config to control whether to add down replicas to the replica pool in operation tracker. If yes, down(offline) replicas\nwill be added to the end.\nPreclude null replicas in the method of getReplicaIdsByState().\nMake \"update replication lag\" in ReplicaSyncUpManager atomic. Added lock to avoid race condition in the process of  updateLag-checkStatus-completeSyncUp. (We saw two threads concurrently attempting to count the same latch in production. The loser threw IllegalStateException because latch was already removed.)", "createdAt": "2020-11-24T23:22:48Z", "url": "https://github.com/linkedin/ambry/pull/1705", "merged": true, "mergeCommit": {"oid": "f7c9892f1a3d1abe397bcb669b446130b3ac97a1"}, "closed": true, "closedAt": "2020-12-16T00:49:27Z", "author": {"login": "jsjtzyy"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdfx3fngH2gAyNTI2ODQ2NTc4OmEwN2RmZWE1YTc2ZmE4Yzk0MmZiZjE0MGFiOWFhMTU0MWI5MTE2YTI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdmkD7lAFqTU1MzE5ODQyNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "a07dfea5a76fa8c942fbf140ab9aa1541b9116a2", "author": {"user": {"login": "jsjtzyy", "name": "Yingyi Zhang"}}, "url": "https://github.com/linkedin/ambry/commit/a07dfea5a76fa8c942fbf140ab9aa1541b9116a2", "committedDate": "2020-11-24T22:54:51Z", "message": "Fix null replica in HelixClusterMap when decommissing replicas.\n\nAmbry frontend now allows GET operation to try on OFFLINE replicas. During replica decommission, the first step is to\ndisable replica. This will eventually bring replica to OFFLINE state and remove its entry from DataNodeConfig. All listeners\n(both frontends and servers) will receive the DataNodeConfig change and remove this replica from in-mem clustermap. However,\nat this point of time, the IdealState in Helix hasn't been changed and this replica still exists in external view. So when\nrouter calls getReplicaIdByState(), the RoutingTableProvider still returns instance(server) associated with this replica. This\ncauses a null replica when router queries in-mem clustermap by this instance and our code doesn't invalidate the null value in\nthe return result, which results in NPEs.\nThis PR makes two changes:\n1. Add a config to control whether to add down replicas to the replica pool in operation tracker. If yes, down(offline) replicas\n   will be added to the end.\n2. Preclude null replicas in the method of getReplicaIdsByState()."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "47d0793fcbec5876b483d5f31bbe6661b7b40497", "author": {"user": {"login": "jsjtzyy", "name": "Yingyi Zhang"}}, "url": "https://github.com/linkedin/ambry/commit/47d0793fcbec5876b483d5f31bbe6661b7b40497", "committedDate": "2020-11-25T18:02:50Z", "message": "fix test failure"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0MzcxNTgy", "url": "https://github.com/linkedin/ambry/pull/1705#pullrequestreview-544371582", "createdAt": "2020-12-03T20:05:56Z", "commit": {"oid": "47d0793fcbec5876b483d5f31bbe6661b7b40497"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMDowNTo1NlrOH-vUCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMTo1MDo1MVrOH-2UBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU0ODkzOQ==", "bodyText": "nit: it looks like this comment should still go above getResourcesInClusterWithTag?", "url": "https://github.com/linkedin/ambry/pull/1705#discussion_r535548939", "createdAt": "2020-12-03T20:05:56Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/test/java/com/github/ambry/clustermap/MockHelixAdmin.java", "diffHunk": "@@ -407,36 +446,15 @@ int getSetInstanceConfigCallCount() {\n     return setInstanceConfigCallCount;\n   }\n \n-  /**\n-   * Private class that holds partition state infos from one data node.\n-   */\n-  class ReplicaStateInfos {\n-    Map<String, Map<String, String>> replicaStateMap;\n-\n-    ReplicaStateInfos() {\n-      replicaStateMap = new HashMap<>();\n-    }\n-\n-    void setReplicaState(String partition, String state) {\n-      Map<String, String> stateMap = new HashMap<>();\n-      stateMap.put(CurrentState.CurrentStateProperty.CURRENT_STATE.name(), state);\n-      replicaStateMap.put(partition, stateMap);\n-    }\n-\n-    Map<String, Map<String, String>> getReplicaStateMap() {\n-      return replicaStateMap;\n-    }\n+  @Override\n+  public List<String> getResourcesInClusterWithTag(String clusterName, String tag) {\n+    throw new IllegalStateException(\"Not implemented\");\n   }\n \n   // ***************************************", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d0793fcbec5876b483d5f31bbe6661b7b40497"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU3NzIxOA==", "bodyText": "So we have removed the option to exclude cross colo replicas that aren't in the originating DC? I thought that we used this option in production to improve latency in the not found case?", "url": "https://github.com/linkedin/ambry/pull/1705#discussion_r535577218", "createdAt": "2020-12-03T20:33:41Z", "author": {"login": "cgtz"}, "path": "ambry-router/src/main/java/com/github/ambry/router/SimpleOperationTracker.java", "diffHunk": "@@ -265,19 +258,29 @@\n     }\n     List<ReplicaId> backupReplicasToCheck = new ArrayList<>(backupReplicas);\n     List<ReplicaId> downReplicasToCheck = new ArrayList<>(downReplicas);\n-    if (includeNonOriginatingDcReplicas || this.originatingDcName == null) {\n-      backupReplicas.forEach(this::addToEndOfPool);\n+\n+    // Add replicas that are neither in local dc nor in originating dc.\n+    backupReplicas.forEach(this::addToEndOfPool);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d0793fcbec5876b483d5f31bbe6661b7b40497"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY2MzYyMw==", "bodyText": "It may be better to do a loop here. Mutating the offlineReplicas list was a little confusing to me and a loop can save a couple list iterations:\nSet remoteOfflineReplicas\nfor (replica in offlineReplicas) {\n  if (replica in originating DC) {\n    numReplicasInOriginatingDc++;\n  }\n  if (replica in local DC) {\n    addToEndOfPool\n  } else {\n    remoteOfflineReplicas.add(replica)\n  }\n}\nremoteOfflineReplicas.forEach(addToEndOfPool);", "url": "https://github.com/linkedin/ambry/pull/1705#discussion_r535663623", "createdAt": "2020-12-03T21:50:51Z", "author": {"login": "cgtz"}, "path": "ambry-router/src/main/java/com/github/ambry/router/SimpleOperationTracker.java", "diffHunk": "@@ -265,19 +258,29 @@\n     }\n     List<ReplicaId> backupReplicasToCheck = new ArrayList<>(backupReplicas);\n     List<ReplicaId> downReplicasToCheck = new ArrayList<>(downReplicas);\n-    if (includeNonOriginatingDcReplicas || this.originatingDcName == null) {\n-      backupReplicas.forEach(this::addToEndOfPool);\n+\n+    // Add replicas that are neither in local dc nor in originating dc.\n+    backupReplicas.forEach(this::addToEndOfPool);\n+\n+    if (routerConfig.routerOperationTrackerIncludeDownReplicas) {\n+      // Add those replicas deemed by native failure detector to be down\n       downReplicas.forEach(this::addToEndOfPool);\n-    } else {\n-      // This is for get request only. Take replicasRequired copy of replicas to do the request\n-      // Please note replicasRequired is 6 because total number of local and originating replicas is always <= 6.\n-      // This may no longer be true with partition classes and flexible replication.\n-      // Don't do this if originatingDcName is unknown.\n-      while (replicaPool.size() < numOfReplicasRequired && backupReplicas.size() > 0) {\n-        addToEndOfPool(backupReplicas.pollFirst());\n-      }\n-      while (replicaPool.size() < numOfReplicasRequired && downReplicas.size() > 0) {\n-        addToEndOfPool(downReplicas.pollFirst());\n+      // Add those replicas deemed by Helix to be down (offline). This only applies to GET operation.\n+      // Adding this logic to mitigate situation where one or more Zookeeper clusters are suddenly unavailable while\n+      // ambry servers are still up.\n+      if (routerOperation == RouterOperation.GetBlobOperation\n+          || routerOperation == RouterOperation.GetBlobInfoOperation) {\n+        Set<ReplicaId> offlineReplicas =\n+            new HashSet<>(getEligibleReplicas(partitionId, null, EnumSet.of(ReplicaState.OFFLINE)));\n+        Set<ReplicaId> originatingReplicas = offlineReplicas.stream().filter(r -> r.getDataNodeId().getDatacenterName().equals(this.originatingDcName)).collect(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47d0793fcbec5876b483d5f31bbe6661b7b40497"}, "originalPosition": 87}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3d3e6c46732448941b37bfd6cec4a438d195c064", "author": {"user": {"login": "jsjtzyy", "name": "Yingyi Zhang"}}, "url": "https://github.com/linkedin/ambry/commit/3d3e6c46732448941b37bfd6cec4a438d195c064", "committedDate": "2020-12-03T22:57:08Z", "message": "address Casey's comment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ1MTI4Nzg2", "url": "https://github.com/linkedin/ambry/pull/1705#pullrequestreview-545128786", "createdAt": "2020-12-04T16:51:15Z", "commit": {"oid": "3d3e6c46732448941b37bfd6cec4a438d195c064"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dfe191909742d6304de1b2ec631e6a5b7bf931a8", "author": {"user": {"login": "jsjtzyy", "name": "Yingyi Zhang"}}, "url": "https://github.com/linkedin/ambry/commit/dfe191909742d6304de1b2ec631e6a5b7bf931a8", "committedDate": "2020-12-06T22:11:17Z", "message": "fix the race condition in ReplicaSyncUpManager (minor refactoring)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzMTk4NDI0", "url": "https://github.com/linkedin/ambry/pull/1705#pullrequestreview-553198424", "createdAt": "2020-12-16T00:47:14Z", "commit": {"oid": "dfe191909742d6304de1b2ec631e6a5b7bf931a8"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1020, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}