{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM2MTU4NDc0", "number": 1570, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxODowMzoyNVrOEG23-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMjo1NTowOVrOEG7-qg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjI1OTc5OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxODowMzoyNVrOGl6cPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxODoxNDoyN1rOGl6zYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQwNzk5OA==", "bodyText": "In this case, we still recovery Undelete index, but P can't be accessed, right?", "url": "https://github.com/linkedin/ambry/pull/1570#discussion_r442407998", "createdAt": "2020-06-18T18:03:25Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "diffHunk": "@@ -980,35 +964,54 @@ IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, lon\n    * @throws StoreException if there is any problem writing the index record\n    */\n   IndexValue markAsUndeleted(StoreKey id, FileSpan fileSpan, long operationTimeMs) throws StoreException {\n-    return markAsUndeleted(id, fileSpan, operationTimeMs, MessageInfo.LIFE_VERSION_FROM_FRONTEND);\n+    return markAsUndeleted(id, fileSpan, null, operationTimeMs, MessageInfo.LIFE_VERSION_FROM_FRONTEND);\n   }\n \n   /**\n    * Marks a blob as undeleted\n    * @param id the {@link StoreKey} of the blob\n    * @param fileSpan the file span represented by this entry in the log\n    * @param operationTimeMs the time of the update operation\n+   * @param info this needs to be non-null in the case of recovery and replicateion. Can be {@code null} otherwise. Used if the PUT\n+   *             record could not be found\n    * @param lifeVersion lifeVersion of this undelete record.\n    * @return the {@link IndexValue} of the undelete record\n    * @throws StoreException if there is any problem writing the index record\n    */\n-  IndexValue markAsUndeleted(StoreKey id, FileSpan fileSpan, long operationTimeMs, short lifeVersion)\n+  IndexValue markAsUndeleted(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs, short lifeVersion)\n       throws StoreException {\n     boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     validateFileSpan(fileSpan, true);\n+    long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     List<IndexValue> values =\n         findAllIndexValuesForKey(id, null, EnumSet.allOf(IndexEntryType.class), validIndexSegments);\n-    validateSanityForUndelete(id, values, lifeVersion);\n-    // This value is the delete IndexValue\n-    IndexValue value = values.get(0);\n-    maybeChangeExpirationDate(value, values);\n-    lifeVersion = hasLifeVersion ? lifeVersion : (short) (value.getLifeVersion() + 1);\n-    long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n-    IndexValue newValue =\n-        new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(), operationTimeMs,\n-            value.getAccountId(), value.getContainerId(), lifeVersion);\n-    newValue.setNewOffset(fileSpan.getStartOffset());\n-    newValue.setNewSize(size);\n+    IndexValue newValue;\n+    if (info != null) {\n+      // This is from recovery. In recovery, we don't need to do any sanity check because\n+      // 1. we already know the IndexValue has it's source in the log.\n+      // 2. some sanity check will fail.\n+      //    assume we have P, D, U, D in the log, then a compaction cycle compacted P and first D,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62ccc8693af1daff936bc398c25a08fa2f3fe523"}, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQxMzkyMg==", "bodyText": "That's right. We used to think that when there is a U, there must be a P before U. This is wrong. After compaction, anything could happen. After compaction, in this case, we are left with only U and D, without any P. And recovery need to work for this case as well.", "url": "https://github.com/linkedin/ambry/pull/1570#discussion_r442413922", "createdAt": "2020-06-18T18:14:27Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "diffHunk": "@@ -980,35 +964,54 @@ IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, lon\n    * @throws StoreException if there is any problem writing the index record\n    */\n   IndexValue markAsUndeleted(StoreKey id, FileSpan fileSpan, long operationTimeMs) throws StoreException {\n-    return markAsUndeleted(id, fileSpan, operationTimeMs, MessageInfo.LIFE_VERSION_FROM_FRONTEND);\n+    return markAsUndeleted(id, fileSpan, null, operationTimeMs, MessageInfo.LIFE_VERSION_FROM_FRONTEND);\n   }\n \n   /**\n    * Marks a blob as undeleted\n    * @param id the {@link StoreKey} of the blob\n    * @param fileSpan the file span represented by this entry in the log\n    * @param operationTimeMs the time of the update operation\n+   * @param info this needs to be non-null in the case of recovery and replicateion. Can be {@code null} otherwise. Used if the PUT\n+   *             record could not be found\n    * @param lifeVersion lifeVersion of this undelete record.\n    * @return the {@link IndexValue} of the undelete record\n    * @throws StoreException if there is any problem writing the index record\n    */\n-  IndexValue markAsUndeleted(StoreKey id, FileSpan fileSpan, long operationTimeMs, short lifeVersion)\n+  IndexValue markAsUndeleted(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs, short lifeVersion)\n       throws StoreException {\n     boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     validateFileSpan(fileSpan, true);\n+    long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     List<IndexValue> values =\n         findAllIndexValuesForKey(id, null, EnumSet.allOf(IndexEntryType.class), validIndexSegments);\n-    validateSanityForUndelete(id, values, lifeVersion);\n-    // This value is the delete IndexValue\n-    IndexValue value = values.get(0);\n-    maybeChangeExpirationDate(value, values);\n-    lifeVersion = hasLifeVersion ? lifeVersion : (short) (value.getLifeVersion() + 1);\n-    long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n-    IndexValue newValue =\n-        new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(), operationTimeMs,\n-            value.getAccountId(), value.getContainerId(), lifeVersion);\n-    newValue.setNewOffset(fileSpan.getStartOffset());\n-    newValue.setNewSize(size);\n+    IndexValue newValue;\n+    if (info != null) {\n+      // This is from recovery. In recovery, we don't need to do any sanity check because\n+      // 1. we already know the IndexValue has it's source in the log.\n+      // 2. some sanity check will fail.\n+      //    assume we have P, D, U, D in the log, then a compaction cycle compacted P and first D,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQwNzk5OA=="}, "originalCommit": {"oid": "62ccc8693af1daff936bc398c25a08fa2f3fe523"}, "originalPosition": 184}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjM2NDc5OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxODozNToyMlrOGl7f1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxODozNToyMlrOGl7f1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQyNTMwMA==", "bodyText": "typo: replication", "url": "https://github.com/linkedin/ambry/pull/1570#discussion_r442425300", "createdAt": "2020-06-18T18:35:22Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "diffHunk": "@@ -980,35 +964,54 @@ IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, lon\n    * @throws StoreException if there is any problem writing the index record\n    */\n   IndexValue markAsUndeleted(StoreKey id, FileSpan fileSpan, long operationTimeMs) throws StoreException {\n-    return markAsUndeleted(id, fileSpan, operationTimeMs, MessageInfo.LIFE_VERSION_FROM_FRONTEND);\n+    return markAsUndeleted(id, fileSpan, null, operationTimeMs, MessageInfo.LIFE_VERSION_FROM_FRONTEND);\n   }\n \n   /**\n    * Marks a blob as undeleted\n    * @param id the {@link StoreKey} of the blob\n    * @param fileSpan the file span represented by this entry in the log\n    * @param operationTimeMs the time of the update operation\n+   * @param info this needs to be non-null in the case of recovery and replicateion. Can be {@code null} otherwise. Used if the PUT", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62ccc8693af1daff936bc398c25a08fa2f3fe523"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njk1NDY4OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTo0Nzo0NVrOGmBVIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTo0Nzo0NVrOGmBVIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyMDg2Nw==", "bodyText": "minor: this can be more descriptive like invalid lifeVersion -1 (although I know this exception should not happen in practice)", "url": "https://github.com/linkedin/ambry/pull/1570#discussion_r442520867", "createdAt": "2020-06-18T21:47:45Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "diffHunk": "@@ -980,35 +964,54 @@ IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, lon\n    * @throws StoreException if there is any problem writing the index record\n    */\n   IndexValue markAsUndeleted(StoreKey id, FileSpan fileSpan, long operationTimeMs) throws StoreException {\n-    return markAsUndeleted(id, fileSpan, operationTimeMs, MessageInfo.LIFE_VERSION_FROM_FRONTEND);\n+    return markAsUndeleted(id, fileSpan, null, operationTimeMs, MessageInfo.LIFE_VERSION_FROM_FRONTEND);\n   }\n \n   /**\n    * Marks a blob as undeleted\n    * @param id the {@link StoreKey} of the blob\n    * @param fileSpan the file span represented by this entry in the log\n    * @param operationTimeMs the time of the update operation\n+   * @param info this needs to be non-null in the case of recovery and replicateion. Can be {@code null} otherwise. Used if the PUT\n+   *             record could not be found\n    * @param lifeVersion lifeVersion of this undelete record.\n    * @return the {@link IndexValue} of the undelete record\n    * @throws StoreException if there is any problem writing the index record\n    */\n-  IndexValue markAsUndeleted(StoreKey id, FileSpan fileSpan, long operationTimeMs, short lifeVersion)\n+  IndexValue markAsUndeleted(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs, short lifeVersion)\n       throws StoreException {\n     boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     validateFileSpan(fileSpan, true);\n+    long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     List<IndexValue> values =\n         findAllIndexValuesForKey(id, null, EnumSet.allOf(IndexEntryType.class), validIndexSegments);\n-    validateSanityForUndelete(id, values, lifeVersion);\n-    // This value is the delete IndexValue\n-    IndexValue value = values.get(0);\n-    maybeChangeExpirationDate(value, values);\n-    lifeVersion = hasLifeVersion ? lifeVersion : (short) (value.getLifeVersion() + 1);\n-    long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n-    IndexValue newValue =\n-        new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(), operationTimeMs,\n-            value.getAccountId(), value.getContainerId(), lifeVersion);\n-    newValue.setNewOffset(fileSpan.getStartOffset());\n-    newValue.setNewSize(size);\n+    IndexValue newValue;\n+    if (info != null) {\n+      // This is from recovery. In recovery, we don't need to do any sanity check because\n+      // 1. we already know the IndexValue has it's source in the log.\n+      // 2. some sanity check will fail.\n+      //    assume we have P, D, U, D in the log, then a compaction cycle compacted P and first D,\n+      //    then we only have U and second D. U in this case, will have no prior records.\n+      if (!hasLifeVersion) {\n+        throw new StoreException(\"MessageInfo of undelete carries invalid lifeVersion\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62ccc8693af1daff936bc398c25a08fa2f3fe523"}, "originalPosition": 187}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njk3NzMxOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTo1NzoxMlrOGmBjbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTo1NzoxMlrOGmBjbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyNDUyNA==", "bodyText": "add a comment here to help understanding the context", "url": "https://github.com/linkedin/ambry/pull/1570#discussion_r442524524", "createdAt": "2020-06-18T21:57:12Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "diffHunk": "@@ -324,15 +324,13 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n           // removes from the tracking structure if a delete was being expected for the key\n           deleteExpectedKeys.remove(info.getStoreKey());\n         } else if (info.isUndeleted()) {\n-          markAsUndeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info.getOperationTimeMs(),\n-              info.getLifeVersion());\n+          markAsUndeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n+              info.getOperationTimeMs(), info.getLifeVersion());\n           logger.info(\n               \"Index : {} updated message with key {} by inserting undelete entry of size {} ttl {} lifeVersion {}\",\n               dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           if (value == null) {\n-            // Undelete record indicates that a put and/or a delete record were expected.\n-            throw new StoreException(\"Put record were expected but were not encountered for key: \" + info.getStoreKey(),\n-                StoreErrorCodes.Initialization_Error);\n+            deleteExpectedKeys.add(info.getStoreKey());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62ccc8693af1daff936bc398c25a08fa2f3fe523"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NzA2NjI2OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMjozOToyMVrOGmCcFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMjozOToyMVrOGmCcFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUzOTAyOA==", "bodyText": "remove -> remote", "url": "https://github.com/linkedin/ambry/pull/1570#discussion_r442539028", "createdAt": "2020-06-18T22:39:21Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "diffHunk": "@@ -722,15 +720,22 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n    */\n   void validateSanityForUndelete(StoreKey key, List<IndexValue> values, short lifeVersion) throws StoreException {\n     if (values == null || values.isEmpty()) {\n-      throw new StoreException(\"Id \" + key + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n+      throw new StoreException(\"Id \" + key + \" not found in \" + dataDir, StoreErrorCodes.ID_Not_Found);\n     }\n     if (!IndexValue.hasLifeVersion(lifeVersion)) {\n       validateSanityForUndeleteWithoutLifeVersion(key, values);\n       return;\n     }\n-    // This is from recovery or replication, make sure the last value is a put and the first value's lifeVersion is strictly\n-    // less than the given lifeVersion. We don't care about the first value's type, it can be a put, ttl_update or delete, it\n-    // can even be an undelete.\n+    // This is from replication. For replication, undelete should be permitted only when\n+    // 1. The oldest record is PUT\n+    // 2. the latest record's lifeVersion is less then undelete's lifeVersion.\n+    // 3. Replication doesn't care if the latest record is delete or not. In local, we can have a PUT record when\n+    // the remote has PUT, DELETE, UNDELETE. When replicating from remote, local has to insert UNDELETE, where\n+    // there is no prior DELETE.\n+    // 4. Replication doesn't care if the latest record is expired or not. In local, we can have a PUT record when\n+    // the remove has PUT, DELETE, UNDELETE TTL_UPDATE. When replicating from remote, PUT might already have expired.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62ccc8693af1daff936bc398c25a08fa2f3fe523"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NzA3OTkzOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/IndexValue.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMjo0NjoxOFrOGmCk2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMjo0NjoxOFrOGmCk2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU0MTI3NA==", "bodyText": "This is only used for testing, right?  (Also format the file please)", "url": "https://github.com/linkedin/ambry/pull/1570#discussion_r442541274", "createdAt": "2020-06-18T22:46:18Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/IndexValue.java", "diffHunk": "@@ -362,6 +362,14 @@ void setNewOffset(Offset newOffset) {\n     setOriginalMessageOffset(oldOffset);\n   }\n \n+  /**\n+   * Updates the {@link #lifeVersion} of this {@link IndexValue}.\n+   * @param lifeVersion the new lifeVersion to set.\n+   */\n+  void setNewLifeVersion(short lifeVersion) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62ccc8693af1daff936bc398c25a08fa2f3fe523"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NzA5NjEwOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMjo1NTowOVrOGmCvEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMzowNTowMVrOGmC6bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU0Mzg4OA==", "bodyText": "Looks like if valueFromTgtIdx != null, we only check version and undelete state, and we still add a new index entry to tgtIndex?", "url": "https://github.com/linkedin/ambry/pull/1570#discussion_r442543888", "createdAt": "2020-06-18T22:55:09Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -767,16 +768,27 @@ private boolean copyRecords(LogSegment logSegmentToCopy, List<IndexEntry> srcInd\n               }\n             } else if (srcValue.isUndelete()) {\n               if (valueFromTgtIdx != null) {\n-                tgtIndex.markAsUndeleted(srcIndexEntry.getKey(), fileSpan, srcValue.getOperationTimeInMs(),\n-                    srcValue.getLifeVersion());\n-              } else {\n-                IndexValue tgtValue = new IndexValue(srcValue.getSize(), fileSpan.getStartOffset(), srcValue.getFlags(),\n-                    srcValue.getExpiresAtMs(), srcValue.getOperationTimeInMs(), srcValue.getAccountId(),\n-                    srcValue.getContainerId(), srcValue.getLifeVersion());\n-                tgtValue.setFlag(IndexValue.Flags.Undelete_Index);\n-                tgtValue.clearOriginalMessageOffset();\n-                tgtIndex.addToIndex(new IndexEntry(srcIndexEntry.getKey(), tgtValue), fileSpan);\n+                // Here don't use markAsUndelete method. In markAsUndelete, multiple sanity check would be applied. But\n+                // target index might only contain incomplete blob history, which would fail sanity check.\n+                // Here we do a different check to make sure we don't insert undelete with lower lifeVersion.\n+                if (valueFromTgtIdx.isUndelete()) {\n+                  throw new StoreException(\n+                      \"Id \" + srcIndexEntry.getKey() + \" already has undelete \" + valueFromTgtIdx + \" in index \"\n+                          + dataDir, StoreErrorCodes.ID_Undeleted);\n+                }\n+                // Undelete would increase the life version\n+                if (valueFromTgtIdx.getLifeVersion() >= srcValue.getLifeVersion()) {\n+                  throw new StoreException(\n+                      \"Id \" + srcIndexEntry.getKey() + \" has bad lifeversion \" + valueFromTgtIdx + \" in index \"\n+                          + dataDir, StoreErrorCodes.Life_Version_Conflict);\n+                }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62ccc8693af1daff936bc398c25a08fa2f3fe523"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU0Njc5OA==", "bodyText": "We are throwing an error when the valueFromTgtIndex is not what we expect. The idea is that, in target index, if there are already index value existed for this key, then it can't be an undelete. Compaction would remove all previous undelete in the same segment. And it's lifeVersion can't be equal or greater than the undelete's lifeVersion. It' pretty easy to understand why it can't be greater than undelete, for example if we have PUT in the target index, and it's lifeVersion has to be smaller than the undelete. Also it can't be equal to undelete, because undelete always increase the lifeVersion.", "url": "https://github.com/linkedin/ambry/pull/1570#discussion_r442546798", "createdAt": "2020-06-18T23:05:01Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -767,16 +768,27 @@ private boolean copyRecords(LogSegment logSegmentToCopy, List<IndexEntry> srcInd\n               }\n             } else if (srcValue.isUndelete()) {\n               if (valueFromTgtIdx != null) {\n-                tgtIndex.markAsUndeleted(srcIndexEntry.getKey(), fileSpan, srcValue.getOperationTimeInMs(),\n-                    srcValue.getLifeVersion());\n-              } else {\n-                IndexValue tgtValue = new IndexValue(srcValue.getSize(), fileSpan.getStartOffset(), srcValue.getFlags(),\n-                    srcValue.getExpiresAtMs(), srcValue.getOperationTimeInMs(), srcValue.getAccountId(),\n-                    srcValue.getContainerId(), srcValue.getLifeVersion());\n-                tgtValue.setFlag(IndexValue.Flags.Undelete_Index);\n-                tgtValue.clearOriginalMessageOffset();\n-                tgtIndex.addToIndex(new IndexEntry(srcIndexEntry.getKey(), tgtValue), fileSpan);\n+                // Here don't use markAsUndelete method. In markAsUndelete, multiple sanity check would be applied. But\n+                // target index might only contain incomplete blob history, which would fail sanity check.\n+                // Here we do a different check to make sure we don't insert undelete with lower lifeVersion.\n+                if (valueFromTgtIdx.isUndelete()) {\n+                  throw new StoreException(\n+                      \"Id \" + srcIndexEntry.getKey() + \" already has undelete \" + valueFromTgtIdx + \" in index \"\n+                          + dataDir, StoreErrorCodes.ID_Undeleted);\n+                }\n+                // Undelete would increase the life version\n+                if (valueFromTgtIdx.getLifeVersion() >= srcValue.getLifeVersion()) {\n+                  throw new StoreException(\n+                      \"Id \" + srcIndexEntry.getKey() + \" has bad lifeversion \" + valueFromTgtIdx + \" in index \"\n+                          + dataDir, StoreErrorCodes.Life_Version_Conflict);\n+                }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU0Mzg4OA=="}, "originalCommit": {"oid": "62ccc8693af1daff936bc398c25a08fa2f3fe523"}, "originalPosition": 63}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1290, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}