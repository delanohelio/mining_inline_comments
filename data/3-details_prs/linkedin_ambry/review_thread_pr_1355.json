{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYwNzE3MDAy", "number": 1355, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTowMjoyNFrODYpAhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxODoxNToyOVrODgzSuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTY0MjkyOnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTowMjoyNFrOFencsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTowMjoyNFrOFencsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY0NzkyMQ==", "bodyText": "javadoc please.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r367647921", "createdAt": "2020-01-16T21:02:24Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,487 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTY1MTcxOnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTowNTo1NFrOFeniiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo1MzoyNlrOFesDFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY0OTQxNw==", "bodyText": "can we add some comments with regards to the znode path in the zookeeper? I think other engineers would benefit a lot from this information.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r367649417", "createdAt": "2020-01-16T21:05:54Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,487 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyMzI4Ng==", "bodyText": "sure, will do", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r367723286", "createdAt": "2020-01-17T00:53:26Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,487 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY0OTQxNw=="}, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTY2OTk5OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToxMzowNVrOFenuGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo1NjoyN1rOFesF-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1MjM3Nw==", "bodyText": "nit: just do instanceConfigInitalized.set(true) in an finally block.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r367652377", "createdAt": "2020-01-16T21:13:05Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,487 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized.get()) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+          instanceConfigInitialized.set(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyNDAyNg==", "bodyText": "make sense", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r367724026", "createdAt": "2020-01-17T00:56:27Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,487 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized.get()) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+          instanceConfigInitialized.set(true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1MjM3Nw=="}, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTY3ODU1OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToxNjowM1rOFenzWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo1OToyNFrOFesIyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1MzcyMw==", "bodyText": "throwing an exception here, just like SimpleClusterChangeHandler.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r367653723", "createdAt": "2020-01-16T21:16:03Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,487 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized.get()) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+          instanceConfigInitialized.set(true);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized.get()) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+            instanceConfigInitialized.set(true);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   * @throws InterruptedException\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext)\n+      throws InterruptedException {\n+    if (!idealStateInitialized.get()) {\n+      logger.info(\"Received initial notification for IdealState change from {}\", dcName);\n+      idealStateInitialized.set(true);\n+    } else {\n+      logger.info(\"IdealState change triggered from {}\", dcName);\n+    }\n+    logger.debug(\"Detailed ideal states in {} are: {}\", dcName, idealState);\n+    // rebuild the entire partition-to-resource map in current dc\n+    ConcurrentHashMap<String, String> partitionToResourceMap = new ConcurrentHashMap<>();\n+    for (IdealState state : idealState) {\n+      String resourceName = state.getResourceName();\n+      state.getPartitionSet().forEach(partitionName -> partitionToResourceMap.put(partitionName, resourceName));\n+    }\n+    partitionNameToResource = partitionToResourceMap;\n+    helixClusterManagerMetrics.idealStateChangeTriggerCount.inc();\n+  }\n+\n+  /**\n+   * Triggered whenever there is a change in the list of live instances.\n+   * @param liveInstances the list of all live instances (not a change set) at the time of this call.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onLiveInstanceChange(List<LiveInstance> liveInstances, NotificationContext changeContext) {\n+    try {\n+      if (!liveStateInitialized.get()) {\n+        logger.info(\"Received initial notification for live instance change from {}\", dcName);\n+        liveStateInitialized.set(true);\n+      } else {\n+        logger.info(\"Live instance change triggered from {}\", dcName);\n+      }\n+      logger.debug(\"Detailed live instances in {} are: {}\", dcName, liveInstances);\n+      synchronized (notificationLock) {\n+        updateInstanceLiveness(liveInstances);\n+        helixClusterManagerMetrics.liveInstanceChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the state of replica in cluster has changed. The snapshot contains up-to-date state of all\n+   * resources(replicas) in this data center.\n+   * @param routingTableSnapshot a snapshot of routing table for this data center.\n+   * @param context additional context associated with this change.\n+   */\n+  @Override\n+  public void onRoutingTableChange(RoutingTableSnapshot routingTableSnapshot, Object context) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+    if (routingTableInitLatch.getCount() == 1) {\n+      logger.info(\"Received initial notification for routing table change from {}\", dcName);\n+      routingTableInitLatch.countDown();\n+    } else {\n+      logger.info(\"Routing table change triggered from {}\", dcName);\n+    }\n+    helixClusterManagerMetrics.routingTableChangeTriggerCount.inc();\n+  }\n+\n+  @Override\n+  public void setRoutingTableSnapshot(RoutingTableSnapshot routingTableSnapshot) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+  }\n+\n+  @Override\n+  public RoutingTableSnapshot getRoutingTableSnapshot() {\n+    return routingTableSnapshotRef.get();\n+  }\n+\n+  @Override\n+  public Map<AmbryDataNode, Set<AmbryDisk>> getDataNodeToDisksMap() {\n+    return Collections.unmodifiableMap(ambryDataNodeToAmbryDisks);\n+  }\n+\n+  @Override\n+  public AmbryDataNode getDataNode(String instanceName) {\n+    return instanceNameToAmbryDataNode.get(instanceName);\n+  }\n+\n+  @Override\n+  public AmbryReplica getReplicaId(AmbryDataNode ambryDataNode, String partitionName) {\n+    return ambryDataNodeToAmbryReplicas.getOrDefault(ambryDataNode, new ConcurrentHashMap<>()).get(partitionName);\n+  }\n+\n+  @Override\n+  public List<AmbryReplica> getReplicaIds(AmbryDataNode ambryDataNode) {\n+    return new ArrayList<>(ambryDataNodeToAmbryReplicas.get(ambryDataNode).values());\n+  }\n+\n+  @Override\n+  public List<AmbryDataNode> getAllDataNodes() {\n+    return new ArrayList<>(instanceNameToAmbryDataNode.values());\n+  }\n+\n+  @Override\n+  public Set<AmbryDisk> getDisks(AmbryDataNode ambryDataNode) {\n+    return ambryDataNodeToAmbryDisks.get(ambryDataNode);\n+  }\n+\n+  @Override\n+  public Map<String, String> getPartitionToResourceMap() {\n+    return Collections.unmodifiableMap(partitionNameToResource);\n+  }\n+\n+  @Override\n+  public long getErrorCount() {\n+    return errorCount.get();\n+  }\n+\n+  @Override\n+  public void waitForInitNotification() throws InterruptedException {\n+    // wait slightly more than 5 mins to ensure routerUpdater refreshes the snapshot.\n+    if (!routingTableInitLatch.await(320, TimeUnit.SECONDS)) {\n+      throw new IllegalStateException(\"Initial routing table change from \" + dcName + \" didn't come within 5 mins\");\n+    }\n+  }\n+\n+  /**\n+   * Add new instance or update existing instance based on {@link InstanceConfig}(s).\n+   * @param instanceConfigs the {@link InstanceConfig}(s) used to update in-mem cluster map.\n+   * @throws Exception\n+   */\n+  private void addOrUpdateInstanceInfos(List<InstanceConfig> instanceConfigs) throws Exception {\n+    for (InstanceConfig instanceConfig : instanceConfigs) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        logger.error(\"Unknown InstanceConfig schema version: {}, ignoring.\", schemaVersion);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 265}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyNDc0Nw==", "bodyText": "will do", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r367724747", "createdAt": "2020-01-17T00:59:24Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,487 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized.get()) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+          instanceConfigInitialized.set(true);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized.get()) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+            instanceConfigInitialized.set(true);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   * @throws InterruptedException\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext)\n+      throws InterruptedException {\n+    if (!idealStateInitialized.get()) {\n+      logger.info(\"Received initial notification for IdealState change from {}\", dcName);\n+      idealStateInitialized.set(true);\n+    } else {\n+      logger.info(\"IdealState change triggered from {}\", dcName);\n+    }\n+    logger.debug(\"Detailed ideal states in {} are: {}\", dcName, idealState);\n+    // rebuild the entire partition-to-resource map in current dc\n+    ConcurrentHashMap<String, String> partitionToResourceMap = new ConcurrentHashMap<>();\n+    for (IdealState state : idealState) {\n+      String resourceName = state.getResourceName();\n+      state.getPartitionSet().forEach(partitionName -> partitionToResourceMap.put(partitionName, resourceName));\n+    }\n+    partitionNameToResource = partitionToResourceMap;\n+    helixClusterManagerMetrics.idealStateChangeTriggerCount.inc();\n+  }\n+\n+  /**\n+   * Triggered whenever there is a change in the list of live instances.\n+   * @param liveInstances the list of all live instances (not a change set) at the time of this call.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onLiveInstanceChange(List<LiveInstance> liveInstances, NotificationContext changeContext) {\n+    try {\n+      if (!liveStateInitialized.get()) {\n+        logger.info(\"Received initial notification for live instance change from {}\", dcName);\n+        liveStateInitialized.set(true);\n+      } else {\n+        logger.info(\"Live instance change triggered from {}\", dcName);\n+      }\n+      logger.debug(\"Detailed live instances in {} are: {}\", dcName, liveInstances);\n+      synchronized (notificationLock) {\n+        updateInstanceLiveness(liveInstances);\n+        helixClusterManagerMetrics.liveInstanceChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the state of replica in cluster has changed. The snapshot contains up-to-date state of all\n+   * resources(replicas) in this data center.\n+   * @param routingTableSnapshot a snapshot of routing table for this data center.\n+   * @param context additional context associated with this change.\n+   */\n+  @Override\n+  public void onRoutingTableChange(RoutingTableSnapshot routingTableSnapshot, Object context) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+    if (routingTableInitLatch.getCount() == 1) {\n+      logger.info(\"Received initial notification for routing table change from {}\", dcName);\n+      routingTableInitLatch.countDown();\n+    } else {\n+      logger.info(\"Routing table change triggered from {}\", dcName);\n+    }\n+    helixClusterManagerMetrics.routingTableChangeTriggerCount.inc();\n+  }\n+\n+  @Override\n+  public void setRoutingTableSnapshot(RoutingTableSnapshot routingTableSnapshot) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+  }\n+\n+  @Override\n+  public RoutingTableSnapshot getRoutingTableSnapshot() {\n+    return routingTableSnapshotRef.get();\n+  }\n+\n+  @Override\n+  public Map<AmbryDataNode, Set<AmbryDisk>> getDataNodeToDisksMap() {\n+    return Collections.unmodifiableMap(ambryDataNodeToAmbryDisks);\n+  }\n+\n+  @Override\n+  public AmbryDataNode getDataNode(String instanceName) {\n+    return instanceNameToAmbryDataNode.get(instanceName);\n+  }\n+\n+  @Override\n+  public AmbryReplica getReplicaId(AmbryDataNode ambryDataNode, String partitionName) {\n+    return ambryDataNodeToAmbryReplicas.getOrDefault(ambryDataNode, new ConcurrentHashMap<>()).get(partitionName);\n+  }\n+\n+  @Override\n+  public List<AmbryReplica> getReplicaIds(AmbryDataNode ambryDataNode) {\n+    return new ArrayList<>(ambryDataNodeToAmbryReplicas.get(ambryDataNode).values());\n+  }\n+\n+  @Override\n+  public List<AmbryDataNode> getAllDataNodes() {\n+    return new ArrayList<>(instanceNameToAmbryDataNode.values());\n+  }\n+\n+  @Override\n+  public Set<AmbryDisk> getDisks(AmbryDataNode ambryDataNode) {\n+    return ambryDataNodeToAmbryDisks.get(ambryDataNode);\n+  }\n+\n+  @Override\n+  public Map<String, String> getPartitionToResourceMap() {\n+    return Collections.unmodifiableMap(partitionNameToResource);\n+  }\n+\n+  @Override\n+  public long getErrorCount() {\n+    return errorCount.get();\n+  }\n+\n+  @Override\n+  public void waitForInitNotification() throws InterruptedException {\n+    // wait slightly more than 5 mins to ensure routerUpdater refreshes the snapshot.\n+    if (!routingTableInitLatch.await(320, TimeUnit.SECONDS)) {\n+      throw new IllegalStateException(\"Initial routing table change from \" + dcName + \" didn't come within 5 mins\");\n+    }\n+  }\n+\n+  /**\n+   * Add new instance or update existing instance based on {@link InstanceConfig}(s).\n+   * @param instanceConfigs the {@link InstanceConfig}(s) used to update in-mem cluster map.\n+   * @throws Exception\n+   */\n+  private void addOrUpdateInstanceInfos(List<InstanceConfig> instanceConfigs) throws Exception {\n+    for (InstanceConfig instanceConfig : instanceConfigs) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        logger.error(\"Unknown InstanceConfig schema version: {}, ignoring.\", schemaVersion);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1MzcyMw=="}, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 265}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTcxNzk4OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTozMDowMlrOFeoL_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwNTozMTo1NlrOFoiL9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2MDAzMQ==", "bodyText": "this function is quite similar to the one in the SimpleClusterChangeHandler. If we are going to keep two implementations for a long time, we can find a way to reuse the code.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r367660031", "createdAt": "2020-01-16T21:30:02Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,487 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized.get()) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+          instanceConfigInitialized.set(true);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized.get()) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+            instanceConfigInitialized.set(true);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   * @throws InterruptedException\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext)\n+      throws InterruptedException {\n+    if (!idealStateInitialized.get()) {\n+      logger.info(\"Received initial notification for IdealState change from {}\", dcName);\n+      idealStateInitialized.set(true);\n+    } else {\n+      logger.info(\"IdealState change triggered from {}\", dcName);\n+    }\n+    logger.debug(\"Detailed ideal states in {} are: {}\", dcName, idealState);\n+    // rebuild the entire partition-to-resource map in current dc\n+    ConcurrentHashMap<String, String> partitionToResourceMap = new ConcurrentHashMap<>();\n+    for (IdealState state : idealState) {\n+      String resourceName = state.getResourceName();\n+      state.getPartitionSet().forEach(partitionName -> partitionToResourceMap.put(partitionName, resourceName));\n+    }\n+    partitionNameToResource = partitionToResourceMap;\n+    helixClusterManagerMetrics.idealStateChangeTriggerCount.inc();\n+  }\n+\n+  /**\n+   * Triggered whenever there is a change in the list of live instances.\n+   * @param liveInstances the list of all live instances (not a change set) at the time of this call.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onLiveInstanceChange(List<LiveInstance> liveInstances, NotificationContext changeContext) {\n+    try {\n+      if (!liveStateInitialized.get()) {\n+        logger.info(\"Received initial notification for live instance change from {}\", dcName);\n+        liveStateInitialized.set(true);\n+      } else {\n+        logger.info(\"Live instance change triggered from {}\", dcName);\n+      }\n+      logger.debug(\"Detailed live instances in {} are: {}\", dcName, liveInstances);\n+      synchronized (notificationLock) {\n+        updateInstanceLiveness(liveInstances);\n+        helixClusterManagerMetrics.liveInstanceChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the state of replica in cluster has changed. The snapshot contains up-to-date state of all\n+   * resources(replicas) in this data center.\n+   * @param routingTableSnapshot a snapshot of routing table for this data center.\n+   * @param context additional context associated with this change.\n+   */\n+  @Override\n+  public void onRoutingTableChange(RoutingTableSnapshot routingTableSnapshot, Object context) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+    if (routingTableInitLatch.getCount() == 1) {\n+      logger.info(\"Received initial notification for routing table change from {}\", dcName);\n+      routingTableInitLatch.countDown();\n+    } else {\n+      logger.info(\"Routing table change triggered from {}\", dcName);\n+    }\n+    helixClusterManagerMetrics.routingTableChangeTriggerCount.inc();\n+  }\n+\n+  @Override\n+  public void setRoutingTableSnapshot(RoutingTableSnapshot routingTableSnapshot) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+  }\n+\n+  @Override\n+  public RoutingTableSnapshot getRoutingTableSnapshot() {\n+    return routingTableSnapshotRef.get();\n+  }\n+\n+  @Override\n+  public Map<AmbryDataNode, Set<AmbryDisk>> getDataNodeToDisksMap() {\n+    return Collections.unmodifiableMap(ambryDataNodeToAmbryDisks);\n+  }\n+\n+  @Override\n+  public AmbryDataNode getDataNode(String instanceName) {\n+    return instanceNameToAmbryDataNode.get(instanceName);\n+  }\n+\n+  @Override\n+  public AmbryReplica getReplicaId(AmbryDataNode ambryDataNode, String partitionName) {\n+    return ambryDataNodeToAmbryReplicas.getOrDefault(ambryDataNode, new ConcurrentHashMap<>()).get(partitionName);\n+  }\n+\n+  @Override\n+  public List<AmbryReplica> getReplicaIds(AmbryDataNode ambryDataNode) {\n+    return new ArrayList<>(ambryDataNodeToAmbryReplicas.get(ambryDataNode).values());\n+  }\n+\n+  @Override\n+  public List<AmbryDataNode> getAllDataNodes() {\n+    return new ArrayList<>(instanceNameToAmbryDataNode.values());\n+  }\n+\n+  @Override\n+  public Set<AmbryDisk> getDisks(AmbryDataNode ambryDataNode) {\n+    return ambryDataNodeToAmbryDisks.get(ambryDataNode);\n+  }\n+\n+  @Override\n+  public Map<String, String> getPartitionToResourceMap() {\n+    return Collections.unmodifiableMap(partitionNameToResource);\n+  }\n+\n+  @Override\n+  public long getErrorCount() {\n+    return errorCount.get();\n+  }\n+\n+  @Override\n+  public void waitForInitNotification() throws InterruptedException {\n+    // wait slightly more than 5 mins to ensure routerUpdater refreshes the snapshot.\n+    if (!routingTableInitLatch.await(320, TimeUnit.SECONDS)) {\n+      throw new IllegalStateException(\"Initial routing table change from \" + dcName + \" didn't come within 5 mins\");\n+    }\n+  }\n+\n+  /**\n+   * Add new instance or update existing instance based on {@link InstanceConfig}(s).\n+   * @param instanceConfigs the {@link InstanceConfig}(s) used to update in-mem cluster map.\n+   * @throws Exception\n+   */\n+  private void addOrUpdateInstanceInfos(List<InstanceConfig> instanceConfigs) throws Exception {\n+    for (InstanceConfig instanceConfig : instanceConfigs) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        logger.error(\"Unknown InstanceConfig schema version: {}, ignoring.\", schemaVersion);\n+        continue;\n+      }\n+      if (instanceNameToAmbryDataNode.containsKey(instanceConfig.getInstanceName())) {\n+        updateInstanceInfo(instanceConfig);\n+      } else {\n+        createNewInstance(instanceConfig);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Update info of an existing instance. This may happen in following cases: (1) new replica is added; (2) old replica\n+   * is removed; (3) replica's state has changed (i.e. becomes seal/unseal).\n+   * @param instanceConfig the {@link InstanceConfig} used to update info of instance.\n+   */\n+  private void updateInstanceInfo(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Updating replicas info for existing node {}\", instanceName);\n+    List<String> sealedReplicas = getSealedReplicas(instanceConfig);\n+    List<String> stoppedReplicas = getStoppedReplicas(instanceConfig);\n+    ConcurrentHashMap<String, AmbryReplica> currentReplicasOnNode =\n+        ambryDataNodeToAmbryReplicas.get(instanceNameToAmbryDataNode.get(instanceName));\n+    ConcurrentHashMap<String, AmbryReplica> replicasFromConfig = new ConcurrentHashMap<>();\n+    Map<String, Map<String, String>> diskInfos = instanceConfig.getRecord().getMapFields();\n+    for (Map.Entry<String, Map<String, String>> entry : diskInfos.entrySet()) {\n+      String mountPath = entry.getKey();\n+      Map<String, String> diskInfo = entry.getValue();\n+      String replicasStr = diskInfo.get(ClusterMapUtils.REPLICAS_STR);\n+      if (!replicasStr.isEmpty()) {\n+        for (String replicaInfo : replicasStr.split(ClusterMapUtils.REPLICAS_DELIM_STR)) {\n+          String[] info = replicaInfo.split(ClusterMapUtils.REPLICAS_STR_SEPARATOR);\n+          // partition name and replica name are the same.\n+          String partitionName = info[0];\n+          if (currentReplicasOnNode.containsKey(partitionName)) {\n+            // if replica is already present\n+            AmbryReplica existingReplica = currentReplicasOnNode.get(partitionName);\n+            // 1. directly add it into \"replicasFromConfig\" map\n+            replicasFromConfig.put(partitionName, existingReplica);\n+            // 2. update replica seal/stop state\n+            updateReplicaStateAndOverrideIfNeeded(existingReplica, sealedReplicas, stoppedReplicas);\n+          } else {\n+            // if this is a new replica and doesn't exist on node\n+            logger.info(\"Adding new replica {} to existing node {}\", partitionName, instanceName);\n+            long replicaCapacity = Long.valueOf(info[1]);\n+            String partitionClass = info.length > 2 ? info[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+            AmbryDataNode dataNode = instanceNameToAmbryDataNode.get(instanceName);\n+            AmbryDisk disk = ambryDataNodeToAmbryDisks.get(dataNode)\n+                .stream()\n+                .filter(d -> d.getMountPath().equals(mountPath))\n+                .findFirst()\n+                .get();\n+            // this can be a brand new partition that is added to an existing node\n+            AmbryPartition mappedPartition =\n+                new AmbryPartition(Long.valueOf(partitionName), partitionClass, helixClusterManagerCallback);\n+            // Ensure only one AmbryPartition instance exists for specific partition.\n+            mappedPartition = clusterChangeHandlerCallback.addPartitionIfAbsent(mappedPartition, replicaCapacity);\n+            ensurePartitionAbsenceOnNodeAndValidateCapacity(mappedPartition, dataNode, replicaCapacity);\n+            // create new replica belonging to this partition\n+            AmbryReplica replica =\n+                new AmbryReplica(clusterMapConfig, mappedPartition, disk, stoppedReplicas.contains(partitionName),\n+                    replicaCapacity, sealedReplicas.contains(partitionName));\n+            updateReplicaStateAndOverrideIfNeeded(replica, sealedReplicas, stoppedReplicas);\n+            // add new created replica to \"replicasFromConfig\" map\n+            replicasFromConfig.put(partitionName, replica);\n+            // add new replica to specific partition\n+            clusterChangeHandlerCallback.addReplicasToPartition(mappedPartition, Collections.singletonList(replica));\n+          }\n+        }\n+      }\n+    }\n+    // update ambryDataNodeToAmbryReplicas map by adding \"replicasFromConfig\"\n+    ambryDataNodeToAmbryReplicas.put(instanceNameToAmbryDataNode.get(instanceName), replicasFromConfig);\n+    // compare replicasFromConfig with current replica set to derive old replicas that are removed\n+    Set<AmbryReplica> previousReplicas = new HashSet<>(currentReplicasOnNode.values());\n+    previousReplicas.removeAll(replicasFromConfig.values());\n+    for (AmbryReplica ambryReplica : previousReplicas) {\n+      logger.info(\"Removing replica {} from existing node {}\", ambryReplica.getPartitionId().toPathString(),\n+          instanceName);\n+      clusterChangeHandlerCallback.removeReplicasFromPartition(ambryReplica.getPartitionId(),\n+          Collections.singletonList(ambryReplica));\n+    }\n+  }\n+\n+  /**\n+   * If partition override is enabled, we override replica SEAL/UNSEAL state based on partitionOverrideMap. If disabled,\n+   * update replica state according to the info from {@link InstanceConfig}.\n+   * @param replica the {@link ReplicaId} whose states (seal,stop) should be updated.\n+   * @param sealedReplicas a list of {@link ReplicaId}(s) that are in SEALED state.\n+   * @param stoppedReplicas a list of {@link ReplicaId}(s) that are in STOPPED state.\n+   */\n+  private void updateReplicaStateAndOverrideIfNeeded(AmbryReplica replica, List<String> sealedReplicas,\n+      List<String> stoppedReplicas) {\n+    String partitionName = replica.getPartitionId().toPathString();\n+    boolean isSealed;\n+    if (clusterMapConfig.clusterMapEnablePartitionOverride && partitionOverrideInfoMap.containsKey(partitionName)) {\n+      isSealed = partitionOverrideInfoMap.get(partitionName)\n+          .get(ClusterMapUtils.PARTITION_STATE)\n+          .equals(ClusterMapUtils.READ_ONLY_STR);\n+    } else {\n+      isSealed = sealedReplicas.contains(partitionName);\n+    }\n+    replica.setSealedState(isSealed);\n+    replica.setStoppedState(stoppedReplicas.contains(partitionName));\n+  }\n+\n+  /**\n+   * Create a new instance(node) and initialize disks/replicas on it.\n+   * @param instanceConfig the {@link InstanceConfig} to create new instance\n+   * @throws Exception\n+   */\n+  private void createNewInstance(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Adding node {} and its disks and replicas\", instanceName);\n+    AmbryDataNode datanode =\n+        new AmbryDataNode(getDcName(instanceConfig), clusterMapConfig, instanceConfig.getHostName(),\n+            Integer.valueOf(instanceConfig.getPort()), getRackId(instanceConfig), getSslPortStr(instanceConfig),\n+            getXid(instanceConfig), helixClusterManagerCallback);\n+    // for new instance, we first set it to unavailable and rely on its participation to update its liveness\n+    if (!instanceName.equals(selfInstanceName)) {\n+      datanode.setState(HardwareState.UNAVAILABLE);\n+    }\n+    initializeDisksAndReplicasOnNode(datanode, instanceConfig);\n+    instanceNameToAmbryDataNode.put(instanceName, datanode);\n+  }\n+\n+  /**\n+   * Initialize the disks and replicas on the given node. Create partitions if this is the first time a replica of\n+   * that partition is being constructed. If partition override is enabled, the seal state of replica is determined by\n+   * partition info in HelixPropertyStore, if disabled, the seal state is determined by instanceConfig.\n+   * @param datanode the {@link AmbryDataNode} that is being initialized.\n+   * @param instanceConfig the {@link InstanceConfig} associated with this datanode.\n+   * @throws Exception if creation of {@link AmbryDisk} throws an Exception.\n+   */\n+  private void initializeDisksAndReplicasOnNode(AmbryDataNode datanode, InstanceConfig instanceConfig)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 399}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyNDcyMg==", "bodyText": "Yes, both of cluster change handlers should be able to share the code. I will probably do this in future PR after dynamic handler gets rolled out to production.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r367724722", "createdAt": "2020-01-17T00:59:16Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,487 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized.get()) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+          instanceConfigInitialized.set(true);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized.get()) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+            instanceConfigInitialized.set(true);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   * @throws InterruptedException\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext)\n+      throws InterruptedException {\n+    if (!idealStateInitialized.get()) {\n+      logger.info(\"Received initial notification for IdealState change from {}\", dcName);\n+      idealStateInitialized.set(true);\n+    } else {\n+      logger.info(\"IdealState change triggered from {}\", dcName);\n+    }\n+    logger.debug(\"Detailed ideal states in {} are: {}\", dcName, idealState);\n+    // rebuild the entire partition-to-resource map in current dc\n+    ConcurrentHashMap<String, String> partitionToResourceMap = new ConcurrentHashMap<>();\n+    for (IdealState state : idealState) {\n+      String resourceName = state.getResourceName();\n+      state.getPartitionSet().forEach(partitionName -> partitionToResourceMap.put(partitionName, resourceName));\n+    }\n+    partitionNameToResource = partitionToResourceMap;\n+    helixClusterManagerMetrics.idealStateChangeTriggerCount.inc();\n+  }\n+\n+  /**\n+   * Triggered whenever there is a change in the list of live instances.\n+   * @param liveInstances the list of all live instances (not a change set) at the time of this call.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onLiveInstanceChange(List<LiveInstance> liveInstances, NotificationContext changeContext) {\n+    try {\n+      if (!liveStateInitialized.get()) {\n+        logger.info(\"Received initial notification for live instance change from {}\", dcName);\n+        liveStateInitialized.set(true);\n+      } else {\n+        logger.info(\"Live instance change triggered from {}\", dcName);\n+      }\n+      logger.debug(\"Detailed live instances in {} are: {}\", dcName, liveInstances);\n+      synchronized (notificationLock) {\n+        updateInstanceLiveness(liveInstances);\n+        helixClusterManagerMetrics.liveInstanceChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the state of replica in cluster has changed. The snapshot contains up-to-date state of all\n+   * resources(replicas) in this data center.\n+   * @param routingTableSnapshot a snapshot of routing table for this data center.\n+   * @param context additional context associated with this change.\n+   */\n+  @Override\n+  public void onRoutingTableChange(RoutingTableSnapshot routingTableSnapshot, Object context) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+    if (routingTableInitLatch.getCount() == 1) {\n+      logger.info(\"Received initial notification for routing table change from {}\", dcName);\n+      routingTableInitLatch.countDown();\n+    } else {\n+      logger.info(\"Routing table change triggered from {}\", dcName);\n+    }\n+    helixClusterManagerMetrics.routingTableChangeTriggerCount.inc();\n+  }\n+\n+  @Override\n+  public void setRoutingTableSnapshot(RoutingTableSnapshot routingTableSnapshot) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+  }\n+\n+  @Override\n+  public RoutingTableSnapshot getRoutingTableSnapshot() {\n+    return routingTableSnapshotRef.get();\n+  }\n+\n+  @Override\n+  public Map<AmbryDataNode, Set<AmbryDisk>> getDataNodeToDisksMap() {\n+    return Collections.unmodifiableMap(ambryDataNodeToAmbryDisks);\n+  }\n+\n+  @Override\n+  public AmbryDataNode getDataNode(String instanceName) {\n+    return instanceNameToAmbryDataNode.get(instanceName);\n+  }\n+\n+  @Override\n+  public AmbryReplica getReplicaId(AmbryDataNode ambryDataNode, String partitionName) {\n+    return ambryDataNodeToAmbryReplicas.getOrDefault(ambryDataNode, new ConcurrentHashMap<>()).get(partitionName);\n+  }\n+\n+  @Override\n+  public List<AmbryReplica> getReplicaIds(AmbryDataNode ambryDataNode) {\n+    return new ArrayList<>(ambryDataNodeToAmbryReplicas.get(ambryDataNode).values());\n+  }\n+\n+  @Override\n+  public List<AmbryDataNode> getAllDataNodes() {\n+    return new ArrayList<>(instanceNameToAmbryDataNode.values());\n+  }\n+\n+  @Override\n+  public Set<AmbryDisk> getDisks(AmbryDataNode ambryDataNode) {\n+    return ambryDataNodeToAmbryDisks.get(ambryDataNode);\n+  }\n+\n+  @Override\n+  public Map<String, String> getPartitionToResourceMap() {\n+    return Collections.unmodifiableMap(partitionNameToResource);\n+  }\n+\n+  @Override\n+  public long getErrorCount() {\n+    return errorCount.get();\n+  }\n+\n+  @Override\n+  public void waitForInitNotification() throws InterruptedException {\n+    // wait slightly more than 5 mins to ensure routerUpdater refreshes the snapshot.\n+    if (!routingTableInitLatch.await(320, TimeUnit.SECONDS)) {\n+      throw new IllegalStateException(\"Initial routing table change from \" + dcName + \" didn't come within 5 mins\");\n+    }\n+  }\n+\n+  /**\n+   * Add new instance or update existing instance based on {@link InstanceConfig}(s).\n+   * @param instanceConfigs the {@link InstanceConfig}(s) used to update in-mem cluster map.\n+   * @throws Exception\n+   */\n+  private void addOrUpdateInstanceInfos(List<InstanceConfig> instanceConfigs) throws Exception {\n+    for (InstanceConfig instanceConfig : instanceConfigs) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        logger.error(\"Unknown InstanceConfig schema version: {}, ignoring.\", schemaVersion);\n+        continue;\n+      }\n+      if (instanceNameToAmbryDataNode.containsKey(instanceConfig.getInstanceName())) {\n+        updateInstanceInfo(instanceConfig);\n+      } else {\n+        createNewInstance(instanceConfig);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Update info of an existing instance. This may happen in following cases: (1) new replica is added; (2) old replica\n+   * is removed; (3) replica's state has changed (i.e. becomes seal/unseal).\n+   * @param instanceConfig the {@link InstanceConfig} used to update info of instance.\n+   */\n+  private void updateInstanceInfo(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Updating replicas info for existing node {}\", instanceName);\n+    List<String> sealedReplicas = getSealedReplicas(instanceConfig);\n+    List<String> stoppedReplicas = getStoppedReplicas(instanceConfig);\n+    ConcurrentHashMap<String, AmbryReplica> currentReplicasOnNode =\n+        ambryDataNodeToAmbryReplicas.get(instanceNameToAmbryDataNode.get(instanceName));\n+    ConcurrentHashMap<String, AmbryReplica> replicasFromConfig = new ConcurrentHashMap<>();\n+    Map<String, Map<String, String>> diskInfos = instanceConfig.getRecord().getMapFields();\n+    for (Map.Entry<String, Map<String, String>> entry : diskInfos.entrySet()) {\n+      String mountPath = entry.getKey();\n+      Map<String, String> diskInfo = entry.getValue();\n+      String replicasStr = diskInfo.get(ClusterMapUtils.REPLICAS_STR);\n+      if (!replicasStr.isEmpty()) {\n+        for (String replicaInfo : replicasStr.split(ClusterMapUtils.REPLICAS_DELIM_STR)) {\n+          String[] info = replicaInfo.split(ClusterMapUtils.REPLICAS_STR_SEPARATOR);\n+          // partition name and replica name are the same.\n+          String partitionName = info[0];\n+          if (currentReplicasOnNode.containsKey(partitionName)) {\n+            // if replica is already present\n+            AmbryReplica existingReplica = currentReplicasOnNode.get(partitionName);\n+            // 1. directly add it into \"replicasFromConfig\" map\n+            replicasFromConfig.put(partitionName, existingReplica);\n+            // 2. update replica seal/stop state\n+            updateReplicaStateAndOverrideIfNeeded(existingReplica, sealedReplicas, stoppedReplicas);\n+          } else {\n+            // if this is a new replica and doesn't exist on node\n+            logger.info(\"Adding new replica {} to existing node {}\", partitionName, instanceName);\n+            long replicaCapacity = Long.valueOf(info[1]);\n+            String partitionClass = info.length > 2 ? info[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+            AmbryDataNode dataNode = instanceNameToAmbryDataNode.get(instanceName);\n+            AmbryDisk disk = ambryDataNodeToAmbryDisks.get(dataNode)\n+                .stream()\n+                .filter(d -> d.getMountPath().equals(mountPath))\n+                .findFirst()\n+                .get();\n+            // this can be a brand new partition that is added to an existing node\n+            AmbryPartition mappedPartition =\n+                new AmbryPartition(Long.valueOf(partitionName), partitionClass, helixClusterManagerCallback);\n+            // Ensure only one AmbryPartition instance exists for specific partition.\n+            mappedPartition = clusterChangeHandlerCallback.addPartitionIfAbsent(mappedPartition, replicaCapacity);\n+            ensurePartitionAbsenceOnNodeAndValidateCapacity(mappedPartition, dataNode, replicaCapacity);\n+            // create new replica belonging to this partition\n+            AmbryReplica replica =\n+                new AmbryReplica(clusterMapConfig, mappedPartition, disk, stoppedReplicas.contains(partitionName),\n+                    replicaCapacity, sealedReplicas.contains(partitionName));\n+            updateReplicaStateAndOverrideIfNeeded(replica, sealedReplicas, stoppedReplicas);\n+            // add new created replica to \"replicasFromConfig\" map\n+            replicasFromConfig.put(partitionName, replica);\n+            // add new replica to specific partition\n+            clusterChangeHandlerCallback.addReplicasToPartition(mappedPartition, Collections.singletonList(replica));\n+          }\n+        }\n+      }\n+    }\n+    // update ambryDataNodeToAmbryReplicas map by adding \"replicasFromConfig\"\n+    ambryDataNodeToAmbryReplicas.put(instanceNameToAmbryDataNode.get(instanceName), replicasFromConfig);\n+    // compare replicasFromConfig with current replica set to derive old replicas that are removed\n+    Set<AmbryReplica> previousReplicas = new HashSet<>(currentReplicasOnNode.values());\n+    previousReplicas.removeAll(replicasFromConfig.values());\n+    for (AmbryReplica ambryReplica : previousReplicas) {\n+      logger.info(\"Removing replica {} from existing node {}\", ambryReplica.getPartitionId().toPathString(),\n+          instanceName);\n+      clusterChangeHandlerCallback.removeReplicasFromPartition(ambryReplica.getPartitionId(),\n+          Collections.singletonList(ambryReplica));\n+    }\n+  }\n+\n+  /**\n+   * If partition override is enabled, we override replica SEAL/UNSEAL state based on partitionOverrideMap. If disabled,\n+   * update replica state according to the info from {@link InstanceConfig}.\n+   * @param replica the {@link ReplicaId} whose states (seal,stop) should be updated.\n+   * @param sealedReplicas a list of {@link ReplicaId}(s) that are in SEALED state.\n+   * @param stoppedReplicas a list of {@link ReplicaId}(s) that are in STOPPED state.\n+   */\n+  private void updateReplicaStateAndOverrideIfNeeded(AmbryReplica replica, List<String> sealedReplicas,\n+      List<String> stoppedReplicas) {\n+    String partitionName = replica.getPartitionId().toPathString();\n+    boolean isSealed;\n+    if (clusterMapConfig.clusterMapEnablePartitionOverride && partitionOverrideInfoMap.containsKey(partitionName)) {\n+      isSealed = partitionOverrideInfoMap.get(partitionName)\n+          .get(ClusterMapUtils.PARTITION_STATE)\n+          .equals(ClusterMapUtils.READ_ONLY_STR);\n+    } else {\n+      isSealed = sealedReplicas.contains(partitionName);\n+    }\n+    replica.setSealedState(isSealed);\n+    replica.setStoppedState(stoppedReplicas.contains(partitionName));\n+  }\n+\n+  /**\n+   * Create a new instance(node) and initialize disks/replicas on it.\n+   * @param instanceConfig the {@link InstanceConfig} to create new instance\n+   * @throws Exception\n+   */\n+  private void createNewInstance(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Adding node {} and its disks and replicas\", instanceName);\n+    AmbryDataNode datanode =\n+        new AmbryDataNode(getDcName(instanceConfig), clusterMapConfig, instanceConfig.getHostName(),\n+            Integer.valueOf(instanceConfig.getPort()), getRackId(instanceConfig), getSslPortStr(instanceConfig),\n+            getXid(instanceConfig), helixClusterManagerCallback);\n+    // for new instance, we first set it to unavailable and rely on its participation to update its liveness\n+    if (!instanceName.equals(selfInstanceName)) {\n+      datanode.setState(HardwareState.UNAVAILABLE);\n+    }\n+    initializeDisksAndReplicasOnNode(datanode, instanceConfig);\n+    instanceNameToAmbryDataNode.put(instanceName, datanode);\n+  }\n+\n+  /**\n+   * Initialize the disks and replicas on the given node. Create partitions if this is the first time a replica of\n+   * that partition is being constructed. If partition override is enabled, the seal state of replica is determined by\n+   * partition info in HelixPropertyStore, if disabled, the seal state is determined by instanceConfig.\n+   * @param datanode the {@link AmbryDataNode} that is being initialized.\n+   * @param instanceConfig the {@link InstanceConfig} associated with this datanode.\n+   * @throws Exception if creation of {@link AmbryDisk} throws an Exception.\n+   */\n+  private void initializeDisksAndReplicasOnNode(AmbryDataNode datanode, InstanceConfig instanceConfig)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2MDAzMQ=="}, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 399}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA0NzQ3Nw==", "bodyText": "Could we remove SimpleClusterChangeHandler after we have gained enough confidence with DynamicClusterChangeHandler?", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r378047477", "createdAt": "2020-02-12T05:31:56Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,487 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized.get()) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+          instanceConfigInitialized.set(true);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized.get()) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+            instanceConfigInitialized.set(true);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   * @throws InterruptedException\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext)\n+      throws InterruptedException {\n+    if (!idealStateInitialized.get()) {\n+      logger.info(\"Received initial notification for IdealState change from {}\", dcName);\n+      idealStateInitialized.set(true);\n+    } else {\n+      logger.info(\"IdealState change triggered from {}\", dcName);\n+    }\n+    logger.debug(\"Detailed ideal states in {} are: {}\", dcName, idealState);\n+    // rebuild the entire partition-to-resource map in current dc\n+    ConcurrentHashMap<String, String> partitionToResourceMap = new ConcurrentHashMap<>();\n+    for (IdealState state : idealState) {\n+      String resourceName = state.getResourceName();\n+      state.getPartitionSet().forEach(partitionName -> partitionToResourceMap.put(partitionName, resourceName));\n+    }\n+    partitionNameToResource = partitionToResourceMap;\n+    helixClusterManagerMetrics.idealStateChangeTriggerCount.inc();\n+  }\n+\n+  /**\n+   * Triggered whenever there is a change in the list of live instances.\n+   * @param liveInstances the list of all live instances (not a change set) at the time of this call.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onLiveInstanceChange(List<LiveInstance> liveInstances, NotificationContext changeContext) {\n+    try {\n+      if (!liveStateInitialized.get()) {\n+        logger.info(\"Received initial notification for live instance change from {}\", dcName);\n+        liveStateInitialized.set(true);\n+      } else {\n+        logger.info(\"Live instance change triggered from {}\", dcName);\n+      }\n+      logger.debug(\"Detailed live instances in {} are: {}\", dcName, liveInstances);\n+      synchronized (notificationLock) {\n+        updateInstanceLiveness(liveInstances);\n+        helixClusterManagerMetrics.liveInstanceChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the state of replica in cluster has changed. The snapshot contains up-to-date state of all\n+   * resources(replicas) in this data center.\n+   * @param routingTableSnapshot a snapshot of routing table for this data center.\n+   * @param context additional context associated with this change.\n+   */\n+  @Override\n+  public void onRoutingTableChange(RoutingTableSnapshot routingTableSnapshot, Object context) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+    if (routingTableInitLatch.getCount() == 1) {\n+      logger.info(\"Received initial notification for routing table change from {}\", dcName);\n+      routingTableInitLatch.countDown();\n+    } else {\n+      logger.info(\"Routing table change triggered from {}\", dcName);\n+    }\n+    helixClusterManagerMetrics.routingTableChangeTriggerCount.inc();\n+  }\n+\n+  @Override\n+  public void setRoutingTableSnapshot(RoutingTableSnapshot routingTableSnapshot) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+  }\n+\n+  @Override\n+  public RoutingTableSnapshot getRoutingTableSnapshot() {\n+    return routingTableSnapshotRef.get();\n+  }\n+\n+  @Override\n+  public Map<AmbryDataNode, Set<AmbryDisk>> getDataNodeToDisksMap() {\n+    return Collections.unmodifiableMap(ambryDataNodeToAmbryDisks);\n+  }\n+\n+  @Override\n+  public AmbryDataNode getDataNode(String instanceName) {\n+    return instanceNameToAmbryDataNode.get(instanceName);\n+  }\n+\n+  @Override\n+  public AmbryReplica getReplicaId(AmbryDataNode ambryDataNode, String partitionName) {\n+    return ambryDataNodeToAmbryReplicas.getOrDefault(ambryDataNode, new ConcurrentHashMap<>()).get(partitionName);\n+  }\n+\n+  @Override\n+  public List<AmbryReplica> getReplicaIds(AmbryDataNode ambryDataNode) {\n+    return new ArrayList<>(ambryDataNodeToAmbryReplicas.get(ambryDataNode).values());\n+  }\n+\n+  @Override\n+  public List<AmbryDataNode> getAllDataNodes() {\n+    return new ArrayList<>(instanceNameToAmbryDataNode.values());\n+  }\n+\n+  @Override\n+  public Set<AmbryDisk> getDisks(AmbryDataNode ambryDataNode) {\n+    return ambryDataNodeToAmbryDisks.get(ambryDataNode);\n+  }\n+\n+  @Override\n+  public Map<String, String> getPartitionToResourceMap() {\n+    return Collections.unmodifiableMap(partitionNameToResource);\n+  }\n+\n+  @Override\n+  public long getErrorCount() {\n+    return errorCount.get();\n+  }\n+\n+  @Override\n+  public void waitForInitNotification() throws InterruptedException {\n+    // wait slightly more than 5 mins to ensure routerUpdater refreshes the snapshot.\n+    if (!routingTableInitLatch.await(320, TimeUnit.SECONDS)) {\n+      throw new IllegalStateException(\"Initial routing table change from \" + dcName + \" didn't come within 5 mins\");\n+    }\n+  }\n+\n+  /**\n+   * Add new instance or update existing instance based on {@link InstanceConfig}(s).\n+   * @param instanceConfigs the {@link InstanceConfig}(s) used to update in-mem cluster map.\n+   * @throws Exception\n+   */\n+  private void addOrUpdateInstanceInfos(List<InstanceConfig> instanceConfigs) throws Exception {\n+    for (InstanceConfig instanceConfig : instanceConfigs) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        logger.error(\"Unknown InstanceConfig schema version: {}, ignoring.\", schemaVersion);\n+        continue;\n+      }\n+      if (instanceNameToAmbryDataNode.containsKey(instanceConfig.getInstanceName())) {\n+        updateInstanceInfo(instanceConfig);\n+      } else {\n+        createNewInstance(instanceConfig);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Update info of an existing instance. This may happen in following cases: (1) new replica is added; (2) old replica\n+   * is removed; (3) replica's state has changed (i.e. becomes seal/unseal).\n+   * @param instanceConfig the {@link InstanceConfig} used to update info of instance.\n+   */\n+  private void updateInstanceInfo(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Updating replicas info for existing node {}\", instanceName);\n+    List<String> sealedReplicas = getSealedReplicas(instanceConfig);\n+    List<String> stoppedReplicas = getStoppedReplicas(instanceConfig);\n+    ConcurrentHashMap<String, AmbryReplica> currentReplicasOnNode =\n+        ambryDataNodeToAmbryReplicas.get(instanceNameToAmbryDataNode.get(instanceName));\n+    ConcurrentHashMap<String, AmbryReplica> replicasFromConfig = new ConcurrentHashMap<>();\n+    Map<String, Map<String, String>> diskInfos = instanceConfig.getRecord().getMapFields();\n+    for (Map.Entry<String, Map<String, String>> entry : diskInfos.entrySet()) {\n+      String mountPath = entry.getKey();\n+      Map<String, String> diskInfo = entry.getValue();\n+      String replicasStr = diskInfo.get(ClusterMapUtils.REPLICAS_STR);\n+      if (!replicasStr.isEmpty()) {\n+        for (String replicaInfo : replicasStr.split(ClusterMapUtils.REPLICAS_DELIM_STR)) {\n+          String[] info = replicaInfo.split(ClusterMapUtils.REPLICAS_STR_SEPARATOR);\n+          // partition name and replica name are the same.\n+          String partitionName = info[0];\n+          if (currentReplicasOnNode.containsKey(partitionName)) {\n+            // if replica is already present\n+            AmbryReplica existingReplica = currentReplicasOnNode.get(partitionName);\n+            // 1. directly add it into \"replicasFromConfig\" map\n+            replicasFromConfig.put(partitionName, existingReplica);\n+            // 2. update replica seal/stop state\n+            updateReplicaStateAndOverrideIfNeeded(existingReplica, sealedReplicas, stoppedReplicas);\n+          } else {\n+            // if this is a new replica and doesn't exist on node\n+            logger.info(\"Adding new replica {} to existing node {}\", partitionName, instanceName);\n+            long replicaCapacity = Long.valueOf(info[1]);\n+            String partitionClass = info.length > 2 ? info[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+            AmbryDataNode dataNode = instanceNameToAmbryDataNode.get(instanceName);\n+            AmbryDisk disk = ambryDataNodeToAmbryDisks.get(dataNode)\n+                .stream()\n+                .filter(d -> d.getMountPath().equals(mountPath))\n+                .findFirst()\n+                .get();\n+            // this can be a brand new partition that is added to an existing node\n+            AmbryPartition mappedPartition =\n+                new AmbryPartition(Long.valueOf(partitionName), partitionClass, helixClusterManagerCallback);\n+            // Ensure only one AmbryPartition instance exists for specific partition.\n+            mappedPartition = clusterChangeHandlerCallback.addPartitionIfAbsent(mappedPartition, replicaCapacity);\n+            ensurePartitionAbsenceOnNodeAndValidateCapacity(mappedPartition, dataNode, replicaCapacity);\n+            // create new replica belonging to this partition\n+            AmbryReplica replica =\n+                new AmbryReplica(clusterMapConfig, mappedPartition, disk, stoppedReplicas.contains(partitionName),\n+                    replicaCapacity, sealedReplicas.contains(partitionName));\n+            updateReplicaStateAndOverrideIfNeeded(replica, sealedReplicas, stoppedReplicas);\n+            // add new created replica to \"replicasFromConfig\" map\n+            replicasFromConfig.put(partitionName, replica);\n+            // add new replica to specific partition\n+            clusterChangeHandlerCallback.addReplicasToPartition(mappedPartition, Collections.singletonList(replica));\n+          }\n+        }\n+      }\n+    }\n+    // update ambryDataNodeToAmbryReplicas map by adding \"replicasFromConfig\"\n+    ambryDataNodeToAmbryReplicas.put(instanceNameToAmbryDataNode.get(instanceName), replicasFromConfig);\n+    // compare replicasFromConfig with current replica set to derive old replicas that are removed\n+    Set<AmbryReplica> previousReplicas = new HashSet<>(currentReplicasOnNode.values());\n+    previousReplicas.removeAll(replicasFromConfig.values());\n+    for (AmbryReplica ambryReplica : previousReplicas) {\n+      logger.info(\"Removing replica {} from existing node {}\", ambryReplica.getPartitionId().toPathString(),\n+          instanceName);\n+      clusterChangeHandlerCallback.removeReplicasFromPartition(ambryReplica.getPartitionId(),\n+          Collections.singletonList(ambryReplica));\n+    }\n+  }\n+\n+  /**\n+   * If partition override is enabled, we override replica SEAL/UNSEAL state based on partitionOverrideMap. If disabled,\n+   * update replica state according to the info from {@link InstanceConfig}.\n+   * @param replica the {@link ReplicaId} whose states (seal,stop) should be updated.\n+   * @param sealedReplicas a list of {@link ReplicaId}(s) that are in SEALED state.\n+   * @param stoppedReplicas a list of {@link ReplicaId}(s) that are in STOPPED state.\n+   */\n+  private void updateReplicaStateAndOverrideIfNeeded(AmbryReplica replica, List<String> sealedReplicas,\n+      List<String> stoppedReplicas) {\n+    String partitionName = replica.getPartitionId().toPathString();\n+    boolean isSealed;\n+    if (clusterMapConfig.clusterMapEnablePartitionOverride && partitionOverrideInfoMap.containsKey(partitionName)) {\n+      isSealed = partitionOverrideInfoMap.get(partitionName)\n+          .get(ClusterMapUtils.PARTITION_STATE)\n+          .equals(ClusterMapUtils.READ_ONLY_STR);\n+    } else {\n+      isSealed = sealedReplicas.contains(partitionName);\n+    }\n+    replica.setSealedState(isSealed);\n+    replica.setStoppedState(stoppedReplicas.contains(partitionName));\n+  }\n+\n+  /**\n+   * Create a new instance(node) and initialize disks/replicas on it.\n+   * @param instanceConfig the {@link InstanceConfig} to create new instance\n+   * @throws Exception\n+   */\n+  private void createNewInstance(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Adding node {} and its disks and replicas\", instanceName);\n+    AmbryDataNode datanode =\n+        new AmbryDataNode(getDcName(instanceConfig), clusterMapConfig, instanceConfig.getHostName(),\n+            Integer.valueOf(instanceConfig.getPort()), getRackId(instanceConfig), getSslPortStr(instanceConfig),\n+            getXid(instanceConfig), helixClusterManagerCallback);\n+    // for new instance, we first set it to unavailable and rely on its participation to update its liveness\n+    if (!instanceName.equals(selfInstanceName)) {\n+      datanode.setState(HardwareState.UNAVAILABLE);\n+    }\n+    initializeDisksAndReplicasOnNode(datanode, instanceConfig);\n+    instanceNameToAmbryDataNode.put(instanceName, datanode);\n+  }\n+\n+  /**\n+   * Initialize the disks and replicas on the given node. Create partitions if this is the first time a replica of\n+   * that partition is being constructed. If partition override is enabled, the seal state of replica is determined by\n+   * partition info in HelixPropertyStore, if disabled, the seal state is determined by instanceConfig.\n+   * @param datanode the {@link AmbryDataNode} that is being initialized.\n+   * @param instanceConfig the {@link InstanceConfig} associated with this datanode.\n+   * @throws Exception if creation of {@link AmbryDisk} throws an Exception.\n+   */\n+  private void initializeDisksAndReplicasOnNode(AmbryDataNode datanode, InstanceConfig instanceConfig)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2MDAzMQ=="}, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 399}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTczODAyOnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/HelixClusterManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTozNzozOFrOFeoYSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQyMToyNTo1NVrOFgJTNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2MzE3Nw==", "bodyText": "we can move this statement to the if block as well, just like partitionMap.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r367663177", "createdAt": "2020-01-16T21:37:38Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/HelixClusterManager.java", "diffHunk": "@@ -589,6 +614,68 @@ long getErrorCount() {\n     }\n   }\n \n+  /**\n+   * A callback class for {@link ClusterChangeHandler} in each dc to update cluster-wide info (i.e partition-to-replica\n+   * mapping, cluster-wide capacity)\n+   */\n+  class ClusterChangeHandlerCallback {\n+    /**\n+     * Add partition if it's not present in cluster-wide partition map and also update cluster-wide allocated usable\n+     * capacity. If the partition already exists, skip addition and return current partition.\n+     * @param partition the {@link AmbryPartition} to add (if not present)\n+     * @param capacityBytes the capacity of partition in bytes\n+     * @return the current {@link AmbryPartition} present in the map.\n+     */\n+    AmbryPartition addPartitionIfAbsent(AmbryPartition partition, long capacityBytes) {\n+      AmbryPartition currentPartition = partitionNameToAmbryPartition.putIfAbsent(partition.toPathString(), partition);\n+      if (currentPartition == null) {\n+        // this means the map previously didn't contain this partition and passed-in partition is successfully added\n+        // into the map\n+        currentPartition = partition;\n+        // it doesn't really need to synchronize this method. \"partitionNameToAmbryPartition\" guarantees each thread\n+        // will get same instance of ambry partition. The first one that succeeds adding partition into\n+        // \"partitionNameToAmbryPartition\" will update \"partitionMap\".\n+        partitionMap.put(ByteBuffer.wrap(currentPartition.getBytes()), currentPartition);\n+        // update cluster-wide capacity\n+        clusterWideAllocatedUsableCapacityBytes.getAndAdd(capacityBytes);\n+      }\n+      ambryPartitionToAmbryReplicas.putIfAbsent(currentPartition, ConcurrentHashMap.newKeySet());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTI1MTEyNw==", "bodyText": "Unfortunately we can't. There could be an edge case where two threads from two dcs are attempting to modify the map concurrently. Will discuss with you offline.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r369251127", "createdAt": "2020-01-21T21:25:55Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/HelixClusterManager.java", "diffHunk": "@@ -589,6 +614,68 @@ long getErrorCount() {\n     }\n   }\n \n+  /**\n+   * A callback class for {@link ClusterChangeHandler} in each dc to update cluster-wide info (i.e partition-to-replica\n+   * mapping, cluster-wide capacity)\n+   */\n+  class ClusterChangeHandlerCallback {\n+    /**\n+     * Add partition if it's not present in cluster-wide partition map and also update cluster-wide allocated usable\n+     * capacity. If the partition already exists, skip addition and return current partition.\n+     * @param partition the {@link AmbryPartition} to add (if not present)\n+     * @param capacityBytes the capacity of partition in bytes\n+     * @return the current {@link AmbryPartition} present in the map.\n+     */\n+    AmbryPartition addPartitionIfAbsent(AmbryPartition partition, long capacityBytes) {\n+      AmbryPartition currentPartition = partitionNameToAmbryPartition.putIfAbsent(partition.toPathString(), partition);\n+      if (currentPartition == null) {\n+        // this means the map previously didn't contain this partition and passed-in partition is successfully added\n+        // into the map\n+        currentPartition = partition;\n+        // it doesn't really need to synchronize this method. \"partitionNameToAmbryPartition\" guarantees each thread\n+        // will get same instance of ambry partition. The first one that succeeds adding partition into\n+        // \"partitionNameToAmbryPartition\" will update \"partitionMap\".\n+        partitionMap.put(ByteBuffer.wrap(currentPartition.getBytes()), currentPartition);\n+        // update cluster-wide capacity\n+        clusterWideAllocatedUsableCapacityBytes.getAndAdd(capacityBytes);\n+      }\n+      ambryPartitionToAmbryReplicas.putIfAbsent(currentPartition, ConcurrentHashMap.newKeySet());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2MzE3Nw=="}, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 135}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTc3MzU0OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo1MTowOVrOFeouqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo1MTowOVrOFeouqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2ODkwNQ==", "bodyText": "same here, share this method with  SimpleClusterChangeHandler.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r367668905", "createdAt": "2020-01-16T21:51:09Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,487 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized.get()) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+          instanceConfigInitialized.set(true);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized.get()) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+            instanceConfigInitialized.set(true);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   * @throws InterruptedException\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext)\n+      throws InterruptedException {\n+    if (!idealStateInitialized.get()) {\n+      logger.info(\"Received initial notification for IdealState change from {}\", dcName);\n+      idealStateInitialized.set(true);\n+    } else {\n+      logger.info(\"IdealState change triggered from {}\", dcName);\n+    }\n+    logger.debug(\"Detailed ideal states in {} are: {}\", dcName, idealState);\n+    // rebuild the entire partition-to-resource map in current dc\n+    ConcurrentHashMap<String, String> partitionToResourceMap = new ConcurrentHashMap<>();\n+    for (IdealState state : idealState) {\n+      String resourceName = state.getResourceName();\n+      state.getPartitionSet().forEach(partitionName -> partitionToResourceMap.put(partitionName, resourceName));\n+    }\n+    partitionNameToResource = partitionToResourceMap;\n+    helixClusterManagerMetrics.idealStateChangeTriggerCount.inc();\n+  }\n+\n+  /**\n+   * Triggered whenever there is a change in the list of live instances.\n+   * @param liveInstances the list of all live instances (not a change set) at the time of this call.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onLiveInstanceChange(List<LiveInstance> liveInstances, NotificationContext changeContext) {\n+    try {\n+      if (!liveStateInitialized.get()) {\n+        logger.info(\"Received initial notification for live instance change from {}\", dcName);\n+        liveStateInitialized.set(true);\n+      } else {\n+        logger.info(\"Live instance change triggered from {}\", dcName);\n+      }\n+      logger.debug(\"Detailed live instances in {} are: {}\", dcName, liveInstances);\n+      synchronized (notificationLock) {\n+        updateInstanceLiveness(liveInstances);\n+        helixClusterManagerMetrics.liveInstanceChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the state of replica in cluster has changed. The snapshot contains up-to-date state of all\n+   * resources(replicas) in this data center.\n+   * @param routingTableSnapshot a snapshot of routing table for this data center.\n+   * @param context additional context associated with this change.\n+   */\n+  @Override\n+  public void onRoutingTableChange(RoutingTableSnapshot routingTableSnapshot, Object context) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+    if (routingTableInitLatch.getCount() == 1) {\n+      logger.info(\"Received initial notification for routing table change from {}\", dcName);\n+      routingTableInitLatch.countDown();\n+    } else {\n+      logger.info(\"Routing table change triggered from {}\", dcName);\n+    }\n+    helixClusterManagerMetrics.routingTableChangeTriggerCount.inc();\n+  }\n+\n+  @Override\n+  public void setRoutingTableSnapshot(RoutingTableSnapshot routingTableSnapshot) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+  }\n+\n+  @Override\n+  public RoutingTableSnapshot getRoutingTableSnapshot() {\n+    return routingTableSnapshotRef.get();\n+  }\n+\n+  @Override\n+  public Map<AmbryDataNode, Set<AmbryDisk>> getDataNodeToDisksMap() {\n+    return Collections.unmodifiableMap(ambryDataNodeToAmbryDisks);\n+  }\n+\n+  @Override\n+  public AmbryDataNode getDataNode(String instanceName) {\n+    return instanceNameToAmbryDataNode.get(instanceName);\n+  }\n+\n+  @Override\n+  public AmbryReplica getReplicaId(AmbryDataNode ambryDataNode, String partitionName) {\n+    return ambryDataNodeToAmbryReplicas.getOrDefault(ambryDataNode, new ConcurrentHashMap<>()).get(partitionName);\n+  }\n+\n+  @Override\n+  public List<AmbryReplica> getReplicaIds(AmbryDataNode ambryDataNode) {\n+    return new ArrayList<>(ambryDataNodeToAmbryReplicas.get(ambryDataNode).values());\n+  }\n+\n+  @Override\n+  public List<AmbryDataNode> getAllDataNodes() {\n+    return new ArrayList<>(instanceNameToAmbryDataNode.values());\n+  }\n+\n+  @Override\n+  public Set<AmbryDisk> getDisks(AmbryDataNode ambryDataNode) {\n+    return ambryDataNodeToAmbryDisks.get(ambryDataNode);\n+  }\n+\n+  @Override\n+  public Map<String, String> getPartitionToResourceMap() {\n+    return Collections.unmodifiableMap(partitionNameToResource);\n+  }\n+\n+  @Override\n+  public long getErrorCount() {\n+    return errorCount.get();\n+  }\n+\n+  @Override\n+  public void waitForInitNotification() throws InterruptedException {\n+    // wait slightly more than 5 mins to ensure routerUpdater refreshes the snapshot.\n+    if (!routingTableInitLatch.await(320, TimeUnit.SECONDS)) {\n+      throw new IllegalStateException(\"Initial routing table change from \" + dcName + \" didn't come within 5 mins\");\n+    }\n+  }\n+\n+  /**\n+   * Add new instance or update existing instance based on {@link InstanceConfig}(s).\n+   * @param instanceConfigs the {@link InstanceConfig}(s) used to update in-mem cluster map.\n+   * @throws Exception\n+   */\n+  private void addOrUpdateInstanceInfos(List<InstanceConfig> instanceConfigs) throws Exception {\n+    for (InstanceConfig instanceConfig : instanceConfigs) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        logger.error(\"Unknown InstanceConfig schema version: {}, ignoring.\", schemaVersion);\n+        continue;\n+      }\n+      if (instanceNameToAmbryDataNode.containsKey(instanceConfig.getInstanceName())) {\n+        updateInstanceInfo(instanceConfig);\n+      } else {\n+        createNewInstance(instanceConfig);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Update info of an existing instance. This may happen in following cases: (1) new replica is added; (2) old replica\n+   * is removed; (3) replica's state has changed (i.e. becomes seal/unseal).\n+   * @param instanceConfig the {@link InstanceConfig} used to update info of instance.\n+   */\n+  private void updateInstanceInfo(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Updating replicas info for existing node {}\", instanceName);\n+    List<String> sealedReplicas = getSealedReplicas(instanceConfig);\n+    List<String> stoppedReplicas = getStoppedReplicas(instanceConfig);\n+    ConcurrentHashMap<String, AmbryReplica> currentReplicasOnNode =\n+        ambryDataNodeToAmbryReplicas.get(instanceNameToAmbryDataNode.get(instanceName));\n+    ConcurrentHashMap<String, AmbryReplica> replicasFromConfig = new ConcurrentHashMap<>();\n+    Map<String, Map<String, String>> diskInfos = instanceConfig.getRecord().getMapFields();\n+    for (Map.Entry<String, Map<String, String>> entry : diskInfos.entrySet()) {\n+      String mountPath = entry.getKey();\n+      Map<String, String> diskInfo = entry.getValue();\n+      String replicasStr = diskInfo.get(ClusterMapUtils.REPLICAS_STR);\n+      if (!replicasStr.isEmpty()) {\n+        for (String replicaInfo : replicasStr.split(ClusterMapUtils.REPLICAS_DELIM_STR)) {\n+          String[] info = replicaInfo.split(ClusterMapUtils.REPLICAS_STR_SEPARATOR);\n+          // partition name and replica name are the same.\n+          String partitionName = info[0];\n+          if (currentReplicasOnNode.containsKey(partitionName)) {\n+            // if replica is already present\n+            AmbryReplica existingReplica = currentReplicasOnNode.get(partitionName);\n+            // 1. directly add it into \"replicasFromConfig\" map\n+            replicasFromConfig.put(partitionName, existingReplica);\n+            // 2. update replica seal/stop state\n+            updateReplicaStateAndOverrideIfNeeded(existingReplica, sealedReplicas, stoppedReplicas);\n+          } else {\n+            // if this is a new replica and doesn't exist on node\n+            logger.info(\"Adding new replica {} to existing node {}\", partitionName, instanceName);\n+            long replicaCapacity = Long.valueOf(info[1]);\n+            String partitionClass = info.length > 2 ? info[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+            AmbryDataNode dataNode = instanceNameToAmbryDataNode.get(instanceName);\n+            AmbryDisk disk = ambryDataNodeToAmbryDisks.get(dataNode)\n+                .stream()\n+                .filter(d -> d.getMountPath().equals(mountPath))\n+                .findFirst()\n+                .get();\n+            // this can be a brand new partition that is added to an existing node\n+            AmbryPartition mappedPartition =\n+                new AmbryPartition(Long.valueOf(partitionName), partitionClass, helixClusterManagerCallback);\n+            // Ensure only one AmbryPartition instance exists for specific partition.\n+            mappedPartition = clusterChangeHandlerCallback.addPartitionIfAbsent(mappedPartition, replicaCapacity);\n+            ensurePartitionAbsenceOnNodeAndValidateCapacity(mappedPartition, dataNode, replicaCapacity);\n+            // create new replica belonging to this partition\n+            AmbryReplica replica =\n+                new AmbryReplica(clusterMapConfig, mappedPartition, disk, stoppedReplicas.contains(partitionName),\n+                    replicaCapacity, sealedReplicas.contains(partitionName));\n+            updateReplicaStateAndOverrideIfNeeded(replica, sealedReplicas, stoppedReplicas);\n+            // add new created replica to \"replicasFromConfig\" map\n+            replicasFromConfig.put(partitionName, replica);\n+            // add new replica to specific partition\n+            clusterChangeHandlerCallback.addReplicasToPartition(mappedPartition, Collections.singletonList(replica));\n+          }\n+        }\n+      }\n+    }\n+    // update ambryDataNodeToAmbryReplicas map by adding \"replicasFromConfig\"\n+    ambryDataNodeToAmbryReplicas.put(instanceNameToAmbryDataNode.get(instanceName), replicasFromConfig);\n+    // compare replicasFromConfig with current replica set to derive old replicas that are removed\n+    Set<AmbryReplica> previousReplicas = new HashSet<>(currentReplicasOnNode.values());\n+    previousReplicas.removeAll(replicasFromConfig.values());\n+    for (AmbryReplica ambryReplica : previousReplicas) {\n+      logger.info(\"Removing replica {} from existing node {}\", ambryReplica.getPartitionId().toPathString(),\n+          instanceName);\n+      clusterChangeHandlerCallback.removeReplicasFromPartition(ambryReplica.getPartitionId(),\n+          Collections.singletonList(ambryReplica));\n+    }\n+  }\n+\n+  /**\n+   * If partition override is enabled, we override replica SEAL/UNSEAL state based on partitionOverrideMap. If disabled,\n+   * update replica state according to the info from {@link InstanceConfig}.\n+   * @param replica the {@link ReplicaId} whose states (seal,stop) should be updated.\n+   * @param sealedReplicas a list of {@link ReplicaId}(s) that are in SEALED state.\n+   * @param stoppedReplicas a list of {@link ReplicaId}(s) that are in STOPPED state.\n+   */\n+  private void updateReplicaStateAndOverrideIfNeeded(AmbryReplica replica, List<String> sealedReplicas,\n+      List<String> stoppedReplicas) {\n+    String partitionName = replica.getPartitionId().toPathString();\n+    boolean isSealed;\n+    if (clusterMapConfig.clusterMapEnablePartitionOverride && partitionOverrideInfoMap.containsKey(partitionName)) {\n+      isSealed = partitionOverrideInfoMap.get(partitionName)\n+          .get(ClusterMapUtils.PARTITION_STATE)\n+          .equals(ClusterMapUtils.READ_ONLY_STR);\n+    } else {\n+      isSealed = sealedReplicas.contains(partitionName);\n+    }\n+    replica.setSealedState(isSealed);\n+    replica.setStoppedState(stoppedReplicas.contains(partitionName));\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 369}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MTc4MzU4OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo1NTowM1rOFeo0vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQyMTo0NjoyM1rOFgJ2EA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY3MDQ2Mg==", "bodyText": "I suggest check if the mountPath can map to a AmbryDisk in the ambryDataNodeToAmbryDisks first. If not, just throw an exception. We can't want to deal with the case of adding a disk.\nAnd also check if it removes a disk or not.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r367670462", "createdAt": "2020-01-16T21:55:03Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,487 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized.get()) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+          instanceConfigInitialized.set(true);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized.get()) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+            instanceConfigInitialized.set(true);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   * @throws InterruptedException\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext)\n+      throws InterruptedException {\n+    if (!idealStateInitialized.get()) {\n+      logger.info(\"Received initial notification for IdealState change from {}\", dcName);\n+      idealStateInitialized.set(true);\n+    } else {\n+      logger.info(\"IdealState change triggered from {}\", dcName);\n+    }\n+    logger.debug(\"Detailed ideal states in {} are: {}\", dcName, idealState);\n+    // rebuild the entire partition-to-resource map in current dc\n+    ConcurrentHashMap<String, String> partitionToResourceMap = new ConcurrentHashMap<>();\n+    for (IdealState state : idealState) {\n+      String resourceName = state.getResourceName();\n+      state.getPartitionSet().forEach(partitionName -> partitionToResourceMap.put(partitionName, resourceName));\n+    }\n+    partitionNameToResource = partitionToResourceMap;\n+    helixClusterManagerMetrics.idealStateChangeTriggerCount.inc();\n+  }\n+\n+  /**\n+   * Triggered whenever there is a change in the list of live instances.\n+   * @param liveInstances the list of all live instances (not a change set) at the time of this call.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onLiveInstanceChange(List<LiveInstance> liveInstances, NotificationContext changeContext) {\n+    try {\n+      if (!liveStateInitialized.get()) {\n+        logger.info(\"Received initial notification for live instance change from {}\", dcName);\n+        liveStateInitialized.set(true);\n+      } else {\n+        logger.info(\"Live instance change triggered from {}\", dcName);\n+      }\n+      logger.debug(\"Detailed live instances in {} are: {}\", dcName, liveInstances);\n+      synchronized (notificationLock) {\n+        updateInstanceLiveness(liveInstances);\n+        helixClusterManagerMetrics.liveInstanceChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the state of replica in cluster has changed. The snapshot contains up-to-date state of all\n+   * resources(replicas) in this data center.\n+   * @param routingTableSnapshot a snapshot of routing table for this data center.\n+   * @param context additional context associated with this change.\n+   */\n+  @Override\n+  public void onRoutingTableChange(RoutingTableSnapshot routingTableSnapshot, Object context) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+    if (routingTableInitLatch.getCount() == 1) {\n+      logger.info(\"Received initial notification for routing table change from {}\", dcName);\n+      routingTableInitLatch.countDown();\n+    } else {\n+      logger.info(\"Routing table change triggered from {}\", dcName);\n+    }\n+    helixClusterManagerMetrics.routingTableChangeTriggerCount.inc();\n+  }\n+\n+  @Override\n+  public void setRoutingTableSnapshot(RoutingTableSnapshot routingTableSnapshot) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+  }\n+\n+  @Override\n+  public RoutingTableSnapshot getRoutingTableSnapshot() {\n+    return routingTableSnapshotRef.get();\n+  }\n+\n+  @Override\n+  public Map<AmbryDataNode, Set<AmbryDisk>> getDataNodeToDisksMap() {\n+    return Collections.unmodifiableMap(ambryDataNodeToAmbryDisks);\n+  }\n+\n+  @Override\n+  public AmbryDataNode getDataNode(String instanceName) {\n+    return instanceNameToAmbryDataNode.get(instanceName);\n+  }\n+\n+  @Override\n+  public AmbryReplica getReplicaId(AmbryDataNode ambryDataNode, String partitionName) {\n+    return ambryDataNodeToAmbryReplicas.getOrDefault(ambryDataNode, new ConcurrentHashMap<>()).get(partitionName);\n+  }\n+\n+  @Override\n+  public List<AmbryReplica> getReplicaIds(AmbryDataNode ambryDataNode) {\n+    return new ArrayList<>(ambryDataNodeToAmbryReplicas.get(ambryDataNode).values());\n+  }\n+\n+  @Override\n+  public List<AmbryDataNode> getAllDataNodes() {\n+    return new ArrayList<>(instanceNameToAmbryDataNode.values());\n+  }\n+\n+  @Override\n+  public Set<AmbryDisk> getDisks(AmbryDataNode ambryDataNode) {\n+    return ambryDataNodeToAmbryDisks.get(ambryDataNode);\n+  }\n+\n+  @Override\n+  public Map<String, String> getPartitionToResourceMap() {\n+    return Collections.unmodifiableMap(partitionNameToResource);\n+  }\n+\n+  @Override\n+  public long getErrorCount() {\n+    return errorCount.get();\n+  }\n+\n+  @Override\n+  public void waitForInitNotification() throws InterruptedException {\n+    // wait slightly more than 5 mins to ensure routerUpdater refreshes the snapshot.\n+    if (!routingTableInitLatch.await(320, TimeUnit.SECONDS)) {\n+      throw new IllegalStateException(\"Initial routing table change from \" + dcName + \" didn't come within 5 mins\");\n+    }\n+  }\n+\n+  /**\n+   * Add new instance or update existing instance based on {@link InstanceConfig}(s).\n+   * @param instanceConfigs the {@link InstanceConfig}(s) used to update in-mem cluster map.\n+   * @throws Exception\n+   */\n+  private void addOrUpdateInstanceInfos(List<InstanceConfig> instanceConfigs) throws Exception {\n+    for (InstanceConfig instanceConfig : instanceConfigs) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        logger.error(\"Unknown InstanceConfig schema version: {}, ignoring.\", schemaVersion);\n+        continue;\n+      }\n+      if (instanceNameToAmbryDataNode.containsKey(instanceConfig.getInstanceName())) {\n+        updateInstanceInfo(instanceConfig);\n+      } else {\n+        createNewInstance(instanceConfig);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Update info of an existing instance. This may happen in following cases: (1) new replica is added; (2) old replica\n+   * is removed; (3) replica's state has changed (i.e. becomes seal/unseal).\n+   * @param instanceConfig the {@link InstanceConfig} used to update info of instance.\n+   */\n+  private void updateInstanceInfo(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Updating replicas info for existing node {}\", instanceName);\n+    List<String> sealedReplicas = getSealedReplicas(instanceConfig);\n+    List<String> stoppedReplicas = getStoppedReplicas(instanceConfig);\n+    ConcurrentHashMap<String, AmbryReplica> currentReplicasOnNode =\n+        ambryDataNodeToAmbryReplicas.get(instanceNameToAmbryDataNode.get(instanceName));\n+    ConcurrentHashMap<String, AmbryReplica> replicasFromConfig = new ConcurrentHashMap<>();\n+    Map<String, Map<String, String>> diskInfos = instanceConfig.getRecord().getMapFields();\n+    for (Map.Entry<String, Map<String, String>> entry : diskInfos.entrySet()) {\n+      String mountPath = entry.getKey();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 291}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTI2MDA0OA==", "bodyText": "Removing disk is not in the plan but adding disk is actually supported in StorageManager. For now, we temporarily skip adding new disk in dynamic handler. We can support this if needed in the future.\nI will take your suggestion to check if the mount path is valid.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r369260048", "createdAt": "2020-01-21T21:46:23Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,487 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized.get()) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+          instanceConfigInitialized.set(true);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized.get()) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+            instanceConfigInitialized.set(true);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   * @throws InterruptedException\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext)\n+      throws InterruptedException {\n+    if (!idealStateInitialized.get()) {\n+      logger.info(\"Received initial notification for IdealState change from {}\", dcName);\n+      idealStateInitialized.set(true);\n+    } else {\n+      logger.info(\"IdealState change triggered from {}\", dcName);\n+    }\n+    logger.debug(\"Detailed ideal states in {} are: {}\", dcName, idealState);\n+    // rebuild the entire partition-to-resource map in current dc\n+    ConcurrentHashMap<String, String> partitionToResourceMap = new ConcurrentHashMap<>();\n+    for (IdealState state : idealState) {\n+      String resourceName = state.getResourceName();\n+      state.getPartitionSet().forEach(partitionName -> partitionToResourceMap.put(partitionName, resourceName));\n+    }\n+    partitionNameToResource = partitionToResourceMap;\n+    helixClusterManagerMetrics.idealStateChangeTriggerCount.inc();\n+  }\n+\n+  /**\n+   * Triggered whenever there is a change in the list of live instances.\n+   * @param liveInstances the list of all live instances (not a change set) at the time of this call.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onLiveInstanceChange(List<LiveInstance> liveInstances, NotificationContext changeContext) {\n+    try {\n+      if (!liveStateInitialized.get()) {\n+        logger.info(\"Received initial notification for live instance change from {}\", dcName);\n+        liveStateInitialized.set(true);\n+      } else {\n+        logger.info(\"Live instance change triggered from {}\", dcName);\n+      }\n+      logger.debug(\"Detailed live instances in {} are: {}\", dcName, liveInstances);\n+      synchronized (notificationLock) {\n+        updateInstanceLiveness(liveInstances);\n+        helixClusterManagerMetrics.liveInstanceChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the state of replica in cluster has changed. The snapshot contains up-to-date state of all\n+   * resources(replicas) in this data center.\n+   * @param routingTableSnapshot a snapshot of routing table for this data center.\n+   * @param context additional context associated with this change.\n+   */\n+  @Override\n+  public void onRoutingTableChange(RoutingTableSnapshot routingTableSnapshot, Object context) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+    if (routingTableInitLatch.getCount() == 1) {\n+      logger.info(\"Received initial notification for routing table change from {}\", dcName);\n+      routingTableInitLatch.countDown();\n+    } else {\n+      logger.info(\"Routing table change triggered from {}\", dcName);\n+    }\n+    helixClusterManagerMetrics.routingTableChangeTriggerCount.inc();\n+  }\n+\n+  @Override\n+  public void setRoutingTableSnapshot(RoutingTableSnapshot routingTableSnapshot) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+  }\n+\n+  @Override\n+  public RoutingTableSnapshot getRoutingTableSnapshot() {\n+    return routingTableSnapshotRef.get();\n+  }\n+\n+  @Override\n+  public Map<AmbryDataNode, Set<AmbryDisk>> getDataNodeToDisksMap() {\n+    return Collections.unmodifiableMap(ambryDataNodeToAmbryDisks);\n+  }\n+\n+  @Override\n+  public AmbryDataNode getDataNode(String instanceName) {\n+    return instanceNameToAmbryDataNode.get(instanceName);\n+  }\n+\n+  @Override\n+  public AmbryReplica getReplicaId(AmbryDataNode ambryDataNode, String partitionName) {\n+    return ambryDataNodeToAmbryReplicas.getOrDefault(ambryDataNode, new ConcurrentHashMap<>()).get(partitionName);\n+  }\n+\n+  @Override\n+  public List<AmbryReplica> getReplicaIds(AmbryDataNode ambryDataNode) {\n+    return new ArrayList<>(ambryDataNodeToAmbryReplicas.get(ambryDataNode).values());\n+  }\n+\n+  @Override\n+  public List<AmbryDataNode> getAllDataNodes() {\n+    return new ArrayList<>(instanceNameToAmbryDataNode.values());\n+  }\n+\n+  @Override\n+  public Set<AmbryDisk> getDisks(AmbryDataNode ambryDataNode) {\n+    return ambryDataNodeToAmbryDisks.get(ambryDataNode);\n+  }\n+\n+  @Override\n+  public Map<String, String> getPartitionToResourceMap() {\n+    return Collections.unmodifiableMap(partitionNameToResource);\n+  }\n+\n+  @Override\n+  public long getErrorCount() {\n+    return errorCount.get();\n+  }\n+\n+  @Override\n+  public void waitForInitNotification() throws InterruptedException {\n+    // wait slightly more than 5 mins to ensure routerUpdater refreshes the snapshot.\n+    if (!routingTableInitLatch.await(320, TimeUnit.SECONDS)) {\n+      throw new IllegalStateException(\"Initial routing table change from \" + dcName + \" didn't come within 5 mins\");\n+    }\n+  }\n+\n+  /**\n+   * Add new instance or update existing instance based on {@link InstanceConfig}(s).\n+   * @param instanceConfigs the {@link InstanceConfig}(s) used to update in-mem cluster map.\n+   * @throws Exception\n+   */\n+  private void addOrUpdateInstanceInfos(List<InstanceConfig> instanceConfigs) throws Exception {\n+    for (InstanceConfig instanceConfig : instanceConfigs) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        logger.error(\"Unknown InstanceConfig schema version: {}, ignoring.\", schemaVersion);\n+        continue;\n+      }\n+      if (instanceNameToAmbryDataNode.containsKey(instanceConfig.getInstanceName())) {\n+        updateInstanceInfo(instanceConfig);\n+      } else {\n+        createNewInstance(instanceConfig);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Update info of an existing instance. This may happen in following cases: (1) new replica is added; (2) old replica\n+   * is removed; (3) replica's state has changed (i.e. becomes seal/unseal).\n+   * @param instanceConfig the {@link InstanceConfig} used to update info of instance.\n+   */\n+  private void updateInstanceInfo(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Updating replicas info for existing node {}\", instanceName);\n+    List<String> sealedReplicas = getSealedReplicas(instanceConfig);\n+    List<String> stoppedReplicas = getStoppedReplicas(instanceConfig);\n+    ConcurrentHashMap<String, AmbryReplica> currentReplicasOnNode =\n+        ambryDataNodeToAmbryReplicas.get(instanceNameToAmbryDataNode.get(instanceName));\n+    ConcurrentHashMap<String, AmbryReplica> replicasFromConfig = new ConcurrentHashMap<>();\n+    Map<String, Map<String, String>> diskInfos = instanceConfig.getRecord().getMapFields();\n+    for (Map.Entry<String, Map<String, String>> entry : diskInfos.entrySet()) {\n+      String mountPath = entry.getKey();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY3MDQ2Mg=="}, "originalCommit": {"oid": "00e247cdbe63fa97f66c76f3d21442bb3f94f399"}, "originalPosition": 291}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyMjY2MDkzOnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQyMjozODozNFrOFmJoJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMjo1MjoxMFrOFn4jQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU0Nzk0MA==", "bodyText": "throws InterruptedException can be removed", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r375547940", "createdAt": "2020-02-05T22:38:34Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,504 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  /**\n+   * Constructor for {@link DynamicClusterChangeHandler}\n+   * @param clusterMapConfig the {@link ClusterMapConfig} used to define some behavior of cluster change handler.\n+   * @param dcName the name of data center this handler is associated with.\n+   * @param selfInstanceName instance name of current node.\n+   * @param partitionOverrideInfoMap a map that records partitions and states they should be overridden to.\n+   * @param helixClusterManagerCallback a call back used to query cluster-wide info.\n+   * @param clusterChangeHandlerCallback a call back that allows current handler to update cluster-wide info.\n+   * @param helixClusterManagerMetrics metrics to keep track of changes and status of {@link HelixClusterManager}.\n+   * @param initializationFailureMap a map recording failure associate with each dc during initialization.\n+   * @param sealedStateChangeCounter a counter indicating if sealed state of any partition has changed.\n+   */\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * (The ZNode path of instance config in Helix is [AmbryClusterName]/CONFIGS/PARTICIPANT/[hostname_port])\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized.get()) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized.get()) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+            instanceConfigInitialized.set(true);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        } finally {\n+          instanceConfigInitialized.set(true);\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   * @throws InterruptedException\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext)\n+      throws InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a544067f0dfcd8c61541632e8e13a2935ce61c73"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM2NTMxNA==", "bodyText": "removed", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r377365314", "createdAt": "2020-02-10T22:52:10Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,504 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  /**\n+   * Constructor for {@link DynamicClusterChangeHandler}\n+   * @param clusterMapConfig the {@link ClusterMapConfig} used to define some behavior of cluster change handler.\n+   * @param dcName the name of data center this handler is associated with.\n+   * @param selfInstanceName instance name of current node.\n+   * @param partitionOverrideInfoMap a map that records partitions and states they should be overridden to.\n+   * @param helixClusterManagerCallback a call back used to query cluster-wide info.\n+   * @param clusterChangeHandlerCallback a call back that allows current handler to update cluster-wide info.\n+   * @param helixClusterManagerMetrics metrics to keep track of changes and status of {@link HelixClusterManager}.\n+   * @param initializationFailureMap a map recording failure associate with each dc during initialization.\n+   * @param sealedStateChangeCounter a counter indicating if sealed state of any partition has changed.\n+   */\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * (The ZNode path of instance config in Helix is [AmbryClusterName]/CONFIGS/PARTICIPANT/[hostname_port])\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized.get()) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized.get()) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+            instanceConfigInitialized.set(true);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        } finally {\n+          instanceConfigInitialized.set(true);\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   * @throws InterruptedException\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext)\n+      throws InterruptedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU0Nzk0MA=="}, "originalCommit": {"oid": "a544067f0dfcd8c61541632e8e13a2935ce61c73"}, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyMjY2ODg5OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQyMjo0MTo1MFrOFmJtKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMjo1MjowMVrOFn4jFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU0OTIyNg==", "bodyText": "These three atomics don't rely on any cas semantics, so volatile booleans can be used instead.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r375549226", "createdAt": "2020-02-05T22:41:50Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,504 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a544067f0dfcd8c61541632e8e13a2935ce61c73"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM2NTI2OA==", "bodyText": "right, I have changed them to volatile", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r377365268", "createdAt": "2020-02-10T22:52:01Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,504 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU0OTIyNg=="}, "originalCommit": {"oid": "a544067f0dfcd8c61541632e8e13a2935ce61c73"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNTM4MjE4OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQxNzoyMDo1NFrOFmjiDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQyMjo1MToyOFrOFn4iIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTk3MjM2NA==", "bodyText": "Don't need this one since it is repeated in the finally block.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r375972364", "createdAt": "2020-02-06T17:20:54Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,504 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  /**\n+   * Constructor for {@link DynamicClusterChangeHandler}\n+   * @param clusterMapConfig the {@link ClusterMapConfig} used to define some behavior of cluster change handler.\n+   * @param dcName the name of data center this handler is associated with.\n+   * @param selfInstanceName instance name of current node.\n+   * @param partitionOverrideInfoMap a map that records partitions and states they should be overridden to.\n+   * @param helixClusterManagerCallback a call back used to query cluster-wide info.\n+   * @param clusterChangeHandlerCallback a call back that allows current handler to update cluster-wide info.\n+   * @param helixClusterManagerMetrics metrics to keep track of changes and status of {@link HelixClusterManager}.\n+   * @param initializationFailureMap a map recording failure associate with each dc during initialization.\n+   * @param sealedStateChangeCounter a counter indicating if sealed state of any partition has changed.\n+   */\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * (The ZNode path of instance config in Helix is [AmbryClusterName]/CONFIGS/PARTICIPANT/[hostname_port])\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized.get()) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized.get()) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+            instanceConfigInitialized.set(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a544067f0dfcd8c61541632e8e13a2935ce61c73"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM2NTAyNA==", "bodyText": "correct, removed.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r377365024", "createdAt": "2020-02-10T22:51:28Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,504 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final AtomicBoolean instanceConfigInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean liveStateInitialized = new AtomicBoolean(false);\n+  private final AtomicBoolean idealStateInitialized = new AtomicBoolean(false);\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  /**\n+   * Constructor for {@link DynamicClusterChangeHandler}\n+   * @param clusterMapConfig the {@link ClusterMapConfig} used to define some behavior of cluster change handler.\n+   * @param dcName the name of data center this handler is associated with.\n+   * @param selfInstanceName instance name of current node.\n+   * @param partitionOverrideInfoMap a map that records partitions and states they should be overridden to.\n+   * @param helixClusterManagerCallback a call back used to query cluster-wide info.\n+   * @param clusterChangeHandlerCallback a call back that allows current handler to update cluster-wide info.\n+   * @param helixClusterManagerMetrics metrics to keep track of changes and status of {@link HelixClusterManager}.\n+   * @param initializationFailureMap a map recording failure associate with each dc during initialization.\n+   * @param sealedStateChangeCounter a counter indicating if sealed state of any partition has changed.\n+   */\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * (The ZNode path of instance config in Helix is [AmbryClusterName]/CONFIGS/PARTICIPANT/[hostname_port])\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized.get()) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized.get()) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+            instanceConfigInitialized.set(true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTk3MjM2NA=="}, "originalCommit": {"oid": "a544067f0dfcd8c61541632e8e13a2935ce61c73"}, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODU4OTI1OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/HelixClusterManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMjowMjozNFrOFofihg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwNDo0NDoxNVrOFq0_8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAwNDEwMg==", "bodyText": "I see that there is a computeIfAbsent call here. Would there be a case where this method was called before addPartitionIfAbsent is called? Also, if that happens, should the other partition set up be done as well (maybe by calling addPartitionIfAbsent)", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r378004102", "createdAt": "2020-02-12T02:02:34Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/HelixClusterManager.java", "diffHunk": "@@ -596,6 +621,68 @@ long getErrorCount() {\n     }\n   }\n \n+  /**\n+   * A callback class for {@link ClusterChangeHandler} in each dc to update cluster-wide info (i.e partition-to-replica\n+   * mapping, cluster-wide capacity)\n+   */\n+  class ClusterChangeHandlerCallback {\n+    /**\n+     * Add partition if it's not present in cluster-wide partition map and also update cluster-wide allocated usable\n+     * capacity. If the partition already exists, skip addition and return current partition.\n+     * @param partition the {@link AmbryPartition} to add (if not present)\n+     * @param capacityBytes the capacity of partition in bytes\n+     * @return the current {@link AmbryPartition} present in the map.\n+     */\n+    AmbryPartition addPartitionIfAbsent(AmbryPartition partition, long capacityBytes) {\n+      AmbryPartition currentPartition = partitionNameToAmbryPartition.putIfAbsent(partition.toPathString(), partition);\n+      if (currentPartition == null) {\n+        // this means the map previously didn't contain this partition and passed-in partition is successfully added\n+        // into the map\n+        currentPartition = partition;\n+        // it doesn't really need to synchronize this method. \"partitionNameToAmbryPartition\" guarantees each thread\n+        // will get same instance of ambry partition. The first one that succeeds adding partition into\n+        // \"partitionNameToAmbryPartition\" will update \"partitionMap\".\n+        partitionMap.put(ByteBuffer.wrap(currentPartition.getBytes()), currentPartition);\n+        // update cluster-wide capacity\n+        clusterWideAllocatedUsableCapacityBytes.getAndAdd(capacityBytes);\n+      }\n+      ambryPartitionToAmbryReplicas.putIfAbsent(currentPartition, ConcurrentHashMap.newKeySet());\n+      return currentPartition;\n+    }\n+\n+    /**\n+     * Add a list of {@link AmbryReplica} to given {@link AmbryPartition} and update cluster-wide capacity stats\n+     * @param partition the {@link AmbryPartition} which replicas should be added to.\n+     * @param replicas list of {@link AmbryReplica} to be added.\n+     */\n+    void addReplicasToPartition(AmbryPartition partition, List<AmbryReplica> replicas) {\n+      ambryPartitionToAmbryReplicas.computeIfAbsent(partition, k -> ConcurrentHashMap.newKeySet()).addAll(replicas);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830d5f761ff7df4b57e1083a808121b1529b747"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQ1Mjg0OA==", "bodyText": "It seems this method is called after addPartitionIfAbsent method is invoked but you're right it's better to call addPartitionIfAbsent if partition is not present (this should happen though).", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r380452848", "createdAt": "2020-02-18T04:44:15Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/HelixClusterManager.java", "diffHunk": "@@ -596,6 +621,68 @@ long getErrorCount() {\n     }\n   }\n \n+  /**\n+   * A callback class for {@link ClusterChangeHandler} in each dc to update cluster-wide info (i.e partition-to-replica\n+   * mapping, cluster-wide capacity)\n+   */\n+  class ClusterChangeHandlerCallback {\n+    /**\n+     * Add partition if it's not present in cluster-wide partition map and also update cluster-wide allocated usable\n+     * capacity. If the partition already exists, skip addition and return current partition.\n+     * @param partition the {@link AmbryPartition} to add (if not present)\n+     * @param capacityBytes the capacity of partition in bytes\n+     * @return the current {@link AmbryPartition} present in the map.\n+     */\n+    AmbryPartition addPartitionIfAbsent(AmbryPartition partition, long capacityBytes) {\n+      AmbryPartition currentPartition = partitionNameToAmbryPartition.putIfAbsent(partition.toPathString(), partition);\n+      if (currentPartition == null) {\n+        // this means the map previously didn't contain this partition and passed-in partition is successfully added\n+        // into the map\n+        currentPartition = partition;\n+        // it doesn't really need to synchronize this method. \"partitionNameToAmbryPartition\" guarantees each thread\n+        // will get same instance of ambry partition. The first one that succeeds adding partition into\n+        // \"partitionNameToAmbryPartition\" will update \"partitionMap\".\n+        partitionMap.put(ByteBuffer.wrap(currentPartition.getBytes()), currentPartition);\n+        // update cluster-wide capacity\n+        clusterWideAllocatedUsableCapacityBytes.getAndAdd(capacityBytes);\n+      }\n+      ambryPartitionToAmbryReplicas.putIfAbsent(currentPartition, ConcurrentHashMap.newKeySet());\n+      return currentPartition;\n+    }\n+\n+    /**\n+     * Add a list of {@link AmbryReplica} to given {@link AmbryPartition} and update cluster-wide capacity stats\n+     * @param partition the {@link AmbryPartition} which replicas should be added to.\n+     * @param replicas list of {@link AmbryReplica} to be added.\n+     */\n+    void addReplicasToPartition(AmbryPartition partition, List<AmbryReplica> replicas) {\n+      ambryPartitionToAmbryReplicas.computeIfAbsent(partition, k -> ConcurrentHashMap.newKeySet()).addAll(replicas);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAwNDEwMg=="}, "originalCommit": {"oid": "3830d5f761ff7df4b57e1083a808121b1529b747"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODc5MjU4OnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwNDoyNTo0NVrOFohbaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwNToyODozNlrOFq1gmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAzNTA0OQ==", "bodyText": "You could create a map from mount path to disk outside of this for loop so that you don't have to search for the disk every iteration\nMap<String, Disk> m = ambryDataNodeToAmbryDisks.get(dataNode).stream().collect(Collectors.toMap(AmbryDisk::getMountPath, disk -> disk);\nAnother option is to precompute the mountpaths and make ambryDataNodeToAmbryDisks into a Map<AmbryDataNode, Map<String, AmbryDisk>>", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r378035049", "createdAt": "2020-02-12T04:25:45Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,500 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile boolean instanceConfigInitialized = false;\n+  private volatile boolean liveStateInitialized = false;\n+  private volatile boolean idealStateInitialized = false;\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  /**\n+   * Constructor for {@link DynamicClusterChangeHandler}\n+   * @param clusterMapConfig the {@link ClusterMapConfig} used to define some behavior of cluster change handler.\n+   * @param dcName the name of data center this handler is associated with.\n+   * @param selfInstanceName instance name of current node.\n+   * @param partitionOverrideInfoMap a map that records partitions and states they should be overridden to.\n+   * @param helixClusterManagerCallback a call back used to query cluster-wide info.\n+   * @param clusterChangeHandlerCallback a call back that allows current handler to update cluster-wide info.\n+   * @param helixClusterManagerMetrics metrics to keep track of changes and status of {@link HelixClusterManager}.\n+   * @param initializationFailureMap a map recording failure associate with each dc during initialization.\n+   * @param sealedStateChangeCounter a counter indicating if sealed state of any partition has changed.\n+   */\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * (The ZNode path of instance config in Helix is [AmbryClusterName]/CONFIGS/PARTICIPANT/[hostname_port])\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        } finally {\n+          instanceConfigInitialized = true;\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext) {\n+    if (!idealStateInitialized) {\n+      logger.info(\"Received initial notification for IdealState change from {}\", dcName);\n+      idealStateInitialized = true;\n+    } else {\n+      logger.info(\"IdealState change triggered from {}\", dcName);\n+    }\n+    logger.debug(\"Detailed ideal states in {} are: {}\", dcName, idealState);\n+    // rebuild the entire partition-to-resource map in current dc\n+    ConcurrentHashMap<String, String> partitionToResourceMap = new ConcurrentHashMap<>();\n+    for (IdealState state : idealState) {\n+      String resourceName = state.getResourceName();\n+      state.getPartitionSet().forEach(partitionName -> partitionToResourceMap.put(partitionName, resourceName));\n+    }\n+    partitionNameToResource = partitionToResourceMap;\n+    helixClusterManagerMetrics.idealStateChangeTriggerCount.inc();\n+  }\n+\n+  /**\n+   * Triggered whenever there is a change in the list of live instances.\n+   * @param liveInstances the list of all live instances (not a change set) at the time of this call.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onLiveInstanceChange(List<LiveInstance> liveInstances, NotificationContext changeContext) {\n+    try {\n+      if (!liveStateInitialized) {\n+        logger.info(\"Received initial notification for live instance change from {}\", dcName);\n+        liveStateInitialized = true;\n+      } else {\n+        logger.info(\"Live instance change triggered from {}\", dcName);\n+      }\n+      logger.debug(\"Detailed live instances in {} are: {}\", dcName, liveInstances);\n+      synchronized (notificationLock) {\n+        updateInstanceLiveness(liveInstances);\n+        helixClusterManagerMetrics.liveInstanceChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the state of replica in cluster has changed. The snapshot contains up-to-date state of all\n+   * resources(replicas) in this data center.\n+   * @param routingTableSnapshot a snapshot of routing table for this data center.\n+   * @param context additional context associated with this change.\n+   */\n+  @Override\n+  public void onRoutingTableChange(RoutingTableSnapshot routingTableSnapshot, Object context) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+    if (routingTableInitLatch.getCount() == 1) {\n+      logger.info(\"Received initial notification for routing table change from {}\", dcName);\n+      routingTableInitLatch.countDown();\n+    } else {\n+      logger.info(\"Routing table change triggered from {}\", dcName);\n+    }\n+    helixClusterManagerMetrics.routingTableChangeTriggerCount.inc();\n+  }\n+\n+  @Override\n+  public void setRoutingTableSnapshot(RoutingTableSnapshot routingTableSnapshot) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+  }\n+\n+  @Override\n+  public RoutingTableSnapshot getRoutingTableSnapshot() {\n+    return routingTableSnapshotRef.get();\n+  }\n+\n+  @Override\n+  public Map<AmbryDataNode, Set<AmbryDisk>> getDataNodeToDisksMap() {\n+    return Collections.unmodifiableMap(ambryDataNodeToAmbryDisks);\n+  }\n+\n+  @Override\n+  public AmbryDataNode getDataNode(String instanceName) {\n+    return instanceNameToAmbryDataNode.get(instanceName);\n+  }\n+\n+  @Override\n+  public AmbryReplica getReplicaId(AmbryDataNode ambryDataNode, String partitionName) {\n+    return ambryDataNodeToAmbryReplicas.getOrDefault(ambryDataNode, new ConcurrentHashMap<>()).get(partitionName);\n+  }\n+\n+  @Override\n+  public List<AmbryReplica> getReplicaIds(AmbryDataNode ambryDataNode) {\n+    return new ArrayList<>(ambryDataNodeToAmbryReplicas.get(ambryDataNode).values());\n+  }\n+\n+  @Override\n+  public List<AmbryDataNode> getAllDataNodes() {\n+    return new ArrayList<>(instanceNameToAmbryDataNode.values());\n+  }\n+\n+  @Override\n+  public Set<AmbryDisk> getDisks(AmbryDataNode ambryDataNode) {\n+    return ambryDataNodeToAmbryDisks.get(ambryDataNode);\n+  }\n+\n+  @Override\n+  public Map<String, String> getPartitionToResourceMap() {\n+    return Collections.unmodifiableMap(partitionNameToResource);\n+  }\n+\n+  @Override\n+  public long getErrorCount() {\n+    return errorCount.get();\n+  }\n+\n+  @Override\n+  public void waitForInitNotification() throws InterruptedException {\n+    // wait slightly more than 5 mins to ensure routerUpdater refreshes the snapshot.\n+    if (!routingTableInitLatch.await(320, TimeUnit.SECONDS)) {\n+      throw new IllegalStateException(\"Initial routing table change from \" + dcName + \" didn't come within 5 mins\");\n+    }\n+  }\n+\n+  /**\n+   * Add new instance or update existing instance based on {@link InstanceConfig}(s).\n+   * @param instanceConfigs the {@link InstanceConfig}(s) used to update in-mem cluster map.\n+   * @throws Exception\n+   */\n+  private void addOrUpdateInstanceInfos(List<InstanceConfig> instanceConfigs) throws Exception {\n+    for (InstanceConfig instanceConfig : instanceConfigs) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        logger.error(\"Unknown InstanceConfig schema version: {}, ignoring.\", schemaVersion);\n+        continue;\n+      }\n+      if (instanceNameToAmbryDataNode.containsKey(instanceConfig.getInstanceName())) {\n+        updateInstanceInfo(instanceConfig);\n+      } else {\n+        createNewInstance(instanceConfig);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Update info of an existing instance. This may happen in following cases: (1) new replica is added; (2) old replica\n+   * is removed; (3) replica's state has changed (i.e. becomes seal/unseal).\n+   * @param instanceConfig the {@link InstanceConfig} used to update info of instance.\n+   */\n+  private void updateInstanceInfo(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Updating replicas info for existing node {}\", instanceName);\n+    List<String> sealedReplicas = getSealedReplicas(instanceConfig);\n+    List<String> stoppedReplicas = getStoppedReplicas(instanceConfig);\n+    AmbryDataNode dataNode = instanceNameToAmbryDataNode.get(instanceName);\n+    ConcurrentHashMap<String, AmbryReplica> currentReplicasOnNode = ambryDataNodeToAmbryReplicas.get(dataNode);\n+    ConcurrentHashMap<String, AmbryReplica> replicasFromConfig = new ConcurrentHashMap<>();\n+    Map<String, Map<String, String>> diskInfos = instanceConfig.getRecord().getMapFields();\n+    for (Map.Entry<String, Map<String, String>> entry : diskInfos.entrySet()) {\n+      String mountPath = entry.getKey();\n+      Map<String, String> diskInfo = entry.getValue();\n+      Optional<AmbryDisk> potentialDisk =\n+          ambryDataNodeToAmbryDisks.get(dataNode).stream().filter(d -> d.getMountPath().equals(mountPath)).findFirst();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830d5f761ff7df4b57e1083a808121b1529b747"}, "originalPosition": 305}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQ2MTIxMQ==", "bodyText": "Good point. Let's create a map out of loop for now.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r380461211", "createdAt": "2020-02-18T05:28:36Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,500 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile boolean instanceConfigInitialized = false;\n+  private volatile boolean liveStateInitialized = false;\n+  private volatile boolean idealStateInitialized = false;\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  /**\n+   * Constructor for {@link DynamicClusterChangeHandler}\n+   * @param clusterMapConfig the {@link ClusterMapConfig} used to define some behavior of cluster change handler.\n+   * @param dcName the name of data center this handler is associated with.\n+   * @param selfInstanceName instance name of current node.\n+   * @param partitionOverrideInfoMap a map that records partitions and states they should be overridden to.\n+   * @param helixClusterManagerCallback a call back used to query cluster-wide info.\n+   * @param clusterChangeHandlerCallback a call back that allows current handler to update cluster-wide info.\n+   * @param helixClusterManagerMetrics metrics to keep track of changes and status of {@link HelixClusterManager}.\n+   * @param initializationFailureMap a map recording failure associate with each dc during initialization.\n+   * @param sealedStateChangeCounter a counter indicating if sealed state of any partition has changed.\n+   */\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * (The ZNode path of instance config in Helix is [AmbryClusterName]/CONFIGS/PARTICIPANT/[hostname_port])\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        } finally {\n+          instanceConfigInitialized = true;\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext) {\n+    if (!idealStateInitialized) {\n+      logger.info(\"Received initial notification for IdealState change from {}\", dcName);\n+      idealStateInitialized = true;\n+    } else {\n+      logger.info(\"IdealState change triggered from {}\", dcName);\n+    }\n+    logger.debug(\"Detailed ideal states in {} are: {}\", dcName, idealState);\n+    // rebuild the entire partition-to-resource map in current dc\n+    ConcurrentHashMap<String, String> partitionToResourceMap = new ConcurrentHashMap<>();\n+    for (IdealState state : idealState) {\n+      String resourceName = state.getResourceName();\n+      state.getPartitionSet().forEach(partitionName -> partitionToResourceMap.put(partitionName, resourceName));\n+    }\n+    partitionNameToResource = partitionToResourceMap;\n+    helixClusterManagerMetrics.idealStateChangeTriggerCount.inc();\n+  }\n+\n+  /**\n+   * Triggered whenever there is a change in the list of live instances.\n+   * @param liveInstances the list of all live instances (not a change set) at the time of this call.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onLiveInstanceChange(List<LiveInstance> liveInstances, NotificationContext changeContext) {\n+    try {\n+      if (!liveStateInitialized) {\n+        logger.info(\"Received initial notification for live instance change from {}\", dcName);\n+        liveStateInitialized = true;\n+      } else {\n+        logger.info(\"Live instance change triggered from {}\", dcName);\n+      }\n+      logger.debug(\"Detailed live instances in {} are: {}\", dcName, liveInstances);\n+      synchronized (notificationLock) {\n+        updateInstanceLiveness(liveInstances);\n+        helixClusterManagerMetrics.liveInstanceChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the state of replica in cluster has changed. The snapshot contains up-to-date state of all\n+   * resources(replicas) in this data center.\n+   * @param routingTableSnapshot a snapshot of routing table for this data center.\n+   * @param context additional context associated with this change.\n+   */\n+  @Override\n+  public void onRoutingTableChange(RoutingTableSnapshot routingTableSnapshot, Object context) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+    if (routingTableInitLatch.getCount() == 1) {\n+      logger.info(\"Received initial notification for routing table change from {}\", dcName);\n+      routingTableInitLatch.countDown();\n+    } else {\n+      logger.info(\"Routing table change triggered from {}\", dcName);\n+    }\n+    helixClusterManagerMetrics.routingTableChangeTriggerCount.inc();\n+  }\n+\n+  @Override\n+  public void setRoutingTableSnapshot(RoutingTableSnapshot routingTableSnapshot) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+  }\n+\n+  @Override\n+  public RoutingTableSnapshot getRoutingTableSnapshot() {\n+    return routingTableSnapshotRef.get();\n+  }\n+\n+  @Override\n+  public Map<AmbryDataNode, Set<AmbryDisk>> getDataNodeToDisksMap() {\n+    return Collections.unmodifiableMap(ambryDataNodeToAmbryDisks);\n+  }\n+\n+  @Override\n+  public AmbryDataNode getDataNode(String instanceName) {\n+    return instanceNameToAmbryDataNode.get(instanceName);\n+  }\n+\n+  @Override\n+  public AmbryReplica getReplicaId(AmbryDataNode ambryDataNode, String partitionName) {\n+    return ambryDataNodeToAmbryReplicas.getOrDefault(ambryDataNode, new ConcurrentHashMap<>()).get(partitionName);\n+  }\n+\n+  @Override\n+  public List<AmbryReplica> getReplicaIds(AmbryDataNode ambryDataNode) {\n+    return new ArrayList<>(ambryDataNodeToAmbryReplicas.get(ambryDataNode).values());\n+  }\n+\n+  @Override\n+  public List<AmbryDataNode> getAllDataNodes() {\n+    return new ArrayList<>(instanceNameToAmbryDataNode.values());\n+  }\n+\n+  @Override\n+  public Set<AmbryDisk> getDisks(AmbryDataNode ambryDataNode) {\n+    return ambryDataNodeToAmbryDisks.get(ambryDataNode);\n+  }\n+\n+  @Override\n+  public Map<String, String> getPartitionToResourceMap() {\n+    return Collections.unmodifiableMap(partitionNameToResource);\n+  }\n+\n+  @Override\n+  public long getErrorCount() {\n+    return errorCount.get();\n+  }\n+\n+  @Override\n+  public void waitForInitNotification() throws InterruptedException {\n+    // wait slightly more than 5 mins to ensure routerUpdater refreshes the snapshot.\n+    if (!routingTableInitLatch.await(320, TimeUnit.SECONDS)) {\n+      throw new IllegalStateException(\"Initial routing table change from \" + dcName + \" didn't come within 5 mins\");\n+    }\n+  }\n+\n+  /**\n+   * Add new instance or update existing instance based on {@link InstanceConfig}(s).\n+   * @param instanceConfigs the {@link InstanceConfig}(s) used to update in-mem cluster map.\n+   * @throws Exception\n+   */\n+  private void addOrUpdateInstanceInfos(List<InstanceConfig> instanceConfigs) throws Exception {\n+    for (InstanceConfig instanceConfig : instanceConfigs) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        logger.error(\"Unknown InstanceConfig schema version: {}, ignoring.\", schemaVersion);\n+        continue;\n+      }\n+      if (instanceNameToAmbryDataNode.containsKey(instanceConfig.getInstanceName())) {\n+        updateInstanceInfo(instanceConfig);\n+      } else {\n+        createNewInstance(instanceConfig);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Update info of an existing instance. This may happen in following cases: (1) new replica is added; (2) old replica\n+   * is removed; (3) replica's state has changed (i.e. becomes seal/unseal).\n+   * @param instanceConfig the {@link InstanceConfig} used to update info of instance.\n+   */\n+  private void updateInstanceInfo(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Updating replicas info for existing node {}\", instanceName);\n+    List<String> sealedReplicas = getSealedReplicas(instanceConfig);\n+    List<String> stoppedReplicas = getStoppedReplicas(instanceConfig);\n+    AmbryDataNode dataNode = instanceNameToAmbryDataNode.get(instanceName);\n+    ConcurrentHashMap<String, AmbryReplica> currentReplicasOnNode = ambryDataNodeToAmbryReplicas.get(dataNode);\n+    ConcurrentHashMap<String, AmbryReplica> replicasFromConfig = new ConcurrentHashMap<>();\n+    Map<String, Map<String, String>> diskInfos = instanceConfig.getRecord().getMapFields();\n+    for (Map.Entry<String, Map<String, String>> entry : diskInfos.entrySet()) {\n+      String mountPath = entry.getKey();\n+      Map<String, String> diskInfo = entry.getValue();\n+      Optional<AmbryDisk> potentialDisk =\n+          ambryDataNodeToAmbryDisks.get(dataNode).stream().filter(d -> d.getMountPath().equals(mountPath)).findFirst();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAzNTA0OQ=="}, "originalCommit": {"oid": "3830d5f761ff7df4b57e1083a808121b1529b747"}, "originalPosition": 305}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODg1OTQxOnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwNToyMjoxN1rOFoiEOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwNTozNDo1M1rOFq1lhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA0NTQ5Nw==", "bodyText": "General question: What happens when exceptions get thrown inside of helix listener callbacks? Would future notifications be received?\nThrowing at this point in the middle of the loop could create some inconsistency since clusterChangeHandlerCallback.addReplicasToPartition is called on every iteration of the loop, but ambryDataNodeToAmbryReplicas is updated after the loop. If this is an issue, you could move the clusterChangeHandlerCallback.addReplicasToPartition call to after the loop.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r378045497", "createdAt": "2020-02-12T05:22:17Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,500 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile boolean instanceConfigInitialized = false;\n+  private volatile boolean liveStateInitialized = false;\n+  private volatile boolean idealStateInitialized = false;\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  /**\n+   * Constructor for {@link DynamicClusterChangeHandler}\n+   * @param clusterMapConfig the {@link ClusterMapConfig} used to define some behavior of cluster change handler.\n+   * @param dcName the name of data center this handler is associated with.\n+   * @param selfInstanceName instance name of current node.\n+   * @param partitionOverrideInfoMap a map that records partitions and states they should be overridden to.\n+   * @param helixClusterManagerCallback a call back used to query cluster-wide info.\n+   * @param clusterChangeHandlerCallback a call back that allows current handler to update cluster-wide info.\n+   * @param helixClusterManagerMetrics metrics to keep track of changes and status of {@link HelixClusterManager}.\n+   * @param initializationFailureMap a map recording failure associate with each dc during initialization.\n+   * @param sealedStateChangeCounter a counter indicating if sealed state of any partition has changed.\n+   */\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * (The ZNode path of instance config in Helix is [AmbryClusterName]/CONFIGS/PARTICIPANT/[hostname_port])\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        } finally {\n+          instanceConfigInitialized = true;\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext) {\n+    if (!idealStateInitialized) {\n+      logger.info(\"Received initial notification for IdealState change from {}\", dcName);\n+      idealStateInitialized = true;\n+    } else {\n+      logger.info(\"IdealState change triggered from {}\", dcName);\n+    }\n+    logger.debug(\"Detailed ideal states in {} are: {}\", dcName, idealState);\n+    // rebuild the entire partition-to-resource map in current dc\n+    ConcurrentHashMap<String, String> partitionToResourceMap = new ConcurrentHashMap<>();\n+    for (IdealState state : idealState) {\n+      String resourceName = state.getResourceName();\n+      state.getPartitionSet().forEach(partitionName -> partitionToResourceMap.put(partitionName, resourceName));\n+    }\n+    partitionNameToResource = partitionToResourceMap;\n+    helixClusterManagerMetrics.idealStateChangeTriggerCount.inc();\n+  }\n+\n+  /**\n+   * Triggered whenever there is a change in the list of live instances.\n+   * @param liveInstances the list of all live instances (not a change set) at the time of this call.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onLiveInstanceChange(List<LiveInstance> liveInstances, NotificationContext changeContext) {\n+    try {\n+      if (!liveStateInitialized) {\n+        logger.info(\"Received initial notification for live instance change from {}\", dcName);\n+        liveStateInitialized = true;\n+      } else {\n+        logger.info(\"Live instance change triggered from {}\", dcName);\n+      }\n+      logger.debug(\"Detailed live instances in {} are: {}\", dcName, liveInstances);\n+      synchronized (notificationLock) {\n+        updateInstanceLiveness(liveInstances);\n+        helixClusterManagerMetrics.liveInstanceChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the state of replica in cluster has changed. The snapshot contains up-to-date state of all\n+   * resources(replicas) in this data center.\n+   * @param routingTableSnapshot a snapshot of routing table for this data center.\n+   * @param context additional context associated with this change.\n+   */\n+  @Override\n+  public void onRoutingTableChange(RoutingTableSnapshot routingTableSnapshot, Object context) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+    if (routingTableInitLatch.getCount() == 1) {\n+      logger.info(\"Received initial notification for routing table change from {}\", dcName);\n+      routingTableInitLatch.countDown();\n+    } else {\n+      logger.info(\"Routing table change triggered from {}\", dcName);\n+    }\n+    helixClusterManagerMetrics.routingTableChangeTriggerCount.inc();\n+  }\n+\n+  @Override\n+  public void setRoutingTableSnapshot(RoutingTableSnapshot routingTableSnapshot) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+  }\n+\n+  @Override\n+  public RoutingTableSnapshot getRoutingTableSnapshot() {\n+    return routingTableSnapshotRef.get();\n+  }\n+\n+  @Override\n+  public Map<AmbryDataNode, Set<AmbryDisk>> getDataNodeToDisksMap() {\n+    return Collections.unmodifiableMap(ambryDataNodeToAmbryDisks);\n+  }\n+\n+  @Override\n+  public AmbryDataNode getDataNode(String instanceName) {\n+    return instanceNameToAmbryDataNode.get(instanceName);\n+  }\n+\n+  @Override\n+  public AmbryReplica getReplicaId(AmbryDataNode ambryDataNode, String partitionName) {\n+    return ambryDataNodeToAmbryReplicas.getOrDefault(ambryDataNode, new ConcurrentHashMap<>()).get(partitionName);\n+  }\n+\n+  @Override\n+  public List<AmbryReplica> getReplicaIds(AmbryDataNode ambryDataNode) {\n+    return new ArrayList<>(ambryDataNodeToAmbryReplicas.get(ambryDataNode).values());\n+  }\n+\n+  @Override\n+  public List<AmbryDataNode> getAllDataNodes() {\n+    return new ArrayList<>(instanceNameToAmbryDataNode.values());\n+  }\n+\n+  @Override\n+  public Set<AmbryDisk> getDisks(AmbryDataNode ambryDataNode) {\n+    return ambryDataNodeToAmbryDisks.get(ambryDataNode);\n+  }\n+\n+  @Override\n+  public Map<String, String> getPartitionToResourceMap() {\n+    return Collections.unmodifiableMap(partitionNameToResource);\n+  }\n+\n+  @Override\n+  public long getErrorCount() {\n+    return errorCount.get();\n+  }\n+\n+  @Override\n+  public void waitForInitNotification() throws InterruptedException {\n+    // wait slightly more than 5 mins to ensure routerUpdater refreshes the snapshot.\n+    if (!routingTableInitLatch.await(320, TimeUnit.SECONDS)) {\n+      throw new IllegalStateException(\"Initial routing table change from \" + dcName + \" didn't come within 5 mins\");\n+    }\n+  }\n+\n+  /**\n+   * Add new instance or update existing instance based on {@link InstanceConfig}(s).\n+   * @param instanceConfigs the {@link InstanceConfig}(s) used to update in-mem cluster map.\n+   * @throws Exception\n+   */\n+  private void addOrUpdateInstanceInfos(List<InstanceConfig> instanceConfigs) throws Exception {\n+    for (InstanceConfig instanceConfig : instanceConfigs) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        logger.error(\"Unknown InstanceConfig schema version: {}, ignoring.\", schemaVersion);\n+        continue;\n+      }\n+      if (instanceNameToAmbryDataNode.containsKey(instanceConfig.getInstanceName())) {\n+        updateInstanceInfo(instanceConfig);\n+      } else {\n+        createNewInstance(instanceConfig);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Update info of an existing instance. This may happen in following cases: (1) new replica is added; (2) old replica\n+   * is removed; (3) replica's state has changed (i.e. becomes seal/unseal).\n+   * @param instanceConfig the {@link InstanceConfig} used to update info of instance.\n+   */\n+  private void updateInstanceInfo(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Updating replicas info for existing node {}\", instanceName);\n+    List<String> sealedReplicas = getSealedReplicas(instanceConfig);\n+    List<String> stoppedReplicas = getStoppedReplicas(instanceConfig);\n+    AmbryDataNode dataNode = instanceNameToAmbryDataNode.get(instanceName);\n+    ConcurrentHashMap<String, AmbryReplica> currentReplicasOnNode = ambryDataNodeToAmbryReplicas.get(dataNode);\n+    ConcurrentHashMap<String, AmbryReplica> replicasFromConfig = new ConcurrentHashMap<>();\n+    Map<String, Map<String, String>> diskInfos = instanceConfig.getRecord().getMapFields();\n+    for (Map.Entry<String, Map<String, String>> entry : diskInfos.entrySet()) {\n+      String mountPath = entry.getKey();\n+      Map<String, String> diskInfo = entry.getValue();\n+      Optional<AmbryDisk> potentialDisk =\n+          ambryDataNodeToAmbryDisks.get(dataNode).stream().filter(d -> d.getMountPath().equals(mountPath)).findFirst();\n+      AmbryDisk disk = potentialDisk.orElse(null);\n+      if (disk == null) {\n+        logger.info(\"Temporarily don't support adding new disk to existing node.\");\n+        // TODO support dynamically adding disk in the future\n+        continue;\n+      }\n+      String replicasStr = diskInfo.get(ClusterMapUtils.REPLICAS_STR);\n+      if (!replicasStr.isEmpty()) {\n+        for (String replicaInfo : replicasStr.split(ClusterMapUtils.REPLICAS_DELIM_STR)) {\n+          String[] info = replicaInfo.split(ClusterMapUtils.REPLICAS_STR_SEPARATOR);\n+          // partition name and replica name are the same.\n+          String partitionName = info[0];\n+          if (currentReplicasOnNode.containsKey(partitionName)) {\n+            // if replica is already present\n+            AmbryReplica existingReplica = currentReplicasOnNode.get(partitionName);\n+            // 1. directly add it into \"replicasFromConfig\" map\n+            replicasFromConfig.put(partitionName, existingReplica);\n+            // 2. update replica seal/stop state\n+            updateReplicaStateAndOverrideIfNeeded(existingReplica, sealedReplicas, stoppedReplicas);\n+          } else {\n+            // if this is a new replica and doesn't exist on node\n+            logger.info(\"Adding new replica {} to existing node {}\", partitionName, instanceName);\n+            long replicaCapacity = Long.valueOf(info[1]);\n+            String partitionClass = info.length > 2 ? info[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+            // this can be a brand new partition that is added to an existing node\n+            AmbryPartition mappedPartition =\n+                new AmbryPartition(Long.valueOf(partitionName), partitionClass, helixClusterManagerCallback);\n+            // Ensure only one AmbryPartition instance exists for specific partition.\n+            mappedPartition = clusterChangeHandlerCallback.addPartitionIfAbsent(mappedPartition, replicaCapacity);\n+            ensurePartitionAbsenceOnNodeAndValidateCapacity(mappedPartition, dataNode, replicaCapacity);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830d5f761ff7df4b57e1083a808121b1529b747"}, "originalPosition": 335}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQ2MjQ2OA==", "bodyText": "I need to check with Helix team. Yeah, we need to move addReplicasToPartition out of loop.", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r380462468", "createdAt": "2020-02-18T05:34:53Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,500 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile boolean instanceConfigInitialized = false;\n+  private volatile boolean liveStateInitialized = false;\n+  private volatile boolean idealStateInitialized = false;\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  /**\n+   * Constructor for {@link DynamicClusterChangeHandler}\n+   * @param clusterMapConfig the {@link ClusterMapConfig} used to define some behavior of cluster change handler.\n+   * @param dcName the name of data center this handler is associated with.\n+   * @param selfInstanceName instance name of current node.\n+   * @param partitionOverrideInfoMap a map that records partitions and states they should be overridden to.\n+   * @param helixClusterManagerCallback a call back used to query cluster-wide info.\n+   * @param clusterChangeHandlerCallback a call back that allows current handler to update cluster-wide info.\n+   * @param helixClusterManagerMetrics metrics to keep track of changes and status of {@link HelixClusterManager}.\n+   * @param initializationFailureMap a map recording failure associate with each dc during initialization.\n+   * @param sealedStateChangeCounter a counter indicating if sealed state of any partition has changed.\n+   */\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * (The ZNode path of instance config in Helix is [AmbryClusterName]/CONFIGS/PARTICIPANT/[hostname_port])\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        } finally {\n+          instanceConfigInitialized = true;\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext) {\n+    if (!idealStateInitialized) {\n+      logger.info(\"Received initial notification for IdealState change from {}\", dcName);\n+      idealStateInitialized = true;\n+    } else {\n+      logger.info(\"IdealState change triggered from {}\", dcName);\n+    }\n+    logger.debug(\"Detailed ideal states in {} are: {}\", dcName, idealState);\n+    // rebuild the entire partition-to-resource map in current dc\n+    ConcurrentHashMap<String, String> partitionToResourceMap = new ConcurrentHashMap<>();\n+    for (IdealState state : idealState) {\n+      String resourceName = state.getResourceName();\n+      state.getPartitionSet().forEach(partitionName -> partitionToResourceMap.put(partitionName, resourceName));\n+    }\n+    partitionNameToResource = partitionToResourceMap;\n+    helixClusterManagerMetrics.idealStateChangeTriggerCount.inc();\n+  }\n+\n+  /**\n+   * Triggered whenever there is a change in the list of live instances.\n+   * @param liveInstances the list of all live instances (not a change set) at the time of this call.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onLiveInstanceChange(List<LiveInstance> liveInstances, NotificationContext changeContext) {\n+    try {\n+      if (!liveStateInitialized) {\n+        logger.info(\"Received initial notification for live instance change from {}\", dcName);\n+        liveStateInitialized = true;\n+      } else {\n+        logger.info(\"Live instance change triggered from {}\", dcName);\n+      }\n+      logger.debug(\"Detailed live instances in {} are: {}\", dcName, liveInstances);\n+      synchronized (notificationLock) {\n+        updateInstanceLiveness(liveInstances);\n+        helixClusterManagerMetrics.liveInstanceChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the state of replica in cluster has changed. The snapshot contains up-to-date state of all\n+   * resources(replicas) in this data center.\n+   * @param routingTableSnapshot a snapshot of routing table for this data center.\n+   * @param context additional context associated with this change.\n+   */\n+  @Override\n+  public void onRoutingTableChange(RoutingTableSnapshot routingTableSnapshot, Object context) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+    if (routingTableInitLatch.getCount() == 1) {\n+      logger.info(\"Received initial notification for routing table change from {}\", dcName);\n+      routingTableInitLatch.countDown();\n+    } else {\n+      logger.info(\"Routing table change triggered from {}\", dcName);\n+    }\n+    helixClusterManagerMetrics.routingTableChangeTriggerCount.inc();\n+  }\n+\n+  @Override\n+  public void setRoutingTableSnapshot(RoutingTableSnapshot routingTableSnapshot) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+  }\n+\n+  @Override\n+  public RoutingTableSnapshot getRoutingTableSnapshot() {\n+    return routingTableSnapshotRef.get();\n+  }\n+\n+  @Override\n+  public Map<AmbryDataNode, Set<AmbryDisk>> getDataNodeToDisksMap() {\n+    return Collections.unmodifiableMap(ambryDataNodeToAmbryDisks);\n+  }\n+\n+  @Override\n+  public AmbryDataNode getDataNode(String instanceName) {\n+    return instanceNameToAmbryDataNode.get(instanceName);\n+  }\n+\n+  @Override\n+  public AmbryReplica getReplicaId(AmbryDataNode ambryDataNode, String partitionName) {\n+    return ambryDataNodeToAmbryReplicas.getOrDefault(ambryDataNode, new ConcurrentHashMap<>()).get(partitionName);\n+  }\n+\n+  @Override\n+  public List<AmbryReplica> getReplicaIds(AmbryDataNode ambryDataNode) {\n+    return new ArrayList<>(ambryDataNodeToAmbryReplicas.get(ambryDataNode).values());\n+  }\n+\n+  @Override\n+  public List<AmbryDataNode> getAllDataNodes() {\n+    return new ArrayList<>(instanceNameToAmbryDataNode.values());\n+  }\n+\n+  @Override\n+  public Set<AmbryDisk> getDisks(AmbryDataNode ambryDataNode) {\n+    return ambryDataNodeToAmbryDisks.get(ambryDataNode);\n+  }\n+\n+  @Override\n+  public Map<String, String> getPartitionToResourceMap() {\n+    return Collections.unmodifiableMap(partitionNameToResource);\n+  }\n+\n+  @Override\n+  public long getErrorCount() {\n+    return errorCount.get();\n+  }\n+\n+  @Override\n+  public void waitForInitNotification() throws InterruptedException {\n+    // wait slightly more than 5 mins to ensure routerUpdater refreshes the snapshot.\n+    if (!routingTableInitLatch.await(320, TimeUnit.SECONDS)) {\n+      throw new IllegalStateException(\"Initial routing table change from \" + dcName + \" didn't come within 5 mins\");\n+    }\n+  }\n+\n+  /**\n+   * Add new instance or update existing instance based on {@link InstanceConfig}(s).\n+   * @param instanceConfigs the {@link InstanceConfig}(s) used to update in-mem cluster map.\n+   * @throws Exception\n+   */\n+  private void addOrUpdateInstanceInfos(List<InstanceConfig> instanceConfigs) throws Exception {\n+    for (InstanceConfig instanceConfig : instanceConfigs) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        logger.error(\"Unknown InstanceConfig schema version: {}, ignoring.\", schemaVersion);\n+        continue;\n+      }\n+      if (instanceNameToAmbryDataNode.containsKey(instanceConfig.getInstanceName())) {\n+        updateInstanceInfo(instanceConfig);\n+      } else {\n+        createNewInstance(instanceConfig);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Update info of an existing instance. This may happen in following cases: (1) new replica is added; (2) old replica\n+   * is removed; (3) replica's state has changed (i.e. becomes seal/unseal).\n+   * @param instanceConfig the {@link InstanceConfig} used to update info of instance.\n+   */\n+  private void updateInstanceInfo(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Updating replicas info for existing node {}\", instanceName);\n+    List<String> sealedReplicas = getSealedReplicas(instanceConfig);\n+    List<String> stoppedReplicas = getStoppedReplicas(instanceConfig);\n+    AmbryDataNode dataNode = instanceNameToAmbryDataNode.get(instanceName);\n+    ConcurrentHashMap<String, AmbryReplica> currentReplicasOnNode = ambryDataNodeToAmbryReplicas.get(dataNode);\n+    ConcurrentHashMap<String, AmbryReplica> replicasFromConfig = new ConcurrentHashMap<>();\n+    Map<String, Map<String, String>> diskInfos = instanceConfig.getRecord().getMapFields();\n+    for (Map.Entry<String, Map<String, String>> entry : diskInfos.entrySet()) {\n+      String mountPath = entry.getKey();\n+      Map<String, String> diskInfo = entry.getValue();\n+      Optional<AmbryDisk> potentialDisk =\n+          ambryDataNodeToAmbryDisks.get(dataNode).stream().filter(d -> d.getMountPath().equals(mountPath)).findFirst();\n+      AmbryDisk disk = potentialDisk.orElse(null);\n+      if (disk == null) {\n+        logger.info(\"Temporarily don't support adding new disk to existing node.\");\n+        // TODO support dynamically adding disk in the future\n+        continue;\n+      }\n+      String replicasStr = diskInfo.get(ClusterMapUtils.REPLICAS_STR);\n+      if (!replicasStr.isEmpty()) {\n+        for (String replicaInfo : replicasStr.split(ClusterMapUtils.REPLICAS_DELIM_STR)) {\n+          String[] info = replicaInfo.split(ClusterMapUtils.REPLICAS_STR_SEPARATOR);\n+          // partition name and replica name are the same.\n+          String partitionName = info[0];\n+          if (currentReplicasOnNode.containsKey(partitionName)) {\n+            // if replica is already present\n+            AmbryReplica existingReplica = currentReplicasOnNode.get(partitionName);\n+            // 1. directly add it into \"replicasFromConfig\" map\n+            replicasFromConfig.put(partitionName, existingReplica);\n+            // 2. update replica seal/stop state\n+            updateReplicaStateAndOverrideIfNeeded(existingReplica, sealedReplicas, stoppedReplicas);\n+          } else {\n+            // if this is a new replica and doesn't exist on node\n+            logger.info(\"Adding new replica {} to existing node {}\", partitionName, instanceName);\n+            long replicaCapacity = Long.valueOf(info[1]);\n+            String partitionClass = info.length > 2 ? info[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+            // this can be a brand new partition that is added to an existing node\n+            AmbryPartition mappedPartition =\n+                new AmbryPartition(Long.valueOf(partitionName), partitionClass, helixClusterManagerCallback);\n+            // Ensure only one AmbryPartition instance exists for specific partition.\n+            mappedPartition = clusterChangeHandlerCallback.addPartitionIfAbsent(mappedPartition, replicaCapacity);\n+            ensurePartitionAbsenceOnNodeAndValidateCapacity(mappedPartition, dataNode, replicaCapacity);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA0NTQ5Nw=="}, "originalCommit": {"oid": "3830d5f761ff7df4b57e1083a808121b1529b747"}, "originalPosition": 335}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODg2ODQzOnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwNToyOTozMlrOFoiJww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwNjowMDozM1rOFq16bQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA0NjkxNQ==", "bodyText": "could this be done with partition IDs without the copy?\ncurrentReplicasOnNode.keySet()\n  .stream()\n  .filter(partitionId -> !replicasFromConfig.containsKey(partitionId))\n  .forEach(do delete);", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r378046915", "createdAt": "2020-02-12T05:29:32Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,500 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile boolean instanceConfigInitialized = false;\n+  private volatile boolean liveStateInitialized = false;\n+  private volatile boolean idealStateInitialized = false;\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  /**\n+   * Constructor for {@link DynamicClusterChangeHandler}\n+   * @param clusterMapConfig the {@link ClusterMapConfig} used to define some behavior of cluster change handler.\n+   * @param dcName the name of data center this handler is associated with.\n+   * @param selfInstanceName instance name of current node.\n+   * @param partitionOverrideInfoMap a map that records partitions and states they should be overridden to.\n+   * @param helixClusterManagerCallback a call back used to query cluster-wide info.\n+   * @param clusterChangeHandlerCallback a call back that allows current handler to update cluster-wide info.\n+   * @param helixClusterManagerMetrics metrics to keep track of changes and status of {@link HelixClusterManager}.\n+   * @param initializationFailureMap a map recording failure associate with each dc during initialization.\n+   * @param sealedStateChangeCounter a counter indicating if sealed state of any partition has changed.\n+   */\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * (The ZNode path of instance config in Helix is [AmbryClusterName]/CONFIGS/PARTICIPANT/[hostname_port])\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        } finally {\n+          instanceConfigInitialized = true;\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext) {\n+    if (!idealStateInitialized) {\n+      logger.info(\"Received initial notification for IdealState change from {}\", dcName);\n+      idealStateInitialized = true;\n+    } else {\n+      logger.info(\"IdealState change triggered from {}\", dcName);\n+    }\n+    logger.debug(\"Detailed ideal states in {} are: {}\", dcName, idealState);\n+    // rebuild the entire partition-to-resource map in current dc\n+    ConcurrentHashMap<String, String> partitionToResourceMap = new ConcurrentHashMap<>();\n+    for (IdealState state : idealState) {\n+      String resourceName = state.getResourceName();\n+      state.getPartitionSet().forEach(partitionName -> partitionToResourceMap.put(partitionName, resourceName));\n+    }\n+    partitionNameToResource = partitionToResourceMap;\n+    helixClusterManagerMetrics.idealStateChangeTriggerCount.inc();\n+  }\n+\n+  /**\n+   * Triggered whenever there is a change in the list of live instances.\n+   * @param liveInstances the list of all live instances (not a change set) at the time of this call.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onLiveInstanceChange(List<LiveInstance> liveInstances, NotificationContext changeContext) {\n+    try {\n+      if (!liveStateInitialized) {\n+        logger.info(\"Received initial notification for live instance change from {}\", dcName);\n+        liveStateInitialized = true;\n+      } else {\n+        logger.info(\"Live instance change triggered from {}\", dcName);\n+      }\n+      logger.debug(\"Detailed live instances in {} are: {}\", dcName, liveInstances);\n+      synchronized (notificationLock) {\n+        updateInstanceLiveness(liveInstances);\n+        helixClusterManagerMetrics.liveInstanceChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the state of replica in cluster has changed. The snapshot contains up-to-date state of all\n+   * resources(replicas) in this data center.\n+   * @param routingTableSnapshot a snapshot of routing table for this data center.\n+   * @param context additional context associated with this change.\n+   */\n+  @Override\n+  public void onRoutingTableChange(RoutingTableSnapshot routingTableSnapshot, Object context) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+    if (routingTableInitLatch.getCount() == 1) {\n+      logger.info(\"Received initial notification for routing table change from {}\", dcName);\n+      routingTableInitLatch.countDown();\n+    } else {\n+      logger.info(\"Routing table change triggered from {}\", dcName);\n+    }\n+    helixClusterManagerMetrics.routingTableChangeTriggerCount.inc();\n+  }\n+\n+  @Override\n+  public void setRoutingTableSnapshot(RoutingTableSnapshot routingTableSnapshot) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+  }\n+\n+  @Override\n+  public RoutingTableSnapshot getRoutingTableSnapshot() {\n+    return routingTableSnapshotRef.get();\n+  }\n+\n+  @Override\n+  public Map<AmbryDataNode, Set<AmbryDisk>> getDataNodeToDisksMap() {\n+    return Collections.unmodifiableMap(ambryDataNodeToAmbryDisks);\n+  }\n+\n+  @Override\n+  public AmbryDataNode getDataNode(String instanceName) {\n+    return instanceNameToAmbryDataNode.get(instanceName);\n+  }\n+\n+  @Override\n+  public AmbryReplica getReplicaId(AmbryDataNode ambryDataNode, String partitionName) {\n+    return ambryDataNodeToAmbryReplicas.getOrDefault(ambryDataNode, new ConcurrentHashMap<>()).get(partitionName);\n+  }\n+\n+  @Override\n+  public List<AmbryReplica> getReplicaIds(AmbryDataNode ambryDataNode) {\n+    return new ArrayList<>(ambryDataNodeToAmbryReplicas.get(ambryDataNode).values());\n+  }\n+\n+  @Override\n+  public List<AmbryDataNode> getAllDataNodes() {\n+    return new ArrayList<>(instanceNameToAmbryDataNode.values());\n+  }\n+\n+  @Override\n+  public Set<AmbryDisk> getDisks(AmbryDataNode ambryDataNode) {\n+    return ambryDataNodeToAmbryDisks.get(ambryDataNode);\n+  }\n+\n+  @Override\n+  public Map<String, String> getPartitionToResourceMap() {\n+    return Collections.unmodifiableMap(partitionNameToResource);\n+  }\n+\n+  @Override\n+  public long getErrorCount() {\n+    return errorCount.get();\n+  }\n+\n+  @Override\n+  public void waitForInitNotification() throws InterruptedException {\n+    // wait slightly more than 5 mins to ensure routerUpdater refreshes the snapshot.\n+    if (!routingTableInitLatch.await(320, TimeUnit.SECONDS)) {\n+      throw new IllegalStateException(\"Initial routing table change from \" + dcName + \" didn't come within 5 mins\");\n+    }\n+  }\n+\n+  /**\n+   * Add new instance or update existing instance based on {@link InstanceConfig}(s).\n+   * @param instanceConfigs the {@link InstanceConfig}(s) used to update in-mem cluster map.\n+   * @throws Exception\n+   */\n+  private void addOrUpdateInstanceInfos(List<InstanceConfig> instanceConfigs) throws Exception {\n+    for (InstanceConfig instanceConfig : instanceConfigs) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        logger.error(\"Unknown InstanceConfig schema version: {}, ignoring.\", schemaVersion);\n+        continue;\n+      }\n+      if (instanceNameToAmbryDataNode.containsKey(instanceConfig.getInstanceName())) {\n+        updateInstanceInfo(instanceConfig);\n+      } else {\n+        createNewInstance(instanceConfig);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Update info of an existing instance. This may happen in following cases: (1) new replica is added; (2) old replica\n+   * is removed; (3) replica's state has changed (i.e. becomes seal/unseal).\n+   * @param instanceConfig the {@link InstanceConfig} used to update info of instance.\n+   */\n+  private void updateInstanceInfo(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Updating replicas info for existing node {}\", instanceName);\n+    List<String> sealedReplicas = getSealedReplicas(instanceConfig);\n+    List<String> stoppedReplicas = getStoppedReplicas(instanceConfig);\n+    AmbryDataNode dataNode = instanceNameToAmbryDataNode.get(instanceName);\n+    ConcurrentHashMap<String, AmbryReplica> currentReplicasOnNode = ambryDataNodeToAmbryReplicas.get(dataNode);\n+    ConcurrentHashMap<String, AmbryReplica> replicasFromConfig = new ConcurrentHashMap<>();\n+    Map<String, Map<String, String>> diskInfos = instanceConfig.getRecord().getMapFields();\n+    for (Map.Entry<String, Map<String, String>> entry : diskInfos.entrySet()) {\n+      String mountPath = entry.getKey();\n+      Map<String, String> diskInfo = entry.getValue();\n+      Optional<AmbryDisk> potentialDisk =\n+          ambryDataNodeToAmbryDisks.get(dataNode).stream().filter(d -> d.getMountPath().equals(mountPath)).findFirst();\n+      AmbryDisk disk = potentialDisk.orElse(null);\n+      if (disk == null) {\n+        logger.info(\"Temporarily don't support adding new disk to existing node.\");\n+        // TODO support dynamically adding disk in the future\n+        continue;\n+      }\n+      String replicasStr = diskInfo.get(ClusterMapUtils.REPLICAS_STR);\n+      if (!replicasStr.isEmpty()) {\n+        for (String replicaInfo : replicasStr.split(ClusterMapUtils.REPLICAS_DELIM_STR)) {\n+          String[] info = replicaInfo.split(ClusterMapUtils.REPLICAS_STR_SEPARATOR);\n+          // partition name and replica name are the same.\n+          String partitionName = info[0];\n+          if (currentReplicasOnNode.containsKey(partitionName)) {\n+            // if replica is already present\n+            AmbryReplica existingReplica = currentReplicasOnNode.get(partitionName);\n+            // 1. directly add it into \"replicasFromConfig\" map\n+            replicasFromConfig.put(partitionName, existingReplica);\n+            // 2. update replica seal/stop state\n+            updateReplicaStateAndOverrideIfNeeded(existingReplica, sealedReplicas, stoppedReplicas);\n+          } else {\n+            // if this is a new replica and doesn't exist on node\n+            logger.info(\"Adding new replica {} to existing node {}\", partitionName, instanceName);\n+            long replicaCapacity = Long.valueOf(info[1]);\n+            String partitionClass = info.length > 2 ? info[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+            // this can be a brand new partition that is added to an existing node\n+            AmbryPartition mappedPartition =\n+                new AmbryPartition(Long.valueOf(partitionName), partitionClass, helixClusterManagerCallback);\n+            // Ensure only one AmbryPartition instance exists for specific partition.\n+            mappedPartition = clusterChangeHandlerCallback.addPartitionIfAbsent(mappedPartition, replicaCapacity);\n+            ensurePartitionAbsenceOnNodeAndValidateCapacity(mappedPartition, dataNode, replicaCapacity);\n+            // create new replica belonging to this partition\n+            AmbryReplica replica =\n+                new AmbryReplica(clusterMapConfig, mappedPartition, disk, stoppedReplicas.contains(partitionName),\n+                    replicaCapacity, sealedReplicas.contains(partitionName));\n+            updateReplicaStateAndOverrideIfNeeded(replica, sealedReplicas, stoppedReplicas);\n+            // add new created replica to \"replicasFromConfig\" map\n+            replicasFromConfig.put(partitionName, replica);\n+            // add new replica to specific partition\n+            clusterChangeHandlerCallback.addReplicasToPartition(mappedPartition, Collections.singletonList(replica));\n+          }\n+        }\n+      }\n+    }\n+    // update ambryDataNodeToAmbryReplicas map by adding \"replicasFromConfig\"\n+    ambryDataNodeToAmbryReplicas.put(instanceNameToAmbryDataNode.get(instanceName), replicasFromConfig);\n+    // compare replicasFromConfig with current replica set to derive old replicas that are removed\n+    Set<AmbryReplica> previousReplicas = new HashSet<>(currentReplicasOnNode.values());\n+    previousReplicas.removeAll(replicasFromConfig.values());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830d5f761ff7df4b57e1083a808121b1529b747"}, "originalPosition": 353}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQ2NzgyMQ==", "bodyText": "Changed", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r380467821", "createdAt": "2020-02-18T06:00:33Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -0,0 +1,500 @@\n+/*\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.helix.NotificationContext;\n+import org.apache.helix.model.IdealState;\n+import org.apache.helix.model.InstanceConfig;\n+import org.apache.helix.model.LiveInstance;\n+import org.apache.helix.spectator.RoutingTableSnapshot;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * A more dynamic implementation of {@link ClusterChangeHandler} which supports adding new nodes/partitions at runtime.\n+ * It is also able to absorb replica location changes in cluster.\n+ */\n+public class DynamicClusterChangeHandler implements ClusterChangeHandler {\n+  private final String dcName;\n+  private final Object notificationLock = new Object();\n+  private final HelixClusterManagerMetrics helixClusterManagerMetrics;\n+  private final AtomicLong sealedStateChangeCounter;\n+  private final ConcurrentHashMap<String, Exception> initializationFailureMap;\n+  private final ClusterMapConfig clusterMapConfig;\n+  private final String selfInstanceName;\n+  private final Map<String, Map<String, String>> partitionOverrideInfoMap;\n+  private final HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback;\n+  private final HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback;\n+  private final CountDownLatch routingTableInitLatch = new CountDownLatch(1);\n+  // A map whose key is ambry datanode and value is a map of partitionName to corresponding replica on this datanode\n+  // Note: the partitionName (inner map key) comes from partitionId.toString() not partitionId.toPathString()\n+  private final ConcurrentHashMap<AmbryDataNode, ConcurrentHashMap<String, AmbryReplica>> ambryDataNodeToAmbryReplicas =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<AmbryDataNode, Set<AmbryDisk>> ambryDataNodeToAmbryDisks = new ConcurrentHashMap<>();\n+  private final AtomicLong errorCount = new AtomicLong(0);\n+\n+  private volatile boolean instanceConfigInitialized = false;\n+  private volatile boolean liveStateInitialized = false;\n+  private volatile boolean idealStateInitialized = false;\n+  private volatile ConcurrentHashMap<String, String> partitionNameToResource = new ConcurrentHashMap<>();\n+  private AtomicReference<RoutingTableSnapshot> routingTableSnapshotRef = new AtomicReference<>();\n+  private ConcurrentHashMap<String, AmbryDataNode> instanceNameToAmbryDataNode = new ConcurrentHashMap<>();\n+  private static final Logger logger = LoggerFactory.getLogger(DynamicClusterChangeHandler.class);\n+\n+  /**\n+   * Constructor for {@link DynamicClusterChangeHandler}\n+   * @param clusterMapConfig the {@link ClusterMapConfig} used to define some behavior of cluster change handler.\n+   * @param dcName the name of data center this handler is associated with.\n+   * @param selfInstanceName instance name of current node.\n+   * @param partitionOverrideInfoMap a map that records partitions and states they should be overridden to.\n+   * @param helixClusterManagerCallback a call back used to query cluster-wide info.\n+   * @param clusterChangeHandlerCallback a call back that allows current handler to update cluster-wide info.\n+   * @param helixClusterManagerMetrics metrics to keep track of changes and status of {@link HelixClusterManager}.\n+   * @param initializationFailureMap a map recording failure associate with each dc during initialization.\n+   * @param sealedStateChangeCounter a counter indicating if sealed state of any partition has changed.\n+   */\n+  DynamicClusterChangeHandler(ClusterMapConfig clusterMapConfig, String dcName, String selfInstanceName,\n+      Map<String, Map<String, String>> partitionOverrideInfoMap,\n+      HelixClusterManager.HelixClusterManagerCallback helixClusterManagerCallback,\n+      HelixClusterManager.ClusterChangeHandlerCallback clusterChangeHandlerCallback,\n+      HelixClusterManagerMetrics helixClusterManagerMetrics,\n+      ConcurrentHashMap<String, Exception> initializationFailureMap, AtomicLong sealedStateChangeCounter) {\n+    this.clusterMapConfig = clusterMapConfig;\n+    this.dcName = dcName;\n+    this.selfInstanceName = selfInstanceName;\n+    this.partitionOverrideInfoMap = partitionOverrideInfoMap;\n+    this.helixClusterManagerCallback = helixClusterManagerCallback;\n+    this.clusterChangeHandlerCallback = clusterChangeHandlerCallback;\n+    this.helixClusterManagerMetrics = helixClusterManagerMetrics;\n+    this.initializationFailureMap = initializationFailureMap;\n+    this.sealedStateChangeCounter = sealedStateChangeCounter;\n+  }\n+\n+  /**\n+   * Handle any {@link InstanceConfig} related change in current datacenter. Several events will trigger instance config\n+   * change: (1) replica's seal or stop state has changed; (2) new node or new partition is added; (3) new replica is\n+   * added to existing node; (4) old replica is removed from existing node; (5) data node is deleted from cluster.\n+   * For now, {@link DynamicClusterChangeHandler} supports (1)~(4). We may consider supporting (5) in the future.\n+   * (The ZNode path of instance config in Helix is [AmbryClusterName]/CONFIGS/PARTICIPANT/[hostname_port])\n+   * @param configs all the {@link InstanceConfig}(s) in current data center. (Note that PreFetch is enabled by default\n+   *                in Helix, which means all instance configs under \"participants\" ZNode will be sent to this method)\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onInstanceConfigChange(List<InstanceConfig> configs, NotificationContext changeContext) {\n+    try {\n+      synchronized (notificationLock) {\n+        if (!instanceConfigInitialized) {\n+          logger.info(\"Received initial notification for instance config change from {}\", dcName);\n+        } else {\n+          logger.info(\"Instance config change triggered from {}\", dcName);\n+        }\n+        logger.debug(\"Detailed instance configs in {} are: {}\", dcName, configs);\n+        try {\n+          addOrUpdateInstanceInfos(configs);\n+        } catch (Exception e) {\n+          if (!instanceConfigInitialized) {\n+            logger.error(\"Exception occurred when initializing instances in {}: \", dcName, e);\n+            initializationFailureMap.putIfAbsent(dcName, e);\n+          } else {\n+            logger.error(\"Exception occurred at runtime when handling instance config changes in {}: \", dcName, e);\n+          }\n+        } finally {\n+          instanceConfigInitialized = true;\n+        }\n+        sealedStateChangeCounter.incrementAndGet();\n+        helixClusterManagerMetrics.instanceConfigChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the IdealState in current data center has changed (for now, it is usually updated by Helix\n+   * Bootstrap tool).\n+   * @param idealState a list of {@link IdealState} that specifies ideal location of replicas.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onIdealStateChange(List<IdealState> idealState, NotificationContext changeContext) {\n+    if (!idealStateInitialized) {\n+      logger.info(\"Received initial notification for IdealState change from {}\", dcName);\n+      idealStateInitialized = true;\n+    } else {\n+      logger.info(\"IdealState change triggered from {}\", dcName);\n+    }\n+    logger.debug(\"Detailed ideal states in {} are: {}\", dcName, idealState);\n+    // rebuild the entire partition-to-resource map in current dc\n+    ConcurrentHashMap<String, String> partitionToResourceMap = new ConcurrentHashMap<>();\n+    for (IdealState state : idealState) {\n+      String resourceName = state.getResourceName();\n+      state.getPartitionSet().forEach(partitionName -> partitionToResourceMap.put(partitionName, resourceName));\n+    }\n+    partitionNameToResource = partitionToResourceMap;\n+    helixClusterManagerMetrics.idealStateChangeTriggerCount.inc();\n+  }\n+\n+  /**\n+   * Triggered whenever there is a change in the list of live instances.\n+   * @param liveInstances the list of all live instances (not a change set) at the time of this call.\n+   * @param changeContext the {@link NotificationContext} associated.\n+   */\n+  @Override\n+  public void onLiveInstanceChange(List<LiveInstance> liveInstances, NotificationContext changeContext) {\n+    try {\n+      if (!liveStateInitialized) {\n+        logger.info(\"Received initial notification for live instance change from {}\", dcName);\n+        liveStateInitialized = true;\n+      } else {\n+        logger.info(\"Live instance change triggered from {}\", dcName);\n+      }\n+      logger.debug(\"Detailed live instances in {} are: {}\", dcName, liveInstances);\n+      synchronized (notificationLock) {\n+        updateInstanceLiveness(liveInstances);\n+        helixClusterManagerMetrics.liveInstanceChangeTriggerCount.inc();\n+      }\n+    } catch (Throwable t) {\n+      errorCount.incrementAndGet();\n+      throw t;\n+    }\n+  }\n+\n+  /**\n+   * Triggered whenever the state of replica in cluster has changed. The snapshot contains up-to-date state of all\n+   * resources(replicas) in this data center.\n+   * @param routingTableSnapshot a snapshot of routing table for this data center.\n+   * @param context additional context associated with this change.\n+   */\n+  @Override\n+  public void onRoutingTableChange(RoutingTableSnapshot routingTableSnapshot, Object context) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+    if (routingTableInitLatch.getCount() == 1) {\n+      logger.info(\"Received initial notification for routing table change from {}\", dcName);\n+      routingTableInitLatch.countDown();\n+    } else {\n+      logger.info(\"Routing table change triggered from {}\", dcName);\n+    }\n+    helixClusterManagerMetrics.routingTableChangeTriggerCount.inc();\n+  }\n+\n+  @Override\n+  public void setRoutingTableSnapshot(RoutingTableSnapshot routingTableSnapshot) {\n+    routingTableSnapshotRef.getAndSet(routingTableSnapshot);\n+  }\n+\n+  @Override\n+  public RoutingTableSnapshot getRoutingTableSnapshot() {\n+    return routingTableSnapshotRef.get();\n+  }\n+\n+  @Override\n+  public Map<AmbryDataNode, Set<AmbryDisk>> getDataNodeToDisksMap() {\n+    return Collections.unmodifiableMap(ambryDataNodeToAmbryDisks);\n+  }\n+\n+  @Override\n+  public AmbryDataNode getDataNode(String instanceName) {\n+    return instanceNameToAmbryDataNode.get(instanceName);\n+  }\n+\n+  @Override\n+  public AmbryReplica getReplicaId(AmbryDataNode ambryDataNode, String partitionName) {\n+    return ambryDataNodeToAmbryReplicas.getOrDefault(ambryDataNode, new ConcurrentHashMap<>()).get(partitionName);\n+  }\n+\n+  @Override\n+  public List<AmbryReplica> getReplicaIds(AmbryDataNode ambryDataNode) {\n+    return new ArrayList<>(ambryDataNodeToAmbryReplicas.get(ambryDataNode).values());\n+  }\n+\n+  @Override\n+  public List<AmbryDataNode> getAllDataNodes() {\n+    return new ArrayList<>(instanceNameToAmbryDataNode.values());\n+  }\n+\n+  @Override\n+  public Set<AmbryDisk> getDisks(AmbryDataNode ambryDataNode) {\n+    return ambryDataNodeToAmbryDisks.get(ambryDataNode);\n+  }\n+\n+  @Override\n+  public Map<String, String> getPartitionToResourceMap() {\n+    return Collections.unmodifiableMap(partitionNameToResource);\n+  }\n+\n+  @Override\n+  public long getErrorCount() {\n+    return errorCount.get();\n+  }\n+\n+  @Override\n+  public void waitForInitNotification() throws InterruptedException {\n+    // wait slightly more than 5 mins to ensure routerUpdater refreshes the snapshot.\n+    if (!routingTableInitLatch.await(320, TimeUnit.SECONDS)) {\n+      throw new IllegalStateException(\"Initial routing table change from \" + dcName + \" didn't come within 5 mins\");\n+    }\n+  }\n+\n+  /**\n+   * Add new instance or update existing instance based on {@link InstanceConfig}(s).\n+   * @param instanceConfigs the {@link InstanceConfig}(s) used to update in-mem cluster map.\n+   * @throws Exception\n+   */\n+  private void addOrUpdateInstanceInfos(List<InstanceConfig> instanceConfigs) throws Exception {\n+    for (InstanceConfig instanceConfig : instanceConfigs) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        logger.error(\"Unknown InstanceConfig schema version: {}, ignoring.\", schemaVersion);\n+        continue;\n+      }\n+      if (instanceNameToAmbryDataNode.containsKey(instanceConfig.getInstanceName())) {\n+        updateInstanceInfo(instanceConfig);\n+      } else {\n+        createNewInstance(instanceConfig);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Update info of an existing instance. This may happen in following cases: (1) new replica is added; (2) old replica\n+   * is removed; (3) replica's state has changed (i.e. becomes seal/unseal).\n+   * @param instanceConfig the {@link InstanceConfig} used to update info of instance.\n+   */\n+  private void updateInstanceInfo(InstanceConfig instanceConfig) throws Exception {\n+    String instanceName = instanceConfig.getInstanceName();\n+    logger.info(\"Updating replicas info for existing node {}\", instanceName);\n+    List<String> sealedReplicas = getSealedReplicas(instanceConfig);\n+    List<String> stoppedReplicas = getStoppedReplicas(instanceConfig);\n+    AmbryDataNode dataNode = instanceNameToAmbryDataNode.get(instanceName);\n+    ConcurrentHashMap<String, AmbryReplica> currentReplicasOnNode = ambryDataNodeToAmbryReplicas.get(dataNode);\n+    ConcurrentHashMap<String, AmbryReplica> replicasFromConfig = new ConcurrentHashMap<>();\n+    Map<String, Map<String, String>> diskInfos = instanceConfig.getRecord().getMapFields();\n+    for (Map.Entry<String, Map<String, String>> entry : diskInfos.entrySet()) {\n+      String mountPath = entry.getKey();\n+      Map<String, String> diskInfo = entry.getValue();\n+      Optional<AmbryDisk> potentialDisk =\n+          ambryDataNodeToAmbryDisks.get(dataNode).stream().filter(d -> d.getMountPath().equals(mountPath)).findFirst();\n+      AmbryDisk disk = potentialDisk.orElse(null);\n+      if (disk == null) {\n+        logger.info(\"Temporarily don't support adding new disk to existing node.\");\n+        // TODO support dynamically adding disk in the future\n+        continue;\n+      }\n+      String replicasStr = diskInfo.get(ClusterMapUtils.REPLICAS_STR);\n+      if (!replicasStr.isEmpty()) {\n+        for (String replicaInfo : replicasStr.split(ClusterMapUtils.REPLICAS_DELIM_STR)) {\n+          String[] info = replicaInfo.split(ClusterMapUtils.REPLICAS_STR_SEPARATOR);\n+          // partition name and replica name are the same.\n+          String partitionName = info[0];\n+          if (currentReplicasOnNode.containsKey(partitionName)) {\n+            // if replica is already present\n+            AmbryReplica existingReplica = currentReplicasOnNode.get(partitionName);\n+            // 1. directly add it into \"replicasFromConfig\" map\n+            replicasFromConfig.put(partitionName, existingReplica);\n+            // 2. update replica seal/stop state\n+            updateReplicaStateAndOverrideIfNeeded(existingReplica, sealedReplicas, stoppedReplicas);\n+          } else {\n+            // if this is a new replica and doesn't exist on node\n+            logger.info(\"Adding new replica {} to existing node {}\", partitionName, instanceName);\n+            long replicaCapacity = Long.valueOf(info[1]);\n+            String partitionClass = info.length > 2 ? info[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+            // this can be a brand new partition that is added to an existing node\n+            AmbryPartition mappedPartition =\n+                new AmbryPartition(Long.valueOf(partitionName), partitionClass, helixClusterManagerCallback);\n+            // Ensure only one AmbryPartition instance exists for specific partition.\n+            mappedPartition = clusterChangeHandlerCallback.addPartitionIfAbsent(mappedPartition, replicaCapacity);\n+            ensurePartitionAbsenceOnNodeAndValidateCapacity(mappedPartition, dataNode, replicaCapacity);\n+            // create new replica belonging to this partition\n+            AmbryReplica replica =\n+                new AmbryReplica(clusterMapConfig, mappedPartition, disk, stoppedReplicas.contains(partitionName),\n+                    replicaCapacity, sealedReplicas.contains(partitionName));\n+            updateReplicaStateAndOverrideIfNeeded(replica, sealedReplicas, stoppedReplicas);\n+            // add new created replica to \"replicasFromConfig\" map\n+            replicasFromConfig.put(partitionName, replica);\n+            // add new replica to specific partition\n+            clusterChangeHandlerCallback.addReplicasToPartition(mappedPartition, Collections.singletonList(replica));\n+          }\n+        }\n+      }\n+    }\n+    // update ambryDataNodeToAmbryReplicas map by adding \"replicasFromConfig\"\n+    ambryDataNodeToAmbryReplicas.put(instanceNameToAmbryDataNode.get(instanceName), replicasFromConfig);\n+    // compare replicasFromConfig with current replica set to derive old replicas that are removed\n+    Set<AmbryReplica> previousReplicas = new HashSet<>(currentReplicasOnNode.values());\n+    previousReplicas.removeAll(replicasFromConfig.values());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA0NjkxNQ=="}, "originalCommit": {"oid": "3830d5f761ff7df4b57e1083a808121b1529b747"}, "originalPosition": 353}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1NzIxNDAwOnYy", "diffSide": "RIGHT", "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxODoxNToyOVrOFrNJBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxODoxNToyOVrOFrNJBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDg0ODM5MA==", "bodyText": "remove commented code", "url": "https://github.com/linkedin/ambry/pull/1355#discussion_r380848390", "createdAt": "2020-02-18T18:15:29Z", "author": {"login": "cgtz"}, "path": "ambry-clustermap/src/main/java/com.github.ambry.clustermap/DynamicClusterChangeHandler.java", "diffHunk": "@@ -340,23 +343,37 @@ private void updateInstanceInfo(InstanceConfig instanceConfig) throws Exception\n             updateReplicaStateAndOverrideIfNeeded(replica, sealedReplicas, stoppedReplicas);\n             // add new created replica to \"replicasFromConfig\" map\n             replicasFromConfig.put(partitionName, replica);\n-            // add new replica to specific partition\n-            clusterChangeHandlerCallback.addReplicasToPartition(mappedPartition, Collections.singletonList(replica));\n+            // Put new replica into partition-to-replica map temporarily (this is to avoid any exception thrown within the\n+            // loop before updating \"ambryDataNodeToAmbryReplicas\" map. If we update call addReplicasToPartition here\n+            // immediately, the exception may cause inconsistency between \"ambryPartitionToAmbryReplicas\" and\n+            // \"ambryDataNodeToAmbryReplicas\")\n+            replicaToAddByPartition.put(mappedPartition, Collections.singletonList(replica));\n           }\n         }\n       }\n     }\n+    // update ambryDataNodeToAmbryReplicas map\n+    replicaToAddByPartition.forEach(clusterChangeHandlerCallback::addReplicasToPartition);\n     // update ambryDataNodeToAmbryReplicas map by adding \"replicasFromConfig\"\n     ambryDataNodeToAmbryReplicas.put(instanceNameToAmbryDataNode.get(instanceName), replicasFromConfig);\n-    // compare replicasFromConfig with current replica set to derive old replicas that are removed\n-    Set<AmbryReplica> previousReplicas = new HashSet<>(currentReplicasOnNode.values());\n+    // Derive old replicas that are removed and delete them from partition\n+/*    Set<AmbryReplica> previousReplicas = new HashSet<>(currentReplicasOnNode.values());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "268d2ccd62a2d085ef1b1a1ef21600dde0c5a68b"}, "originalPosition": 59}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1644, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}