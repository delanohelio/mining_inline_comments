{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY2NjMwOTIy", "number": 1365, "title": "Azure batch deletes", "bodyText": "", "createdAt": "2020-01-24T00:27:47Z", "url": "https://github.com/linkedin/ambry/pull/1365", "merged": true, "mergeCommit": {"oid": "db13bc31de1f3526a07fbcd426d7deda8e02ed3e"}, "closed": true, "closedAt": "2020-02-04T23:48:31Z", "author": {"login": "lightningrob"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABbyYsPNgH2gAyMzY2NjMwOTIyOjQ2YzU0NTc3M2Q1Yjg1ZmVhMWRmNTRjZTA2NmRlNDA1ZjEwODU4YzA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcBJaPkgH2gAyMzY2NjMwOTIyOjA2ZTViM2Y1OWZjYmY3ZmZjOGUxNGJiOGExOGFjOGJjMzlkM2FlNWM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "46c545773d5b85fea1df54ce066de405f10858c0", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/46c545773d5b85fea1df54ce066de405f10858c0", "committedDate": "2019-12-21T01:57:11Z", "message": "First cut at batch blob deletes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2d404f35c365a4485406744f64ca76eac3b9b8f3", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/2d404f35c365a4485406744f64ca76eac3b9b8f3", "committedDate": "2020-01-23T23:53:30Z", "message": "Update azure storage lib to fix batch delete bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e1afe24679a3e699c57998612cd2c7d82dd7fd9", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/5e1afe24679a3e699c57998612cd2c7d82dd7fd9", "committedDate": "2020-01-24T00:15:51Z", "message": "Merge branch 'master' of github.com:linkedin/ambry into azure-batch-deletes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4fb33e7869127836470c7330767266ea4c36e2bf", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/4fb33e7869127836470c7330767266ea4c36e2bf", "committedDate": "2020-01-24T00:24:47Z", "message": "Fix merge issues from rebase."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d", "committedDate": "2020-01-24T00:52:24Z", "message": "Fix test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ5NjEwMjc0", "url": "https://github.com/linkedin/ambry/pull/1365#pullrequestreview-349610274", "createdAt": "2020-01-28T18:39:25Z", "commit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxODozOToyNVrOFiwKIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQyMzo1NDo0NVrOFi4rPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTk4NDkyOQ==", "bodyText": "Lets make this final. Also add the comment about batch size <= 256 here. Along with that add additional comment that once added to AzureCloudConfig, we should validate this is <= 256.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r371984929", "createdAt": "2020-01-28T18:39:25Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -55,12 +63,15 @@\n   private static final Logger logger = LoggerFactory.getLogger(AzureBlobDataAccessor.class);\n   private static final String SEPARATOR = \"-\";\n   private final BlobServiceClient storageClient;\n+  private final BlobBatchClient blobBatchClient;\n   private final Configuration storageConfiguration;\n   private final AzureMetrics azureMetrics;\n   private final String clusterName;\n   // Containers known to exist in the storage account\n   private final Set<String> knownContainers = ConcurrentHashMap.newKeySet();\n   private ProxyOptions proxyOptions;\n+  // TODO: add to AzureCloudConfig\n+  private int purgeBatchSize = 100;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTk4NTE4MA==", "bodyText": "I think we should move this comment to where we define the purgeBatchSize variable. Also see more comments about purgeBatchSize above.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r371985180", "createdAt": "2020-01-28T18:39:57Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +317,47 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    // Per docs.microsoft.com/en-us/rest/api/storageservices/blob-batch, must use batch size <= 256", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTk4NzY4NQ==", "bodyText": "minor-XXS: maybe rename \"someBlobs\" to \"batchofBlobs\"", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r371987685", "createdAt": "2020-01-28T18:45:02Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +317,47 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    // Per docs.microsoft.com/en-us/rest/api/storageservices/blob-batch, must use batch size <= 256\n+    List<CloudBlobMetadata> deletedBlobs = new ArrayList<>();\n+    List<List<CloudBlobMetadata>> partitionedLists = Lists.partition(blobMetadataList, purgeBatchSize);\n+    for (List<CloudBlobMetadata> someBlobs : partitionedLists) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAwMjk0MQ==", "bodyText": "We should make the Duration.ofHours(1) configurable. Atleast final private for now.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r372002941", "createdAt": "2020-01-28T19:15:24Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +317,47 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    // Per docs.microsoft.com/en-us/rest/api/storageservices/blob-batch, must use batch size <= 256\n+    List<CloudBlobMetadata> deletedBlobs = new ArrayList<>();\n+    List<List<CloudBlobMetadata>> partitionedLists = Lists.partition(blobMetadataList, purgeBatchSize);\n+    for (List<CloudBlobMetadata> someBlobs : partitionedLists) {\n+      BlobBatch blobBatch = blobBatchClient.getBlobBatch();\n+      List<Response<Void>> responseList = new ArrayList<>();\n+      for (CloudBlobMetadata blobMetadata : someBlobs) {\n+        String containerName = getAzureContainerName(blobMetadata);\n+        String blobName = getAzureBlobName(blobMetadata);\n+        responseList.add(blobBatch.deleteBlob(containerName, blobName));\n+      }\n+      blobBatchClient.submitBatchWithResponse(blobBatch, false, Duration.ofHours(1), Context.NONE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjEyNDQ3OQ==", "bodyText": "can the statuscode returned by exception be one of OK, ACCEPTED, NOT_FOUND or GONE? If not, then maybe we can move the try catch block to include switch as well.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r372124479", "createdAt": "2020-01-28T23:54:45Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +317,47 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    // Per docs.microsoft.com/en-us/rest/api/storageservices/blob-batch, must use batch size <= 256\n+    List<CloudBlobMetadata> deletedBlobs = new ArrayList<>();\n+    List<List<CloudBlobMetadata>> partitionedLists = Lists.partition(blobMetadataList, purgeBatchSize);\n+    for (List<CloudBlobMetadata> someBlobs : partitionedLists) {\n+      BlobBatch blobBatch = blobBatchClient.getBlobBatch();\n+      List<Response<Void>> responseList = new ArrayList<>();\n+      for (CloudBlobMetadata blobMetadata : someBlobs) {\n+        String containerName = getAzureContainerName(blobMetadata);\n+        String blobName = getAzureBlobName(blobMetadata);\n+        responseList.add(blobBatch.deleteBlob(containerName, blobName));\n+      }\n+      blobBatchClient.submitBatchWithResponse(blobBatch, false, Duration.ofHours(1), Context.NONE);\n+      for (int j = 0; j < responseList.size(); j++) {\n+        Response<Void> response = responseList.get(j);\n+        CloudBlobMetadata blobMetadata = someBlobs.get(j);\n+        // Note: Response.getStatusCode() throws exception on any error.\n+        int statusCode;\n+        try {\n+          statusCode = response.getStatusCode();\n+        } catch (BlobStorageException bex) {\n+          statusCode = bex.getStatusCode();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d"}, "originalPosition": 109}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "805a9932963048e079de165b1ed757116477437f", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/805a9932963048e079de165b1ed757116477437f", "committedDate": "2020-01-31T02:21:30Z", "message": "Address Ankur's review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "994decafe82416deb29570ed0b991fc3c89afd90", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/994decafe82416deb29570ed0b991fc3c89afd90", "committedDate": "2020-01-31T05:53:22Z", "message": "Merge branch 'master' of github.com:linkedin/ambry into azure-batch-deletes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/0a174038358ca7cbeeb6389145131970c7e9bd5f", "committedDate": "2020-01-31T19:43:19Z", "message": "Fix test issue"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxODQwMTUx", "url": "https://github.com/linkedin/ambry/pull/1365#pullrequestreview-351840151", "createdAt": "2020-01-31T23:58:18Z", "commit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzo1ODoxOVrOFka9hA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxNzoyNzo1NlrOFk5jCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzczNDc4OA==", "bodyText": "make static?", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r373734788", "createdAt": "2020-01-31T23:58:19Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -55,12 +62,15 @@\n   private static final Logger logger = LoggerFactory.getLogger(AzureBlobDataAccessor.class);\n   private static final String SEPARATOR = \"-\";\n   private final BlobServiceClient storageClient;\n+  private final BlobBatchClient blobBatchClient;\n   private final Configuration storageConfiguration;\n   private final AzureMetrics azureMetrics;\n   private final String clusterName;\n   // Containers known to exist in the storage account\n   private final Set<String> knownContainers = ConcurrentHashMap.newKeySet();\n   private ProxyOptions proxyOptions;\n+  private final int purgeBatchSize;\n+  private final int batchPurgeTimeoutSec = 60;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIxNzAyMw==", "bodyText": "So, the caller of this method will know to retry on the blobs that are not in the returned list?", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374217023", "createdAt": "2020-02-03T16:51:38Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +318,46 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    List<CloudBlobMetadata> deletedBlobs = new ArrayList<>();\n+    List<List<CloudBlobMetadata>> partitionedLists = Lists.partition(blobMetadataList, purgeBatchSize);\n+    for (List<CloudBlobMetadata> batchOfBlobs : partitionedLists) {\n+      BlobBatch blobBatch = blobBatchClient.getBlobBatch();\n+      List<Response<Void>> responseList = new ArrayList<>();\n+      for (CloudBlobMetadata blobMetadata : batchOfBlobs) {\n+        String containerName = getAzureContainerName(blobMetadata);\n+        String blobName = getAzureBlobName(blobMetadata);\n+        responseList.add(blobBatch.deleteBlob(containerName, blobName));\n+      }\n+      blobBatchClient.submitBatchWithResponse(blobBatch, false, Duration.ofSeconds(batchPurgeTimeoutSec), Context.NONE);\n+      for (int j = 0; j < responseList.size(); j++) {\n+        Response<Void> response = responseList.get(j);\n+        CloudBlobMetadata blobMetadata = batchOfBlobs.get(j);\n+        // Note: Response.getStatusCode() throws exception on any error.\n+        int statusCode;\n+        try {\n+          statusCode = response.getStatusCode();\n+        } catch (BlobStorageException bex) {\n+          statusCode = bex.getStatusCode();\n+        }\n+        switch (statusCode) {\n+          case HttpURLConnection.HTTP_OK:\n+          case HttpURLConnection.HTTP_ACCEPTED:\n+          case HttpURLConnection.HTTP_NOT_FOUND:\n+          case HttpURLConnection.HTTP_GONE:\n+            // blob was deleted or already gone\n+            deletedBlobs.add(blobMetadata);\n+            break;\n+          default:\n+            logger.error(\"Deleting blob {} got status {}\", blobMetadata.getId(), statusCode);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIyMDA3Mw==", "bodyText": "Apparently, the client will throw a general RuntimeException (looking at the code its IllegalStateException) and not BlobStorageException if it times out. We should make sure that the callers are prepared to handle that. It may be worth documenting in the javadocs too.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374220073", "createdAt": "2020-02-03T16:57:07Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +318,46 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    List<CloudBlobMetadata> deletedBlobs = new ArrayList<>();\n+    List<List<CloudBlobMetadata>> partitionedLists = Lists.partition(blobMetadataList, purgeBatchSize);\n+    for (List<CloudBlobMetadata> batchOfBlobs : partitionedLists) {\n+      BlobBatch blobBatch = blobBatchClient.getBlobBatch();\n+      List<Response<Void>> responseList = new ArrayList<>();\n+      for (CloudBlobMetadata blobMetadata : batchOfBlobs) {\n+        String containerName = getAzureContainerName(blobMetadata);\n+        String blobName = getAzureBlobName(blobMetadata);\n+        responseList.add(blobBatch.deleteBlob(containerName, blobName));\n+      }\n+      blobBatchClient.submitBatchWithResponse(blobBatch, false, Duration.ofSeconds(batchPurgeTimeoutSec), Context.NONE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIzMTYyNQ==", "bodyText": "I just want to confirm my understanding of the compaction logic: In the case where deletedBlobs.size() < blobMetadataList.size(), we rely on the fact that not deleting the record from cosmos will allow the dead blobs query to return the same blob ID in future compaction iterations?", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374231625", "createdAt": "2020-02-03T17:19:05Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureCloudDestination.java", "diffHunk": "@@ -349,51 +352,48 @@ private boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value\n   }\n \n   @Override\n-  public boolean purgeBlob(CloudBlobMetadata blobMetadata) throws CloudStorageException {\n-    String blobId = blobMetadata.getId();\n-    String blobFileName = azureBlobDataAccessor.getAzureBlobName(blobMetadata);\n-    String containerName = azureBlobDataAccessor.getAzureContainerName(blobMetadata);\n-    String partitionPath = blobMetadata.getPartitionId();\n-    azureMetrics.blobDeleteRequestCount.inc();\n+  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageException {\n+    if (blobMetadataList.isEmpty()) {\n+      return 0;\n+    }\n+    azureMetrics.blobDeleteRequestCount.inc(blobMetadataList.size());\n     Timer.Context deleteTimer = azureMetrics.blobDeletionTime.time();\n     try {\n-      // delete blob from storage\n-      boolean deletionDone = azureBlobDataAccessor.deleteFile(containerName, blobFileName);\n-\n-      // Delete the document too\n-      try {\n-        cosmosDataAccessor.deleteMetadata(blobMetadata);\n-        deletionDone = true;\n-        logger.debug(\"Purged blob {} from partition {}.\", blobId, partitionPath);\n-      } catch (DocumentClientException dex) {\n-        if (dex.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {\n-          logger.warn(\"Could not find metadata for blob {} to delete\", blobId);\n-        } else {\n-          throw dex;\n-        }\n+      List<CloudBlobMetadata> deletedBlobs = azureBlobDataAccessor.purgeBlobs(blobMetadataList);\n+      azureMetrics.blobDeletedCount.inc(deletedBlobs.size());\n+      azureMetrics.blobDeleteErrorCount.inc(blobMetadataList.size() - deletedBlobs.size());\n+\n+      // Remove them from Cosmos too\n+      for (CloudBlobMetadata blobMetadata : deletedBlobs) {\n+        deleteFromCosmos(blobMetadata);\n       }\n-      azureMetrics.blobDeletedCount.inc(deletionDone ? 1 : 0);\n-      return deletionDone;\n-    } catch (Exception e) {\n-      azureMetrics.blobDeleteErrorCount.inc();\n-      String error = (e instanceof DocumentClientException) ? \"Failed to delete metadata document for blob \" + blobId\n-          : \"Failed to delete blob \" + blobId + \", storage path: \" + containerName + \"/\" + blobFileName;\n-      throw toCloudStorageException(error, e);\n+      return deletedBlobs.size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIzMzY4Mw==", "bodyText": "If we want to use guava directly in Ambry (I'm not against this since it has a lot of useful things), it might be good to explicitly reference the desired version in our build files. Interestingly, our dependencies seem to transitively bring in 3 versions of guava.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374233683", "createdAt": "2020-02-03T17:23:17Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -32,12 +36,15 @@\n import com.github.ambry.commons.BlobId;\n import com.github.ambry.config.CloudConfig;\n import com.github.ambry.utils.Utils;\n+import com.google.common.collect.Lists;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIzNTkxNA==", "bodyText": "If we decide not to use guava directly we can add a partition method to Utils.java (the impl is pretty simple), or just use List::subList", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374235914", "createdAt": "2020-02-03T17:27:56Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -32,12 +36,15 @@\n import com.github.ambry.commons.BlobId;\n import com.github.ambry.config.CloudConfig;\n import com.github.ambry.utils.Utils;\n+import com.google.common.collect.Lists;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIzMzY4Mw=="}, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 20}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "73c5d5935acc0bf9f0fcae9da577c951a676c69d", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/73c5d5935acc0bf9f0fcae9da577c951a676c69d", "committedDate": "2020-02-03T20:39:11Z", "message": "Moved list partitioning logic to Utils.partitionList()"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUyNzIzOTE1", "url": "https://github.com/linkedin/ambry/pull/1365#pullrequestreview-352723915", "createdAt": "2020-02-04T04:55:39Z", "commit": {"oid": "73c5d5935acc0bf9f0fcae9da577c951a676c69d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQwNDo1NTozOVrOFlIFgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQwNDo1NTozOVrOFlIFgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDQ3NDExNA==", "bodyText": "could this loop be simplified to:\nfor (int start = 0; start < inputList.size(); start += batchSize) {\n  int end = Math.min(start + batchSize, inputList.size());\n  partitionedLists.add(inputList.subList(start, end));\n}\n\nI may be missing an edge case here though", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374474114", "createdAt": "2020-02-04T04:55:39Z", "author": {"login": "cgtz"}, "path": "ambry-utils/src/main/java/com.github.ambry.utils/Utils.java", "diffHunk": "@@ -912,6 +913,30 @@ public static DataInputStream createDataInputStreamFromBuffer(Object buffer, boo\n         .collect(Collectors.toCollection(collectionFactory));\n   }\n \n+  /**\n+   * Partition the input list into a List of smaller sublists, each one limited to the specified batch size.\n+   * Method inspired by the Guava utility Lists.partition(List<T> list, int size).\n+   * @param inputList the input list to partition.\n+   * @param batchSize the maximum size of the returned sublists.\n+   * @return the partitioned list of sublists.\n+   */\n+  public static <T> List<List<T>> partitionList(List<T> inputList, int batchSize) {\n+    Objects.requireNonNull(inputList, \"Input list cannot be null\");\n+    if (batchSize < 1) {\n+      throw new IllegalArgumentException(\"Invalid batchSize: \" + batchSize);\n+    }\n+    List<List<T>> partitionedLists = new ArrayList<>();\n+    for (int j = 0; j < inputList.size() / batchSize + 1; j++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73c5d5935acc0bf9f0fcae9da577c951a676c69d"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUyNzI0MDUw", "url": "https://github.com/linkedin/ambry/pull/1365#pullrequestreview-352724050", "createdAt": "2020-02-04T04:56:20Z", "commit": {"oid": "73c5d5935acc0bf9f0fcae9da577c951a676c69d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "06e5b3f59fcbf7ffc8e14bb8a18ac8bc39d3ae5c", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/06e5b3f59fcbf7ffc8e14bb8a18ac8bc39d3ae5c", "committedDate": "2020-02-04T22:37:49Z", "message": "Simplify Utils.partitionList() per Casey's feedback."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1566, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}