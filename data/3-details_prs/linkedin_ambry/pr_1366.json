{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY2OTYyMDAz", "number": 1366, "title": "Adding undelete record to persistent index", "bodyText": "Adding undelete record to persistent index.\nThis would introduces new assumptions to the system.\n\nDeletes are not the final state of blobs any more, instead, deletes and undeletes are the final possible state.\nDeletes will be receiving less special treatment from the code. In findDeleteEntriesSince method, we have to exam if a deleted record is revived later.\nDeletes following undelete will not have original message offset anymore.", "createdAt": "2020-01-24T18:32:20Z", "url": "https://github.com/linkedin/ambry/pull/1366", "merged": true, "mergeCommit": {"oid": "8f884cfcbd024126d10e206458293ba693c8d263"}, "closed": true, "closedAt": "2020-02-03T20:30:02Z", "author": {"login": "justinlin-linkedin"}, "timelineItems": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb-nAK3gBqjI5ODM5ODMyNTE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcAyQS2AH2gAyMzY2OTYyMDAzOjk0NTQ3M2Y3OWJkZWZiMzFjMDUwMzhjZDQ2OTkxNDc4NmMxOTQyNDE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "204d07c1a1261d3ba5b6c54f11a84cd97b222f2b", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/204d07c1a1261d3ba5b6c54f11a84cd97b222f2b", "committedDate": "2020-01-24T18:47:16Z", "message": "Fix"}, "afterCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/09d5ddedaea1c3a9a8c465187e556da1c082f5ac", "committedDate": "2020-01-28T01:17:06Z", "message": "Fix"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ5ODg0MTYy", "url": "https://github.com/linkedin/ambry/pull/1366#pullrequestreview-349884162", "createdAt": "2020-01-29T06:26:45Z", "commit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNjoyNjo0NVrOFi9w2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQwMDowMDowOFrOFjbmzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjIwNzgzMw==", "bodyText": "Do we still need ID_Deleted?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372207833", "createdAt": "2020-01-29T06:26:45Z", "author": {"login": "jsjtzyy"}, "path": "ambry-api/src/main/java/com.github.ambry/store/StoreErrorCodes.java", "diffHunk": "@@ -35,5 +35,9 @@\n   Already_Updated,\n   Update_Not_Allowed,\n   File_Not_Found,\n-  Channel_Closed\n+  Channel_Closed,\n+  Life_Version_Conflict,\n+  ID_Not_Deleted,\n+  ID_Undeleted,\n+  ID_Deleted_Permanently", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY0MDQ4Mw==", "bodyText": "Can you explain a little bit more about difference between life version from frontend and that is not?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372640483", "createdAt": "2020-01-29T21:30:31Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "diffHunk": "@@ -55,6 +55,8 @@\n \n   final static byte FLAGS_DEFAULT_VALUE = (byte) 0;\n   final static long UNKNOWN_ORIGINAL_MESSAGE_OFFSET = -1;\n+  // The life version when the operation is trigger by the requests from frontend.\n+  final static short LIFE_VERSION_FROM_FRONTEND = -1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY0MTU1Nw==", "bodyText": "Replace isFlagSet(Flags.Undelete_Index) with isUndelete(). Same for DELETE and TTLUpdate.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372641557", "createdAt": "2020-01-29T21:32:57Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "diffHunk": "@@ -386,11 +429,11 @@ ByteBuffer getBytes() {\n   @Override\n   public String toString() {\n     return \"Offset: \" + offset + \", Size: \" + getSize() + \", Deleted: \" + isFlagSet(Flags.Delete_Index)\n-        + \", TTL Updated: \" + isFlagSet(Flags.Ttl_Update_Index) + \", Undelete: \" + isFlagSet(\n-        Flags.Undelete_Index) + \", ExpiresAtMs: \" + getExpiresAtMs() + \", Original Message Offset: \"\n-        + getOriginalMessageOffset() + (formatVersion != PersistentIndex.VERSION_0 ? (\", OperationTimeAtSecs \"\n-        + getOperationTimeInMs() + \", AccountId \" + getAccountId() + \", ContainerId \" + getContainerId())\n-        : \"\") + (formatVersion > PersistentIndex.VERSION_2 ? \", Life Version:\" + lifeVersion : \"\");\n+        + \", TTL Updated: \" + isFlagSet(Flags.Ttl_Update_Index) + \", Undelete: \" + isFlagSet(Flags.Undelete_Index)\n+        + \", ExpiresAtMs: \" + getExpiresAtMs() + \", Original Message Offset: \" + getOriginalMessageOffset() + (", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY0NjUyNQ==", "bodyText": "If life version is not from frontend, dose it mean that the version comes from replication or compaction?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372646525", "createdAt": "2020-01-29T21:42:55Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "diffHunk": "@@ -279,6 +313,15 @@ short getContainerId() {\n     return containerId;\n   }\n \n+  /**\n+   * True when the life version is not from frontend requests.\n+   * @param lifeVersion the given life version.\n+   * @return true when it's not from frontend requests.\n+   */\n+  static boolean hasLifeVersion(short lifeVersion) {\n+    return lifeVersion > LIFE_VERSION_FROM_FRONTEND;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY2MTQ2Ng==", "bodyText": "Can this be simplified as flags == FLAGS_DEFAULT_VALUE ?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372661466", "createdAt": "2020-01-29T22:16:31Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "diffHunk": "@@ -243,6 +245,38 @@ boolean isFlagSet(Flags flag) {\n     return ((getFlags() & (1 << flag.ordinal())) != 0);\n   }\n \n+  /**\n+   * Helper function for isFlagSet(Flags.Ttl_Update_Index).\n+   * @return true when the Ttl_Update_Index is set.\n+   */\n+  boolean isTTLUpdate() {\n+    return isFlagSet(Flags.Ttl_Update_Index);\n+  }\n+\n+  /**\n+   * Helper function for isFlagSet(Flags.Delete_Index).\n+   * @return true when the Delete_Index is set.\n+   */\n+  boolean isDelete() {\n+    return isFlagSet(Flags.Delete_Index);\n+  }\n+\n+  /**\n+   * Helper function for isFlagSet(Flags.Undelete_Index).\n+   * @return true when the Undelete_Index is set.\n+   */\n+  boolean isUndelete() {\n+    return isFlagSet(Flags.Undelete_Index);\n+  }\n+\n+  /**\n+   * Helper function to decide if this value is a put value or not.\n+   * @return true when it's not a put record.\n+   */\n+  boolean isPut() {\n+    return !(isTTLUpdate() || isDelete() || isUndelete());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY3Mjc1Mg==", "bodyText": "If this is for testing purpose, I suggest removing this and either use mockito or extends PersisentIndex to override getVersion() method.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372672752", "createdAt": "2020-01-29T22:44:24Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -69,6 +70,30 @@\n   static final short CURRENT_VERSION = VERSION_2;\n   static final String CLEAN_SHUTDOWN_FILENAME = \"cleanshutdown\";\n \n+  // set by the setVersion method from test cases to test IndexValue at next version.\n+  private static Short externalSetVersion = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY3MzcwNg==", "bodyText": "same here, we probably could remove this method", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372673706", "createdAt": "2020-01-29T22:46:49Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -69,6 +70,30 @@\n   static final short CURRENT_VERSION = VERSION_2;\n   static final String CLEAN_SHUTDOWN_FILENAME = \"cleanshutdown\";\n \n+  // set by the setVersion method from test cases to test IndexValue at next version.\n+  private static Short externalSetVersion = null;\n+\n+  /**\n+   * Return the version. If {@link #setVersion(Short)} was invoked before, the version provided in the {@link #setVersion(Short)}\n+   * would be returned here.\n+   * @return The version of {@link PersistentIndex}.\n+   */\n+  static short getVersion() {\n+    if (externalSetVersion != null) {\n+      return externalSetVersion;\n+    }\n+    return CURRENT_VERSION;\n+  }\n+\n+  /**\n+   * Set the version for {@link PersistentIndex}. To resume the version to default one, pass a null to this method.\n+   * This method should only be used in the test cases.\n+   * @param external the version to set for {@link PersistentIndex}.\n+   */\n+  static void setVersion(Short external) {\n+    externalSetVersion = external;\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY4MjU0MQ==", "bodyText": "The info here comes from recovery.recover() but I don't see BlobStoreRecovery handles UNDELETE case. Are you planning to add that in future PR?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372682541", "createdAt": "2020-01-29T23:12:23Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -332,14 +357,24 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n             // DELETE must have been present)\n             deleteExpectedKeys.add(info.getStoreKey());\n           }\n+        } else if (info.isUndeleted()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY5NjA4Mg==", "bodyText": "why we throw exception here?  If ttlUpdate doesn't have version, it is invalid?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372696082", "createdAt": "2020-01-29T23:57:47Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -712,37 +897,106 @@ IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, long operationTimeMs)\n    * @return the {@link IndexValue} of the ttl update record\n    * @throws StoreException if there is any problem writing the index record\n    */\n-  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs)\n-      throws StoreException {\n+  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs,\n+      short lifeVersion) throws StoreException {\n     validateFileSpan(fileSpan, true);\n+    boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     IndexValue value = findKey(id);\n     if (value == null && info == null) {\n       throw new StoreException(\"Id \" + id + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n-    } else if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+    }\n+    short retrievedLifeVersion = value == null ? info.getLifeVersion() : value.getLifeVersion();\n+    if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n       throw new StoreException(\"Id \" + id + \" deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n     } else if (value != null && value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n       throw new StoreException(\"TTL of \" + id + \" already updated in index\" + dataDir, StoreErrorCodes.Already_Updated);\n+    } else if (hasLifeVersion && retrievedLifeVersion > lifeVersion) {\n+      throw new StoreException(\"LifeVersion conflict in index. Id \" + id + \" LifeVersion: \" + retrievedLifeVersion\n+          + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n     }\n     long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     IndexValue newValue;\n+\n     if (value == null) {\n       // It is possible that the PUT has been cleaned by compaction\n+      // but the TTL update is going to still be placed?\n+      if (!hasLifeVersion) {\n+        throw new StoreException(\"MessageInfo of ttlUpdate carries invalid lifeVersion\",\n+            StoreErrorCodes.Initialization_Error);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 360}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY5Njc4MA==", "bodyText": "update the java doc of this method by adding lifeVersion", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372696780", "createdAt": "2020-01-30T00:00:08Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -712,37 +897,106 @@ IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, long operationTimeMs)\n    * @return the {@link IndexValue} of the ttl update record\n    * @throws StoreException if there is any problem writing the index record\n    */\n-  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs)\n-      throws StoreException {\n+  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs,\n+      short lifeVersion) throws StoreException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 335}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/09d5ddedaea1c3a9a8c465187e556da1c082f5ac", "committedDate": "2020-01-28T01:17:06Z", "message": "Fix"}, "afterCommit": {"oid": "d4630e28a026a0cea4ec501b9002bde97a401073", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/d4630e28a026a0cea4ec501b9002bde97a401073", "committedDate": "2020-01-30T01:02:05Z", "message": "Address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUwNTI0MjQ4", "url": "https://github.com/linkedin/ambry/pull/1366#pullrequestreview-350524248", "createdAt": "2020-01-30T00:51:49Z", "commit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQwMDo1MTo0OVrOFjcdAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQwMDo1NTo1N1rOFjcg_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjcxMDY1Nw==", "bodyText": "+1", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372710657", "createdAt": "2020-01-30T00:51:49Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "diffHunk": "@@ -243,6 +245,38 @@ boolean isFlagSet(Flags flag) {\n     return ((getFlags() & (1 << flag.ordinal())) != 0);\n   }\n \n+  /**\n+   * Helper function for isFlagSet(Flags.Ttl_Update_Index).\n+   * @return true when the Ttl_Update_Index is set.\n+   */\n+  boolean isTTLUpdate() {\n+    return isFlagSet(Flags.Ttl_Update_Index);\n+  }\n+\n+  /**\n+   * Helper function for isFlagSet(Flags.Delete_Index).\n+   * @return true when the Delete_Index is set.\n+   */\n+  boolean isDelete() {\n+    return isFlagSet(Flags.Delete_Index);\n+  }\n+\n+  /**\n+   * Helper function for isFlagSet(Flags.Undelete_Index).\n+   * @return true when the Undelete_Index is set.\n+   */\n+  boolean isUndelete() {\n+    return isFlagSet(Flags.Undelete_Index);\n+  }\n+\n+  /**\n+   * Helper function to decide if this value is a put value or not.\n+   * @return true when it's not a put record.\n+   */\n+  boolean isPut() {\n+    return !(isTTLUpdate() || isDelete() || isUndelete());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY2MTQ2Ng=="}, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjcxMTQ1Ng==", "bodyText": "logger.info add info.getLifeVersion()?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372711456", "createdAt": "2020-01-30T00:55:02Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -315,15 +340,15 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n         Offset infoEndOffset = new Offset(runningOffset.getName(), runningOffset.getOffset() + info.getSize());\n         IndexValue value = findKey(info.getStoreKey());\n         if (info.isDeleted()) {\n-          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n+          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info, info.getOperationTimeMs(),\n+              info.getLifeVersion());\n           logger.info(\"Index : {} updated message with key {} by inserting delete entry of size {} ttl {}\", dataDir,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjcxMTY3OA==", "bodyText": "same for logs below.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372711678", "createdAt": "2020-01-30T00:55:57Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -315,15 +340,15 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n         Offset infoEndOffset = new Offset(runningOffset.getName(), runningOffset.getOffset() + info.getSize());\n         IndexValue value = findKey(info.getStoreKey());\n         if (info.isDeleted()) {\n-          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n+          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info, info.getOperationTimeMs(),\n+              info.getLifeVersion());\n           logger.info(\"Index : {} updated message with key {} by inserting delete entry of size {} ttl {}\", dataDir,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjcxMTQ1Ng=="}, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxNjk5NjQ0", "url": "https://github.com/linkedin/ambry/pull/1366#pullrequestreview-351699644", "createdAt": "2020-01-31T18:41:53Z", "commit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0MTo1M1rOFkUW7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxOTowMDozMlrOFkU3HA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyNjYwNg==", "bodyText": "Minor: Life_Version_Invalid", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373626606", "createdAt": "2020-01-31T18:41:53Z", "author": {"login": "zzmao"}, "path": "ambry-api/src/main/java/com.github.ambry/store/StoreErrorCodes.java", "diffHunk": "@@ -35,5 +35,9 @@\n   Already_Updated,\n   Update_Not_Allowed,\n   File_Not_Found,\n-  Channel_Closed\n+  Channel_Closed,\n+  Life_Version_Conflict,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyODQ3Mg==", "bodyText": "Does U/D carry expiration time?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373628472", "createdAt": "2020-01-31T18:45:57Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -518,17 +532,18 @@ private boolean needToRollOverIndex(IndexEntry entry) {\n   }\n \n   /**\n-   * Finds a key in the index and returns the blob index value associated with it. If not found,\n-   * returns null\n+   * Finds {@link IndexValue} that represents the latest state of the given {@code key} in the index and return it. If\n+   * not found, returns null.\n    * <br>\n-   * This method only returns PUT or DELETE index entries. It does not return TTL_UPDATE entries but accounts for\n-   * TTL updates by updating the flag and expiry time (if applicable).\n+   * This method returns the final state of the given {@code key}. The final state of a key can be a Put value, a Delete\n+   * value and a Undelete value. Ttl_update isn't considered as final state as it just update the expiration date.\n+   * {@link IndexValue} returned by this method would carry expiration date from Ttl_update if there is one.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyOTc5OA==", "bodyText": "The word reversed is confused.\nBecause the default order in our mind is latest to oldest. How about not use reverse in function name, just add some comments about what the order is.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373629798", "createdAt": "2020-01-31T18:48:56Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKeyInReverseOrder(StoreKey key, FileSpan fileSpan,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYzMjY1NA==", "bodyText": "Is it possible to embed TTL_UPDATE FLAG in D/U all the time?\nIf we support UNDELETE to DELETE after we release this feature, there won't be backward compatible  issue.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373632654", "createdAt": "2020-01-31T18:55:33Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -518,17 +532,18 @@ private boolean needToRollOverIndex(IndexEntry entry) {\n   }\n \n   /**\n-   * Finds a key in the index and returns the blob index value associated with it. If not found,\n-   * returns null\n+   * Finds {@link IndexValue} that represents the latest state of the given {@code key} in the index and return it. If\n+   * not found, returns null.\n    * <br>\n-   * This method only returns PUT or DELETE index entries. It does not return TTL_UPDATE entries but accounts for\n-   * TTL updates by updating the flag and expiry time (if applicable).\n+   * This method returns the final state of the given {@code key}. The final state of a key can be a Put value, a Delete\n+   * value and a Undelete value. Ttl_update isn't considered as final state as it just update the expiration date.\n+   * {@link IndexValue} returned by this method would carry expiration date from Ttl_update if there is one.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyODQ3Mg=="}, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYzMzI0MA==", "bodyText": "Why getDeletedBlobReadOptions is so complicated?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373633240", "createdAt": "2020-01-31T18:56:45Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -839,6 +1099,33 @@ private BlobReadOptions getDeletedBlobReadOptions(IndexValue value, StoreKey key\n     return readOptions;\n   }\n \n+  /**\n+   * Gets {@link BlobReadOptions} for a undeleted blob.\n+   * @param value the {@link IndexValue} of the delete index entry for the blob.\n+   * @param key the {@link StoreKey} for which {@code value} is the delete {@link IndexValue}\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return the {@link BlobReadOptions} that contains the information for the given {@code id}\n+   * @throws StoreException\n+   */\n+  private BlobReadOptions getUndeletedBlobReadOptions(IndexValue value, StoreKey key,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 502}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYzMzYzNQ==", "bodyText": "Why is size changed?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373633635", "createdAt": "2020-01-31T18:57:38Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/test/java/com.github.ambry.store/CuratedLogIndexState.java", "diffHunk": "@@ -77,7 +77,10 @@\n   // deliberately do not divide the capacities perfectly.\n   static final long PUT_RECORD_SIZE = 53;\n   static final long DELETE_RECORD_SIZE = 29;\n-  static final long TTL_UPDATE_RECORD_SIZE = 37;\n+  static final long TTL_UPDATE_RECORD_SIZE = 29;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYzNDg0NA==", "bodyText": "Or, can we set it permanent if a blob with TTL is undeleted?  To make TTL_UPDATE easier.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373634844", "createdAt": "2020-01-31T19:00:32Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -518,17 +532,18 @@ private boolean needToRollOverIndex(IndexEntry entry) {\n   }\n \n   /**\n-   * Finds a key in the index and returns the blob index value associated with it. If not found,\n-   * returns null\n+   * Finds {@link IndexValue} that represents the latest state of the given {@code key} in the index and return it. If\n+   * not found, returns null.\n    * <br>\n-   * This method only returns PUT or DELETE index entries. It does not return TTL_UPDATE entries but accounts for\n-   * TTL updates by updating the flag and expiry time (if applicable).\n+   * This method returns the final state of the given {@code key}. The final state of a key can be a Put value, a Delete\n+   * value and a Undelete value. Ttl_update isn't considered as final state as it just update the expiration date.\n+   * {@link IndexValue} returned by this method would carry expiration date from Ttl_update if there is one.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyODQ3Mg=="}, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 85}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0ed6e5c1a76f0fad0c6fbb244ea78f69f494b13a", "author": {"user": {"login": "dharju", "name": "David Harju"}}, "url": "https://github.com/linkedin/ambry/commit/0ed6e5c1a76f0fad0c6fbb244ea78f69f494b13a", "committedDate": "2020-01-31T19:25:23Z", "message": "Added behavioral changes for PersistentIndex for Undeletes\n\nAdded behavioral changes for PersistentIndex for Undeletes\n\nfix for IndexTest failure\n\nAdded test for bad undelete, test infra for undelete in CuratedLogIndexState\n\nAdded tests for index undelete\n\nthing to trigger change add\n\nthing to trigger build del\n\nthing to trigger change add\n\nthing to trigger build del\n\nthing to trigger change add\n\nthing to trigger build del\n\nfixed test\n\nadded undelete/lifeVersion awareness to findEntriesSince method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eafaf0ad50234f17f6f9464376790697285a89e5", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/eafaf0ad50234f17f6f9464376790697285a89e5", "committedDate": "2020-01-31T19:25:23Z", "message": "wip"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eedf9524b48fae96f5d55e7f31d809d035a495f6", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/eedf9524b48fae96f5d55e7f31d809d035a495f6", "committedDate": "2020-01-31T19:25:23Z", "message": "Fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f02fe7a6961288805c5861b1cdfcd70d900e5632", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/f02fe7a6961288805c5861b1cdfcd70d900e5632", "committedDate": "2020-01-31T19:25:23Z", "message": "Address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf39281166bdcb85c404ba7aa69bd4cff5eadb9a", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/cf39281166bdcb85c404ba7aa69bd4cff5eadb9a", "committedDate": "2020-01-31T19:25:23Z", "message": "Updates"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c73c0395656a5d0dfcb643b4e406831693ee995d", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/c73c0395656a5d0dfcb643b4e406831693ee995d", "committedDate": "2020-01-31T19:25:23Z", "message": "Comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/cc0dd0855f4405b60609475b40e1f25f66c3e336", "committedDate": "2020-01-31T20:10:08Z", "message": "Comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/d52a85b804a74458ad517fcd7c373895a3ce2b1f", "committedDate": "2020-01-30T20:07:55Z", "message": "Comments"}, "afterCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/cc0dd0855f4405b60609475b40e1f25f66c3e336", "committedDate": "2020-01-31T20:10:08Z", "message": "Comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxNzg5NTEw", "url": "https://github.com/linkedin/ambry/pull/1366#pullrequestreview-351789510", "createdAt": "2020-01-31T21:30:45Z", "commit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxODA4OTkz", "url": "https://github.com/linkedin/ambry/pull/1366#pullrequestreview-351808993", "createdAt": "2020-01-31T22:14:54Z", "commit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "state": "COMMENTED", "comments": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMjoxNDo1NFrOFkZaFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzo1NTo1MFrOFka7rQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcwOTMzNQ==", "bodyText": "nit: inserting undelete entry of size ...", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373709335", "createdAt": "2020-01-31T22:14:54Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -315,31 +316,44 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n         Offset infoEndOffset = new Offset(runningOffset.getName(), runningOffset.getOffset() + info.getSize());\n         IndexValue value = findKey(info.getStoreKey());\n         if (info.isDeleted()) {\n-          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n-          logger.info(\"Index : {} updated message with key {} by inserting delete entry of size {} ttl {}\", dataDir,\n-              info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs());\n+          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info, info.getOperationTimeMs(),\n+              info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting delete entry of size {} ttl {} lifeVersion{}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           // removes from the tracking structure if a delete was being expected for the key\n           deleteExpectedKeys.remove(info.getStoreKey());\n         } else if (info.isTtlUpdated()) {\n           markAsPermanent(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n-          logger.info(\"Index : {} updated message with key {} by inserting TTL update entry of size {} ttl {}\", dataDir,\n-              info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs());\n+              info.getOperationTimeMs(), info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting TTL update entry of size {} ttl {} lifeVersion {}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           if (value == null) {\n             // this TTL update was forced even though there was no equivalent PUT record - this means that we MUST see\n             // a DELETE for this key (because the PUT record is gone, compaction must have cleaned it up because a\n             // DELETE must have been present)\n             deleteExpectedKeys.add(info.getStoreKey());\n           }\n+        } else if (info.isUndeleted()) {\n+          markAsUndeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info.getOperationTimeMs(),\n+              info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting undelete update entry of size {} ttl {} lifeVersion {}\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcxMzUzMQ==", "bodyText": "minor: PUT or DELETE were expected, right?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373713531", "createdAt": "2020-01-31T22:28:25Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -315,31 +316,44 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n         Offset infoEndOffset = new Offset(runningOffset.getName(), runningOffset.getOffset() + info.getSize());\n         IndexValue value = findKey(info.getStoreKey());\n         if (info.isDeleted()) {\n-          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n-          logger.info(\"Index : {} updated message with key {} by inserting delete entry of size {} ttl {}\", dataDir,\n-              info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs());\n+          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info, info.getOperationTimeMs(),\n+              info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting delete entry of size {} ttl {} lifeVersion{}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           // removes from the tracking structure if a delete was being expected for the key\n           deleteExpectedKeys.remove(info.getStoreKey());\n         } else if (info.isTtlUpdated()) {\n           markAsPermanent(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n-          logger.info(\"Index : {} updated message with key {} by inserting TTL update entry of size {} ttl {}\", dataDir,\n-              info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs());\n+              info.getOperationTimeMs(), info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting TTL update entry of size {} ttl {} lifeVersion {}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           if (value == null) {\n             // this TTL update was forced even though there was no equivalent PUT record - this means that we MUST see\n             // a DELETE for this key (because the PUT record is gone, compaction must have cleaned it up because a\n             // DELETE must have been present)\n             deleteExpectedKeys.add(info.getStoreKey());\n           }\n+        } else if (info.isUndeleted()) {\n+          markAsUndeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info.getOperationTimeMs(),\n+              info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting undelete update entry of size {} ttl {} lifeVersion {}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n+          if (value == null) {\n+            // Undelete record indicates that there might be a put and delete record before it.\n+            throw new StoreException(\"Put record were expected but were not encountered for key: \" + info.getStoreKey(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcxNDc4Ng==", "bodyText": "After a second thought, is it possible that Put record has been compacted. Just like value == null for TTL update? My point is, it may not need an exception here.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373714786", "createdAt": "2020-01-31T22:32:49Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -315,31 +316,44 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n         Offset infoEndOffset = new Offset(runningOffset.getName(), runningOffset.getOffset() + info.getSize());\n         IndexValue value = findKey(info.getStoreKey());\n         if (info.isDeleted()) {\n-          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n-          logger.info(\"Index : {} updated message with key {} by inserting delete entry of size {} ttl {}\", dataDir,\n-              info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs());\n+          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info, info.getOperationTimeMs(),\n+              info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting delete entry of size {} ttl {} lifeVersion{}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           // removes from the tracking structure if a delete was being expected for the key\n           deleteExpectedKeys.remove(info.getStoreKey());\n         } else if (info.isTtlUpdated()) {\n           markAsPermanent(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n-          logger.info(\"Index : {} updated message with key {} by inserting TTL update entry of size {} ttl {}\", dataDir,\n-              info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs());\n+              info.getOperationTimeMs(), info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting TTL update entry of size {} ttl {} lifeVersion {}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           if (value == null) {\n             // this TTL update was forced even though there was no equivalent PUT record - this means that we MUST see\n             // a DELETE for this key (because the PUT record is gone, compaction must have cleaned it up because a\n             // DELETE must have been present)\n             deleteExpectedKeys.add(info.getStoreKey());\n           }\n+        } else if (info.isUndeleted()) {\n+          markAsUndeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info.getOperationTimeMs(),\n+              info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting undelete update entry of size {} ttl {} lifeVersion {}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n+          if (value == null) {\n+            // Undelete record indicates that there might be a put and delete record before it.\n+            throw new StoreException(\"Put record were expected but were not encountered for key: \" + info.getStoreKey(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcxMzUzMQ=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyMjg0NQ==", "bodyText": "Can you present a case where we do need to check !value.isDelete() && !value.isUndelete()? I know the piece of code comes from previous version, I just don't quite understand why we need such additional check.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373722845", "createdAt": "2020-01-31T23:01:36Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -590,15 +605,17 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n             }\n             logger.trace(\"Index : {} found value offset {} size {} ttl {}\", dataDir, value.getOffset(), value.getSize(),\n                 value.getExpiresAtMs());\n-            if (types.contains(IndexEntryType.DELETE) && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+            if (types.contains(IndexEntryType.DELETE) && value.isDelete()) {\n+              retCandidate = value;\n+              break;\n+            } else if (types.contains(IndexEntryType.UNDELETE) && value.isUndelete()) {\n               retCandidate = value;\n               break;\n-            } else if (types.contains(IndexEntryType.TTL_UPDATE) && !value.isFlagSet(IndexValue.Flags.Delete_Index)\n-                && value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+            } else if (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete() && !value.isUndelete()\n+                && value.isTTLUpdate()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyNTQzNA==", "bodyText": "can be private", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373725434", "createdAt": "2020-01-31T23:12:26Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyNjEzMw==", "bodyText": "minor: change to Searching all index values for to distinguish from findKey method", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373726133", "createdAt": "2020-01-31T23:15:38Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyNjQxMg==", "bodyText": "same here", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373726412", "createdAt": "2020-01-31T23:16:53Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyNjYyMg==", "bodyText": "same here", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373726622", "createdAt": "2020-01-31T23:17:45Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyNzQ0Mw==", "bodyText": "types.contains(IndexEntryType.TTL_UPDATE logic doesn't consider !value.isUndelete() like that in findKey method. Any specific reason?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373727443", "createdAt": "2020-01-31T23:21:33Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyODAxNw==", "bodyText": "Optional: probably worth adding a constructor for IndexValue that takes in another index value for deep copy purpose. (Pass in many parameters is error-prone)", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373728017", "createdAt": "2020-01-31T23:24:19Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyODI1Nw==", "bodyText": "typo Returning", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373728257", "createdAt": "2020-01-31T23:25:34Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyODk5Ng==", "bodyText": "Looks like result may be null. Can you explicitly mention this in java doc?\nAlso, if null means not find index value related to given key. We can make result = new ArrayList<>() at the very beginning and in the end, return result.isEmpty() ? null : result;\nThen we can remove\n if (result == null) {\n            result = new LinkedList<>();\n  }", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373728996", "createdAt": "2020-01-31T23:28:04Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyOTExMA==", "bodyText": "Or probably we can accept an empty result as a return value.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373729110", "createdAt": "2020-01-31T23:28:30Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyODk5Ng=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzczMjQ4OA==", "bodyText": "I get you point here but I would suggest reverse values before hand which makes logic easier to understand and more consistent with your java docs of this method.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373732488", "createdAt": "2020-01-31T23:46:04Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key}.\n+   * <p/>\n+   * Undelete should be permitted only when the first record is a Put and last record is a Delete, and the Put record\n+   * hasn't expired yet.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   * @param lifeVersion lifeVersion for the undelete record, it's only valid when in recovery or replication.\n+   */\n+  void validateSanityForUndelete(StoreKey key, List<IndexValue> values, short lifeVersion) throws StoreException {\n+    if (values == null || values.isEmpty()) {\n+      throw new StoreException(\"Id \" + key + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n+    }\n+    if (!IndexValue.hasLifeVersion(lifeVersion)) {\n+      validateSanityForUndeleteWithoutLifeVersion(key, values);\n+      return;\n+    }\n+    IndexValue firstValue = values.get(values.size() - 1);\n+    IndexValue lastValue = values.get(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzczMjk1Mg==", "bodyText": "Why we didn't check last value type and expiration time of PUT in this method?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373732952", "createdAt": "2020-01-31T23:48:31Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key}.\n+   * <p/>\n+   * Undelete should be permitted only when the first record is a Put and last record is a Delete, and the Put record\n+   * hasn't expired yet.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   * @param lifeVersion lifeVersion for the undelete record, it's only valid when in recovery or replication.\n+   */\n+  void validateSanityForUndelete(StoreKey key, List<IndexValue> values, short lifeVersion) throws StoreException {\n+    if (values == null || values.isEmpty()) {\n+      throw new StoreException(\"Id \" + key + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n+    }\n+    if (!IndexValue.hasLifeVersion(lifeVersion)) {\n+      validateSanityForUndeleteWithoutLifeVersion(key, values);\n+      return;\n+    }\n+    IndexValue firstValue = values.get(values.size() - 1);\n+    IndexValue lastValue = values.get(0);\n+    if (!firstValue.isPut()) {\n+      throw new StoreException(\"Id \" + key + \" requires first value to be a put in index \" + dataDir,\n+          StoreErrorCodes.ID_Deleted_Permanently);\n+    }\n+    if (lastValue.getLifeVersion() >= lifeVersion) {\n+      throw new StoreException(\n+          \"LifeVersion conflict in index. Id \" + key + \" LifeVersion: \" + lastValue.getLifeVersion()\n+              + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzczNDMxNw==", "bodyText": "if target is already updated or expiration time = -1, in either case we can skip update", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373734317", "createdAt": "2020-01-31T23:55:50Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key}.\n+   * <p/>\n+   * Undelete should be permitted only when the first record is a Put and last record is a Delete, and the Put record\n+   * hasn't expired yet.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   * @param lifeVersion lifeVersion for the undelete record, it's only valid when in recovery or replication.\n+   */\n+  void validateSanityForUndelete(StoreKey key, List<IndexValue> values, short lifeVersion) throws StoreException {\n+    if (values == null || values.isEmpty()) {\n+      throw new StoreException(\"Id \" + key + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n+    }\n+    if (!IndexValue.hasLifeVersion(lifeVersion)) {\n+      validateSanityForUndeleteWithoutLifeVersion(key, values);\n+      return;\n+    }\n+    IndexValue firstValue = values.get(values.size() - 1);\n+    IndexValue lastValue = values.get(0);\n+    if (!firstValue.isPut()) {\n+      throw new StoreException(\"Id \" + key + \" requires first value to be a put in index \" + dataDir,\n+          StoreErrorCodes.ID_Deleted_Permanently);\n+    }\n+    if (lastValue.getLifeVersion() >= lifeVersion) {\n+      throw new StoreException(\n+          \"LifeVersion conflict in index. Id \" + key + \" LifeVersion: \" + lastValue.getLifeVersion()\n+              + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n+    }\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key} when there is\n+   * no lifeVersion provided.\n+   * <p/>\n+   * Undelete should be permitted only when the first record is a Put and last record is a Delete, and the Put record\n+   * hasn't expired yet.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   */\n+  void validateSanityForUndeleteWithoutLifeVersion(StoreKey key, List<IndexValue> values) throws StoreException {\n+    // When it's valid to undelete this key\n+    // P/T + D\n+    // P/T + D + U + D\n+    if (values.size() == 1) {\n+      IndexValue value = values.get(0);\n+      if (value.isDelete() || value.isTTLUpdate()) {\n+        throw new StoreException(\"Id \" + key + \" is compacted in index\" + dataDir,\n+            StoreErrorCodes.ID_Deleted_Permanently);\n+      } else if (value.isPut()) {\n+        throw new StoreException(\"Id \" + key + \" is not deleted yet in index \" + dataDir,\n+            StoreErrorCodes.ID_Not_Deleted);\n+      } else {\n+        throw new StoreException(\"Id \" + key + \" is already undeleted in index\" + dataDir,\n+            StoreErrorCodes.ID_Undeleted);\n+      }\n+    }\n+    // First item has to be put and last item has to be a delete.\n+    // PutRecord can't expire and delete record can't be older than the delete retention time.\n+    IndexValue firstValue = values.get(values.size() - 1);\n+    IndexValue lastValue = values.get(0);\n+    if (lastValue.isUndelete()) {\n+      throw new StoreException(\"Id \" + key + \" is already undeleted in index\" + dataDir, StoreErrorCodes.ID_Undeleted);\n+    }\n+    if (!firstValue.isPut() || !lastValue.isDelete()) {\n+      throw new StoreException(\n+          \"Id \" + key + \" requires first value to be a put and last value to be a delete in index \" + dataDir,\n+          StoreErrorCodes.ID_Not_Deleted);\n+    }\n+    if (lastValue.getOperationTimeInMs() + TimeUnit.DAYS.toMillis(config.storeDeletedMessageRetentionDays)\n+        < time.milliseconds()) {\n+      throw new StoreException(\"Id \" + key + \" already permanently deleted in index \" + dataDir,\n+          StoreErrorCodes.ID_Deleted_Permanently);\n+    }\n+    maybeChangeExpirationDate(firstValue, values);\n+    if (isExpired(firstValue)) {\n+      throw new StoreException(\"Id \" + key + \" already expired in index \" + dataDir, StoreErrorCodes.TTL_Expired);\n+    }\n+  }\n+\n+  /**\n+   * Change the target's expiration date and set the ttl_update_index to be true if there is a ttl index in given list.\n+   * @param target the {@link IndexValue} to change expiration date.\n+   * @param allValues the given list of {@link IndexValue}s.\n+   */\n+  void maybeChangeExpirationDate(IndexValue target, List<IndexValue> allValues) {\n+    if (target.isTTLUpdate()) {\n+      return;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 271}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxODU1OTc2", "url": "https://github.com/linkedin/ambry/pull/1366#pullrequestreview-351855976", "createdAt": "2020-02-01T01:40:05Z", "commit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQwMTo0MDowNVrOFkbxlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQwMTo0OTo0OFrOFkb0rg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc0ODExOA==", "bodyText": "nit: carries", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373748118", "createdAt": "2020-02-01T01:40:05Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -658,33 +834,55 @@ IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, long deletionTimeMs) th\n    * @param info this needs to be non-null in the case of recovery. Can be {@code null} otherwise. Used if the PUT\n    *             record could not be found\n    * @param deletionTimeMs deletion time of the blob. In-case of recovery, deletion time is obtained from {@code info}.\n+   * @param lifeVersion lifeVersion of this undelete record.\n    * @return the {@link IndexValue} of the delete record\n    * @throws StoreException\n    */\n-  private IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, MessageInfo info, long deletionTimeMs)\n+  IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, MessageInfo info, long deletionTimeMs, short lifeVersion)\n       throws StoreException {\n+    boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     validateFileSpan(fileSpan, true);\n     IndexValue value = findKey(id);\n     if (value == null && info == null) {\n       throw new StoreException(\"Id \" + id + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n-    } else if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n-      throw new StoreException(\"Id \" + id + \" already deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n+    } else if (value != null) {\n+      if (hasLifeVersion) {\n+        // When this method is invoked in either recovery or replication, delete can follow any index value.\n+        if ((value.isDelete() && value.getLifeVersion() >= lifeVersion) || (value.getLifeVersion() > lifeVersion)) {\n+          throw new StoreException(\"LifeVersion conflict in index. Id \" + id + \" LifeVersion: \" + value.getLifeVersion()\n+              + \" Delete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n+        }\n+      } else {\n+        if (value.isDelete()) {\n+          throw new StoreException(\"Id \" + id + \" already deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n+        }\n+      }\n     }\n     long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     IndexValue newValue;\n     if (value == null) {\n       // It is possible that the PUT has been cleaned by compaction\n+      if (!hasLifeVersion) {\n+        throw new StoreException(\"MessageInfo of delete carry invalid lifeVersion\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 330}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc0ODkxMA==", "bodyText": "should we clear TTL Update flag as well?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373748910", "createdAt": "2020-02-01T01:49:48Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -658,33 +834,55 @@ IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, long deletionTimeMs) th\n    * @param info this needs to be non-null in the case of recovery. Can be {@code null} otherwise. Used if the PUT\n    *             record could not be found\n    * @param deletionTimeMs deletion time of the blob. In-case of recovery, deletion time is obtained from {@code info}.\n+   * @param lifeVersion lifeVersion of this undelete record.\n    * @return the {@link IndexValue} of the delete record\n    * @throws StoreException\n    */\n-  private IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, MessageInfo info, long deletionTimeMs)\n+  IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, MessageInfo info, long deletionTimeMs, short lifeVersion)\n       throws StoreException {\n+    boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     validateFileSpan(fileSpan, true);\n     IndexValue value = findKey(id);\n     if (value == null && info == null) {\n       throw new StoreException(\"Id \" + id + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n-    } else if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n-      throw new StoreException(\"Id \" + id + \" already deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n+    } else if (value != null) {\n+      if (hasLifeVersion) {\n+        // When this method is invoked in either recovery or replication, delete can follow any index value.\n+        if ((value.isDelete() && value.getLifeVersion() >= lifeVersion) || (value.getLifeVersion() > lifeVersion)) {\n+          throw new StoreException(\"LifeVersion conflict in index. Id \" + id + \" LifeVersion: \" + value.getLifeVersion()\n+              + \" Delete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n+        }\n+      } else {\n+        if (value.isDelete()) {\n+          throw new StoreException(\"Id \" + id + \" already deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n+        }\n+      }\n     }\n     long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     IndexValue newValue;\n     if (value == null) {\n       // It is possible that the PUT has been cleaned by compaction\n+      if (!hasLifeVersion) {\n+        throw new StoreException(\"MessageInfo of delete carry invalid lifeVersion\",\n+            StoreErrorCodes.Initialization_Error);\n+      }\n       newValue =\n-          new IndexValue(size, fileSpan.getStartOffset(), info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n-              info.getAccountId(), info.getContainerId());\n+          new IndexValue(size, fileSpan.getStartOffset(), IndexValue.FLAGS_DEFAULT_VALUE, info.getExpirationTimeInMs(),\n+              info.getOperationTimeMs(), info.getAccountId(), info.getContainerId(), lifeVersion);\n       newValue.clearOriginalMessageOffset();\n     } else {\n+      lifeVersion = hasLifeVersion ? lifeVersion : value.getLifeVersion();\n       newValue =\n           new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(), deletionTimeMs,\n-              value.getAccountId(), value.getContainerId(), (short) 0);\n+              value.getAccountId(), value.getContainerId(), lifeVersion);\n       newValue.setNewOffset(fileSpan.getStartOffset());\n+      // Only set the original message offset when the value is put\n+      if (!value.isPut()) {\n+        newValue.clearOriginalMessageOffset();\n+      }\n       newValue.setNewSize(size);\n     }\n+    newValue.clearFlag(IndexValue.Flags.Undelete_Index);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 352}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ea2ba25eebd566b2d94119211b5f549a59fc2da3", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/ea2ba25eebd566b2d94119211b5f549a59fc2da3", "committedDate": "2020-02-01T23:45:19Z", "message": "More comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUyMDIwNzA2", "url": "https://github.com/linkedin/ambry/pull/1366#pullrequestreview-352020706", "createdAt": "2020-02-03T04:55:07Z", "commit": {"oid": "ea2ba25eebd566b2d94119211b5f549a59fc2da3"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QwNDo1NTowN1rOFkmYxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QwNjoxNjozOFrOFknPag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkyMTk5MQ==", "bodyText": "I see your comment. Also, like I said, we are able to remove\n if (result == null) {\n            result = new LinkedList<>();\n  }\n\nin the for loop.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373921991", "createdAt": "2020-02-03T04:55:07Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyODk5Ng=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkyNjA2MA==", "bodyText": "Looks like firstValue/lastValue ordering is different from that in method validateSanityForUndelete().  I don't see any issue here but could we make them consistent?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373926060", "createdAt": "2020-02-03T05:22:38Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,167 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order. If there is\n+   * no matched {@link IndexValue}, this method would return null;\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching all indexes for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching all indexes for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset()\n+                + \" to \" + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching all indexes with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && !value.isUndelete() && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT)\n+                && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              result.add(new IndexValue(value));\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returning values {}\", dataDir, result);\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key}.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   * @param lifeVersion lifeVersion for the undelete record, it's only valid when in recovery or replication.\n+   */\n+  void validateSanityForUndelete(StoreKey key, List<IndexValue> values, short lifeVersion) throws StoreException {\n+    if (values == null || values.isEmpty()) {\n+      throw new StoreException(\"Id \" + key + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n+    }\n+    if (!IndexValue.hasLifeVersion(lifeVersion)) {\n+      validateSanityForUndeleteWithoutLifeVersion(key, values);\n+      return;\n+    }\n+    // This is from recovery or replication, make sure the last value is a put and the first value's lifeVersion is strictly\n+    // less than the given lifeVersion. We don't care about the first value's type, it can be a put, ttl_update or delete, it\n+    // can even be an undelete.\n+    IndexValue lastValue = values.get(values.size() - 1);\n+    IndexValue firstValue = values.get(0);\n+    if (!lastValue.isPut()) {\n+      throw new StoreException(\"Id \" + key + \" requires first value to be a put in index \" + dataDir,\n+          StoreErrorCodes.ID_Deleted_Permanently);\n+    }\n+    if (firstValue.getLifeVersion() >= lifeVersion) {\n+      throw new StoreException(\n+          \"LifeVersion conflict in index. Id \" + key + \" LifeVersion: \" + firstValue.getLifeVersion()\n+              + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n+    }\n+    maybeChangeExpirationDate(lastValue, values);\n+    if (isExpired(lastValue)) {\n+      throw new StoreException(\"Id \" + key + \" already expired in index \" + dataDir, StoreErrorCodes.TTL_Expired);\n+    }\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key} when there is\n+   * no lifeVersion provided.\n+   * <p/>\n+   * Undelete should be permitted only when the last value is a Put and first record is a Delete, and the Put record\n+   * hasn't expired yet.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   */\n+  void validateSanityForUndeleteWithoutLifeVersion(StoreKey key, List<IndexValue> values) throws StoreException {\n+    // When it's valid to undelete this key\n+    // P/T + D\n+    // P/T + D + U + D\n+    if (values.size() == 1) {\n+      IndexValue value = values.get(0);\n+      if (value.isDelete() || value.isTTLUpdate()) {\n+        throw new StoreException(\"Id \" + key + \" is compacted in index\" + dataDir,\n+            StoreErrorCodes.ID_Deleted_Permanently);\n+      } else if (value.isPut()) {\n+        throw new StoreException(\"Id \" + key + \" is not deleted yet in index \" + dataDir,\n+            StoreErrorCodes.ID_Not_Deleted);\n+      } else {\n+        throw new StoreException(\"Id \" + key + \" is already undeleted in index\" + dataDir,\n+            StoreErrorCodes.ID_Undeleted);\n+      }\n+    }\n+    // First item has to be put and last item has to be a delete.\n+    // PutRecord can't expire and delete record can't be older than the delete retention time.\n+    IndexValue firstValue = values.get(0);\n+    IndexValue lastValue = values.get(values.size() - 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ea2ba25eebd566b2d94119211b5f549a59fc2da3"}, "originalPosition": 242}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkzNDc1MQ==", "bodyText": "typo: as undelete", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373934751", "createdAt": "2020-02-03T06:11:06Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -709,40 +909,100 @@ IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, long operationTimeMs)\n    * @param operationTimeMs the time of the update operation\n    * @param info this needs to be non-null in the case of recovery. Can be {@code null} otherwise. Used if the PUT\n    *             record could not be found\n+   * @param lifeVersion lifeVersion of this ttlUpdate record.\n    * @return the {@link IndexValue} of the ttl update record\n    * @throws StoreException if there is any problem writing the index record\n    */\n-  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs)\n-      throws StoreException {\n+  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs,\n+      short lifeVersion) throws StoreException {\n     validateFileSpan(fileSpan, true);\n+    boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     IndexValue value = findKey(id);\n     if (value == null && info == null) {\n       throw new StoreException(\"Id \" + id + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n-    } else if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+    }\n+    short retrievedLifeVersion = value == null ? info.getLifeVersion() : value.getLifeVersion();\n+    if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n       throw new StoreException(\"Id \" + id + \" deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n     } else if (value != null && value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n       throw new StoreException(\"TTL of \" + id + \" already updated in index\" + dataDir, StoreErrorCodes.Already_Updated);\n+    } else if (hasLifeVersion && retrievedLifeVersion > lifeVersion) {\n+      throw new StoreException(\"LifeVersion conflict in index. Id \" + id + \" LifeVersion: \" + retrievedLifeVersion\n+          + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n     }\n     long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     IndexValue newValue;\n+\n     if (value == null) {\n       // It is possible that the PUT has been cleaned by compaction\n+      // but the TTL update is going to still be placed?\n+      if (!hasLifeVersion) {\n+        throw new StoreException(\"MessageInfo of ttlUpdate carries invalid lifeVersion\",\n+            StoreErrorCodes.Initialization_Error);\n+      }\n       newValue =\n-          new IndexValue(size, fileSpan.getStartOffset(), info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n-              info.getAccountId(), info.getContainerId());\n+          new IndexValue(size, fileSpan.getStartOffset(), IndexValue.FLAGS_DEFAULT_VALUE, info.getExpirationTimeInMs(),\n+              info.getOperationTimeMs(), info.getAccountId(), info.getContainerId(), lifeVersion);\n       newValue.clearOriginalMessageOffset();\n     } else {\n+      lifeVersion = hasLifeVersion ? lifeVersion : value.getLifeVersion();\n       newValue =\n           new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), Utils.Infinite_Time, operationTimeMs,\n-              value.getAccountId(), value.getContainerId(), (short) 0);\n+              value.getAccountId(), value.getContainerId(), lifeVersion);\n       newValue.setNewOffset(fileSpan.getStartOffset());\n       newValue.setNewSize(size);\n     }\n+    newValue.clearFlag(IndexValue.Flags.Undelete_Index);\n     newValue.setFlag(IndexValue.Flags.Ttl_Update_Index);\n     addToIndex(new IndexEntry(id, newValue, null), fileSpan);\n     return newValue;\n   }\n \n+  /**\n+   * Marks a blob as permanent", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ea2ba25eebd566b2d94119211b5f549a59fc2da3"}, "originalPosition": 423}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkzNTk3OA==", "bodyText": "I have same question here, do you have any idea why TTL update is placed even when PUT has been cleaned up?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373935978", "createdAt": "2020-02-03T06:16:38Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -709,40 +909,100 @@ IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, long operationTimeMs)\n    * @param operationTimeMs the time of the update operation\n    * @param info this needs to be non-null in the case of recovery. Can be {@code null} otherwise. Used if the PUT\n    *             record could not be found\n+   * @param lifeVersion lifeVersion of this ttlUpdate record.\n    * @return the {@link IndexValue} of the ttl update record\n    * @throws StoreException if there is any problem writing the index record\n    */\n-  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs)\n-      throws StoreException {\n+  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs,\n+      short lifeVersion) throws StoreException {\n     validateFileSpan(fileSpan, true);\n+    boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     IndexValue value = findKey(id);\n     if (value == null && info == null) {\n       throw new StoreException(\"Id \" + id + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n-    } else if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+    }\n+    short retrievedLifeVersion = value == null ? info.getLifeVersion() : value.getLifeVersion();\n+    if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n       throw new StoreException(\"Id \" + id + \" deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n     } else if (value != null && value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n       throw new StoreException(\"TTL of \" + id + \" already updated in index\" + dataDir, StoreErrorCodes.Already_Updated);\n+    } else if (hasLifeVersion && retrievedLifeVersion > lifeVersion) {\n+      throw new StoreException(\"LifeVersion conflict in index. Id \" + id + \" LifeVersion: \" + retrievedLifeVersion\n+          + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n     }\n     long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     IndexValue newValue;\n+\n     if (value == null) {\n       // It is possible that the PUT has been cleaned by compaction\n+      // but the TTL update is going to still be placed?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ea2ba25eebd566b2d94119211b5f549a59fc2da3"}, "originalPosition": 396}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f65940cac1fe13b1d839c94d2ff81df94299c6fa", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/f65940cac1fe13b1d839c94d2ff81df94299c6fa", "committedDate": "2020-02-03T18:08:31Z", "message": "More comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUyNDY5MDM4", "url": "https://github.com/linkedin/ambry/pull/1366#pullrequestreview-352469038", "createdAt": "2020-02-03T18:32:44Z", "commit": {"oid": "f65940cac1fe13b1d839c94d2ff81df94299c6fa"}, "state": "APPROVED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxODozMjo0NFrOFk7d9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxODo0NjoxMVrOFk74Wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI2NzM4Mw==", "bodyText": "please update comment for value and key parameters (change delete to undelete)", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r374267383", "createdAt": "2020-02-03T18:32:44Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -839,6 +1098,33 @@ private BlobReadOptions getDeletedBlobReadOptions(IndexValue value, StoreKey key\n     return readOptions;\n   }\n \n+  /**\n+   * Gets {@link BlobReadOptions} for a undeleted blob.\n+   * @param value the {@link IndexValue} of the delete index entry for the blob.\n+   * @param key the {@link StoreKey} for which {@code value} is the delete {@link IndexValue}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f65940cac1fe13b1d839c94d2ff81df94299c6fa"}, "originalPosition": 492}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI3MDg4MQ==", "bodyText": "nit: value.isTTLUpdate()", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r374270881", "createdAt": "2020-02-03T18:39:28Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -839,6 +1098,33 @@ private BlobReadOptions getDeletedBlobReadOptions(IndexValue value, StoreKey key\n     return readOptions;\n   }\n \n+  /**\n+   * Gets {@link BlobReadOptions} for a undeleted blob.\n+   * @param value the {@link IndexValue} of the delete index entry for the blob.\n+   * @param key the {@link StoreKey} for which {@code value} is the delete {@link IndexValue}\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return the {@link BlobReadOptions} that contains the information for the given {@code id}\n+   * @throws StoreException\n+   */\n+  private BlobReadOptions getUndeletedBlobReadOptions(IndexValue value, StoreKey key,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    IndexValue putValue =\n+        findKey(key, new FileSpan(getStartOffset(indexSegments), value.getOffset()), EnumSet.of(IndexEntryType.PUT),\n+            indexSegments);\n+    if (putValue != null) {\n+      // use the expiration time from the original value because it may have been updated\n+      // since we are here dealing with undelete blob, we have to return the right life version\n+      return new BlobReadOptions(log, putValue.getOffset(),\n+          new MessageInfo(key, putValue.getSize(), false, value.isFlagSet(IndexValue.Flags.Ttl_Update_Index), true,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f65940cac1fe13b1d839c94d2ff81df94299c6fa"}, "originalPosition": 506}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI3MzgxNA==", "bodyText": "why here we filter delete entries again? Is this because messageInfo is updated in updateStateForMessages?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r374273814", "createdAt": "2020-02-03T18:45:31Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -1555,6 +1839,10 @@ FindInfo findDeletedEntriesSince(FindToken token, long maxTotalSizeOfEntries, lo\n         }\n       }\n     }\n+    // Filter out all the messages that are not \"deleted\", then update state for remaining deleted message.\n+    filterDeleteEntries(messageEntries);\n+    updateStateForMessages(messageEntries);\n+\n     filterDeleteEntries(messageEntries);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f65940cac1fe13b1d839c94d2ff81df94299c6fa"}, "originalPosition": 593}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI3NDEzOA==", "bodyText": "minor: please update java doc for this method.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r374274138", "createdAt": "2020-02-03T18:46:11Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -1342,17 +1627,16 @@ private void updateStateForMessages(List<MessageInfo> messageEntries) throws Sto\n     ListIterator<MessageInfo> messageEntriesIterator = messageEntries.listIterator();\n     while (messageEntriesIterator.hasNext()) {\n       MessageInfo messageInfo = messageEntriesIterator.next();\n-      if (!messageInfo.isDeleted()) {\n-        // ok to use most recent ref\n-        IndexValue indexValue =\n-            findKey(messageInfo.getStoreKey(), null, EnumSet.of(IndexEntryType.TTL_UPDATE, IndexEntryType.DELETE));\n-        if (indexValue != null) {\n-          messageInfo = new MessageInfo(messageInfo.getStoreKey(), messageInfo.getSize(),\n-              indexValue.isFlagSet(IndexValue.Flags.Delete_Index),\n-              indexValue.isFlagSet(IndexValue.Flags.Ttl_Update_Index), indexValue.getExpiresAtMs(),\n-              indexValue.getAccountId(), indexValue.getContainerId(), indexValue.getOperationTimeInMs());\n-          messageEntriesIterator.set(messageInfo);\n-        }\n+      // for all the message info, we use most recent ref even for delete. since a deleted entry can be undeleted.\n+      // ok to use most recent ref\n+      IndexValue indexValue = findKey(messageInfo.getStoreKey(), null,\n+          EnumSet.of(IndexEntryType.TTL_UPDATE, IndexEntryType.DELETE, IndexEntryType.UNDELETE));\n+      if (indexValue != null) {\n+        messageInfo = new MessageInfo(messageInfo.getStoreKey(), indexValue.getSize(), indexValue.isDelete(),\n+            indexValue.isTTLUpdate(), indexValue.isUndelete(), indexValue.getExpiresAtMs(), null,\n+            indexValue.getAccountId(), indexValue.getContainerId(), indexValue.getOperationTimeInMs(),\n+            indexValue.getLifeVersion());\n+        messageEntriesIterator.set(messageInfo);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f65940cac1fe13b1d839c94d2ff81df94299c6fa"}, "originalPosition": 581}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "945473f79bdefb31c05038cd469914786c194241", "author": {"user": {"login": "justinlin-linkedin", "name": "Justin Lin"}}, "url": "https://github.com/linkedin/ambry/commit/945473f79bdefb31c05038cd469914786c194241", "committedDate": "2020-02-03T19:39:08Z", "message": "More comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1569, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}