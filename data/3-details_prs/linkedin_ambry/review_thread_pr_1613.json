{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc2NDU5NTcy", "number": 1613, "reviewThreads": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QwNTo0Mzo1NFrOEf1Hig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwMzo1MTo1M1rOEjaLMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxODExNTk0OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QwNTo0Mzo1NFrOHMWtGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQyMjoxMzo1NFrOHRox8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjcxNjk1NQ==", "bodyText": "Why TtlUpdate is also counted here?", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r482716955", "createdAt": "2020-09-03T05:43:54Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "diffHunk": "@@ -572,6 +574,12 @@ public void delete(List<MessageInfo> infosToDelete) throws StoreException {\n           indexValuesToDelete.add(value);\n           lifeVersions.add(info.getLifeVersion());\n         }\n+        if (!value.isDelete() && !value.isUndelete()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fbeda4c80b949fe346b2f23fad17fee6beb7ae"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzIxNDU3MQ==", "bodyText": "Because we use findKey to get the value, and in findKey, we are not returning TTL_UPDATE record.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r483214571", "createdAt": "2020-09-03T19:45:26Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "diffHunk": "@@ -572,6 +574,12 @@ public void delete(List<MessageInfo> infosToDelete) throws StoreException {\n           indexValuesToDelete.add(value);\n           lifeVersions.add(info.getLifeVersion());\n         }\n+        if (!value.isDelete() && !value.isUndelete()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjcxNjk1NQ=="}, "originalCommit": {"oid": "b3fbeda4c80b949fe346b2f23fad17fee6beb7ae"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY0ODgwMg==", "bodyText": "Ok, then why not use if(value.isPut()) here?", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r484648802", "createdAt": "2020-09-08T04:52:48Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "diffHunk": "@@ -572,6 +574,12 @@ public void delete(List<MessageInfo> infosToDelete) throws StoreException {\n           indexValuesToDelete.add(value);\n           lifeVersions.add(info.getLifeVersion());\n         }\n+        if (!value.isDelete() && !value.isUndelete()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjcxNjk1NQ=="}, "originalCommit": {"oid": "b3fbeda4c80b949fe346b2f23fad17fee6beb7ae"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE4MDc1Mg==", "bodyText": "because then the TTL_UPDATE flag is true, value.isPut will be false. findKey has this weird logic where it will populate TTL_UPDATE flag even if we are not trying to find TTL_UPDATE record.\nSo when we have PUT, TTL_UPDATE and DELETE for this blob and we call findKey(blobId, [INDEX_TYPE.PUT]),  we will get a PUT value with TTL_UPDATE flag being true. And if we do value.isPut, it will return false.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r485180752", "createdAt": "2020-09-08T20:34:48Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "diffHunk": "@@ -572,6 +574,12 @@ public void delete(List<MessageInfo> infosToDelete) throws StoreException {\n           indexValuesToDelete.add(value);\n           lifeVersions.add(info.getLifeVersion());\n         }\n+        if (!value.isDelete() && !value.isUndelete()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjcxNjk1NQ=="}, "originalCommit": {"oid": "b3fbeda4c80b949fe346b2f23fad17fee6beb7ae"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI1NTk4Ng==", "bodyText": "I see. Thanks for explanation. I think it probably justifies a comment here.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488255986", "createdAt": "2020-09-14T22:13:54Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "diffHunk": "@@ -572,6 +574,12 @@ public void delete(List<MessageInfo> infosToDelete) throws StoreException {\n           indexValuesToDelete.add(value);\n           lifeVersions.add(info.getLifeVersion());\n         }\n+        if (!value.isDelete() && !value.isUndelete()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjcxNjk1NQ=="}, "originalCommit": {"oid": "b3fbeda4c80b949fe346b2f23fad17fee6beb7ae"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxODE0NzY0OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QwNTo1ODo0N1rOHMW_vQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QwNTo1ODo0N1rOHMW_vQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjcyMTcyNQ==", "bodyText": "minor: can we call this indexValuePriorToNewDelete or the more descriptive name?", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r482721725", "createdAt": "2020-09-03T05:58:47Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "diffHunk": "@@ -605,6 +613,7 @@ public void delete(List<MessageInfo> infosToDelete) throws StoreException {\n                     StoreErrorCodes.Life_Version_Conflict);\n               }\n             }\n+            indexValuesToDelete.set(i, value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fbeda4c80b949fe346b2f23fad17fee6beb7ae"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDYyNDA1OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QyMDo1Mjo0N1rOHOHfVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMDozMDoxMFrOHOs8lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU2NDgyMw==", "bodyText": "nit: removeFromStates -> removeFinalStateOnPut.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r484564823", "createdAt": "2020-09-07T20:52:47Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -493,17 +526,18 @@ private void enqueueNewValue(IndexValue newValue, IndexValue originalPutValue) {\n    * Perform an action each valid {@link IndexEntry} from a given {@link List} of {@link IndexEntry}s that belong to the\n    * same {@link IndexSegment}.\n    * @param indexSegment the {@link IndexSegment} where the entries came from\n-   * @param referenceTimeInMs the reference time in ms until which deletes and expiration are relevant\n+   * @param deleteReferenceTimeInMs the reference time in ms until which deletes are relevant\n+   * @param expiryReferenceTimeInMs the reference time in ms until which expiration are relevant\n    * @param keyFinalStates a {@link Map} of key to {@link IndexFinalState}.\n    * @param removeFromStates if {@code True}, then remove the {@link IndexFinalState} from the given map {@code keyFinalStates}\n    *                         when encountering PUT IndexValue. This method iterates through IndexValues from most recent one to\n    *                         earliest one, so PUT IndexValue is the last IndexValue for the same key.\n    * @param validIndexEntryAction the action to take on each valid {@link IndexEntry} found.\n    * @throws StoreException if there are problems reading the index.\n    */\n-  private void forEachValidIndexEntry(IndexSegment indexSegment, long referenceTimeInMs,\n-      Map<StoreKey, IndexFinalState> keyFinalStates, boolean removeFromStates, IndexEntryAction validIndexEntryAction)\n-      throws StoreException {\n+  private void forEachValidIndexEntry(IndexSegment indexSegment, long deleteReferenceTimeInMs,\n+      long expiryReferenceTimeInMs, Map<StoreKey, IndexFinalState> keyFinalStates, boolean removeFromStates,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef03846acf002bcc3ba905ebcc6b7f4e34a70f90"}, "originalPosition": 264}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE3ODUxNg==", "bodyText": "will update", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r485178516", "createdAt": "2020-09-08T20:30:10Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -493,17 +526,18 @@ private void enqueueNewValue(IndexValue newValue, IndexValue originalPutValue) {\n    * Perform an action each valid {@link IndexEntry} from a given {@link List} of {@link IndexEntry}s that belong to the\n    * same {@link IndexSegment}.\n    * @param indexSegment the {@link IndexSegment} where the entries came from\n-   * @param referenceTimeInMs the reference time in ms until which deletes and expiration are relevant\n+   * @param deleteReferenceTimeInMs the reference time in ms until which deletes are relevant\n+   * @param expiryReferenceTimeInMs the reference time in ms until which expiration are relevant\n    * @param keyFinalStates a {@link Map} of key to {@link IndexFinalState}.\n    * @param removeFromStates if {@code True}, then remove the {@link IndexFinalState} from the given map {@code keyFinalStates}\n    *                         when encountering PUT IndexValue. This method iterates through IndexValues from most recent one to\n    *                         earliest one, so PUT IndexValue is the last IndexValue for the same key.\n    * @param validIndexEntryAction the action to take on each valid {@link IndexEntry} found.\n    * @throws StoreException if there are problems reading the index.\n    */\n-  private void forEachValidIndexEntry(IndexSegment indexSegment, long referenceTimeInMs,\n-      Map<StoreKey, IndexFinalState> keyFinalStates, boolean removeFromStates, IndexEntryAction validIndexEntryAction)\n-      throws StoreException {\n+  private void forEachValidIndexEntry(IndexSegment indexSegment, long deleteReferenceTimeInMs,\n+      long expiryReferenceTimeInMs, Map<StoreKey, IndexFinalState> keyFinalStates, boolean removeFromStates,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU2NDgyMw=="}, "originalCommit": {"oid": "ef03846acf002bcc3ba905ebcc6b7f4e34a70f90"}, "originalPosition": 264}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDcyMzMyOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QyMjoyNzo0MVrOHOISIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQwNTo1NDo1M1rOHQPRkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU3NzgyNw==", "bodyText": "How many buckets do you want to set? (If bucket based blob stats is enabled)", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r484577827", "createdAt": "2020-09-07T22:27:41Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -139,6 +140,7 @@ static StatsSnapshot convertStoreUsageToContainerStatsSnapshot(Map<String, Map<S\n     this.logSegmentForecastOffsetMs = logSegmentForecastOffsetMs;\n     this.waitTimeoutInSecs = waitTimeoutInSecs;\n     this.metrics = metrics;\n+    this.enableBucketForLogSegmentReports = enableBucketForLogSegmentReports;\n \n     if (bucketCount > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef03846acf002bcc3ba905ebcc6b7f4e34a70f90"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE3OTEyNg==", "bodyText": "I am thinking about 24 buckets. One bucket for 10 minutes and 4 hours. I am willing to extend to 8 or 10 hours. So 24, 48 or 60 are all good options for me.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r485179126", "createdAt": "2020-09-08T20:31:27Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -139,6 +140,7 @@ static StatsSnapshot convertStoreUsageToContainerStatsSnapshot(Map<String, Map<S\n     this.logSegmentForecastOffsetMs = logSegmentForecastOffsetMs;\n     this.waitTimeoutInSecs = waitTimeoutInSecs;\n     this.metrics = metrics;\n+    this.enableBucketForLogSegmentReports = enableBucketForLogSegmentReports;\n \n     if (bucketCount > 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU3NzgyNw=="}, "originalCommit": {"oid": "ef03846acf002bcc3ba905ebcc6b7f4e34a70f90"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc4OTUyMw==", "bodyText": "I think bucket span depends on how frequently we will do node-wide aggregation (10 mins is the lower bound I guess). Number of buckets is more flexible, if it doesn't take up too much memory,  buckets span * bucket count = 1 day is good enough.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r486789523", "createdAt": "2020-09-11T05:54:53Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -139,6 +140,7 @@ static StatsSnapshot convertStoreUsageToContainerStatsSnapshot(Map<String, Map<S\n     this.logSegmentForecastOffsetMs = logSegmentForecastOffsetMs;\n     this.waitTimeoutInSecs = waitTimeoutInSecs;\n     this.metrics = metrics;\n+    this.enableBucketForLogSegmentReports = enableBucketForLogSegmentReports;\n \n     if (bucketCount > 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU3NzgyNw=="}, "originalCommit": {"oid": "ef03846acf002bcc3ba905ebcc6b7f4e34a70f90"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMTIyMjEwOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwNDo1NDo1OFrOHOMpUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMDozNjoyMFrOHOtIRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY0OTI5OQ==", "bodyText": "line  560-561 and line 574-575 can be moved out of if-else block (no need to write them twice)", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r484649299", "createdAt": "2020-09-08T04:54:58Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "diffHunk": "@@ -569,9 +571,15 @@ public void delete(List<MessageInfo> infosToDelete) throws StoreException {\n                 \"Cannot delete id \" + info.getStoreKey() + \" since it has a higher lifeVersion than the message info: \"\n                     + value.getLifeVersion() + \">\" + info.getLifeVersion(), StoreErrorCodes.Life_Version_Conflict);\n           }\n-          indexValuesToDelete.add(value);\n+          indexValuesPriorToDelete.add(value);\n           lifeVersions.add(info.getLifeVersion());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef03846acf002bcc3ba905ebcc6b7f4e34a70f90"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE4MTUwOQ==", "bodyText": "will update", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r485181509", "createdAt": "2020-09-08T20:36:20Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "diffHunk": "@@ -569,9 +571,15 @@ public void delete(List<MessageInfo> infosToDelete) throws StoreException {\n                 \"Cannot delete id \" + info.getStoreKey() + \" since it has a higher lifeVersion than the message info: \"\n                     + value.getLifeVersion() + \">\" + info.getLifeVersion(), StoreErrorCodes.Life_Version_Conflict);\n           }\n-          indexValuesToDelete.add(value);\n+          indexValuesPriorToDelete.add(value);\n           lifeVersions.add(info.getLifeVersion());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY0OTI5OQ=="}, "originalCommit": {"oid": "ef03846acf002bcc3ba905ebcc6b7f4e34a70f90"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMTIzMTkxOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwNTowMTowMVrOHOMvDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMDoyODo1M1rOHOs6AA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY1MDc2Nw==", "bodyText": "Where is latestValue used? In this if block, no matter what the value is, the StoreException will be thrown and latestValue is never used.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r484650767", "createdAt": "2020-09-08T05:01:01Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "diffHunk": "@@ -818,6 +830,7 @@ public short undelete(MessageInfo info) throws StoreException {\n           IndexValue value = index.findKey(info.getStoreKey(), fileSpan,\n               EnumSet.of(PersistentIndex.IndexEntryType.DELETE, PersistentIndex.IndexEntryType.UNDELETE));\n           if (value != null) {\n+            latestValue = value;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef03846acf002bcc3ba905ebcc6b7f4e34a70f90"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE3Nzg1Ng==", "bodyText": "you are right, will update.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r485177856", "createdAt": "2020-09-08T20:28:53Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "diffHunk": "@@ -818,6 +830,7 @@ public short undelete(MessageInfo info) throws StoreException {\n           IndexValue value = index.findKey(info.getStoreKey(), fileSpan,\n               EnumSet.of(PersistentIndex.IndexEntryType.DELETE, PersistentIndex.IndexEntryType.UNDELETE));\n           if (value != null) {\n+            latestValue = value;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY1MDc2Nw=="}, "originalCommit": {"oid": "ef03846acf002bcc3ba905ebcc6b7f4e34a70f90"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMTI4MTQ2OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/ScanResults.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwNToyODoyMFrOHONLIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMDoyNTo0OFrOHOs0Yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY1Nzk1Mw==", "bodyText": "Let's call this containerDeltaBuckets.  Btw, what does bucketCount comprise? Is bucketCount = 1 baseBucket + number of delta buckets?", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r484657953", "createdAt": "2020-09-08T05:28:20Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/ScanResults.java", "diffHunk": "@@ -26,46 +26,124 @@\n  * used to modify and access the stored data structures.\n  */\n class ScanResults {\n-  // A NavigableMap that stores buckets for container valid data size. The key of the map is the end time of each\n-  // bucket and the value is the corresponding valid data size map. For example, there are two buckets with end time\n-  // t1 and t2. Bucket with end time t2 includes all events whose operation time is greater than or equal to t1 but\n-  // strictly less than t2.\n-  // Each bucket except for the very first one contains the delta in valid data size that occurred prior to the bucket\n-  // end time. The very first bucket's end time is the forecast start time for containers and it contains the valid data\n-  // size map at the forecast start time. The very first bucket is used as a base value, requested valid data size is\n-  // computed by applying the deltas from appropriate buckets on the base value.\n-  private final NavigableMap<Long, Map<String, Map<String, Long>>> containerBuckets = new TreeMap<>();\n-\n-  // A NavigableMap that stores buckets for log segment valid data size. The rest of the structure is similar\n-  // to containerBuckets.\n-  private final NavigableMap<Long, NavigableMap<String, Long>> logSegmentBuckets = new TreeMap<>();\n+  // A bucket is a period of time, whose length is defined by bucketSpanInMs parameter in constructor. In this class,\n+  // several maps are created to represent different purposes of bucket.\n+  // An object is created to represent base bucket and another map, whose value is the same type as the base bucket,\n+  // is created to represent delta in each time bucket. This is an example to illustrate how base and delta bucket work.\n+  //\n+  // If we want to calculate valid data size for all account and container, we create at least two fields\n+  // <pre><code>\n+  //  private long validSize;\n+  //  private final NavigableMap<Long, Long> deltaBuckets = new TreeMap<>();\n+  // </code></pre>\n+  //\n+  // When starting calculating, we add valid data size to {@code validSize} and add delta to its corresponding bucket.\n+  // Assuming each bucket is one hour long and the start time is T0, then we have deltaBuckets as\n+  //    T0                    T0+1H                   T0+2H                 T0+3H\n+  // baseBucket   [              ]    [                  ]    [                 ]   .....\n+  //\n+  // T0+1H is the first key in the deltaBuckets. All the deltas that happen before T0+1H but after T0 will be added to\n+  // its value. T0+2H is the second key in the deltaBucket. All the deltas that happen before T0+2H but after T0+1H will\n+  // be added to its value. But what are deltas? This is an example to show the answer.\n+  //\n+  // Now let's go through some IndexValues as\n+  // 1. PUT[ID1]: created at T0-1D, but will expire at T0+2.5H\n+  // 2. PUT[ID2]: created at T0-1D, permanent blob\n+  // The validSize at T0 should be PUT[ID1] + PUT[ID2] because PUT[ID1] haven't expired yet. But we know at T0+2.5H, the\n+  // validSize should be PUT[ID2] because PUT[ID1] is expired. Since we are not going to handle PUT[ID1] again, we need\n+  // to put a delta in T0+3H bucket. And it looks like this:\n+  //    T0                           T0+1H                   T0+2H                 T0+3H\n+  // PUT[ID1]+PUT[ID2]   [              ]    [                  ]    [ -PUT[ID1]     ]   .....\n+  // Please notice that the delta is negative. So when we pick any point of time and add the base validSize and all the\n+  // delta values in the buckets before this point of time, we will get the correct answer. For instance, validSize of\n+  // T0 is PUT[ID1]+PUT[ID2], validSize of T0+1H is PUT[ID1]+PUT[ID2]+delta of T0+1H. And validSize of T0+2H is PUT[ID1]\n+  // +PUT[ID2]-PUT[ID1] because the delta of T0+3H bucket is -PUT[ID1].\n+  //\n+  // As new IndexValue comes in, we have to deal with them and fill the delta for them as well.\n+  // 1. DELETE[ID1]: deleted at T0+0.5H\n+  // 2. PUT[ID3]: created at T0+1.2H, but will expire at T0+2.8H\n+  // 3. DELETE[ID2]: deleted at T0+1.5H\n+  // For DELETE[ID1], it adds -PUT[ID1] delta to T0+1H bucket since it makes PUT[ID1] invalid. However, PUT[ID1] will\n+  // expire at T0+2.5H and delta has already been added to T0+3H bucket. In order to avoid double adding, we have to add\n+  // PUT[ID1] back to T0+3H bucket. PUT[ID3] adds delta in T0+2H bucket but it will expire at T0+2.8H, so it has be to\n+  // subtracted in T0+3H bucket. DELETE[ID2] makes PUT[ID2] invalid, so a delta will be added to T0+2H bucket.\n+  //    T0                           T0+1H                   T0+2H                             T0+3H\n+  // PUT[ID1]+PUT[ID2]   [-PUT[ID1]      ]    [PUT[ID3]-PUT[ID2] ]    [-PUT[ID1]+PUT[ID1]-PUT[ID3] ]   .....\n+  //\n+  // This is how delta bucket works. We can pick any point of time and add base validSize and all the delta value in those\n+  // buckets prior to this point of time and get the answer we need.\n \n+  // Container buckets are very close to the example above, except that it doesn't aggregate all containers' valid data,\n+  // rather it keeps them in a map. The key of this map is the AccountId, and the value of this map is yet another map,\n+  // whose key is the ContainerId and the value is the valid size of this container.\n+  // So {@code containerBaseBucket} is base value for all containers and the {@code containerBuckets} is the delta values\n+  // on each time bucket.\n+  private final Map<String, Map<String, Long>> containerBaseBucket = new HashMap<>();\n+  private final NavigableMap<Long, Map<String, Map<String, Long>>> containerBuckets = new TreeMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef03846acf002bcc3ba905ebcc6b7f4e34a70f90"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE3NjQxOA==", "bodyText": "sure.\nSo when bucketCount is 1, we only have baseBucket.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r485176418", "createdAt": "2020-09-08T20:25:48Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/ScanResults.java", "diffHunk": "@@ -26,46 +26,124 @@\n  * used to modify and access the stored data structures.\n  */\n class ScanResults {\n-  // A NavigableMap that stores buckets for container valid data size. The key of the map is the end time of each\n-  // bucket and the value is the corresponding valid data size map. For example, there are two buckets with end time\n-  // t1 and t2. Bucket with end time t2 includes all events whose operation time is greater than or equal to t1 but\n-  // strictly less than t2.\n-  // Each bucket except for the very first one contains the delta in valid data size that occurred prior to the bucket\n-  // end time. The very first bucket's end time is the forecast start time for containers and it contains the valid data\n-  // size map at the forecast start time. The very first bucket is used as a base value, requested valid data size is\n-  // computed by applying the deltas from appropriate buckets on the base value.\n-  private final NavigableMap<Long, Map<String, Map<String, Long>>> containerBuckets = new TreeMap<>();\n-\n-  // A NavigableMap that stores buckets for log segment valid data size. The rest of the structure is similar\n-  // to containerBuckets.\n-  private final NavigableMap<Long, NavigableMap<String, Long>> logSegmentBuckets = new TreeMap<>();\n+  // A bucket is a period of time, whose length is defined by bucketSpanInMs parameter in constructor. In this class,\n+  // several maps are created to represent different purposes of bucket.\n+  // An object is created to represent base bucket and another map, whose value is the same type as the base bucket,\n+  // is created to represent delta in each time bucket. This is an example to illustrate how base and delta bucket work.\n+  //\n+  // If we want to calculate valid data size for all account and container, we create at least two fields\n+  // <pre><code>\n+  //  private long validSize;\n+  //  private final NavigableMap<Long, Long> deltaBuckets = new TreeMap<>();\n+  // </code></pre>\n+  //\n+  // When starting calculating, we add valid data size to {@code validSize} and add delta to its corresponding bucket.\n+  // Assuming each bucket is one hour long and the start time is T0, then we have deltaBuckets as\n+  //    T0                    T0+1H                   T0+2H                 T0+3H\n+  // baseBucket   [              ]    [                  ]    [                 ]   .....\n+  //\n+  // T0+1H is the first key in the deltaBuckets. All the deltas that happen before T0+1H but after T0 will be added to\n+  // its value. T0+2H is the second key in the deltaBucket. All the deltas that happen before T0+2H but after T0+1H will\n+  // be added to its value. But what are deltas? This is an example to show the answer.\n+  //\n+  // Now let's go through some IndexValues as\n+  // 1. PUT[ID1]: created at T0-1D, but will expire at T0+2.5H\n+  // 2. PUT[ID2]: created at T0-1D, permanent blob\n+  // The validSize at T0 should be PUT[ID1] + PUT[ID2] because PUT[ID1] haven't expired yet. But we know at T0+2.5H, the\n+  // validSize should be PUT[ID2] because PUT[ID1] is expired. Since we are not going to handle PUT[ID1] again, we need\n+  // to put a delta in T0+3H bucket. And it looks like this:\n+  //    T0                           T0+1H                   T0+2H                 T0+3H\n+  // PUT[ID1]+PUT[ID2]   [              ]    [                  ]    [ -PUT[ID1]     ]   .....\n+  // Please notice that the delta is negative. So when we pick any point of time and add the base validSize and all the\n+  // delta values in the buckets before this point of time, we will get the correct answer. For instance, validSize of\n+  // T0 is PUT[ID1]+PUT[ID2], validSize of T0+1H is PUT[ID1]+PUT[ID2]+delta of T0+1H. And validSize of T0+2H is PUT[ID1]\n+  // +PUT[ID2]-PUT[ID1] because the delta of T0+3H bucket is -PUT[ID1].\n+  //\n+  // As new IndexValue comes in, we have to deal with them and fill the delta for them as well.\n+  // 1. DELETE[ID1]: deleted at T0+0.5H\n+  // 2. PUT[ID3]: created at T0+1.2H, but will expire at T0+2.8H\n+  // 3. DELETE[ID2]: deleted at T0+1.5H\n+  // For DELETE[ID1], it adds -PUT[ID1] delta to T0+1H bucket since it makes PUT[ID1] invalid. However, PUT[ID1] will\n+  // expire at T0+2.5H and delta has already been added to T0+3H bucket. In order to avoid double adding, we have to add\n+  // PUT[ID1] back to T0+3H bucket. PUT[ID3] adds delta in T0+2H bucket but it will expire at T0+2.8H, so it has be to\n+  // subtracted in T0+3H bucket. DELETE[ID2] makes PUT[ID2] invalid, so a delta will be added to T0+2H bucket.\n+  //    T0                           T0+1H                   T0+2H                             T0+3H\n+  // PUT[ID1]+PUT[ID2]   [-PUT[ID1]      ]    [PUT[ID3]-PUT[ID2] ]    [-PUT[ID1]+PUT[ID1]-PUT[ID3] ]   .....\n+  //\n+  // This is how delta bucket works. We can pick any point of time and add base validSize and all the delta value in those\n+  // buckets prior to this point of time and get the answer we need.\n \n+  // Container buckets are very close to the example above, except that it doesn't aggregate all containers' valid data,\n+  // rather it keeps them in a map. The key of this map is the AccountId, and the value of this map is yet another map,\n+  // whose key is the ContainerId and the value is the valid size of this container.\n+  // So {@code containerBaseBucket} is base value for all containers and the {@code containerBuckets} is the delta values\n+  // on each time bucket.\n+  private final Map<String, Map<String, Long>> containerBaseBucket = new HashMap<>();\n+  private final NavigableMap<Long, Map<String, Map<String, Long>>> containerBuckets = new TreeMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY1Nzk1Mw=="}, "originalCommit": {"oid": "ef03846acf002bcc3ba905ebcc6b7f4e34a70f90"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMTMzMTYxOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/ScanResults.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwNTo1MTo0NVrOHONnJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwNTo1MTo0NVrOHONnJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY2NTEyNQ==", "bodyText": "containerForecastEndTimeMs,  logSegmentForecastEndTimeMsForExpired and  logSegmentForecastEndTimeMsForDeleted all seem to be extending another bucketSpanInMs from last bucket time. Can you explain why? We can talk this offline if needed.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r484665125", "createdAt": "2020-09-08T05:51:45Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/ScanResults.java", "diffHunk": "@@ -26,46 +26,124 @@\n  * used to modify and access the stored data structures.\n  */\n class ScanResults {\n-  // A NavigableMap that stores buckets for container valid data size. The key of the map is the end time of each\n-  // bucket and the value is the corresponding valid data size map. For example, there are two buckets with end time\n-  // t1 and t2. Bucket with end time t2 includes all events whose operation time is greater than or equal to t1 but\n-  // strictly less than t2.\n-  // Each bucket except for the very first one contains the delta in valid data size that occurred prior to the bucket\n-  // end time. The very first bucket's end time is the forecast start time for containers and it contains the valid data\n-  // size map at the forecast start time. The very first bucket is used as a base value, requested valid data size is\n-  // computed by applying the deltas from appropriate buckets on the base value.\n-  private final NavigableMap<Long, Map<String, Map<String, Long>>> containerBuckets = new TreeMap<>();\n-\n-  // A NavigableMap that stores buckets for log segment valid data size. The rest of the structure is similar\n-  // to containerBuckets.\n-  private final NavigableMap<Long, NavigableMap<String, Long>> logSegmentBuckets = new TreeMap<>();\n+  // A bucket is a period of time, whose length is defined by bucketSpanInMs parameter in constructor. In this class,\n+  // several maps are created to represent different purposes of bucket.\n+  // An object is created to represent base bucket and another map, whose value is the same type as the base bucket,\n+  // is created to represent delta in each time bucket. This is an example to illustrate how base and delta bucket work.\n+  //\n+  // If we want to calculate valid data size for all account and container, we create at least two fields\n+  // <pre><code>\n+  //  private long validSize;\n+  //  private final NavigableMap<Long, Long> deltaBuckets = new TreeMap<>();\n+  // </code></pre>\n+  //\n+  // When starting calculating, we add valid data size to {@code validSize} and add delta to its corresponding bucket.\n+  // Assuming each bucket is one hour long and the start time is T0, then we have deltaBuckets as\n+  //    T0                    T0+1H                   T0+2H                 T0+3H\n+  // baseBucket   [              ]    [                  ]    [                 ]   .....\n+  //\n+  // T0+1H is the first key in the deltaBuckets. All the deltas that happen before T0+1H but after T0 will be added to\n+  // its value. T0+2H is the second key in the deltaBucket. All the deltas that happen before T0+2H but after T0+1H will\n+  // be added to its value. But what are deltas? This is an example to show the answer.\n+  //\n+  // Now let's go through some IndexValues as\n+  // 1. PUT[ID1]: created at T0-1D, but will expire at T0+2.5H\n+  // 2. PUT[ID2]: created at T0-1D, permanent blob\n+  // The validSize at T0 should be PUT[ID1] + PUT[ID2] because PUT[ID1] haven't expired yet. But we know at T0+2.5H, the\n+  // validSize should be PUT[ID2] because PUT[ID1] is expired. Since we are not going to handle PUT[ID1] again, we need\n+  // to put a delta in T0+3H bucket. And it looks like this:\n+  //    T0                           T0+1H                   T0+2H                 T0+3H\n+  // PUT[ID1]+PUT[ID2]   [              ]    [                  ]    [ -PUT[ID1]     ]   .....\n+  // Please notice that the delta is negative. So when we pick any point of time and add the base validSize and all the\n+  // delta values in the buckets before this point of time, we will get the correct answer. For instance, validSize of\n+  // T0 is PUT[ID1]+PUT[ID2], validSize of T0+1H is PUT[ID1]+PUT[ID2]+delta of T0+1H. And validSize of T0+2H is PUT[ID1]\n+  // +PUT[ID2]-PUT[ID1] because the delta of T0+3H bucket is -PUT[ID1].\n+  //\n+  // As new IndexValue comes in, we have to deal with them and fill the delta for them as well.\n+  // 1. DELETE[ID1]: deleted at T0+0.5H\n+  // 2. PUT[ID3]: created at T0+1.2H, but will expire at T0+2.8H\n+  // 3. DELETE[ID2]: deleted at T0+1.5H\n+  // For DELETE[ID1], it adds -PUT[ID1] delta to T0+1H bucket since it makes PUT[ID1] invalid. However, PUT[ID1] will\n+  // expire at T0+2.5H and delta has already been added to T0+3H bucket. In order to avoid double adding, we have to add\n+  // PUT[ID1] back to T0+3H bucket. PUT[ID3] adds delta in T0+2H bucket but it will expire at T0+2.8H, so it has be to\n+  // subtracted in T0+3H bucket. DELETE[ID2] makes PUT[ID2] invalid, so a delta will be added to T0+2H bucket.\n+  //    T0                           T0+1H                   T0+2H                             T0+3H\n+  // PUT[ID1]+PUT[ID2]   [-PUT[ID1]      ]    [PUT[ID3]-PUT[ID2] ]    [-PUT[ID1]+PUT[ID1]-PUT[ID3] ]   .....\n+  //\n+  // This is how delta bucket works. We can pick any point of time and add base validSize and all the delta value in those\n+  // buckets prior to this point of time and get the answer we need.\n \n+  // Container buckets are very close to the example above, except that it doesn't aggregate all containers' valid data,\n+  // rather it keeps them in a map. The key of this map is the AccountId, and the value of this map is yet another map,\n+  // whose key is the ContainerId and the value is the valid size of this container.\n+  // So {@code containerBaseBucket} is base value for all containers and the {@code containerBuckets} is the delta values\n+  // on each time bucket.\n+  private final Map<String, Map<String, Long>> containerBaseBucket = new HashMap<>();\n+  private final NavigableMap<Long, Map<String, Map<String, Long>>> containerBuckets = new TreeMap<>();\n   final long containerForecastStartTimeMs;\n   final long containerLastBucketTimeMs;\n   final long containerForecastEndTimeMs;\n-  final long logSegmentForecastStartTimeMs;\n-  final long logSegmentLastBucketTimeMs;\n-  final long logSegmentForecastEndTimeMs;\n+\n+  // LogSegment buckets keep track of valid IndexValue size in each log segments. So the base value of log segment bucket\n+  // is a map, whose key is the log segment name and the value is the sum of valid IndexValues' sizes. To test if an\n+  // IndexValue is valid or not, we follow the same rule in the {@link BlobStoreCompactor}.\n+  //\n+  // LogSegment buckets is very close to container buckets, except for several differences.\n+  // 1. LogSegment has two delta buckets. one for deleted blobs, the other for expired blobs.\n+  // 2. Since we have a retention period for deleted blob, deleted delta's first bucket would start earlier than expired\n+  //    delta.\n+  // The reason to have two delta buckets is because deleted blobs and expired blobs are invalidated at different time.\n+  // When a blob is deleted, we have to wait for a retention duration(7 days in prod) to invalidate it. But when a blob\n+  // is expired, it's invalidated right aways. So when calculating valid size for log segment, we have to pass two time\n+  // stamps, a delete reference time, usually is now - 7 days, and a expiry time, usually is now.\n+  private final NavigableMap<String, Long> logSegmentBaseBucket = new TreeMap<>();\n+  private final NavigableMap<Long, NavigableMap<String, Long>> logSegmentBucketsDeltaForDeleted = new TreeMap<>();\n+  private final NavigableMap<Long, NavigableMap<String, Long>> logSegmentBucketsDeltaForExpired = new TreeMap<>();\n+\n+  final long logSegmentForecastStartTimeMsForDeleted;\n+  final long logSegmentLastBucketTimeMsForDeleted;\n+  final long logSegmentForecastEndTimeMsForDeleted;\n+  final long logSegmentForecastStartTimeMsForExpired;\n+  final long logSegmentLastBucketTimeMsForExpired;\n+  final long logSegmentForecastEndTimeMsForExpired;\n   Offset scannedEndOffset = null;\n \n   /**\n    * Create the bucket data structures in advance based on the given scanStartTime and segmentScanTimeOffset.\n    */\n   ScanResults(long startTimeInMs, long logSegmentForecastOffsetMs, int bucketCount, long bucketSpanInMs) {\n-    long containerBucketTimeMs = startTimeInMs;\n-    long logSegmentBucketTimeMs = startTimeInMs - logSegmentForecastOffsetMs;\n-    for (int i = 0; i < bucketCount; i++) {\n+    // Set up container buckets\n+    containerForecastStartTimeMs = startTimeInMs;\n+    long containerBucketTimeMs = containerForecastStartTimeMs + bucketSpanInMs;\n+    for (int i = 1; i < bucketCount; i++) {\n       containerBuckets.put(containerBucketTimeMs, new HashMap<>());\n-      logSegmentBuckets.put(logSegmentBucketTimeMs, new TreeMap<>(LogSegmentNameHelper.COMPARATOR));\n       containerBucketTimeMs += bucketSpanInMs;\n-      logSegmentBucketTimeMs += bucketSpanInMs;\n     }\n-    containerForecastStartTimeMs = containerBuckets.firstKey();\n-    containerLastBucketTimeMs = containerBuckets.lastKey();\n+    containerLastBucketTimeMs = containerBuckets.isEmpty() ? containerForecastStartTimeMs : containerBuckets.lastKey();\n     containerForecastEndTimeMs = containerLastBucketTimeMs + bucketSpanInMs;\n-    logSegmentForecastStartTimeMs = logSegmentBuckets.firstKey();\n-    logSegmentLastBucketTimeMs = logSegmentBuckets.lastKey();\n-    logSegmentForecastEndTimeMs = logSegmentLastBucketTimeMs + bucketSpanInMs;\n+\n+    // Set up log segment buckets\n+    logSegmentForecastStartTimeMsForExpired = startTimeInMs;\n+    long bucketTimeMs = logSegmentForecastStartTimeMsForExpired + bucketSpanInMs;\n+    for (int i = 1; i < bucketCount; i++) {\n+      logSegmentBucketsDeltaForExpired.put(bucketTimeMs, new TreeMap<>(LogSegmentNameHelper.COMPARATOR));\n+      bucketTimeMs += bucketSpanInMs;\n+    }\n+    logSegmentLastBucketTimeMsForExpired =\n+        logSegmentBucketsDeltaForExpired.isEmpty() ? logSegmentForecastStartTimeMsForExpired\n+            : logSegmentBucketsDeltaForExpired.lastKey();\n+    logSegmentForecastEndTimeMsForExpired = logSegmentLastBucketTimeMsForExpired + bucketSpanInMs;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef03846acf002bcc3ba905ebcc6b7f4e34a70f90"}, "originalPosition": 136}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMTM0MjE2OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/ScanResults.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwNTo1NjowN1rOHONtAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMDoyNToxNFrOHOszQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY2NjYyNw==", "bodyText": "I think the maps in this class are better to concurrent map as IndexScanner and QueueProcessor may concurrently update the map.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r484666627", "createdAt": "2020-09-08T05:56:07Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/ScanResults.java", "diffHunk": "@@ -26,46 +26,124 @@\n  * used to modify and access the stored data structures.\n  */\n class ScanResults {\n-  // A NavigableMap that stores buckets for container valid data size. The key of the map is the end time of each\n-  // bucket and the value is the corresponding valid data size map. For example, there are two buckets with end time\n-  // t1 and t2. Bucket with end time t2 includes all events whose operation time is greater than or equal to t1 but\n-  // strictly less than t2.\n-  // Each bucket except for the very first one contains the delta in valid data size that occurred prior to the bucket\n-  // end time. The very first bucket's end time is the forecast start time for containers and it contains the valid data\n-  // size map at the forecast start time. The very first bucket is used as a base value, requested valid data size is\n-  // computed by applying the deltas from appropriate buckets on the base value.\n-  private final NavigableMap<Long, Map<String, Map<String, Long>>> containerBuckets = new TreeMap<>();\n-\n-  // A NavigableMap that stores buckets for log segment valid data size. The rest of the structure is similar\n-  // to containerBuckets.\n-  private final NavigableMap<Long, NavigableMap<String, Long>> logSegmentBuckets = new TreeMap<>();\n+  // A bucket is a period of time, whose length is defined by bucketSpanInMs parameter in constructor. In this class,\n+  // several maps are created to represent different purposes of bucket.\n+  // An object is created to represent base bucket and another map, whose value is the same type as the base bucket,\n+  // is created to represent delta in each time bucket. This is an example to illustrate how base and delta bucket work.\n+  //\n+  // If we want to calculate valid data size for all account and container, we create at least two fields\n+  // <pre><code>\n+  //  private long validSize;\n+  //  private final NavigableMap<Long, Long> deltaBuckets = new TreeMap<>();\n+  // </code></pre>\n+  //\n+  // When starting calculating, we add valid data size to {@code validSize} and add delta to its corresponding bucket.\n+  // Assuming each bucket is one hour long and the start time is T0, then we have deltaBuckets as\n+  //    T0                    T0+1H                   T0+2H                 T0+3H\n+  // baseBucket   [              ]    [                  ]    [                 ]   .....\n+  //\n+  // T0+1H is the first key in the deltaBuckets. All the deltas that happen before T0+1H but after T0 will be added to\n+  // its value. T0+2H is the second key in the deltaBucket. All the deltas that happen before T0+2H but after T0+1H will\n+  // be added to its value. But what are deltas? This is an example to show the answer.\n+  //\n+  // Now let's go through some IndexValues as\n+  // 1. PUT[ID1]: created at T0-1D, but will expire at T0+2.5H\n+  // 2. PUT[ID2]: created at T0-1D, permanent blob\n+  // The validSize at T0 should be PUT[ID1] + PUT[ID2] because PUT[ID1] haven't expired yet. But we know at T0+2.5H, the\n+  // validSize should be PUT[ID2] because PUT[ID1] is expired. Since we are not going to handle PUT[ID1] again, we need\n+  // to put a delta in T0+3H bucket. And it looks like this:\n+  //    T0                           T0+1H                   T0+2H                 T0+3H\n+  // PUT[ID1]+PUT[ID2]   [              ]    [                  ]    [ -PUT[ID1]     ]   .....\n+  // Please notice that the delta is negative. So when we pick any point of time and add the base validSize and all the\n+  // delta values in the buckets before this point of time, we will get the correct answer. For instance, validSize of\n+  // T0 is PUT[ID1]+PUT[ID2], validSize of T0+1H is PUT[ID1]+PUT[ID2]+delta of T0+1H. And validSize of T0+2H is PUT[ID1]\n+  // +PUT[ID2]-PUT[ID1] because the delta of T0+3H bucket is -PUT[ID1].\n+  //\n+  // As new IndexValue comes in, we have to deal with them and fill the delta for them as well.\n+  // 1. DELETE[ID1]: deleted at T0+0.5H\n+  // 2. PUT[ID3]: created at T0+1.2H, but will expire at T0+2.8H\n+  // 3. DELETE[ID2]: deleted at T0+1.5H\n+  // For DELETE[ID1], it adds -PUT[ID1] delta to T0+1H bucket since it makes PUT[ID1] invalid. However, PUT[ID1] will\n+  // expire at T0+2.5H and delta has already been added to T0+3H bucket. In order to avoid double adding, we have to add\n+  // PUT[ID1] back to T0+3H bucket. PUT[ID3] adds delta in T0+2H bucket but it will expire at T0+2.8H, so it has be to\n+  // subtracted in T0+3H bucket. DELETE[ID2] makes PUT[ID2] invalid, so a delta will be added to T0+2H bucket.\n+  //    T0                           T0+1H                   T0+2H                             T0+3H\n+  // PUT[ID1]+PUT[ID2]   [-PUT[ID1]      ]    [PUT[ID3]-PUT[ID2] ]    [-PUT[ID1]+PUT[ID1]-PUT[ID3] ]   .....\n+  //\n+  // This is how delta bucket works. We can pick any point of time and add base validSize and all the delta value in those\n+  // buckets prior to this point of time and get the answer we need.\n \n+  // Container buckets are very close to the example above, except that it doesn't aggregate all containers' valid data,\n+  // rather it keeps them in a map. The key of this map is the AccountId, and the value of this map is yet another map,\n+  // whose key is the ContainerId and the value is the valid size of this container.\n+  // So {@code containerBaseBucket} is base value for all containers and the {@code containerBuckets} is the delta values\n+  // on each time bucket.\n+  private final Map<String, Map<String, Long>> containerBaseBucket = new HashMap<>();\n+  private final NavigableMap<Long, Map<String, Map<String, Long>>> containerBuckets = new TreeMap<>();\n   final long containerForecastStartTimeMs;\n   final long containerLastBucketTimeMs;\n   final long containerForecastEndTimeMs;\n-  final long logSegmentForecastStartTimeMs;\n-  final long logSegmentLastBucketTimeMs;\n-  final long logSegmentForecastEndTimeMs;\n+\n+  // LogSegment buckets keep track of valid IndexValue size in each log segments. So the base value of log segment bucket\n+  // is a map, whose key is the log segment name and the value is the sum of valid IndexValues' sizes. To test if an\n+  // IndexValue is valid or not, we follow the same rule in the {@link BlobStoreCompactor}.\n+  //\n+  // LogSegment buckets is very close to container buckets, except for several differences.\n+  // 1. LogSegment has two delta buckets. one for deleted blobs, the other for expired blobs.\n+  // 2. Since we have a retention period for deleted blob, deleted delta's first bucket would start earlier than expired\n+  //    delta.\n+  // The reason to have two delta buckets is because deleted blobs and expired blobs are invalidated at different time.\n+  // When a blob is deleted, we have to wait for a retention duration(7 days in prod) to invalidate it. But when a blob\n+  // is expired, it's invalidated right aways. So when calculating valid size for log segment, we have to pass two time\n+  // stamps, a delete reference time, usually is now - 7 days, and a expiry time, usually is now.\n+  private final NavigableMap<String, Long> logSegmentBaseBucket = new TreeMap<>();\n+  private final NavigableMap<Long, NavigableMap<String, Long>> logSegmentBucketsDeltaForDeleted = new TreeMap<>();\n+  private final NavigableMap<Long, NavigableMap<String, Long>> logSegmentBucketsDeltaForExpired = new TreeMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef03846acf002bcc3ba905ebcc6b7f4e34a70f90"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE3NjEzMQ==", "bodyText": "QueueProcessor shouldn't be running at the same time with IndexScanner. IndexScanner would change isScanning to true while scanning the index files. And when the isScanning is true, QueueProcessor won't do anything.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r485176131", "createdAt": "2020-09-08T20:25:14Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/ScanResults.java", "diffHunk": "@@ -26,46 +26,124 @@\n  * used to modify and access the stored data structures.\n  */\n class ScanResults {\n-  // A NavigableMap that stores buckets for container valid data size. The key of the map is the end time of each\n-  // bucket and the value is the corresponding valid data size map. For example, there are two buckets with end time\n-  // t1 and t2. Bucket with end time t2 includes all events whose operation time is greater than or equal to t1 but\n-  // strictly less than t2.\n-  // Each bucket except for the very first one contains the delta in valid data size that occurred prior to the bucket\n-  // end time. The very first bucket's end time is the forecast start time for containers and it contains the valid data\n-  // size map at the forecast start time. The very first bucket is used as a base value, requested valid data size is\n-  // computed by applying the deltas from appropriate buckets on the base value.\n-  private final NavigableMap<Long, Map<String, Map<String, Long>>> containerBuckets = new TreeMap<>();\n-\n-  // A NavigableMap that stores buckets for log segment valid data size. The rest of the structure is similar\n-  // to containerBuckets.\n-  private final NavigableMap<Long, NavigableMap<String, Long>> logSegmentBuckets = new TreeMap<>();\n+  // A bucket is a period of time, whose length is defined by bucketSpanInMs parameter in constructor. In this class,\n+  // several maps are created to represent different purposes of bucket.\n+  // An object is created to represent base bucket and another map, whose value is the same type as the base bucket,\n+  // is created to represent delta in each time bucket. This is an example to illustrate how base and delta bucket work.\n+  //\n+  // If we want to calculate valid data size for all account and container, we create at least two fields\n+  // <pre><code>\n+  //  private long validSize;\n+  //  private final NavigableMap<Long, Long> deltaBuckets = new TreeMap<>();\n+  // </code></pre>\n+  //\n+  // When starting calculating, we add valid data size to {@code validSize} and add delta to its corresponding bucket.\n+  // Assuming each bucket is one hour long and the start time is T0, then we have deltaBuckets as\n+  //    T0                    T0+1H                   T0+2H                 T0+3H\n+  // baseBucket   [              ]    [                  ]    [                 ]   .....\n+  //\n+  // T0+1H is the first key in the deltaBuckets. All the deltas that happen before T0+1H but after T0 will be added to\n+  // its value. T0+2H is the second key in the deltaBucket. All the deltas that happen before T0+2H but after T0+1H will\n+  // be added to its value. But what are deltas? This is an example to show the answer.\n+  //\n+  // Now let's go through some IndexValues as\n+  // 1. PUT[ID1]: created at T0-1D, but will expire at T0+2.5H\n+  // 2. PUT[ID2]: created at T0-1D, permanent blob\n+  // The validSize at T0 should be PUT[ID1] + PUT[ID2] because PUT[ID1] haven't expired yet. But we know at T0+2.5H, the\n+  // validSize should be PUT[ID2] because PUT[ID1] is expired. Since we are not going to handle PUT[ID1] again, we need\n+  // to put a delta in T0+3H bucket. And it looks like this:\n+  //    T0                           T0+1H                   T0+2H                 T0+3H\n+  // PUT[ID1]+PUT[ID2]   [              ]    [                  ]    [ -PUT[ID1]     ]   .....\n+  // Please notice that the delta is negative. So when we pick any point of time and add the base validSize and all the\n+  // delta values in the buckets before this point of time, we will get the correct answer. For instance, validSize of\n+  // T0 is PUT[ID1]+PUT[ID2], validSize of T0+1H is PUT[ID1]+PUT[ID2]+delta of T0+1H. And validSize of T0+2H is PUT[ID1]\n+  // +PUT[ID2]-PUT[ID1] because the delta of T0+3H bucket is -PUT[ID1].\n+  //\n+  // As new IndexValue comes in, we have to deal with them and fill the delta for them as well.\n+  // 1. DELETE[ID1]: deleted at T0+0.5H\n+  // 2. PUT[ID3]: created at T0+1.2H, but will expire at T0+2.8H\n+  // 3. DELETE[ID2]: deleted at T0+1.5H\n+  // For DELETE[ID1], it adds -PUT[ID1] delta to T0+1H bucket since it makes PUT[ID1] invalid. However, PUT[ID1] will\n+  // expire at T0+2.5H and delta has already been added to T0+3H bucket. In order to avoid double adding, we have to add\n+  // PUT[ID1] back to T0+3H bucket. PUT[ID3] adds delta in T0+2H bucket but it will expire at T0+2.8H, so it has be to\n+  // subtracted in T0+3H bucket. DELETE[ID2] makes PUT[ID2] invalid, so a delta will be added to T0+2H bucket.\n+  //    T0                           T0+1H                   T0+2H                             T0+3H\n+  // PUT[ID1]+PUT[ID2]   [-PUT[ID1]      ]    [PUT[ID3]-PUT[ID2] ]    [-PUT[ID1]+PUT[ID1]-PUT[ID3] ]   .....\n+  //\n+  // This is how delta bucket works. We can pick any point of time and add base validSize and all the delta value in those\n+  // buckets prior to this point of time and get the answer we need.\n \n+  // Container buckets are very close to the example above, except that it doesn't aggregate all containers' valid data,\n+  // rather it keeps them in a map. The key of this map is the AccountId, and the value of this map is yet another map,\n+  // whose key is the ContainerId and the value is the valid size of this container.\n+  // So {@code containerBaseBucket} is base value for all containers and the {@code containerBuckets} is the delta values\n+  // on each time bucket.\n+  private final Map<String, Map<String, Long>> containerBaseBucket = new HashMap<>();\n+  private final NavigableMap<Long, Map<String, Map<String, Long>>> containerBuckets = new TreeMap<>();\n   final long containerForecastStartTimeMs;\n   final long containerLastBucketTimeMs;\n   final long containerForecastEndTimeMs;\n-  final long logSegmentForecastStartTimeMs;\n-  final long logSegmentLastBucketTimeMs;\n-  final long logSegmentForecastEndTimeMs;\n+\n+  // LogSegment buckets keep track of valid IndexValue size in each log segments. So the base value of log segment bucket\n+  // is a map, whose key is the log segment name and the value is the sum of valid IndexValues' sizes. To test if an\n+  // IndexValue is valid or not, we follow the same rule in the {@link BlobStoreCompactor}.\n+  //\n+  // LogSegment buckets is very close to container buckets, except for several differences.\n+  // 1. LogSegment has two delta buckets. one for deleted blobs, the other for expired blobs.\n+  // 2. Since we have a retention period for deleted blob, deleted delta's first bucket would start earlier than expired\n+  //    delta.\n+  // The reason to have two delta buckets is because deleted blobs and expired blobs are invalidated at different time.\n+  // When a blob is deleted, we have to wait for a retention duration(7 days in prod) to invalidate it. But when a blob\n+  // is expired, it's invalidated right aways. So when calculating valid size for log segment, we have to pass two time\n+  // stamps, a delete reference time, usually is now - 7 days, and a expiry time, usually is now.\n+  private final NavigableMap<String, Long> logSegmentBaseBucket = new TreeMap<>();\n+  private final NavigableMap<Long, NavigableMap<String, Long>> logSegmentBucketsDeltaForDeleted = new TreeMap<>();\n+  private final NavigableMap<Long, NavigableMap<String, Long>> logSegmentBucketsDeltaForExpired = new TreeMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY2NjYyNw=="}, "originalCommit": {"oid": "ef03846acf002bcc3ba905ebcc6b7f4e34a70f90"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0Mzg1NjMyOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQyMDo0MjowM1rOHQFAqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxODowMDozMVrOHRglKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjYyMTM1Mw==", "bodyText": "I am actually confused about this scanLock. If IndexScanner is running (which means isScanning == true), current thread is able to acquire and hold scanLock (because it can be released by IndexScanner at line 1089). After that, current thread can be blocked by waitCondition (without releasing lock), however, IndexScanner has to re-acquire the  lock before signaling the waitCondition (see line 1141 - 1146).  It seems like a deadlock except for waitCondition can time out eventually. Is this intended?", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r486621353", "createdAt": "2020-09-10T20:42:03Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -240,53 +242,74 @@ long getMaxBlobSize() {\n    * @throws StoreException if BlobStoreStats is not enabled or closed\n    */\n   Pair<Long, NavigableMap<String, Long>> getValidDataSizeByLogSegment(TimeRange timeRange) throws StoreException {\n+    return getValidDataSizeByLogSegment(timeRange, time.milliseconds());\n+  }\n+\n+  /**\n+   * Same as {@link #getValidDataSizeByLogSegment(TimeRange)}, but provides a expiry reference time. Blobs with expiration\n+   * time less than the given expiry reference time, are considered as expired.\n+   * @param timeRange the delete reference {@link TimeRange} at which the data is requested. Defines both the reference time\n+   *                  and the acceptable resolution.\n+   * @param expiryReferenceTime the reference time for expired blobs. Blobs with expiration time less than it would be\n+   *                            considered as expired. Usually it's now.\n+   * @return a {@link Pair} whose first element is the time at which stats was collected (in ms) and whose second\n+   * element is the valid data size for each segment in the form of a {@link NavigableMap} of segment names to\n+   * valid data sizes.\n+   * @throws StoreException\n+   */\n+  Pair<Long, NavigableMap<String, Long>> getValidDataSizeByLogSegment(TimeRange timeRange, long expiryReferenceTime)\n+      throws StoreException {\n     if (!enabled.get()) {\n       throw new StoreException(String.format(\"BlobStoreStats is not enabled or closing for store %s\", storeId),\n           StoreErrorCodes.Store_Shutting_Down);\n     }\n-    ScanResults currentScanResults = scanResults.get();\n     Pair<Long, NavigableMap<String, Long>> retValue = null;\n+    ScanResults currentScanResults = scanResults.get();\n     long referenceTimeInMs = getLogSegmentRefTimeMs(currentScanResults, timeRange);\n-    if (referenceTimeInMs != REF_TIME_OUT_OF_BOUNDS) {\n-      retValue = currentScanResults.getValidSizePerLogSegment(referenceTimeInMs);\n-    } else {\n-      if (isScanning && getLogSegmentRefTimeMs(indexScanner.newScanResults, timeRange) != REF_TIME_OUT_OF_BOUNDS) {\n-        scanLock.lock();\n-        try {\n-          if (isScanning) {\n-            if (waitCondition.await(waitTimeoutInSecs, TimeUnit.SECONDS)) {\n+    if (enableBucketForLogSegmentReports) {\n+      if (referenceTimeInMs != REF_TIME_OUT_OF_BOUNDS) {\n+        retValue = currentScanResults.getValidSizePerLogSegment(referenceTimeInMs, expiryReferenceTime);\n+      } else {\n+        if (isScanning && getLogSegmentRefTimeMs(indexScanner.newScanResults, timeRange) != REF_TIME_OUT_OF_BOUNDS) {\n+          scanLock.lock();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09dc26934ff89aea399bb4f1c24c6b8d6096fbef"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODEwOTgwNw==", "bodyText": "waitCondition is a condition on scanLock, when calling waitCondition.await, it will release the scan lock. This is pretty much how the condition and lock work.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488109807", "createdAt": "2020-09-14T17:39:58Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -240,53 +242,74 @@ long getMaxBlobSize() {\n    * @throws StoreException if BlobStoreStats is not enabled or closed\n    */\n   Pair<Long, NavigableMap<String, Long>> getValidDataSizeByLogSegment(TimeRange timeRange) throws StoreException {\n+    return getValidDataSizeByLogSegment(timeRange, time.milliseconds());\n+  }\n+\n+  /**\n+   * Same as {@link #getValidDataSizeByLogSegment(TimeRange)}, but provides a expiry reference time. Blobs with expiration\n+   * time less than the given expiry reference time, are considered as expired.\n+   * @param timeRange the delete reference {@link TimeRange} at which the data is requested. Defines both the reference time\n+   *                  and the acceptable resolution.\n+   * @param expiryReferenceTime the reference time for expired blobs. Blobs with expiration time less than it would be\n+   *                            considered as expired. Usually it's now.\n+   * @return a {@link Pair} whose first element is the time at which stats was collected (in ms) and whose second\n+   * element is the valid data size for each segment in the form of a {@link NavigableMap} of segment names to\n+   * valid data sizes.\n+   * @throws StoreException\n+   */\n+  Pair<Long, NavigableMap<String, Long>> getValidDataSizeByLogSegment(TimeRange timeRange, long expiryReferenceTime)\n+      throws StoreException {\n     if (!enabled.get()) {\n       throw new StoreException(String.format(\"BlobStoreStats is not enabled or closing for store %s\", storeId),\n           StoreErrorCodes.Store_Shutting_Down);\n     }\n-    ScanResults currentScanResults = scanResults.get();\n     Pair<Long, NavigableMap<String, Long>> retValue = null;\n+    ScanResults currentScanResults = scanResults.get();\n     long referenceTimeInMs = getLogSegmentRefTimeMs(currentScanResults, timeRange);\n-    if (referenceTimeInMs != REF_TIME_OUT_OF_BOUNDS) {\n-      retValue = currentScanResults.getValidSizePerLogSegment(referenceTimeInMs);\n-    } else {\n-      if (isScanning && getLogSegmentRefTimeMs(indexScanner.newScanResults, timeRange) != REF_TIME_OUT_OF_BOUNDS) {\n-        scanLock.lock();\n-        try {\n-          if (isScanning) {\n-            if (waitCondition.await(waitTimeoutInSecs, TimeUnit.SECONDS)) {\n+    if (enableBucketForLogSegmentReports) {\n+      if (referenceTimeInMs != REF_TIME_OUT_OF_BOUNDS) {\n+        retValue = currentScanResults.getValidSizePerLogSegment(referenceTimeInMs, expiryReferenceTime);\n+      } else {\n+        if (isScanning && getLogSegmentRefTimeMs(indexScanner.newScanResults, timeRange) != REF_TIME_OUT_OF_BOUNDS) {\n+          scanLock.lock();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjYyMTM1Mw=="}, "originalCommit": {"oid": "09dc26934ff89aea399bb4f1c24c6b8d6096fbef"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODEyMTY0MQ==", "bodyText": "Got it, I didn't notice waitCondition will release the lock.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488121641", "createdAt": "2020-09-14T18:00:31Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -240,53 +242,74 @@ long getMaxBlobSize() {\n    * @throws StoreException if BlobStoreStats is not enabled or closed\n    */\n   Pair<Long, NavigableMap<String, Long>> getValidDataSizeByLogSegment(TimeRange timeRange) throws StoreException {\n+    return getValidDataSizeByLogSegment(timeRange, time.milliseconds());\n+  }\n+\n+  /**\n+   * Same as {@link #getValidDataSizeByLogSegment(TimeRange)}, but provides a expiry reference time. Blobs with expiration\n+   * time less than the given expiry reference time, are considered as expired.\n+   * @param timeRange the delete reference {@link TimeRange} at which the data is requested. Defines both the reference time\n+   *                  and the acceptable resolution.\n+   * @param expiryReferenceTime the reference time for expired blobs. Blobs with expiration time less than it would be\n+   *                            considered as expired. Usually it's now.\n+   * @return a {@link Pair} whose first element is the time at which stats was collected (in ms) and whose second\n+   * element is the valid data size for each segment in the form of a {@link NavigableMap} of segment names to\n+   * valid data sizes.\n+   * @throws StoreException\n+   */\n+  Pair<Long, NavigableMap<String, Long>> getValidDataSizeByLogSegment(TimeRange timeRange, long expiryReferenceTime)\n+      throws StoreException {\n     if (!enabled.get()) {\n       throw new StoreException(String.format(\"BlobStoreStats is not enabled or closing for store %s\", storeId),\n           StoreErrorCodes.Store_Shutting_Down);\n     }\n-    ScanResults currentScanResults = scanResults.get();\n     Pair<Long, NavigableMap<String, Long>> retValue = null;\n+    ScanResults currentScanResults = scanResults.get();\n     long referenceTimeInMs = getLogSegmentRefTimeMs(currentScanResults, timeRange);\n-    if (referenceTimeInMs != REF_TIME_OUT_OF_BOUNDS) {\n-      retValue = currentScanResults.getValidSizePerLogSegment(referenceTimeInMs);\n-    } else {\n-      if (isScanning && getLogSegmentRefTimeMs(indexScanner.newScanResults, timeRange) != REF_TIME_OUT_OF_BOUNDS) {\n-        scanLock.lock();\n-        try {\n-          if (isScanning) {\n-            if (waitCondition.await(waitTimeoutInSecs, TimeUnit.SECONDS)) {\n+    if (enableBucketForLogSegmentReports) {\n+      if (referenceTimeInMs != REF_TIME_OUT_OF_BOUNDS) {\n+        retValue = currentScanResults.getValidSizePerLogSegment(referenceTimeInMs, expiryReferenceTime);\n+      } else {\n+        if (isScanning && getLogSegmentRefTimeMs(indexScanner.newScanResults, timeRange) != REF_TIME_OUT_OF_BOUNDS) {\n+          scanLock.lock();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjYyMTM1Mw=="}, "originalCommit": {"oid": "09dc26934ff89aea399bb4f1c24c6b8d6096fbef"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0NzUzMDQ0OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxNzo0NzoyOFrOHQoJcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNzo0NzoyNFrOHRgHZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzE5NzA0MQ==", "bodyText": "Could you elaborate why removeFinalStateOnPut is false here?", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r487197041", "createdAt": "2020-09-11T17:47:28Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -1054,18 +1235,19 @@ private void processIndexSegmentEntriesBackward(IndexSegment indexSegment, Predi\n           indexSegment.getFile().getName(), storeId);\n \n       // valid index entries wrt log segment reference time\n-      forEachValidIndexEntry(indexSegment, newScanResults.logSegmentForecastStartTimeMs, keyFinalStates, false,\n-          entry -> {\n+      forEachValidIndexEntry(indexSegment, newScanResults.logSegmentForecastStartTimeMsForDeleted,\n+          newScanResults.logSegmentForecastStartTimeMsForExpired, keyFinalStates, false, entry -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09dc26934ff89aea399bb4f1c24c6b8d6096fbef"}, "originalPosition": 653}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODExNDAyMw==", "bodyText": "removeFinalStateOnPut will remove the final state of a storeKey when encountering PUT record, since PUT record is the first record for the storeKey. This is for reducing memory usage. We don't want to keep final states of all store key in the map throughout the iteration of all index segment.\nHowever, in this method, we will process same index segment twice, first time for calculating valid size of log segment, second time for calculating valid size for container. So when calculating for log segment, we don't remove the final state  since later we would need the final state when calculating for containers.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488114023", "createdAt": "2020-09-14T17:47:24Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -1054,18 +1235,19 @@ private void processIndexSegmentEntriesBackward(IndexSegment indexSegment, Predi\n           indexSegment.getFile().getName(), storeId);\n \n       // valid index entries wrt log segment reference time\n-      forEachValidIndexEntry(indexSegment, newScanResults.logSegmentForecastStartTimeMs, keyFinalStates, false,\n-          entry -> {\n+      forEachValidIndexEntry(indexSegment, newScanResults.logSegmentForecastStartTimeMsForDeleted,\n+          newScanResults.logSegmentForecastStartTimeMsForExpired, keyFinalStates, false, entry -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzE5NzA0MQ=="}, "originalCommit": {"oid": "09dc26934ff89aea399bb4f1c24c6b8d6096fbef"}, "originalPosition": 653}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0NzU3ODQ2OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxODowMjozNVrOHQonVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNzo0OToyNVrOHRgLzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIwNDY5NA==", "bodyText": "Minor: maybe you can update the java doc of this method to reflect that it's delete forecast range.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r487204694", "createdAt": "2020-09-11T18:02:35Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -616,11 +657,11 @@ private boolean isTtlUpdateEntryValid(StoreKey key, IndexValue ttlUpdateValue, l\n    * is returned\n    */\n   private long getLogSegmentRefTimeMs(ScanResults results, TimeRange timeRange) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09dc26934ff89aea399bb4f1c24c6b8d6096fbef"}, "originalPosition": 320}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODExNTE1MA==", "bodyText": "updated.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488115150", "createdAt": "2020-09-14T17:49:25Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -616,11 +657,11 @@ private boolean isTtlUpdateEntryValid(StoreKey key, IndexValue ttlUpdateValue, l\n    * is returned\n    */\n   private long getLogSegmentRefTimeMs(ScanResults results, TimeRange timeRange) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIwNDY5NA=="}, "originalCommit": {"oid": "09dc26934ff89aea399bb4f1c24c6b8d6096fbef"}, "originalPosition": 320}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0NzYxMzQxOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxODoxMzo0MlrOHQo9FA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNzo1MDoyN1rOHRgOIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIxMDI2MA==", "bodyText": "why expiration in this comment was removed?", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r487210260", "createdAt": "2020-09-11T18:13:42Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -665,11 +706,12 @@ private void updateMapHelper(Map<String, Long> map, String key, Long value) {\n    * Helper function for container buckets for blob expiration/deletion related updates.\n    * @param results the {@link ScanResults} to be updated\n    * @param indexValue the PUT {@link IndexValue} of the expiring/deleting blob\n-   * @param expOrDelTimeInMs either the expiration or deletion time of the blob in ms\n+   * @param expOrDelTimeInMs deletion time of the blob in ms", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09dc26934ff89aea399bb4f1c24c6b8d6096fbef"}, "originalPosition": 338}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODExNTc0Ng==", "bodyText": "that was a mistake. updated.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488115746", "createdAt": "2020-09-14T17:50:27Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -665,11 +706,12 @@ private void updateMapHelper(Map<String, Long> map, String key, Long value) {\n    * Helper function for container buckets for blob expiration/deletion related updates.\n    * @param results the {@link ScanResults} to be updated\n    * @param indexValue the PUT {@link IndexValue} of the expiring/deleting blob\n-   * @param expOrDelTimeInMs either the expiration or deletion time of the blob in ms\n+   * @param expOrDelTimeInMs deletion time of the blob in ms", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIxMDI2MA=="}, "originalCommit": {"oid": "09dc26934ff89aea399bb4f1c24c6b8d6096fbef"}, "originalPosition": 338}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0NzY4MjE4OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxODozNjoyM1rOHQpn_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxODozNjoyM1rOHQpn_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzIyMTI0Nw==", "bodyText": "[Optional] in the future, we should consider making use of this on-demand scanning to populate ScanResult.  So for IndexScanner, it can check last previous scan result to decide whether it's worthwhile to perform another round of full scan.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r487221247", "createdAt": "2020-09-11T18:36:23Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -240,53 +242,74 @@ long getMaxBlobSize() {\n    * @throws StoreException if BlobStoreStats is not enabled or closed\n    */\n   Pair<Long, NavigableMap<String, Long>> getValidDataSizeByLogSegment(TimeRange timeRange) throws StoreException {\n+    return getValidDataSizeByLogSegment(timeRange, time.milliseconds());\n+  }\n+\n+  /**\n+   * Same as {@link #getValidDataSizeByLogSegment(TimeRange)}, but provides a expiry reference time. Blobs with expiration\n+   * time less than the given expiry reference time, are considered as expired.\n+   * @param timeRange the delete reference {@link TimeRange} at which the data is requested. Defines both the reference time\n+   *                  and the acceptable resolution.\n+   * @param expiryReferenceTime the reference time for expired blobs. Blobs with expiration time less than it would be\n+   *                            considered as expired. Usually it's now.\n+   * @return a {@link Pair} whose first element is the time at which stats was collected (in ms) and whose second\n+   * element is the valid data size for each segment in the form of a {@link NavigableMap} of segment names to\n+   * valid data sizes.\n+   * @throws StoreException\n+   */\n+  Pair<Long, NavigableMap<String, Long>> getValidDataSizeByLogSegment(TimeRange timeRange, long expiryReferenceTime)\n+      throws StoreException {\n     if (!enabled.get()) {\n       throw new StoreException(String.format(\"BlobStoreStats is not enabled or closing for store %s\", storeId),\n           StoreErrorCodes.Store_Shutting_Down);\n     }\n-    ScanResults currentScanResults = scanResults.get();\n     Pair<Long, NavigableMap<String, Long>> retValue = null;\n+    ScanResults currentScanResults = scanResults.get();\n     long referenceTimeInMs = getLogSegmentRefTimeMs(currentScanResults, timeRange);\n-    if (referenceTimeInMs != REF_TIME_OUT_OF_BOUNDS) {\n-      retValue = currentScanResults.getValidSizePerLogSegment(referenceTimeInMs);\n-    } else {\n-      if (isScanning && getLogSegmentRefTimeMs(indexScanner.newScanResults, timeRange) != REF_TIME_OUT_OF_BOUNDS) {\n-        scanLock.lock();\n-        try {\n-          if (isScanning) {\n-            if (waitCondition.await(waitTimeoutInSecs, TimeUnit.SECONDS)) {\n+    if (enableBucketForLogSegmentReports) {\n+      if (referenceTimeInMs != REF_TIME_OUT_OF_BOUNDS) {\n+        retValue = currentScanResults.getValidSizePerLogSegment(referenceTimeInMs, expiryReferenceTime);\n+      } else {\n+        if (isScanning && getLogSegmentRefTimeMs(indexScanner.newScanResults, timeRange) != REF_TIME_OUT_OF_BOUNDS) {\n+          scanLock.lock();\n+          try {\n+            if (isScanning) {\n+              if (waitCondition.await(waitTimeoutInSecs, TimeUnit.SECONDS)) {\n+                currentScanResults = scanResults.get();\n+                referenceTimeInMs = getLogSegmentRefTimeMs(currentScanResults, timeRange);\n+                if (referenceTimeInMs != REF_TIME_OUT_OF_BOUNDS) {\n+                  retValue = currentScanResults.getValidSizePerLogSegment(referenceTimeInMs, expiryReferenceTime);\n+                }\n+              } else {\n+                metrics.blobStoreStatsIndexScannerErrorCount.inc();\n+                logger.error(\"Timed out while waiting for BlobStoreStats index scan to complete for store {}\", storeId);\n+              }\n+            } else {\n               currentScanResults = scanResults.get();\n               referenceTimeInMs = getLogSegmentRefTimeMs(currentScanResults, timeRange);\n               if (referenceTimeInMs != REF_TIME_OUT_OF_BOUNDS) {\n-                retValue = currentScanResults.getValidSizePerLogSegment(referenceTimeInMs);\n+                retValue = currentScanResults.getValidSizePerLogSegment(referenceTimeInMs, expiryReferenceTime);\n               }\n-            } else {\n-              metrics.blobStoreStatsIndexScannerErrorCount.inc();\n-              logger.error(\"Timed out while waiting for BlobStoreStats index scan to complete for store {}\", storeId);\n-            }\n-          } else {\n-            currentScanResults = scanResults.get();\n-            referenceTimeInMs = getLogSegmentRefTimeMs(currentScanResults, timeRange);\n-            if (referenceTimeInMs != REF_TIME_OUT_OF_BOUNDS) {\n-              retValue = currentScanResults.getValidSizePerLogSegment(referenceTimeInMs);\n             }\n+          } catch (InterruptedException e) {\n+            metrics.blobStoreStatsIndexScannerErrorCount.inc();\n+            throw new IllegalStateException(\n+                String.format(\"Illegal state, wait for scan to complete is interrupted for store %s\", storeId), e);\n+          } finally {\n+            scanLock.unlock();\n           }\n-        } catch (InterruptedException e) {\n-          metrics.blobStoreStatsIndexScannerErrorCount.inc();\n-          throw new IllegalStateException(\n-              String.format(\"Illegal state, wait for scan to complete is interrupted for store %s\", storeId), e);\n-        } finally {\n-          scanLock.unlock();\n         }\n       }\n-      if (retValue == null) {\n-        // retValue could be null in three scenarios:\n-        // 1. timeRange is outside of current forecast coverage and there is no ongoing scan.\n-        // 2. timed out while waiting for an ongoing scan.\n-        // 3. rare edge case where currentScanResults updated twice since the start of the wait.\n-        referenceTimeInMs = timeRange.getEndTimeInMs();\n-        retValue = new Pair<>(referenceTimeInMs, collectValidDataSizeByLogSegment(referenceTimeInMs));\n-      }\n+    }\n+\n+    if (retValue == null) {\n+      // retValue could be null in three scenarios:\n+      // 1. timeRange is outside of current forecast coverage and there is no ongoing scan.\n+      // 2. timed out while waiting for an ongoing scan.\n+      // 3. rare edge case where currentScanResults updated twice since the start of the wait.\n+      referenceTimeInMs = timeRange.getEndTimeInMs();\n+      retValue =\n+          new Pair<>(referenceTimeInMs, collectValidDataSizeByLogSegment(referenceTimeInMs, expiryReferenceTime));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09dc26934ff89aea399bb4f1c24c6b8d6096fbef"}, "originalPosition": 147}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0ODE1MTkyOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQyMToxNjoyN1rOHQuGUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQyMzozNjoxMlrOHRrMnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI5NDU0Nw==", "bodyText": "Why we need to add PUT's size ?  You mean we first add PUT's size int previous DELETE's operation time bucket and then subtract from current DELETE's operation time bucket?  If so, I would suggest adding step 3 into the comment.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r487294547", "createdAt": "2020-09-11T21:16:27Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -706,46 +765,132 @@ private void processNewPut(ScanResults results, IndexValue putValue) {\n         handleContainerBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n-    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMs)) {\n+    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMsForExpired)) {\n       results.updateLogSegmentBaseBucket(putValue.getOffset().getName(), putValue.getSize());\n       if (expiresAtMs != Utils.Infinite_Time) {\n-        handleLogSegmentBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n+        handleLogSegmentExpiredBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n   }\n \n   /**\n    * Helper function to process new DELETE entries and make appropriate updates to the given {@link ScanResults}.\n    * @param results the {@link ScanResults} to apply the updates to\n+   * @param key the {@link StoreKey} of the new DELETE\n    * @param deleteValue the {@link IndexValue} of the new DELETE\n    * @param originalPutValue the {@link IndexValue} of the original PUT that is getting deleted\n+   * @param previousValue the {@link IndexValue} comes prior to the new DELETE\n    */\n-  private void processNewDelete(ScanResults results, IndexValue deleteValue, IndexValue originalPutValue) {\n+  private void processNewDelete(ScanResults results, StoreKey key, IndexValue deleteValue, IndexValue originalPutValue,\n+      IndexValue previousValue) {\n+    long operationTimeInMs = deleteValue.getOperationTimeInMs();\n+    if (operationTimeInMs == Utils.Infinite_Time) {\n+      operationTimeInMs =\n+          index.getIndexSegments().floorEntry(deleteValue.getOffset()).getValue().getLastModifiedTimeMs();\n+    }\n+    // First deal with container bucket\n+    if (previousValue.isDelete()) {\n+      // The previous value is already DELETE, current DELETE would not change container bucket.\n+      // Assume the record history looks like this: PUT, DELETE, DELETE.\n+      // First DELETE already updated container bucket.\n+    } else {\n+      // If original PUT expires before this DELETE, then it will not change container bucket.\n+      if (!isExpired(originalPutValue.getExpiresAtMs(), operationTimeInMs)) {\n+        handleContainerBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n+        handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n+      }\n+    }\n+    // Now deal with log segment bucket\n+    // Current DELETE is always valid\n+    if (previousValue.isDelete()) {\n+      // Previous IndexValue is a DELETE, it should already\n+      // 1. Added it's size to the first bucket in the scan result\n+      // 2. Subtracted original PUT's size from the previousValue.getOperationTimeInMs() bucket.\n+      // We need to recover from these two operations, so\n+      // 1. Subtract previous DELETE's size from bucket in the scan result.\n+      // 2. Add original PUT's size to the previousValue.getOperationTimeInMs() bucket.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09dc26934ff89aea399bb4f1c24c6b8d6096fbef"}, "originalPosition": 434}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODEyMDY3OQ==", "bodyText": "if we are under this if statement, then we know the current IndexValue is a DELETE and the previous IndexValue is also a DELETE, the IndexValue history can go like this, PUT, DELETE(v0), DELETE(v1).\nWhen we see the DELETE(v0), we will 1) add size of DELETE to base bucket, then 2)subtract size of PUT int the DELETE's operation time bucket.\nWhen we see the DELETE(v1), we know it will invalidate DELETE(v0). So one way to do this is to revert what DELETE(v0) did before, so we 1)subtract size of DELETE from base bucket, then 3)add size of PUT in the DELETE(v0)'s operation time bucket. After doing this, we can treat DELETE(v0) just like it never happened. Then we move on to deal with DELETE(v1).", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488120679", "createdAt": "2020-09-14T17:58:58Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -706,46 +765,132 @@ private void processNewPut(ScanResults results, IndexValue putValue) {\n         handleContainerBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n-    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMs)) {\n+    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMsForExpired)) {\n       results.updateLogSegmentBaseBucket(putValue.getOffset().getName(), putValue.getSize());\n       if (expiresAtMs != Utils.Infinite_Time) {\n-        handleLogSegmentBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n+        handleLogSegmentExpiredBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n   }\n \n   /**\n    * Helper function to process new DELETE entries and make appropriate updates to the given {@link ScanResults}.\n    * @param results the {@link ScanResults} to apply the updates to\n+   * @param key the {@link StoreKey} of the new DELETE\n    * @param deleteValue the {@link IndexValue} of the new DELETE\n    * @param originalPutValue the {@link IndexValue} of the original PUT that is getting deleted\n+   * @param previousValue the {@link IndexValue} comes prior to the new DELETE\n    */\n-  private void processNewDelete(ScanResults results, IndexValue deleteValue, IndexValue originalPutValue) {\n+  private void processNewDelete(ScanResults results, StoreKey key, IndexValue deleteValue, IndexValue originalPutValue,\n+      IndexValue previousValue) {\n+    long operationTimeInMs = deleteValue.getOperationTimeInMs();\n+    if (operationTimeInMs == Utils.Infinite_Time) {\n+      operationTimeInMs =\n+          index.getIndexSegments().floorEntry(deleteValue.getOffset()).getValue().getLastModifiedTimeMs();\n+    }\n+    // First deal with container bucket\n+    if (previousValue.isDelete()) {\n+      // The previous value is already DELETE, current DELETE would not change container bucket.\n+      // Assume the record history looks like this: PUT, DELETE, DELETE.\n+      // First DELETE already updated container bucket.\n+    } else {\n+      // If original PUT expires before this DELETE, then it will not change container bucket.\n+      if (!isExpired(originalPutValue.getExpiresAtMs(), operationTimeInMs)) {\n+        handleContainerBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n+        handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n+      }\n+    }\n+    // Now deal with log segment bucket\n+    // Current DELETE is always valid\n+    if (previousValue.isDelete()) {\n+      // Previous IndexValue is a DELETE, it should already\n+      // 1. Added it's size to the first bucket in the scan result\n+      // 2. Subtracted original PUT's size from the previousValue.getOperationTimeInMs() bucket.\n+      // We need to recover from these two operations, so\n+      // 1. Subtract previous DELETE's size from bucket in the scan result.\n+      // 2. Add original PUT's size to the previousValue.getOperationTimeInMs() bucket.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI5NDU0Nw=="}, "originalCommit": {"oid": "09dc26934ff89aea399bb4f1c24c6b8d6096fbef"}, "originalPosition": 434}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI3NzU3NQ==", "bodyText": "Correct, this is what I thought about. My suggestion is to make the comments more accurate:\nAdded it's size to the first bucket in the scan result -> Added it's size to the base bucket;\nSubtract previous DELETE's size from bucket in the scan result -> Subtract previous DELETE's size from base bucket;\nAlso add one sentence in the end something like:\nAfter recovering changes made by previous DELETE, we update both base bucket and operation time bucket for new DELETE.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488277575", "createdAt": "2020-09-14T22:42:22Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -706,46 +765,132 @@ private void processNewPut(ScanResults results, IndexValue putValue) {\n         handleContainerBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n-    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMs)) {\n+    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMsForExpired)) {\n       results.updateLogSegmentBaseBucket(putValue.getOffset().getName(), putValue.getSize());\n       if (expiresAtMs != Utils.Infinite_Time) {\n-        handleLogSegmentBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n+        handleLogSegmentExpiredBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n   }\n \n   /**\n    * Helper function to process new DELETE entries and make appropriate updates to the given {@link ScanResults}.\n    * @param results the {@link ScanResults} to apply the updates to\n+   * @param key the {@link StoreKey} of the new DELETE\n    * @param deleteValue the {@link IndexValue} of the new DELETE\n    * @param originalPutValue the {@link IndexValue} of the original PUT that is getting deleted\n+   * @param previousValue the {@link IndexValue} comes prior to the new DELETE\n    */\n-  private void processNewDelete(ScanResults results, IndexValue deleteValue, IndexValue originalPutValue) {\n+  private void processNewDelete(ScanResults results, StoreKey key, IndexValue deleteValue, IndexValue originalPutValue,\n+      IndexValue previousValue) {\n+    long operationTimeInMs = deleteValue.getOperationTimeInMs();\n+    if (operationTimeInMs == Utils.Infinite_Time) {\n+      operationTimeInMs =\n+          index.getIndexSegments().floorEntry(deleteValue.getOffset()).getValue().getLastModifiedTimeMs();\n+    }\n+    // First deal with container bucket\n+    if (previousValue.isDelete()) {\n+      // The previous value is already DELETE, current DELETE would not change container bucket.\n+      // Assume the record history looks like this: PUT, DELETE, DELETE.\n+      // First DELETE already updated container bucket.\n+    } else {\n+      // If original PUT expires before this DELETE, then it will not change container bucket.\n+      if (!isExpired(originalPutValue.getExpiresAtMs(), operationTimeInMs)) {\n+        handleContainerBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n+        handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n+      }\n+    }\n+    // Now deal with log segment bucket\n+    // Current DELETE is always valid\n+    if (previousValue.isDelete()) {\n+      // Previous IndexValue is a DELETE, it should already\n+      // 1. Added it's size to the first bucket in the scan result\n+      // 2. Subtracted original PUT's size from the previousValue.getOperationTimeInMs() bucket.\n+      // We need to recover from these two operations, so\n+      // 1. Subtract previous DELETE's size from bucket in the scan result.\n+      // 2. Add original PUT's size to the previousValue.getOperationTimeInMs() bucket.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI5NDU0Nw=="}, "originalCommit": {"oid": "09dc26934ff89aea399bb4f1c24c6b8d6096fbef"}, "originalPosition": 434}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI5NTU4MA==", "bodyText": "I reword the comments, please take another look, see if it's good for you.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488295580", "createdAt": "2020-09-14T23:36:12Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -706,46 +765,132 @@ private void processNewPut(ScanResults results, IndexValue putValue) {\n         handleContainerBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n-    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMs)) {\n+    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMsForExpired)) {\n       results.updateLogSegmentBaseBucket(putValue.getOffset().getName(), putValue.getSize());\n       if (expiresAtMs != Utils.Infinite_Time) {\n-        handleLogSegmentBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n+        handleLogSegmentExpiredBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n   }\n \n   /**\n    * Helper function to process new DELETE entries and make appropriate updates to the given {@link ScanResults}.\n    * @param results the {@link ScanResults} to apply the updates to\n+   * @param key the {@link StoreKey} of the new DELETE\n    * @param deleteValue the {@link IndexValue} of the new DELETE\n    * @param originalPutValue the {@link IndexValue} of the original PUT that is getting deleted\n+   * @param previousValue the {@link IndexValue} comes prior to the new DELETE\n    */\n-  private void processNewDelete(ScanResults results, IndexValue deleteValue, IndexValue originalPutValue) {\n+  private void processNewDelete(ScanResults results, StoreKey key, IndexValue deleteValue, IndexValue originalPutValue,\n+      IndexValue previousValue) {\n+    long operationTimeInMs = deleteValue.getOperationTimeInMs();\n+    if (operationTimeInMs == Utils.Infinite_Time) {\n+      operationTimeInMs =\n+          index.getIndexSegments().floorEntry(deleteValue.getOffset()).getValue().getLastModifiedTimeMs();\n+    }\n+    // First deal with container bucket\n+    if (previousValue.isDelete()) {\n+      // The previous value is already DELETE, current DELETE would not change container bucket.\n+      // Assume the record history looks like this: PUT, DELETE, DELETE.\n+      // First DELETE already updated container bucket.\n+    } else {\n+      // If original PUT expires before this DELETE, then it will not change container bucket.\n+      if (!isExpired(originalPutValue.getExpiresAtMs(), operationTimeInMs)) {\n+        handleContainerBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n+        handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n+      }\n+    }\n+    // Now deal with log segment bucket\n+    // Current DELETE is always valid\n+    if (previousValue.isDelete()) {\n+      // Previous IndexValue is a DELETE, it should already\n+      // 1. Added it's size to the first bucket in the scan result\n+      // 2. Subtracted original PUT's size from the previousValue.getOperationTimeInMs() bucket.\n+      // We need to recover from these two operations, so\n+      // 1. Subtract previous DELETE's size from bucket in the scan result.\n+      // 2. Add original PUT's size to the previousValue.getOperationTimeInMs() bucket.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI5NDU0Nw=="}, "originalCommit": {"oid": "09dc26934ff89aea399bb4f1c24c6b8d6096fbef"}, "originalPosition": 434}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1NTA2MTU4OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQyMjo0Nzo1OVrOHRqQ5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQyMjo0Nzo1OVrOHRqQ5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI4MDI5NA==", "bodyText": "same here, first bucket -> base bucket to be more accurate", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488280294", "createdAt": "2020-09-14T22:47:59Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -706,46 +766,132 @@ private void processNewPut(ScanResults results, IndexValue putValue) {\n         handleContainerBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n-    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMs)) {\n+    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMsForExpired)) {\n       results.updateLogSegmentBaseBucket(putValue.getOffset().getName(), putValue.getSize());\n       if (expiresAtMs != Utils.Infinite_Time) {\n-        handleLogSegmentBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n+        handleLogSegmentExpiredBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n   }\n \n   /**\n    * Helper function to process new DELETE entries and make appropriate updates to the given {@link ScanResults}.\n    * @param results the {@link ScanResults} to apply the updates to\n+   * @param key the {@link StoreKey} of the new DELETE\n    * @param deleteValue the {@link IndexValue} of the new DELETE\n    * @param originalPutValue the {@link IndexValue} of the original PUT that is getting deleted\n+   * @param previousValue the {@link IndexValue} comes prior to the new DELETE\n    */\n-  private void processNewDelete(ScanResults results, IndexValue deleteValue, IndexValue originalPutValue) {\n+  private void processNewDelete(ScanResults results, StoreKey key, IndexValue deleteValue, IndexValue originalPutValue,\n+      IndexValue previousValue) {\n+    long operationTimeInMs = deleteValue.getOperationTimeInMs();\n+    if (operationTimeInMs == Utils.Infinite_Time) {\n+      operationTimeInMs =\n+          index.getIndexSegments().floorEntry(deleteValue.getOffset()).getValue().getLastModifiedTimeMs();\n+    }\n+    // First deal with container bucket\n+    if (previousValue.isDelete()) {\n+      // The previous value is already DELETE, current DELETE would not change container bucket.\n+      // Assume the record history looks like this: PUT, DELETE, DELETE.\n+      // First DELETE already updated container bucket.\n+    } else {\n+      // If original PUT expires before this DELETE, then it will not change container bucket.\n+      if (!isExpired(originalPutValue.getExpiresAtMs(), operationTimeInMs)) {\n+        handleContainerBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n+        handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n+      }\n+    }\n+    // Now deal with log segment bucket\n+    // Current DELETE is always valid\n+    if (previousValue.isDelete()) {\n+      // Previous IndexValue is a DELETE, it should already\n+      // 1. Added it's size to the first bucket in the scan result\n+      // 2. Subtracted original PUT's size from the previousValue.getOperationTimeInMs() bucket.\n+      // We need to recover from these two operations, so\n+      // 1. Subtract previous DELETE's size from bucket in the scan result.\n+      // 2. Add original PUT's size to the previousValue.getOperationTimeInMs() bucket.\n+      processDeleteUpdateLogSegmentHelper(results, key, previousValue, originalPutValue, SUBTRACT);\n+    } else if (previousValue.isUndelete()) {\n+      // Previous IndexValue is an UNDELETE, it should already\n+      // 1. Added it's size to the first bucket of the scan result", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756785e07ee435de886d8b444b9740f686e282e9"}, "originalPosition": 445}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1NTA5NjIxOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQyMzowMzoyNlrOHRqlBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwMDoxMDowMFrOHRrzWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI4NTQ0Ng==", "bodyText": "I understand this part and just want to confirm again about UNDELETE. The UNDELETE operation always checks if original PUT has expired. So, if we reach here, it means original PUT hasn't expired and should be subtracted, correct?\nI am asking this because I am thinking about a case where original PUT expires before DELETE and there is an UNDELETE in the end. In this case, original PUT is subtracted twice but I think this case would never happen.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488285446", "createdAt": "2020-09-14T23:03:26Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -706,46 +766,132 @@ private void processNewPut(ScanResults results, IndexValue putValue) {\n         handleContainerBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n-    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMs)) {\n+    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMsForExpired)) {\n       results.updateLogSegmentBaseBucket(putValue.getOffset().getName(), putValue.getSize());\n       if (expiresAtMs != Utils.Infinite_Time) {\n-        handleLogSegmentBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n+        handleLogSegmentExpiredBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n   }\n \n   /**\n    * Helper function to process new DELETE entries and make appropriate updates to the given {@link ScanResults}.\n    * @param results the {@link ScanResults} to apply the updates to\n+   * @param key the {@link StoreKey} of the new DELETE\n    * @param deleteValue the {@link IndexValue} of the new DELETE\n    * @param originalPutValue the {@link IndexValue} of the original PUT that is getting deleted\n+   * @param previousValue the {@link IndexValue} comes prior to the new DELETE\n    */\n-  private void processNewDelete(ScanResults results, IndexValue deleteValue, IndexValue originalPutValue) {\n+  private void processNewDelete(ScanResults results, StoreKey key, IndexValue deleteValue, IndexValue originalPutValue,\n+      IndexValue previousValue) {\n+    long operationTimeInMs = deleteValue.getOperationTimeInMs();\n+    if (operationTimeInMs == Utils.Infinite_Time) {\n+      operationTimeInMs =\n+          index.getIndexSegments().floorEntry(deleteValue.getOffset()).getValue().getLastModifiedTimeMs();\n+    }\n+    // First deal with container bucket\n+    if (previousValue.isDelete()) {\n+      // The previous value is already DELETE, current DELETE would not change container bucket.\n+      // Assume the record history looks like this: PUT, DELETE, DELETE.\n+      // First DELETE already updated container bucket.\n+    } else {\n+      // If original PUT expires before this DELETE, then it will not change container bucket.\n+      if (!isExpired(originalPutValue.getExpiresAtMs(), operationTimeInMs)) {\n+        handleContainerBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n+        handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n+      }\n+    }\n+    // Now deal with log segment bucket\n+    // Current DELETE is always valid\n+    if (previousValue.isDelete()) {\n+      // Previous IndexValue is a DELETE, it should already\n+      // 1. Added it's size to the first bucket in the scan result\n+      // 2. Subtracted original PUT's size from the previousValue.getOperationTimeInMs() bucket.\n+      // We need to recover from these two operations, so\n+      // 1. Subtract previous DELETE's size from bucket in the scan result.\n+      // 2. Add original PUT's size to the previousValue.getOperationTimeInMs() bucket.\n+      processDeleteUpdateLogSegmentHelper(results, key, previousValue, originalPutValue, SUBTRACT);\n+    } else if (previousValue.isUndelete()) {\n+      // Previous IndexValue is an UNDELETE, it should already\n+      // 1. Added it's size to the first bucket of the scan result\n+      // we need to recover from this operation\n+      results.updateLogSegmentBaseBucket(previousValue.getOffset().getName(), SUBTRACT * previousValue.getSize());\n+    }\n+    processDeleteUpdateLogSegmentHelper(results, key, deleteValue, originalPutValue, ADD);\n+  }\n+\n+  private void processDeleteUpdateLogSegmentHelper(ScanResults results, StoreKey key, IndexValue deleteValue,\n+      IndexValue originalPutValue, int operator) {\n     long operationTimeInMs = deleteValue.getOperationTimeInMs();\n     if (operationTimeInMs == Utils.Infinite_Time) {\n       operationTimeInMs =\n           index.getIndexSegments().floorEntry(deleteValue.getOffset()).getValue().getLastModifiedTimeMs();\n     }\n-    results.updateLogSegmentBaseBucket(deleteValue.getOffset().getName(), deleteValue.getSize());\n+    results.updateLogSegmentBaseBucket(deleteValue.getOffset().getName(), operator * deleteValue.getSize());\n     if (!isExpired(originalPutValue.getExpiresAtMs(), operationTimeInMs)) {\n-      handleContainerBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n-      handleLogSegmentBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n+      handleLogSegmentDeletedBucketUpdate(results, originalPutValue, operationTimeInMs, operator * SUBTRACT);\n       if (originalPutValue.getExpiresAtMs() != Utils.Infinite_Time) {\n         // make appropriate updates to avoid double counting\n-        handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n-        handleLogSegmentBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n+        handleLogSegmentExpiredBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(),\n+            operator * ADD);\n+      } else if (deleteValue.isTtlUpdate()) {\n+        // This blob has a PUT and TTL_UPDATE, because of DELETE, now TTL_UPDATE is not valid any more.\n+        try {\n+          IndexValue ttlUpdateValue =\n+              index.findKey(key, new FileSpan(originalPutValue.getOffset(), deleteValue.getOffset()),\n+                  EnumSet.of(PersistentIndex.IndexEntryType.TTL_UPDATE));\n+          if (ttlUpdateValue != null) {\n+            handleLogSegmentDeletedBucketUpdate(results, ttlUpdateValue, operationTimeInMs, operator * SUBTRACT);\n+          }\n+        } catch (StoreException e) {\n+          logger.error(\n+              \"Failed to find TTL_UPDATE IndexValue for \" + key + \" when processing new DELETE: \" + deleteValue, e);\n+        }\n       }\n     }\n   }\n \n   /**\n    * Helper function to process new UNDELETE entries and make appropriate updates to the given {@link ScanResults}.\n    * @param results the {@link ScanResults} to apply the updates to\n+   * @param key the {@link StoreKey} of the new UNDELETE\n    * @param undeleteValue the {@link IndexValue} of the new UNDELETE\n    * @param originalPutValue the {@link IndexValue} of the original PUT\n+   * @param previousValue the {@link IndexValue} comes prior to the new UNDELETE\n    */\n-  private void processNewUndelete(ScanResults results, IndexValue undeleteValue, IndexValue originalPutValue) {\n-    // TODO: future work to support online updates\n+  private void processNewUndelete(ScanResults results, StoreKey key, IndexValue undeleteValue,\n+      IndexValue originalPutValue, IndexValue previousValue) {\n+    // First deal with container bucket\n+    if (!previousValue.isDelete()) {\n+      // This previous value is not DELETE, current UNDELETE would not change container bucket.\n+      // There are several possibility when the previous value is not DELETE.\n+      // 1. PUT UNDELETE: UNDELETE doesn't change any bucket\n+      // 2. PUT TTL_UPDATE UNDELETE: UNDELETE doesn't change any bucket\n+      // 3. PUT DELETE UNDELETE UNDELETE: the last UNDELETE doesn't change any bucket\n+      // UNDELETE would resurrect a deleted IndexValue, if the previous value is not DELETE, then the IndexValue doesn't\n+      // need to be resurrected.\n+    } else {\n+      handleContainerBucketUpdate(results, originalPutValue, undeleteValue.getOperationTimeInMs(), ADD);\n+      handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), SUBTRACT);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756785e07ee435de886d8b444b9740f686e282e9"}, "originalPosition": 511}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNTQ5Ng==", "bodyText": "Your first statement is correct, and there is no chance when the PUT is expired before DELETE and UNDELETE would succeeds. If we have PUT and DELETE for the blob, but PUT is already expired, then UNDELETE will fail since it can't undelete an expired PUT.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488305496", "createdAt": "2020-09-15T00:10:00Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -706,46 +766,132 @@ private void processNewPut(ScanResults results, IndexValue putValue) {\n         handleContainerBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n-    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMs)) {\n+    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMsForExpired)) {\n       results.updateLogSegmentBaseBucket(putValue.getOffset().getName(), putValue.getSize());\n       if (expiresAtMs != Utils.Infinite_Time) {\n-        handleLogSegmentBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n+        handleLogSegmentExpiredBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n   }\n \n   /**\n    * Helper function to process new DELETE entries and make appropriate updates to the given {@link ScanResults}.\n    * @param results the {@link ScanResults} to apply the updates to\n+   * @param key the {@link StoreKey} of the new DELETE\n    * @param deleteValue the {@link IndexValue} of the new DELETE\n    * @param originalPutValue the {@link IndexValue} of the original PUT that is getting deleted\n+   * @param previousValue the {@link IndexValue} comes prior to the new DELETE\n    */\n-  private void processNewDelete(ScanResults results, IndexValue deleteValue, IndexValue originalPutValue) {\n+  private void processNewDelete(ScanResults results, StoreKey key, IndexValue deleteValue, IndexValue originalPutValue,\n+      IndexValue previousValue) {\n+    long operationTimeInMs = deleteValue.getOperationTimeInMs();\n+    if (operationTimeInMs == Utils.Infinite_Time) {\n+      operationTimeInMs =\n+          index.getIndexSegments().floorEntry(deleteValue.getOffset()).getValue().getLastModifiedTimeMs();\n+    }\n+    // First deal with container bucket\n+    if (previousValue.isDelete()) {\n+      // The previous value is already DELETE, current DELETE would not change container bucket.\n+      // Assume the record history looks like this: PUT, DELETE, DELETE.\n+      // First DELETE already updated container bucket.\n+    } else {\n+      // If original PUT expires before this DELETE, then it will not change container bucket.\n+      if (!isExpired(originalPutValue.getExpiresAtMs(), operationTimeInMs)) {\n+        handleContainerBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n+        handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n+      }\n+    }\n+    // Now deal with log segment bucket\n+    // Current DELETE is always valid\n+    if (previousValue.isDelete()) {\n+      // Previous IndexValue is a DELETE, it should already\n+      // 1. Added it's size to the first bucket in the scan result\n+      // 2. Subtracted original PUT's size from the previousValue.getOperationTimeInMs() bucket.\n+      // We need to recover from these two operations, so\n+      // 1. Subtract previous DELETE's size from bucket in the scan result.\n+      // 2. Add original PUT's size to the previousValue.getOperationTimeInMs() bucket.\n+      processDeleteUpdateLogSegmentHelper(results, key, previousValue, originalPutValue, SUBTRACT);\n+    } else if (previousValue.isUndelete()) {\n+      // Previous IndexValue is an UNDELETE, it should already\n+      // 1. Added it's size to the first bucket of the scan result\n+      // we need to recover from this operation\n+      results.updateLogSegmentBaseBucket(previousValue.getOffset().getName(), SUBTRACT * previousValue.getSize());\n+    }\n+    processDeleteUpdateLogSegmentHelper(results, key, deleteValue, originalPutValue, ADD);\n+  }\n+\n+  private void processDeleteUpdateLogSegmentHelper(ScanResults results, StoreKey key, IndexValue deleteValue,\n+      IndexValue originalPutValue, int operator) {\n     long operationTimeInMs = deleteValue.getOperationTimeInMs();\n     if (operationTimeInMs == Utils.Infinite_Time) {\n       operationTimeInMs =\n           index.getIndexSegments().floorEntry(deleteValue.getOffset()).getValue().getLastModifiedTimeMs();\n     }\n-    results.updateLogSegmentBaseBucket(deleteValue.getOffset().getName(), deleteValue.getSize());\n+    results.updateLogSegmentBaseBucket(deleteValue.getOffset().getName(), operator * deleteValue.getSize());\n     if (!isExpired(originalPutValue.getExpiresAtMs(), operationTimeInMs)) {\n-      handleContainerBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n-      handleLogSegmentBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n+      handleLogSegmentDeletedBucketUpdate(results, originalPutValue, operationTimeInMs, operator * SUBTRACT);\n       if (originalPutValue.getExpiresAtMs() != Utils.Infinite_Time) {\n         // make appropriate updates to avoid double counting\n-        handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n-        handleLogSegmentBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n+        handleLogSegmentExpiredBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(),\n+            operator * ADD);\n+      } else if (deleteValue.isTtlUpdate()) {\n+        // This blob has a PUT and TTL_UPDATE, because of DELETE, now TTL_UPDATE is not valid any more.\n+        try {\n+          IndexValue ttlUpdateValue =\n+              index.findKey(key, new FileSpan(originalPutValue.getOffset(), deleteValue.getOffset()),\n+                  EnumSet.of(PersistentIndex.IndexEntryType.TTL_UPDATE));\n+          if (ttlUpdateValue != null) {\n+            handleLogSegmentDeletedBucketUpdate(results, ttlUpdateValue, operationTimeInMs, operator * SUBTRACT);\n+          }\n+        } catch (StoreException e) {\n+          logger.error(\n+              \"Failed to find TTL_UPDATE IndexValue for \" + key + \" when processing new DELETE: \" + deleteValue, e);\n+        }\n       }\n     }\n   }\n \n   /**\n    * Helper function to process new UNDELETE entries and make appropriate updates to the given {@link ScanResults}.\n    * @param results the {@link ScanResults} to apply the updates to\n+   * @param key the {@link StoreKey} of the new UNDELETE\n    * @param undeleteValue the {@link IndexValue} of the new UNDELETE\n    * @param originalPutValue the {@link IndexValue} of the original PUT\n+   * @param previousValue the {@link IndexValue} comes prior to the new UNDELETE\n    */\n-  private void processNewUndelete(ScanResults results, IndexValue undeleteValue, IndexValue originalPutValue) {\n-    // TODO: future work to support online updates\n+  private void processNewUndelete(ScanResults results, StoreKey key, IndexValue undeleteValue,\n+      IndexValue originalPutValue, IndexValue previousValue) {\n+    // First deal with container bucket\n+    if (!previousValue.isDelete()) {\n+      // This previous value is not DELETE, current UNDELETE would not change container bucket.\n+      // There are several possibility when the previous value is not DELETE.\n+      // 1. PUT UNDELETE: UNDELETE doesn't change any bucket\n+      // 2. PUT TTL_UPDATE UNDELETE: UNDELETE doesn't change any bucket\n+      // 3. PUT DELETE UNDELETE UNDELETE: the last UNDELETE doesn't change any bucket\n+      // UNDELETE would resurrect a deleted IndexValue, if the previous value is not DELETE, then the IndexValue doesn't\n+      // need to be resurrected.\n+    } else {\n+      handleContainerBucketUpdate(results, originalPutValue, undeleteValue.getOperationTimeInMs(), ADD);\n+      handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), SUBTRACT);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI4NTQ0Ng=="}, "originalCommit": {"oid": "756785e07ee435de886d8b444b9740f686e282e9"}, "originalPosition": 511}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1NTE5ODk4OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQyMzo1Mzo1OFrOHRrgpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwMDowNzozNVrOHRrw6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwMDcxMA==", "bodyText": "Let's consider a case: PUT, DELETE, UNDELETE and PUT expires after UNDELETE op time. Based on my understanding, it will go through following steps:\n Operation  | Base bucket    | Delete Op bucket | Undelete Op bucket | Expired Bucket\n    DELETE   | + D size        |  - P size  |                           |  + P size\n UNDELETE    | - D + U size  |  + P size   |                       |  + P size\n\nLooks like the expired bucket adds PUT record size twice, did I misunderstand anything?", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488300710", "createdAt": "2020-09-14T23:53:58Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -706,46 +766,132 @@ private void processNewPut(ScanResults results, IndexValue putValue) {\n         handleContainerBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n-    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMs)) {\n+    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMsForExpired)) {\n       results.updateLogSegmentBaseBucket(putValue.getOffset().getName(), putValue.getSize());\n       if (expiresAtMs != Utils.Infinite_Time) {\n-        handleLogSegmentBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n+        handleLogSegmentExpiredBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n   }\n \n   /**\n    * Helper function to process new DELETE entries and make appropriate updates to the given {@link ScanResults}.\n    * @param results the {@link ScanResults} to apply the updates to\n+   * @param key the {@link StoreKey} of the new DELETE\n    * @param deleteValue the {@link IndexValue} of the new DELETE\n    * @param originalPutValue the {@link IndexValue} of the original PUT that is getting deleted\n+   * @param previousValue the {@link IndexValue} comes prior to the new DELETE\n    */\n-  private void processNewDelete(ScanResults results, IndexValue deleteValue, IndexValue originalPutValue) {\n+  private void processNewDelete(ScanResults results, StoreKey key, IndexValue deleteValue, IndexValue originalPutValue,\n+      IndexValue previousValue) {\n+    long operationTimeInMs = deleteValue.getOperationTimeInMs();\n+    if (operationTimeInMs == Utils.Infinite_Time) {\n+      operationTimeInMs =\n+          index.getIndexSegments().floorEntry(deleteValue.getOffset()).getValue().getLastModifiedTimeMs();\n+    }\n+    // First deal with container bucket\n+    if (previousValue.isDelete()) {\n+      // The previous value is already DELETE, current DELETE would not change container bucket.\n+      // Assume the record history looks like this: PUT, DELETE, DELETE.\n+      // First DELETE already updated container bucket.\n+    } else {\n+      // If original PUT expires before this DELETE, then it will not change container bucket.\n+      if (!isExpired(originalPutValue.getExpiresAtMs(), operationTimeInMs)) {\n+        handleContainerBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n+        handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n+      }\n+    }\n+    // Now deal with log segment bucket\n+    // Current DELETE is always valid\n+    if (previousValue.isDelete()) {\n+      // Previous IndexValue is a DELETE, it should already\n+      // 1. Added it's size to the first bucket in the scan result\n+      // 2. Subtracted original PUT's size from the previousValue.getOperationTimeInMs() bucket.\n+      // We need to recover from these two operations, so\n+      // 1. Subtract previous DELETE's size from bucket in the scan result.\n+      // 2. Add original PUT's size to the previousValue.getOperationTimeInMs() bucket.\n+      processDeleteUpdateLogSegmentHelper(results, key, previousValue, originalPutValue, SUBTRACT);\n+    } else if (previousValue.isUndelete()) {\n+      // Previous IndexValue is an UNDELETE, it should already\n+      // 1. Added it's size to the first bucket of the scan result\n+      // we need to recover from this operation\n+      results.updateLogSegmentBaseBucket(previousValue.getOffset().getName(), SUBTRACT * previousValue.getSize());\n+    }\n+    processDeleteUpdateLogSegmentHelper(results, key, deleteValue, originalPutValue, ADD);\n+  }\n+\n+  private void processDeleteUpdateLogSegmentHelper(ScanResults results, StoreKey key, IndexValue deleteValue,\n+      IndexValue originalPutValue, int operator) {\n     long operationTimeInMs = deleteValue.getOperationTimeInMs();\n     if (operationTimeInMs == Utils.Infinite_Time) {\n       operationTimeInMs =\n           index.getIndexSegments().floorEntry(deleteValue.getOffset()).getValue().getLastModifiedTimeMs();\n     }\n-    results.updateLogSegmentBaseBucket(deleteValue.getOffset().getName(), deleteValue.getSize());\n+    results.updateLogSegmentBaseBucket(deleteValue.getOffset().getName(), operator * deleteValue.getSize());\n     if (!isExpired(originalPutValue.getExpiresAtMs(), operationTimeInMs)) {\n-      handleContainerBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n-      handleLogSegmentBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n+      handleLogSegmentDeletedBucketUpdate(results, originalPutValue, operationTimeInMs, operator * SUBTRACT);\n       if (originalPutValue.getExpiresAtMs() != Utils.Infinite_Time) {\n         // make appropriate updates to avoid double counting\n-        handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n-        handleLogSegmentBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n+        handleLogSegmentExpiredBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(),\n+            operator * ADD);\n+      } else if (deleteValue.isTtlUpdate()) {\n+        // This blob has a PUT and TTL_UPDATE, because of DELETE, now TTL_UPDATE is not valid any more.\n+        try {\n+          IndexValue ttlUpdateValue =\n+              index.findKey(key, new FileSpan(originalPutValue.getOffset(), deleteValue.getOffset()),\n+                  EnumSet.of(PersistentIndex.IndexEntryType.TTL_UPDATE));\n+          if (ttlUpdateValue != null) {\n+            handleLogSegmentDeletedBucketUpdate(results, ttlUpdateValue, operationTimeInMs, operator * SUBTRACT);\n+          }\n+        } catch (StoreException e) {\n+          logger.error(\n+              \"Failed to find TTL_UPDATE IndexValue for \" + key + \" when processing new DELETE: \" + deleteValue, e);\n+        }\n       }\n     }\n   }\n \n   /**\n    * Helper function to process new UNDELETE entries and make appropriate updates to the given {@link ScanResults}.\n    * @param results the {@link ScanResults} to apply the updates to\n+   * @param key the {@link StoreKey} of the new UNDELETE\n    * @param undeleteValue the {@link IndexValue} of the new UNDELETE\n    * @param originalPutValue the {@link IndexValue} of the original PUT\n+   * @param previousValue the {@link IndexValue} comes prior to the new UNDELETE\n    */\n-  private void processNewUndelete(ScanResults results, IndexValue undeleteValue, IndexValue originalPutValue) {\n-    // TODO: future work to support online updates\n+  private void processNewUndelete(ScanResults results, StoreKey key, IndexValue undeleteValue,\n+      IndexValue originalPutValue, IndexValue previousValue) {\n+    // First deal with container bucket\n+    if (!previousValue.isDelete()) {\n+      // This previous value is not DELETE, current UNDELETE would not change container bucket.\n+      // There are several possibility when the previous value is not DELETE.\n+      // 1. PUT UNDELETE: UNDELETE doesn't change any bucket\n+      // 2. PUT TTL_UPDATE UNDELETE: UNDELETE doesn't change any bucket\n+      // 3. PUT DELETE UNDELETE UNDELETE: the last UNDELETE doesn't change any bucket\n+      // UNDELETE would resurrect a deleted IndexValue, if the previous value is not DELETE, then the IndexValue doesn't\n+      // need to be resurrected.\n+    } else {\n+      handleContainerBucketUpdate(results, originalPutValue, undeleteValue.getOperationTimeInMs(), ADD);\n+      handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), SUBTRACT);\n+    }\n+\n+    // Now deal with log segment bucket\n+    // Current UNDELETE is always valid\n+    results.updateLogSegmentBaseBucket(undeleteValue.getOffset().getName(), undeleteValue.getSize());\n+    if (previousValue.isDelete()) {\n+      // Previous IndexValue is a DELETE, it should already\n+      // 1. Added it's size to the first bucket in the scan result\n+      // 2. Subtracted original PUT's size from the previousValue.getOperationTimeInMs() bucket.\n+      // We need to recover from these two operations, so\n+      // 1. Subtract previous DELETE's size from first bucket in the scan result.\n+      // 2. Add original PUT's size to the previousValue.getOperationTimeInMs() bucket.\n+      processDeleteUpdateLogSegmentHelper(results, key, previousValue, originalPutValue, SUBTRACT);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756785e07ee435de886d8b444b9740f686e282e9"}, "originalPosition": 524}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwNDg3Mg==", "bodyText": "if the current IndexValue is UNDELETE and the previous IndexValue is DELETE, then we will first revert the change made by DELETE, so for UNDELETE, at Expired Bucket, it will do -P size, not, +P size.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488304872", "createdAt": "2020-09-15T00:07:35Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -706,46 +766,132 @@ private void processNewPut(ScanResults results, IndexValue putValue) {\n         handleContainerBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n-    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMs)) {\n+    if (!isExpired(expiresAtMs, results.logSegmentForecastStartTimeMsForExpired)) {\n       results.updateLogSegmentBaseBucket(putValue.getOffset().getName(), putValue.getSize());\n       if (expiresAtMs != Utils.Infinite_Time) {\n-        handleLogSegmentBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n+        handleLogSegmentExpiredBucketUpdate(results, putValue, expiresAtMs, SUBTRACT);\n       }\n     }\n   }\n \n   /**\n    * Helper function to process new DELETE entries and make appropriate updates to the given {@link ScanResults}.\n    * @param results the {@link ScanResults} to apply the updates to\n+   * @param key the {@link StoreKey} of the new DELETE\n    * @param deleteValue the {@link IndexValue} of the new DELETE\n    * @param originalPutValue the {@link IndexValue} of the original PUT that is getting deleted\n+   * @param previousValue the {@link IndexValue} comes prior to the new DELETE\n    */\n-  private void processNewDelete(ScanResults results, IndexValue deleteValue, IndexValue originalPutValue) {\n+  private void processNewDelete(ScanResults results, StoreKey key, IndexValue deleteValue, IndexValue originalPutValue,\n+      IndexValue previousValue) {\n+    long operationTimeInMs = deleteValue.getOperationTimeInMs();\n+    if (operationTimeInMs == Utils.Infinite_Time) {\n+      operationTimeInMs =\n+          index.getIndexSegments().floorEntry(deleteValue.getOffset()).getValue().getLastModifiedTimeMs();\n+    }\n+    // First deal with container bucket\n+    if (previousValue.isDelete()) {\n+      // The previous value is already DELETE, current DELETE would not change container bucket.\n+      // Assume the record history looks like this: PUT, DELETE, DELETE.\n+      // First DELETE already updated container bucket.\n+    } else {\n+      // If original PUT expires before this DELETE, then it will not change container bucket.\n+      if (!isExpired(originalPutValue.getExpiresAtMs(), operationTimeInMs)) {\n+        handleContainerBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n+        handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n+      }\n+    }\n+    // Now deal with log segment bucket\n+    // Current DELETE is always valid\n+    if (previousValue.isDelete()) {\n+      // Previous IndexValue is a DELETE, it should already\n+      // 1. Added it's size to the first bucket in the scan result\n+      // 2. Subtracted original PUT's size from the previousValue.getOperationTimeInMs() bucket.\n+      // We need to recover from these two operations, so\n+      // 1. Subtract previous DELETE's size from bucket in the scan result.\n+      // 2. Add original PUT's size to the previousValue.getOperationTimeInMs() bucket.\n+      processDeleteUpdateLogSegmentHelper(results, key, previousValue, originalPutValue, SUBTRACT);\n+    } else if (previousValue.isUndelete()) {\n+      // Previous IndexValue is an UNDELETE, it should already\n+      // 1. Added it's size to the first bucket of the scan result\n+      // we need to recover from this operation\n+      results.updateLogSegmentBaseBucket(previousValue.getOffset().getName(), SUBTRACT * previousValue.getSize());\n+    }\n+    processDeleteUpdateLogSegmentHelper(results, key, deleteValue, originalPutValue, ADD);\n+  }\n+\n+  private void processDeleteUpdateLogSegmentHelper(ScanResults results, StoreKey key, IndexValue deleteValue,\n+      IndexValue originalPutValue, int operator) {\n     long operationTimeInMs = deleteValue.getOperationTimeInMs();\n     if (operationTimeInMs == Utils.Infinite_Time) {\n       operationTimeInMs =\n           index.getIndexSegments().floorEntry(deleteValue.getOffset()).getValue().getLastModifiedTimeMs();\n     }\n-    results.updateLogSegmentBaseBucket(deleteValue.getOffset().getName(), deleteValue.getSize());\n+    results.updateLogSegmentBaseBucket(deleteValue.getOffset().getName(), operator * deleteValue.getSize());\n     if (!isExpired(originalPutValue.getExpiresAtMs(), operationTimeInMs)) {\n-      handleContainerBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n-      handleLogSegmentBucketUpdate(results, originalPutValue, operationTimeInMs, SUBTRACT);\n+      handleLogSegmentDeletedBucketUpdate(results, originalPutValue, operationTimeInMs, operator * SUBTRACT);\n       if (originalPutValue.getExpiresAtMs() != Utils.Infinite_Time) {\n         // make appropriate updates to avoid double counting\n-        handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n-        handleLogSegmentBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), ADD);\n+        handleLogSegmentExpiredBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(),\n+            operator * ADD);\n+      } else if (deleteValue.isTtlUpdate()) {\n+        // This blob has a PUT and TTL_UPDATE, because of DELETE, now TTL_UPDATE is not valid any more.\n+        try {\n+          IndexValue ttlUpdateValue =\n+              index.findKey(key, new FileSpan(originalPutValue.getOffset(), deleteValue.getOffset()),\n+                  EnumSet.of(PersistentIndex.IndexEntryType.TTL_UPDATE));\n+          if (ttlUpdateValue != null) {\n+            handleLogSegmentDeletedBucketUpdate(results, ttlUpdateValue, operationTimeInMs, operator * SUBTRACT);\n+          }\n+        } catch (StoreException e) {\n+          logger.error(\n+              \"Failed to find TTL_UPDATE IndexValue for \" + key + \" when processing new DELETE: \" + deleteValue, e);\n+        }\n       }\n     }\n   }\n \n   /**\n    * Helper function to process new UNDELETE entries and make appropriate updates to the given {@link ScanResults}.\n    * @param results the {@link ScanResults} to apply the updates to\n+   * @param key the {@link StoreKey} of the new UNDELETE\n    * @param undeleteValue the {@link IndexValue} of the new UNDELETE\n    * @param originalPutValue the {@link IndexValue} of the original PUT\n+   * @param previousValue the {@link IndexValue} comes prior to the new UNDELETE\n    */\n-  private void processNewUndelete(ScanResults results, IndexValue undeleteValue, IndexValue originalPutValue) {\n-    // TODO: future work to support online updates\n+  private void processNewUndelete(ScanResults results, StoreKey key, IndexValue undeleteValue,\n+      IndexValue originalPutValue, IndexValue previousValue) {\n+    // First deal with container bucket\n+    if (!previousValue.isDelete()) {\n+      // This previous value is not DELETE, current UNDELETE would not change container bucket.\n+      // There are several possibility when the previous value is not DELETE.\n+      // 1. PUT UNDELETE: UNDELETE doesn't change any bucket\n+      // 2. PUT TTL_UPDATE UNDELETE: UNDELETE doesn't change any bucket\n+      // 3. PUT DELETE UNDELETE UNDELETE: the last UNDELETE doesn't change any bucket\n+      // UNDELETE would resurrect a deleted IndexValue, if the previous value is not DELETE, then the IndexValue doesn't\n+      // need to be resurrected.\n+    } else {\n+      handleContainerBucketUpdate(results, originalPutValue, undeleteValue.getOperationTimeInMs(), ADD);\n+      handleContainerBucketUpdate(results, originalPutValue, originalPutValue.getExpiresAtMs(), SUBTRACT);\n+    }\n+\n+    // Now deal with log segment bucket\n+    // Current UNDELETE is always valid\n+    results.updateLogSegmentBaseBucket(undeleteValue.getOffset().getName(), undeleteValue.getSize());\n+    if (previousValue.isDelete()) {\n+      // Previous IndexValue is a DELETE, it should already\n+      // 1. Added it's size to the first bucket in the scan result\n+      // 2. Subtracted original PUT's size from the previousValue.getOperationTimeInMs() bucket.\n+      // We need to recover from these two operations, so\n+      // 1. Subtract previous DELETE's size from first bucket in the scan result.\n+      // 2. Add original PUT's size to the previousValue.getOperationTimeInMs() bucket.\n+      processDeleteUpdateLogSegmentHelper(results, key, previousValue, originalPutValue, SUBTRACT);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMwMDcxMA=="}, "originalCommit": {"oid": "756785e07ee435de886d8b444b9740f686e282e9"}, "originalPosition": 524}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1NTYxMjAwOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwMzozMjoyNFrOHRvPWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwNDozMDo0NFrOHRwIag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM2MTgxNg==", "bodyText": "I understand the piece of code is to check whether TtlUpdate is valid or not. If PUT and TtlUpdate are in same log segment and final state is delete, then TtlUpdate is not valid.\nI wonder if we can search the log segment of TtlUpdate only instead of findKey from the very beginning of whole PersistentIndex.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488361816", "createdAt": "2020-09-15T03:32:24Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -799,17 +963,33 @@ private void processEntryForLogSegmentBucket(ScanResults results, IndexEntry ind\n     IndexValue indexValue = indexEntry.getValue();\n     results.updateLogSegmentBaseBucket(indexValue.getOffset().getName(), indexValue.getSize());\n     if (indexValue.isPut()) {\n-      long expOrDelTimeInMs = indexValue.getExpiresAtMs();\n+      long expiresAtMs = indexValue.getExpiresAtMs();\n+      long deleteAtMs = Utils.Infinite_Time;\n       IndexFinalState finalState = keyFinalStates.get(indexEntry.getKey());\n       if (finalState != null && finalState.isDelete()) {\n-        long deleteTimeInMs = finalState.getOperationTime();\n-        expOrDelTimeInMs =\n-            expOrDelTimeInMs != Utils.Infinite_Time && expOrDelTimeInMs < deleteTimeInMs ? expOrDelTimeInMs\n-                : deleteTimeInMs;\n+        deleteAtMs = finalState.getOperationTime();\n+      }\n+      if (expiresAtMs == Utils.Infinite_Time && deleteAtMs != Utils.Infinite_Time) {\n+        handleLogSegmentDeletedBucketUpdate(results, indexValue, deleteAtMs, SUBTRACT);\n+      } else if (expiresAtMs != Utils.Infinite_Time && deleteAtMs == Utils.Infinite_Time) {\n+        handleLogSegmentExpiredBucketUpdate(results, indexValue, expiresAtMs, SUBTRACT);\n+      } else if (expiresAtMs != Utils.Infinite_Time && deleteAtMs != Utils.Infinite_Time) {\n+        if (expiresAtMs < deleteAtMs) {\n+          handleLogSegmentExpiredBucketUpdate(results, indexValue, expiresAtMs, SUBTRACT);\n+        } else {\n+          handleLogSegmentDeletedBucketUpdate(results, indexValue, deleteAtMs, SUBTRACT);\n+        }\n       }\n-      if (!indexValue.isTtlUpdate() || !isTtlUpdateEntryValid(indexEntry.getKey(), indexValue,\n-          expOrDelTimeInMs == Utils.Infinite_Time ? expOrDelTimeInMs : expOrDelTimeInMs + 1, keyFinalStates)) {\n-        handleLogSegmentBucketUpdate(results, indexValue, expOrDelTimeInMs, SUBTRACT);\n+    } else if (!indexValue.isDelete() && !indexValue.isUndelete() && indexValue.isTtlUpdate()) {\n+      IndexFinalState finalState = keyFinalStates.get(indexEntry.getKey());\n+      if (finalState != null && finalState.isDelete()) {\n+        IndexValue putValue =\n+            index.findKey(indexEntry.getKey(), new FileSpan(index.getStartOffset(), indexValue.getOffset()),\n+                EnumSet.of(PersistentIndex.IndexEntryType.PUT));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98708ddec2f4f90e11af249b587a7ca2f43102c9"}, "originalPosition": 595}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM3NjQyNg==", "bodyText": "that's a good idea, updated.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488376426", "createdAt": "2020-09-15T04:30:44Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -799,17 +963,33 @@ private void processEntryForLogSegmentBucket(ScanResults results, IndexEntry ind\n     IndexValue indexValue = indexEntry.getValue();\n     results.updateLogSegmentBaseBucket(indexValue.getOffset().getName(), indexValue.getSize());\n     if (indexValue.isPut()) {\n-      long expOrDelTimeInMs = indexValue.getExpiresAtMs();\n+      long expiresAtMs = indexValue.getExpiresAtMs();\n+      long deleteAtMs = Utils.Infinite_Time;\n       IndexFinalState finalState = keyFinalStates.get(indexEntry.getKey());\n       if (finalState != null && finalState.isDelete()) {\n-        long deleteTimeInMs = finalState.getOperationTime();\n-        expOrDelTimeInMs =\n-            expOrDelTimeInMs != Utils.Infinite_Time && expOrDelTimeInMs < deleteTimeInMs ? expOrDelTimeInMs\n-                : deleteTimeInMs;\n+        deleteAtMs = finalState.getOperationTime();\n+      }\n+      if (expiresAtMs == Utils.Infinite_Time && deleteAtMs != Utils.Infinite_Time) {\n+        handleLogSegmentDeletedBucketUpdate(results, indexValue, deleteAtMs, SUBTRACT);\n+      } else if (expiresAtMs != Utils.Infinite_Time && deleteAtMs == Utils.Infinite_Time) {\n+        handleLogSegmentExpiredBucketUpdate(results, indexValue, expiresAtMs, SUBTRACT);\n+      } else if (expiresAtMs != Utils.Infinite_Time && deleteAtMs != Utils.Infinite_Time) {\n+        if (expiresAtMs < deleteAtMs) {\n+          handleLogSegmentExpiredBucketUpdate(results, indexValue, expiresAtMs, SUBTRACT);\n+        } else {\n+          handleLogSegmentDeletedBucketUpdate(results, indexValue, deleteAtMs, SUBTRACT);\n+        }\n       }\n-      if (!indexValue.isTtlUpdate() || !isTtlUpdateEntryValid(indexEntry.getKey(), indexValue,\n-          expOrDelTimeInMs == Utils.Infinite_Time ? expOrDelTimeInMs : expOrDelTimeInMs + 1, keyFinalStates)) {\n-        handleLogSegmentBucketUpdate(results, indexValue, expOrDelTimeInMs, SUBTRACT);\n+    } else if (!indexValue.isDelete() && !indexValue.isUndelete() && indexValue.isTtlUpdate()) {\n+      IndexFinalState finalState = keyFinalStates.get(indexEntry.getKey());\n+      if (finalState != null && finalState.isDelete()) {\n+        IndexValue putValue =\n+            index.findKey(indexEntry.getKey(), new FileSpan(index.getStartOffset(), indexValue.getOffset()),\n+                EnumSet.of(PersistentIndex.IndexEntryType.PUT));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM2MTgxNg=="}, "originalCommit": {"oid": "98708ddec2f4f90e11af249b587a7ca2f43102c9"}, "originalPosition": 595}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1NTYxNDAxOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwMzozMzo0MlrOHRvQeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwNDozMTozMFrOHRwJJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM2MjEwNg==", "bodyText": "Since the indexValue is not from findKey() method, then I believe this can be simplified to indexValue.isTtlUpdate().", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488362106", "createdAt": "2020-09-15T03:33:42Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -799,17 +963,33 @@ private void processEntryForLogSegmentBucket(ScanResults results, IndexEntry ind\n     IndexValue indexValue = indexEntry.getValue();\n     results.updateLogSegmentBaseBucket(indexValue.getOffset().getName(), indexValue.getSize());\n     if (indexValue.isPut()) {\n-      long expOrDelTimeInMs = indexValue.getExpiresAtMs();\n+      long expiresAtMs = indexValue.getExpiresAtMs();\n+      long deleteAtMs = Utils.Infinite_Time;\n       IndexFinalState finalState = keyFinalStates.get(indexEntry.getKey());\n       if (finalState != null && finalState.isDelete()) {\n-        long deleteTimeInMs = finalState.getOperationTime();\n-        expOrDelTimeInMs =\n-            expOrDelTimeInMs != Utils.Infinite_Time && expOrDelTimeInMs < deleteTimeInMs ? expOrDelTimeInMs\n-                : deleteTimeInMs;\n+        deleteAtMs = finalState.getOperationTime();\n+      }\n+      if (expiresAtMs == Utils.Infinite_Time && deleteAtMs != Utils.Infinite_Time) {\n+        handleLogSegmentDeletedBucketUpdate(results, indexValue, deleteAtMs, SUBTRACT);\n+      } else if (expiresAtMs != Utils.Infinite_Time && deleteAtMs == Utils.Infinite_Time) {\n+        handleLogSegmentExpiredBucketUpdate(results, indexValue, expiresAtMs, SUBTRACT);\n+      } else if (expiresAtMs != Utils.Infinite_Time && deleteAtMs != Utils.Infinite_Time) {\n+        if (expiresAtMs < deleteAtMs) {\n+          handleLogSegmentExpiredBucketUpdate(results, indexValue, expiresAtMs, SUBTRACT);\n+        } else {\n+          handleLogSegmentDeletedBucketUpdate(results, indexValue, deleteAtMs, SUBTRACT);\n+        }\n       }\n-      if (!indexValue.isTtlUpdate() || !isTtlUpdateEntryValid(indexEntry.getKey(), indexValue,\n-          expOrDelTimeInMs == Utils.Infinite_Time ? expOrDelTimeInMs : expOrDelTimeInMs + 1, keyFinalStates)) {\n-        handleLogSegmentBucketUpdate(results, indexValue, expOrDelTimeInMs, SUBTRACT);\n+    } else if (!indexValue.isDelete() && !indexValue.isUndelete() && indexValue.isTtlUpdate()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98708ddec2f4f90e11af249b587a7ca2f43102c9"}, "originalPosition": 590}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM3NjYxMg==", "bodyText": "A DELETE or UNDELETE IndexValue can still return true when calling isTtlUpdate(), so there we have to do this kinda check.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488376612", "createdAt": "2020-09-15T04:31:30Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -799,17 +963,33 @@ private void processEntryForLogSegmentBucket(ScanResults results, IndexEntry ind\n     IndexValue indexValue = indexEntry.getValue();\n     results.updateLogSegmentBaseBucket(indexValue.getOffset().getName(), indexValue.getSize());\n     if (indexValue.isPut()) {\n-      long expOrDelTimeInMs = indexValue.getExpiresAtMs();\n+      long expiresAtMs = indexValue.getExpiresAtMs();\n+      long deleteAtMs = Utils.Infinite_Time;\n       IndexFinalState finalState = keyFinalStates.get(indexEntry.getKey());\n       if (finalState != null && finalState.isDelete()) {\n-        long deleteTimeInMs = finalState.getOperationTime();\n-        expOrDelTimeInMs =\n-            expOrDelTimeInMs != Utils.Infinite_Time && expOrDelTimeInMs < deleteTimeInMs ? expOrDelTimeInMs\n-                : deleteTimeInMs;\n+        deleteAtMs = finalState.getOperationTime();\n+      }\n+      if (expiresAtMs == Utils.Infinite_Time && deleteAtMs != Utils.Infinite_Time) {\n+        handleLogSegmentDeletedBucketUpdate(results, indexValue, deleteAtMs, SUBTRACT);\n+      } else if (expiresAtMs != Utils.Infinite_Time && deleteAtMs == Utils.Infinite_Time) {\n+        handleLogSegmentExpiredBucketUpdate(results, indexValue, expiresAtMs, SUBTRACT);\n+      } else if (expiresAtMs != Utils.Infinite_Time && deleteAtMs != Utils.Infinite_Time) {\n+        if (expiresAtMs < deleteAtMs) {\n+          handleLogSegmentExpiredBucketUpdate(results, indexValue, expiresAtMs, SUBTRACT);\n+        } else {\n+          handleLogSegmentDeletedBucketUpdate(results, indexValue, deleteAtMs, SUBTRACT);\n+        }\n       }\n-      if (!indexValue.isTtlUpdate() || !isTtlUpdateEntryValid(indexEntry.getKey(), indexValue,\n-          expOrDelTimeInMs == Utils.Infinite_Time ? expOrDelTimeInMs : expOrDelTimeInMs + 1, keyFinalStates)) {\n-        handleLogSegmentBucketUpdate(results, indexValue, expOrDelTimeInMs, SUBTRACT);\n+    } else if (!indexValue.isDelete() && !indexValue.isUndelete() && indexValue.isTtlUpdate()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM2MjEwNg=="}, "originalCommit": {"oid": "98708ddec2f4f90e11af249b587a7ca2f43102c9"}, "originalPosition": 590}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1NTY0NDY2OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwMzo1MTo1M1rOHRviBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwNDozMzoyMFrOHRwK_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM2NjU5Ng==", "bodyText": "I wonder if it's worthwhile to add an additional check in this \"for\" statement. That is, checking if there is IndexScanner is ongoing.\nfor (int i = 0; i < entryCount && !cancelled &&  !isScanning; i++)\n\nIt's possible QueueProcessor starts before IndexScanner. When IndexScanner may begin while QueueProcessor is still ongoing.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488366596", "createdAt": "2020-09-15T03:51:53Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -850,26 +1030,29 @@ public void run() {\n         }\n         long processStartTimeMs = time.milliseconds();\n         for (int i = 0; i < entryCount && !cancelled; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98708ddec2f4f90e11af249b587a7ca2f43102c9"}, "originalPosition": 606}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM3NzA4NA==", "bodyText": "that's right, will put a check just in case.", "url": "https://github.com/linkedin/ambry/pull/1613#discussion_r488377084", "createdAt": "2020-09-15T04:33:20Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreStats.java", "diffHunk": "@@ -850,26 +1030,29 @@ public void run() {\n         }\n         long processStartTimeMs = time.milliseconds();\n         for (int i = 0; i < entryCount && !cancelled; i++) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM2NjU5Ng=="}, "originalCommit": {"oid": "98708ddec2f4f90e11af249b587a7ca2f43102c9"}, "originalPosition": 606}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1359, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}