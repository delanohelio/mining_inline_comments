{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMxMzYxMTMy", "number": 1555, "title": "Introduce PropertyStoreToDataNodeConfigAdapter", "bodyText": "Introduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\nTo prepare for future participant changes this also adds set and get\nmethods to DataNodeConfigSource for reading and updating\nDataNodeConfigs.", "createdAt": "2020-06-08T19:51:00Z", "url": "https://github.com/linkedin/ambry/pull/1555", "merged": true, "mergeCommit": {"oid": "486d708d8e860096fd120431e504e3b716ec5f5d"}, "closed": true, "closedAt": "2020-07-13T16:17:12Z", "author": {"login": "cgtz"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABct3U5ugBqjM0NzAxNDc4NTY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc0jRwcgH2gAyNDMxMzYxMTMyOjAzZjcxYmY0NmU0YTQ2M2NiMjcyMWVlMjdkYzQxMWFkYzFlY2ZlOWI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2005c2965415eccd58775684820b4fc618613dce", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/2005c2965415eccd58775684820b4fc618613dce", "committedDate": "2020-06-08T19:46:50Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners."}, "afterCommit": {"oid": "a5200b08fc9562d7816f339fafe801d4d07fe263", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/a5200b08fc9562d7816f339fafe801d4d07fe263", "committedDate": "2020-06-22T20:59:46Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\n\nAlso include changes to use DataNodeConfigSource in HelixParticipant."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a5200b08fc9562d7816f339fafe801d4d07fe263", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/a5200b08fc9562d7816f339fafe801d4d07fe263", "committedDate": "2020-06-22T20:59:46Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\n\nAlso include changes to use DataNodeConfigSource in HelixParticipant."}, "afterCommit": {"oid": "3aa04b1099e3a02e2ba47b9969b78f27f3de1576", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/3aa04b1099e3a02e2ba47b9969b78f27f3de1576", "committedDate": "2020-06-23T01:00:52Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter, Use DataNodeConfigSource in HelixParticipant\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\n\nAlso include changes to use DataNodeConfigSource in HelixParticipant."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3aa04b1099e3a02e2ba47b9969b78f27f3de1576", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/3aa04b1099e3a02e2ba47b9969b78f27f3de1576", "committedDate": "2020-06-23T01:00:52Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter, Use DataNodeConfigSource in HelixParticipant\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\n\nAlso include changes to use DataNodeConfigSource in HelixParticipant."}, "afterCommit": {"oid": "f2f28015f4ac956041dbf96a4c3586e6cb8adbe0", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/f2f28015f4ac956041dbf96a4c3586e6cb8adbe0", "committedDate": "2020-06-23T01:43:08Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter, Use DataNodeConfigSource in HelixParticipant\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\n\nAlso include changes to use DataNodeConfigSource in HelixParticipant."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f2f28015f4ac956041dbf96a4c3586e6cb8adbe0", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/f2f28015f4ac956041dbf96a4c3586e6cb8adbe0", "committedDate": "2020-06-23T01:43:08Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter, Use DataNodeConfigSource in HelixParticipant\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\n\nAlso include changes to use DataNodeConfigSource in HelixParticipant."}, "afterCommit": {"oid": "9dddd29c9c1d77dcdb40814f290aa0aad25523f5", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/9dddd29c9c1d77dcdb40814f290aa0aad25523f5", "committedDate": "2020-06-23T18:47:16Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\n\nTo prepare for future participant changes this also adds set and get\nmethods to DataNodeConfigSource for reading and updating\nDataNodeConfigs."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MzM3MzMy", "url": "https://github.com/linkedin/ambry/pull/1555#pullrequestreview-436337332", "createdAt": "2020-06-24T05:39:18Z", "commit": {"oid": "9dddd29c9c1d77dcdb40814f290aa0aad25523f5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNTozOToxOVrOGoDnmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNTozOToxOVrOGoDnmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1NTUxNQ==", "bodyText": "Do we need to check if replicasStr is null in this situation?", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r444655515", "createdAt": "2020-06-24T05:39:19Z", "author": {"login": "SophieGuo410"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "diffHunk": "@@ -33,82 +40,146 @@\n public class InstanceConfigToDataNodeConfigAdapter implements DataNodeConfigSource {\n   private static final Logger LOGGER = LoggerFactory.getLogger(InstanceConfigToDataNodeConfigAdapter.class);\n   private final HelixManager helixManager;\n-  private final ClusterMapConfig clusterMapConfig;\n+  private final Converter converter;\n+  private final String clusterName;\n+  private volatile HelixAdmin helixAdmin = null;\n \n   /**\n    * @param helixManager the {@link HelixManager} to use as the source of truth for {@link InstanceConfig}s.\n    * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n    */\n   public InstanceConfigToDataNodeConfigAdapter(HelixManager helixManager, ClusterMapConfig clusterMapConfig) {\n     this.helixManager = helixManager;\n-    this.clusterMapConfig = clusterMapConfig;\n+    this.converter = new Converter(clusterMapConfig);\n+    clusterName = clusterMapConfig.clusterMapClusterName;\n   }\n \n   @Override\n   public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n     helixManager.addInstanceConfigChangeListener((InstanceConfigChangeListener) (instanceConfigs, context) -> {\n       Iterable<DataNodeConfig> dataNodeConfigs =\n-          () -> instanceConfigs.stream().map(this::convert).filter(Objects::nonNull).iterator();\n+          () -> instanceConfigs.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n       listener.onDataNodeConfigChange(dataNodeConfigs);\n     });\n   }\n \n-  /**\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  private DataNodeConfig convert(InstanceConfig instanceConfig) {\n-    return convert(instanceConfig, clusterMapConfig);\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    InstanceConfig instanceConfig = converter.convert(config);\n+    return getHelixAdmin().setInstanceConfig(clusterName, instanceConfig.getInstanceName(), instanceConfig);\n   }\n \n-  /**\n-   * Exposed for testing.\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @param clusterMapConfig the {@link ClusterMapConfig} containing any default values that may be needed.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  static DataNodeConfig convert(InstanceConfig instanceConfig, ClusterMapConfig clusterMapConfig) {\n-    int schemaVersion = getSchemaVersion(instanceConfig);\n-    if (schemaVersion != 0) {\n-      LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n-      return null;\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    InstanceConfig instanceConfig = getHelixAdmin().getInstanceConfig(clusterName, instanceName);\n+    return instanceConfig != null ? converter.convert(instanceConfig) : null;\n+  }\n+\n+  void setHelixAdmin(HelixAdmin helixAdmin) {\n+    this.helixAdmin = helixAdmin;\n+  }\n+\n+  private HelixAdmin getHelixAdmin() {\n+    return Objects.requireNonNull(helixAdmin, \"helixAdmin not set\");\n+  }\n+\n+  static class Converter {\n+    private final ClusterMapConfig clusterMapConfig;\n+\n+    Converter(ClusterMapConfig clusterMapConfig) {\n+      this.clusterMapConfig = clusterMapConfig;\n     }\n-    DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n-        Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n-        getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n-    dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n-    dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n-    // TODO uncomment this line once 1534 is merged\n-    // dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n-    instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n-      if (diskProps.get(DISK_STATE) == null) {\n-        // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n-        // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n-        // state to InstanceConfig.\n-        LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n-            instanceConfig.getInstanceName());\n-      } else {\n-        DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n-            diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n-            Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n-        String replicasStr = diskProps.get(REPLICAS_STR);\n-        if (!replicasStr.isEmpty()) {\n-          for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n-            String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n-            // partition name and replica name are the same.\n-            String partitionName = replicaStrParts[0];\n-            long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n-            String partitionClass =\n-                replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n-            disk.getReplicaConfigs()\n-                .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+\n+    /**\n+     * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported\n+     *         schema version.\n+     */\n+    DataNodeConfig convert(InstanceConfig instanceConfig) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n+        return null;\n+      }\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n+          Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n+          getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n+      instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n+        if (diskProps.get(DISK_STATE) == null) {\n+          // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n+          // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n+          // state to InstanceConfig.\n+          LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n+              instanceConfig.getInstanceName());\n+        } else {\n+          DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n+              diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n+              Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n+          String replicasStr = diskProps.get(REPLICAS_STR);\n+          if (!replicasStr.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9dddd29c9c1d77dcdb40814f290aa0aad25523f5"}, "originalPosition": 149}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9dddd29c9c1d77dcdb40814f290aa0aad25523f5", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/9dddd29c9c1d77dcdb40814f290aa0aad25523f5", "committedDate": "2020-06-23T18:47:16Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\n\nTo prepare for future participant changes this also adds set and get\nmethods to DataNodeConfigSource for reading and updating\nDataNodeConfigs."}, "afterCommit": {"oid": "ea88b2938ca7a34397d26da2e44ae020b0968f2c", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/ea88b2938ca7a34397d26da2e44ae020b0968f2c", "committedDate": "2020-06-24T22:04:04Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\n\nTo prepare for future participant changes this also adds set and get\nmethods to DataNodeConfigSource for reading and updating\nDataNodeConfigs."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ea88b2938ca7a34397d26da2e44ae020b0968f2c", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/ea88b2938ca7a34397d26da2e44ae020b0968f2c", "committedDate": "2020-06-24T22:04:04Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\n\nTo prepare for future participant changes this also adds set and get\nmethods to DataNodeConfigSource for reading and updating\nDataNodeConfigs."}, "afterCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8", "committedDate": "2020-06-24T22:15:23Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\n\nTo prepare for future participant changes this also adds set and get\nmethods to DataNodeConfigSource for reading and updating\nDataNodeConfigs."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MDAzMzIy", "url": "https://github.com/linkedin/ambry/pull/1555#pullrequestreview-438003322", "createdAt": "2020-06-26T04:19:38Z", "commit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwNDoxOTozOFrOGpTLqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwNToyMDo0OVrOGpUBVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk1OTA4Mw==", "bodyText": "minor: can be removed", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r445959083", "createdAt": "2020-06-26T04:19:38Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/DataNodeConfig.java", "diffHunk": "@@ -15,9 +15,11 @@\n \n package com.github.ambry.clustermap;\n \n+import java.util.Comparator;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2OTAzMw==", "bodyText": "Looks like we will change the format of mount path in PropertyStore record.  Unlike InstanceConfig (which is a pure mount path), here we add a prefix mount- ahead of each mount path. I just wonder why not use the same way in InstanceConfigToDataNodeConfigAdapter to determine if this is a valid mount path.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r445969033", "createdAt": "2020-06-26T05:04:44Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *\n+ */\n+\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import com.github.ambry.utils.Utils;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.helix.AccessOption;\n+import org.apache.helix.store.HelixPropertyListener;\n+import org.apache.helix.store.HelixPropertyStore;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+import org.apache.zookeeper.data.Stat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * An implementation of {@link DataNodeConfigSource} that reads {@link ZNRecord}s stored under the\n+ * \"/DataNodeConfigs\" path in a {@link HelixPropertyStore}.\n+ */\n+public class PropertyStoreToDataNodeConfigAdapter implements DataNodeConfigSource {\n+  static final String CONFIG_PATH = \"/DataNodeConfigs\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(PropertyStoreToDataNodeConfigAdapter.class);\n+  private final HelixPropertyStore<ZNRecord> propertyStore;\n+  private final Converter converter;\n+  private final Executor eventExecutor;\n+\n+  /**\n+   * @param propertyStore the {@link HelixPropertyStore} instance to use to interact with zookeeper.\n+   * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n+   * @param dcName the datacenter name for for the property store.\n+   */\n+  public PropertyStoreToDataNodeConfigAdapter(HelixPropertyStore<ZNRecord> propertyStore,\n+      ClusterMapConfig clusterMapConfig, String dcName) {\n+    this.propertyStore = propertyStore;\n+    this.converter = new Converter(clusterMapConfig.clusterMapDefaultPartitionClass, dcName);\n+    this.eventExecutor = Executors.newSingleThreadExecutor();\n+  }\n+\n+  @Override\n+  public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) {\n+    Subscription subscription = new Subscription(listener);\n+    propertyStore.subscribe(CONFIG_PATH, subscription);\n+    subscription.start();\n+  }\n+\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    ZNRecord record = converter.convert(config);\n+    String path = CONFIG_PATH + \"/\" + record.getId();\n+    return propertyStore.set(path, record, AccessOption.PERSISTENT);\n+  }\n+\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    String path = CONFIG_PATH + \"/\" + instanceName;\n+    ZNRecord record = propertyStore.get(path, new Stat(), AccessOption.PERSISTENT);\n+    return record != null ? converter.convert(record) : null;\n+  }\n+\n+  private class Subscription implements HelixPropertyListener {\n+    private final DataNodeConfigChangeListener listener;\n+    private final ReentrantLock lock = new ReentrantLock();\n+    // reading/writing this variable should be protected by the lock.\n+    private boolean initialized = false;\n+\n+    Subscription(DataNodeConfigChangeListener listener) {\n+      this.listener = listener;\n+    }\n+\n+    /**\n+     * Tell the background executor to perform any initialization needed. This should be called after registering this\n+     * listener using {@link HelixPropertyStore#subscribe}. This will ensure that the first event sent to\n+     * {@link #listener} will contain the entire set of current configs.\n+     */\n+    void start() {\n+      eventExecutor.execute(this::initializeIfNeeded);\n+    }\n+\n+    @Override\n+    public void onDataChange(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} changed\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataCreate(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} created\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataDelete(String path) {\n+      // TODO handle node deletions dynamically. Doing so requires further work in the ClusterChangeHandler impl\n+      LOGGER.info(\"DataNodeConfig path {} deleted. This requires a restart to handle\", path);\n+    }\n+\n+    private void onPathChange(String path) {\n+      eventExecutor.execute(() -> updateOrInitialize(path));\n+    }\n+\n+    private Iterable<DataNodeConfig> lazyIterable(Collection<ZNRecord> records) {\n+      return () -> records.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n+    }\n+\n+    /**\n+     * Either perform a full initialization or notify the listener about an incremental update to a path. This should be\n+     * called from the event executor thread.\n+     * @param changedPath the path that changed.\n+     */\n+    private void updateOrInitialize(String changedPath) {\n+      lock.lock();\n+      try {\n+        if (!initializeIfNeeded()) {\n+          ZNRecord record = propertyStore.get(changedPath, null, AccessOption.PERSISTENT);\n+          if (record != null) {\n+            listener.onDataNodeConfigChange(lazyIterable(Collections.singleton(record)));\n+          } else {\n+            LOGGER.info(\"DataNodeConfig at path {} not found\", changedPath);\n+          }\n+        }\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig change\", e);\n+      } finally {\n+        lock.unlock();\n+      }\n+    }\n+\n+    /**\n+     * If initialization has not yet been completed succesfully, attempt an initialization. This should be called from\n+     * the event executor thread.\n+     * @return {@code true} if a successful full initialization (whether successful or not) was attempted in this call.\n+     */\n+    private boolean initializeIfNeeded() {\n+      lock.lock();\n+      try {\n+        if (initialized) {\n+          return false;\n+        }\n+        List<ZNRecord> records = propertyStore.getChildren(CONFIG_PATH, null, AccessOption.PERSISTENT);\n+        listener.onDataNodeConfigChange(lazyIterable(records));\n+        initialized = true;\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig initialization\", e);\n+      } finally {\n+        lock.unlock();\n+      }\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Convert between {@link DataNodeConfig}s and {@link ZNRecord}s.\n+   */\n+  static class Converter {\n+    private static final int VERSION_0 = 0;\n+    private static final String MOUNT_PREFIX = \"mount-\";\n+    private static final String HOSTNAME_FIELD = \"hostname\";\n+    private static final String PORT_FIELD = \"port\";\n+    private final String defaultPartitionClass;\n+    private final String dcName;\n+\n+    /**\n+     * @param defaultPartitionClass the default partition class for these nodes.\n+     * @param dcName the datacenter name for these nodes.\n+     */\n+    Converter(String defaultPartitionClass, String dcName) {\n+      this.defaultPartitionClass = defaultPartitionClass;\n+      this.dcName = dcName;\n+    }\n+\n+    /**\n+     * @param record the {@link ZNRecord} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link ZNRecord} provided has an unsupported schema\n+     *         version.\n+     */\n+    DataNodeConfig convert(ZNRecord record) {\n+      int schemaVersion = getSchemaVersion(record);\n+      if (schemaVersion != VERSION_0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, record);\n+        return null;\n+      }\n+\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(record.getId(), record.getSimpleField(HOSTNAME_FIELD),\n+          record.getIntField(PORT_FIELD, DataNodeId.UNKNOWN_PORT), dcName, getSslPortStr(record),\n+          getHttp2PortStr(record), getRackId(record), DEFAULT_XID);\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(record));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(record));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(record));\n+      record.getMapFields().forEach((key, diskProps) -> {\n+        if (key.startsWith(MOUNT_PREFIX)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2OTg2NA==", "bodyText": "typo: successfully", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r445969864", "createdAt": "2020-06-26T05:08:24Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *\n+ */\n+\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import com.github.ambry.utils.Utils;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.helix.AccessOption;\n+import org.apache.helix.store.HelixPropertyListener;\n+import org.apache.helix.store.HelixPropertyStore;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+import org.apache.zookeeper.data.Stat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * An implementation of {@link DataNodeConfigSource} that reads {@link ZNRecord}s stored under the\n+ * \"/DataNodeConfigs\" path in a {@link HelixPropertyStore}.\n+ */\n+public class PropertyStoreToDataNodeConfigAdapter implements DataNodeConfigSource {\n+  static final String CONFIG_PATH = \"/DataNodeConfigs\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(PropertyStoreToDataNodeConfigAdapter.class);\n+  private final HelixPropertyStore<ZNRecord> propertyStore;\n+  private final Converter converter;\n+  private final Executor eventExecutor;\n+\n+  /**\n+   * @param propertyStore the {@link HelixPropertyStore} instance to use to interact with zookeeper.\n+   * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n+   * @param dcName the datacenter name for for the property store.\n+   */\n+  public PropertyStoreToDataNodeConfigAdapter(HelixPropertyStore<ZNRecord> propertyStore,\n+      ClusterMapConfig clusterMapConfig, String dcName) {\n+    this.propertyStore = propertyStore;\n+    this.converter = new Converter(clusterMapConfig.clusterMapDefaultPartitionClass, dcName);\n+    this.eventExecutor = Executors.newSingleThreadExecutor();\n+  }\n+\n+  @Override\n+  public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) {\n+    Subscription subscription = new Subscription(listener);\n+    propertyStore.subscribe(CONFIG_PATH, subscription);\n+    subscription.start();\n+  }\n+\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    ZNRecord record = converter.convert(config);\n+    String path = CONFIG_PATH + \"/\" + record.getId();\n+    return propertyStore.set(path, record, AccessOption.PERSISTENT);\n+  }\n+\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    String path = CONFIG_PATH + \"/\" + instanceName;\n+    ZNRecord record = propertyStore.get(path, new Stat(), AccessOption.PERSISTENT);\n+    return record != null ? converter.convert(record) : null;\n+  }\n+\n+  private class Subscription implements HelixPropertyListener {\n+    private final DataNodeConfigChangeListener listener;\n+    private final ReentrantLock lock = new ReentrantLock();\n+    // reading/writing this variable should be protected by the lock.\n+    private boolean initialized = false;\n+\n+    Subscription(DataNodeConfigChangeListener listener) {\n+      this.listener = listener;\n+    }\n+\n+    /**\n+     * Tell the background executor to perform any initialization needed. This should be called after registering this\n+     * listener using {@link HelixPropertyStore#subscribe}. This will ensure that the first event sent to\n+     * {@link #listener} will contain the entire set of current configs.\n+     */\n+    void start() {\n+      eventExecutor.execute(this::initializeIfNeeded);\n+    }\n+\n+    @Override\n+    public void onDataChange(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} changed\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataCreate(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} created\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataDelete(String path) {\n+      // TODO handle node deletions dynamically. Doing so requires further work in the ClusterChangeHandler impl\n+      LOGGER.info(\"DataNodeConfig path {} deleted. This requires a restart to handle\", path);\n+    }\n+\n+    private void onPathChange(String path) {\n+      eventExecutor.execute(() -> updateOrInitialize(path));\n+    }\n+\n+    private Iterable<DataNodeConfig> lazyIterable(Collection<ZNRecord> records) {\n+      return () -> records.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n+    }\n+\n+    /**\n+     * Either perform a full initialization or notify the listener about an incremental update to a path. This should be\n+     * called from the event executor thread.\n+     * @param changedPath the path that changed.\n+     */\n+    private void updateOrInitialize(String changedPath) {\n+      lock.lock();\n+      try {\n+        if (!initializeIfNeeded()) {\n+          ZNRecord record = propertyStore.get(changedPath, null, AccessOption.PERSISTENT);\n+          if (record != null) {\n+            listener.onDataNodeConfigChange(lazyIterable(Collections.singleton(record)));\n+          } else {\n+            LOGGER.info(\"DataNodeConfig at path {} not found\", changedPath);\n+          }\n+        }\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig change\", e);\n+      } finally {\n+        lock.unlock();\n+      }\n+    }\n+\n+    /**\n+     * If initialization has not yet been completed succesfully, attempt an initialization. This should be called from", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk3MjgyMQ==", "bodyText": "Just curious, if exception occurs, how can we block the startup of current node?", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r445972821", "createdAt": "2020-06-26T05:20:49Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *\n+ */\n+\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import com.github.ambry.utils.Utils;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.helix.AccessOption;\n+import org.apache.helix.store.HelixPropertyListener;\n+import org.apache.helix.store.HelixPropertyStore;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+import org.apache.zookeeper.data.Stat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * An implementation of {@link DataNodeConfigSource} that reads {@link ZNRecord}s stored under the\n+ * \"/DataNodeConfigs\" path in a {@link HelixPropertyStore}.\n+ */\n+public class PropertyStoreToDataNodeConfigAdapter implements DataNodeConfigSource {\n+  static final String CONFIG_PATH = \"/DataNodeConfigs\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(PropertyStoreToDataNodeConfigAdapter.class);\n+  private final HelixPropertyStore<ZNRecord> propertyStore;\n+  private final Converter converter;\n+  private final Executor eventExecutor;\n+\n+  /**\n+   * @param propertyStore the {@link HelixPropertyStore} instance to use to interact with zookeeper.\n+   * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n+   * @param dcName the datacenter name for for the property store.\n+   */\n+  public PropertyStoreToDataNodeConfigAdapter(HelixPropertyStore<ZNRecord> propertyStore,\n+      ClusterMapConfig clusterMapConfig, String dcName) {\n+    this.propertyStore = propertyStore;\n+    this.converter = new Converter(clusterMapConfig.clusterMapDefaultPartitionClass, dcName);\n+    this.eventExecutor = Executors.newSingleThreadExecutor();\n+  }\n+\n+  @Override\n+  public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) {\n+    Subscription subscription = new Subscription(listener);\n+    propertyStore.subscribe(CONFIG_PATH, subscription);\n+    subscription.start();\n+  }\n+\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    ZNRecord record = converter.convert(config);\n+    String path = CONFIG_PATH + \"/\" + record.getId();\n+    return propertyStore.set(path, record, AccessOption.PERSISTENT);\n+  }\n+\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    String path = CONFIG_PATH + \"/\" + instanceName;\n+    ZNRecord record = propertyStore.get(path, new Stat(), AccessOption.PERSISTENT);\n+    return record != null ? converter.convert(record) : null;\n+  }\n+\n+  private class Subscription implements HelixPropertyListener {\n+    private final DataNodeConfigChangeListener listener;\n+    private final ReentrantLock lock = new ReentrantLock();\n+    // reading/writing this variable should be protected by the lock.\n+    private boolean initialized = false;\n+\n+    Subscription(DataNodeConfigChangeListener listener) {\n+      this.listener = listener;\n+    }\n+\n+    /**\n+     * Tell the background executor to perform any initialization needed. This should be called after registering this\n+     * listener using {@link HelixPropertyStore#subscribe}. This will ensure that the first event sent to\n+     * {@link #listener} will contain the entire set of current configs.\n+     */\n+    void start() {\n+      eventExecutor.execute(this::initializeIfNeeded);\n+    }\n+\n+    @Override\n+    public void onDataChange(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} changed\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataCreate(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} created\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataDelete(String path) {\n+      // TODO handle node deletions dynamically. Doing so requires further work in the ClusterChangeHandler impl\n+      LOGGER.info(\"DataNodeConfig path {} deleted. This requires a restart to handle\", path);\n+    }\n+\n+    private void onPathChange(String path) {\n+      eventExecutor.execute(() -> updateOrInitialize(path));\n+    }\n+\n+    private Iterable<DataNodeConfig> lazyIterable(Collection<ZNRecord> records) {\n+      return () -> records.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n+    }\n+\n+    /**\n+     * Either perform a full initialization or notify the listener about an incremental update to a path. This should be\n+     * called from the event executor thread.\n+     * @param changedPath the path that changed.\n+     */\n+    private void updateOrInitialize(String changedPath) {\n+      lock.lock();\n+      try {\n+        if (!initializeIfNeeded()) {\n+          ZNRecord record = propertyStore.get(changedPath, null, AccessOption.PERSISTENT);\n+          if (record != null) {\n+            listener.onDataNodeConfigChange(lazyIterable(Collections.singleton(record)));\n+          } else {\n+            LOGGER.info(\"DataNodeConfig at path {} not found\", changedPath);\n+          }\n+        }\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig change\", e);\n+      } finally {\n+        lock.unlock();\n+      }\n+    }\n+\n+    /**\n+     * If initialization has not yet been completed succesfully, attempt an initialization. This should be called from\n+     * the event executor thread.\n+     * @return {@code true} if a successful full initialization (whether successful or not) was attempted in this call.\n+     */\n+    private boolean initializeIfNeeded() {\n+      lock.lock();\n+      try {\n+        if (initialized) {\n+          return false;\n+        }\n+        List<ZNRecord> records = propertyStore.getChildren(CONFIG_PATH, null, AccessOption.PERSISTENT);\n+        listener.onDataNodeConfigChange(lazyIterable(records));\n+        initialized = true;\n+      } catch (Exception e) {\n+        LOGGER.error(\"Exception during DataNodeConfig initialization\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 168}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MDY0OTkx", "url": "https://github.com/linkedin/ambry/pull/1555#pullrequestreview-438064991", "createdAt": "2020-06-26T07:16:35Z", "commit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwNzoxNjozNVrOGpWQyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwNzoxNjozNVrOGpWQyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjAwOTU0NQ==", "bodyText": "A quick check, do we have any test cases cover this method?", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r446009545", "createdAt": "2020-06-26T07:16:35Z", "author": {"login": "SophieGuo410"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "diffHunk": "@@ -33,82 +38,146 @@\n public class InstanceConfigToDataNodeConfigAdapter implements DataNodeConfigSource {\n   private static final Logger LOGGER = LoggerFactory.getLogger(InstanceConfigToDataNodeConfigAdapter.class);\n   private final HelixManager helixManager;\n-  private final ClusterMapConfig clusterMapConfig;\n+  private final Converter converter;\n+  private final String clusterName;\n+  private volatile HelixAdmin helixAdmin = null;\n \n   /**\n    * @param helixManager the {@link HelixManager} to use as the source of truth for {@link InstanceConfig}s.\n    * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n    */\n   public InstanceConfigToDataNodeConfigAdapter(HelixManager helixManager, ClusterMapConfig clusterMapConfig) {\n     this.helixManager = helixManager;\n-    this.clusterMapConfig = clusterMapConfig;\n+    this.converter = new Converter(clusterMapConfig);\n+    clusterName = clusterMapConfig.clusterMapClusterName;\n   }\n \n   @Override\n   public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n     helixManager.addInstanceConfigChangeListener((InstanceConfigChangeListener) (instanceConfigs, context) -> {\n       Iterable<DataNodeConfig> dataNodeConfigs =\n-          () -> instanceConfigs.stream().map(this::convert).filter(Objects::nonNull).iterator();\n+          () -> instanceConfigs.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n       listener.onDataNodeConfigChange(dataNodeConfigs);\n     });\n   }\n \n-  /**\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  private DataNodeConfig convert(InstanceConfig instanceConfig) {\n-    return convert(instanceConfig, clusterMapConfig);\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    InstanceConfig instanceConfig = converter.convert(config);\n+    return getHelixAdmin().setInstanceConfig(clusterName, instanceConfig.getInstanceName(), instanceConfig);\n   }\n \n-  /**\n-   * Exposed for testing.\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @param clusterMapConfig the {@link ClusterMapConfig} containing any default values that may be needed.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  static DataNodeConfig convert(InstanceConfig instanceConfig, ClusterMapConfig clusterMapConfig) {\n-    int schemaVersion = getSchemaVersion(instanceConfig);\n-    if (schemaVersion != 0) {\n-      LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n-      return null;\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    InstanceConfig instanceConfig = getHelixAdmin().getInstanceConfig(clusterName, instanceName);\n+    return instanceConfig != null ? converter.convert(instanceConfig) : null;\n+  }\n+\n+  void setHelixAdmin(HelixAdmin helixAdmin) {\n+    this.helixAdmin = helixAdmin;\n+  }\n+\n+  private HelixAdmin getHelixAdmin() {\n+    return Objects.requireNonNull(helixAdmin, \"helixAdmin not set\");\n+  }\n+\n+  static class Converter {\n+    private final ClusterMapConfig clusterMapConfig;\n+\n+    Converter(ClusterMapConfig clusterMapConfig) {\n+      this.clusterMapConfig = clusterMapConfig;\n     }\n-    DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n-        Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n-        getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n-    dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n-    dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n-    // TODO uncomment this line once 1534 is merged\n-    // dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n-    instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n-      if (diskProps.get(DISK_STATE) == null) {\n-        // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n-        // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n-        // state to InstanceConfig.\n-        LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n-            instanceConfig.getInstanceName());\n-      } else {\n-        DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n-            diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n-            Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n-        String replicasStr = diskProps.get(REPLICAS_STR);\n-        if (!replicasStr.isEmpty()) {\n-          for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n-            String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n-            // partition name and replica name are the same.\n-            String partitionName = replicaStrParts[0];\n-            long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n-            String partitionClass =\n-                replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n-            disk.getReplicaConfigs()\n-                .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+\n+    /**\n+     * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported\n+     *         schema version.\n+     */\n+    DataNodeConfig convert(InstanceConfig instanceConfig) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n+        return null;\n+      }\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n+          Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n+          getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n+      instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n+        if (diskProps.get(DISK_STATE) == null) {\n+          // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n+          // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n+          // state to InstanceConfig.\n+          LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n+              instanceConfig.getInstanceName());\n+        } else {\n+          DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n+              diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n+              Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n+          String replicasStr = diskProps.get(REPLICAS_STR);\n+          if (!Utils.isNullOrEmpty(replicasStr)) {\n+            for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n+              String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n+              // partition name and replica name are the same.\n+              String partitionName = replicaStrParts[0];\n+              long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n+              String partitionClass =\n+                  replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+              disk.getReplicaConfigs()\n+                  .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+            }\n           }\n+          dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n         }\n-        dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n+      });\n+      return dataNodeConfig;\n+    }\n+\n+    /**\n+     * @param dataNodeConfig the {@link DataNodeConfig} to convert into an {@link InstanceConfig}.\n+     * @return the {@link InstanceConfig}.\n+     */\n+    InstanceConfig convert(DataNodeConfig dataNodeConfig) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8"}, "originalPosition": 170}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/67404bee4d56348f4856b7a32fce14dd66159d20", "committedDate": "2020-07-01T00:34:23Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\n\nTo prepare for future participant changes this also adds set and get\nmethods to DataNodeConfigSource for reading and updating\nDataNodeConfigs."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/fc0873c90fb8b8109c2d5bd4571d74286dd3d0e8", "committedDate": "2020-06-24T22:15:23Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\n\nTo prepare for future participant changes this also adds set and get\nmethods to DataNodeConfigSource for reading and updating\nDataNodeConfigs."}, "afterCommit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/67404bee4d56348f4856b7a32fce14dd66159d20", "committedDate": "2020-07-01T00:34:23Z", "message": "Introduce PropertyStoreToDataNodeConfigAdapter\n\nIntroduce a DataNodeConfigSource implementation that can be used to\nlisten for changes to configs using HelixPropertyStore.\n\nTo avoid blocking the threads used by helix to send property store\nchange events, this implementation uses a background thread to read\nrecords from the property store and notify any downstream listeners.\n\nTo prepare for future participant changes this also adds set and get\nmethods to DataNodeConfigSource for reading and updating\nDataNodeConfigs."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMjEyODQ1", "url": "https://github.com/linkedin/ambry/pull/1555#pullrequestreview-441212845", "createdAt": "2020-07-01T21:37:08Z", "commit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMzM1MTA0", "url": "https://github.com/linkedin/ambry/pull/1555#pullrequestreview-441335104", "createdAt": "2020-07-02T04:03:19Z", "commit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "state": "APPROVED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNDowMzoxOVrOGr8qRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNTowODowNVrOGr9n-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODczNTgxNA==", "bodyText": "Minor: suggest moving port == that.port && xid == that.xid to the position close to Objects.equals(sslPort, that.sslPort). We can compare instanceName first. The port and xid are usually same.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r448735814", "createdAt": "2020-07-02T04:03:19Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/DataNodeConfig.java", "diffHunk": "@@ -152,6 +154,27 @@ public String toString() {\n         + stoppedReplicas + \", disabledReplicas=\" + disabledReplicas + \", diskConfigs=\" + diskConfigs + '}';\n   }\n \n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    DataNodeConfig that = (DataNodeConfig) o;\n+    return port == that.port && xid == that.xid && Objects.equals(instanceName, that.instanceName) && Objects.equals(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODc0MTYyMw==", "bodyText": "It seems this method is only called in test. I guess the invocation in production code will be in next PR?", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r448741623", "createdAt": "2020-07-02T04:27:45Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "diffHunk": "@@ -33,82 +38,146 @@\n public class InstanceConfigToDataNodeConfigAdapter implements DataNodeConfigSource {\n   private static final Logger LOGGER = LoggerFactory.getLogger(InstanceConfigToDataNodeConfigAdapter.class);\n   private final HelixManager helixManager;\n-  private final ClusterMapConfig clusterMapConfig;\n+  private final Converter converter;\n+  private final String clusterName;\n+  private volatile HelixAdmin helixAdmin = null;\n \n   /**\n    * @param helixManager the {@link HelixManager} to use as the source of truth for {@link InstanceConfig}s.\n    * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n    */\n   public InstanceConfigToDataNodeConfigAdapter(HelixManager helixManager, ClusterMapConfig clusterMapConfig) {\n     this.helixManager = helixManager;\n-    this.clusterMapConfig = clusterMapConfig;\n+    this.converter = new Converter(clusterMapConfig);\n+    clusterName = clusterMapConfig.clusterMapClusterName;\n   }\n \n   @Override\n   public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n     helixManager.addInstanceConfigChangeListener((InstanceConfigChangeListener) (instanceConfigs, context) -> {\n       Iterable<DataNodeConfig> dataNodeConfigs =\n-          () -> instanceConfigs.stream().map(this::convert).filter(Objects::nonNull).iterator();\n+          () -> instanceConfigs.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n       listener.onDataNodeConfigChange(dataNodeConfigs);\n     });\n   }\n \n-  /**\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  private DataNodeConfig convert(InstanceConfig instanceConfig) {\n-    return convert(instanceConfig, clusterMapConfig);\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    InstanceConfig instanceConfig = converter.convert(config);\n+    return getHelixAdmin().setInstanceConfig(clusterName, instanceConfig.getInstanceName(), instanceConfig);\n   }\n \n-  /**\n-   * Exposed for testing.\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @param clusterMapConfig the {@link ClusterMapConfig} containing any default values that may be needed.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  static DataNodeConfig convert(InstanceConfig instanceConfig, ClusterMapConfig clusterMapConfig) {\n-    int schemaVersion = getSchemaVersion(instanceConfig);\n-    if (schemaVersion != 0) {\n-      LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n-      return null;\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    InstanceConfig instanceConfig = getHelixAdmin().getInstanceConfig(clusterName, instanceName);\n+    return instanceConfig != null ? converter.convert(instanceConfig) : null;\n+  }\n+\n+  void setHelixAdmin(HelixAdmin helixAdmin) {\n+    this.helixAdmin = helixAdmin;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODc0NDAwOA==", "bodyText": "This reminds me that we probably can use setMapField instead of setMapFields to avoid race condition that Helix concurrently writes to HELIX_DISABLED_PARTITION field.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r448744008", "createdAt": "2020-07-02T04:38:06Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/InstanceConfigToDataNodeConfigAdapter.java", "diffHunk": "@@ -33,82 +38,146 @@\n public class InstanceConfigToDataNodeConfigAdapter implements DataNodeConfigSource {\n   private static final Logger LOGGER = LoggerFactory.getLogger(InstanceConfigToDataNodeConfigAdapter.class);\n   private final HelixManager helixManager;\n-  private final ClusterMapConfig clusterMapConfig;\n+  private final Converter converter;\n+  private final String clusterName;\n+  private volatile HelixAdmin helixAdmin = null;\n \n   /**\n    * @param helixManager the {@link HelixManager} to use as the source of truth for {@link InstanceConfig}s.\n    * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n    */\n   public InstanceConfigToDataNodeConfigAdapter(HelixManager helixManager, ClusterMapConfig clusterMapConfig) {\n     this.helixManager = helixManager;\n-    this.clusterMapConfig = clusterMapConfig;\n+    this.converter = new Converter(clusterMapConfig);\n+    clusterName = clusterMapConfig.clusterMapClusterName;\n   }\n \n   @Override\n   public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n     helixManager.addInstanceConfigChangeListener((InstanceConfigChangeListener) (instanceConfigs, context) -> {\n       Iterable<DataNodeConfig> dataNodeConfigs =\n-          () -> instanceConfigs.stream().map(this::convert).filter(Objects::nonNull).iterator();\n+          () -> instanceConfigs.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n       listener.onDataNodeConfigChange(dataNodeConfigs);\n     });\n   }\n \n-  /**\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  private DataNodeConfig convert(InstanceConfig instanceConfig) {\n-    return convert(instanceConfig, clusterMapConfig);\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    InstanceConfig instanceConfig = converter.convert(config);\n+    return getHelixAdmin().setInstanceConfig(clusterName, instanceConfig.getInstanceName(), instanceConfig);\n   }\n \n-  /**\n-   * Exposed for testing.\n-   * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n-   * @param clusterMapConfig the {@link ClusterMapConfig} containing any default values that may be needed.\n-   * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported schema\n-   *         version.\n-   */\n-  static DataNodeConfig convert(InstanceConfig instanceConfig, ClusterMapConfig clusterMapConfig) {\n-    int schemaVersion = getSchemaVersion(instanceConfig);\n-    if (schemaVersion != 0) {\n-      LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n-      return null;\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    InstanceConfig instanceConfig = getHelixAdmin().getInstanceConfig(clusterName, instanceName);\n+    return instanceConfig != null ? converter.convert(instanceConfig) : null;\n+  }\n+\n+  void setHelixAdmin(HelixAdmin helixAdmin) {\n+    this.helixAdmin = helixAdmin;\n+  }\n+\n+  private HelixAdmin getHelixAdmin() {\n+    return Objects.requireNonNull(helixAdmin, \"helixAdmin not set\");\n+  }\n+\n+  static class Converter {\n+    private final ClusterMapConfig clusterMapConfig;\n+\n+    Converter(ClusterMapConfig clusterMapConfig) {\n+      this.clusterMapConfig = clusterMapConfig;\n     }\n-    DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n-        Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n-        getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n-    dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n-    dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n-    // TODO uncomment this line once 1534 is merged\n-    // dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n-    instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n-      if (diskProps.get(DISK_STATE) == null) {\n-        // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n-        // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n-        // state to InstanceConfig.\n-        LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n-            instanceConfig.getInstanceName());\n-      } else {\n-        DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n-            diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n-            Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n-        String replicasStr = diskProps.get(REPLICAS_STR);\n-        if (!replicasStr.isEmpty()) {\n-          for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n-            String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n-            // partition name and replica name are the same.\n-            String partitionName = replicaStrParts[0];\n-            long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n-            String partitionClass =\n-                replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n-            disk.getReplicaConfigs()\n-                .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+\n+    /**\n+     * @param instanceConfig the {@link InstanceConfig} to convert to a {@link DataNodeConfig} object.\n+     * @return the {@link DataNodeConfig}, or {@code null} if the {@link InstanceConfig} provided has an unsupported\n+     *         schema version.\n+     */\n+    DataNodeConfig convert(InstanceConfig instanceConfig) {\n+      int schemaVersion = getSchemaVersion(instanceConfig);\n+      if (schemaVersion != 0) {\n+        LOGGER.warn(\"Unknown InstanceConfig schema version {} in {}. Ignoring.\", schemaVersion, instanceConfig);\n+        return null;\n+      }\n+      DataNodeConfig dataNodeConfig = new DataNodeConfig(instanceConfig.getInstanceName(), instanceConfig.getHostName(),\n+          Integer.parseInt(instanceConfig.getPort()), getDcName(instanceConfig), getSslPortStr(instanceConfig),\n+          getHttp2PortStr(instanceConfig), getRackId(instanceConfig), getXid(instanceConfig));\n+      dataNodeConfig.getSealedReplicas().addAll(getSealedReplicas(instanceConfig));\n+      dataNodeConfig.getStoppedReplicas().addAll(getStoppedReplicas(instanceConfig));\n+      dataNodeConfig.getDisabledReplicas().addAll(getDisabledReplicas(instanceConfig));\n+      instanceConfig.getRecord().getMapFields().forEach((mountPath, diskProps) -> {\n+        if (diskProps.get(DISK_STATE) == null) {\n+          // Check if this map field actually holds disk properties, since we can't tell from just the field key (the\n+          // mount path with no special prefix). There may be extra fields when Helix controller adds partitions in ERROR\n+          // state to InstanceConfig.\n+          LOGGER.warn(\"{} field does not contain disk info on {}. Skip it and continue on next one.\", mountPath,\n+              instanceConfig.getInstanceName());\n+        } else {\n+          DataNodeConfig.DiskConfig disk = new DataNodeConfig.DiskConfig(\n+              diskProps.get(DISK_STATE).equals(AVAILABLE_STR) ? HardwareState.AVAILABLE : HardwareState.UNAVAILABLE,\n+              Long.parseLong(diskProps.get(DISK_CAPACITY_STR)));\n+          String replicasStr = diskProps.get(REPLICAS_STR);\n+          if (!Utils.isNullOrEmpty(replicasStr)) {\n+            for (String replicaStr : replicasStr.split(REPLICAS_DELIM_STR)) {\n+              String[] replicaStrParts = replicaStr.split(REPLICAS_STR_SEPARATOR);\n+              // partition name and replica name are the same.\n+              String partitionName = replicaStrParts[0];\n+              long replicaCapacity = Long.parseLong(replicaStrParts[1]);\n+              String partitionClass =\n+                  replicaStrParts.length > 2 ? replicaStrParts[2] : clusterMapConfig.clusterMapDefaultPartitionClass;\n+              disk.getReplicaConfigs()\n+                  .put(partitionName, new DataNodeConfig.ReplicaConfig(replicaCapacity, partitionClass));\n+            }\n           }\n+          dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n         }\n-        dataNodeConfig.getDiskConfigs().put(mountPath, disk);\n+      });\n+      return dataNodeConfig;\n+    }\n+\n+    /**\n+     * @param dataNodeConfig the {@link DataNodeConfig} to convert into an {@link InstanceConfig}.\n+     * @return the {@link InstanceConfig}.\n+     */\n+    InstanceConfig convert(DataNodeConfig dataNodeConfig) {\n+      InstanceConfig instanceConfig = new InstanceConfig(dataNodeConfig.getInstanceName());\n+      instanceConfig.setHostName(dataNodeConfig.getHostName());\n+      instanceConfig.setPort(Integer.toString(dataNodeConfig.getPort()));\n+      if (dataNodeConfig.getSslPort() != null) {\n+        instanceConfig.getRecord().setIntField(SSL_PORT_STR, dataNodeConfig.getSslPort());\n       }\n-    });\n-    return dataNodeConfig;\n+      if (dataNodeConfig.getHttp2Port() != null) {\n+        instanceConfig.getRecord().setIntField(HTTP2_PORT_STR, dataNodeConfig.getHttp2Port());\n+      }\n+      instanceConfig.getRecord().setSimpleField(DATACENTER_STR, dataNodeConfig.getDatacenterName());\n+      instanceConfig.getRecord().setSimpleField(RACKID_STR, dataNodeConfig.getRackId());\n+      long xid = dataNodeConfig.getXid();\n+      if (xid != DEFAULT_XID) {\n+        // Set the XID only if it is not the default, in order to avoid unnecessary updates.\n+        instanceConfig.getRecord().setLongField(XID_STR, xid);\n+      }\n+      instanceConfig.getRecord().setIntField(SCHEMA_VERSION_STR, CURRENT_SCHEMA_VERSION);\n+      instanceConfig.getRecord().setListField(SEALED_STR, new ArrayList<>(dataNodeConfig.getSealedReplicas()));\n+      instanceConfig.getRecord()\n+          .setListField(STOPPED_REPLICAS_STR, new ArrayList<>(dataNodeConfig.getStoppedReplicas()));\n+      instanceConfig.getRecord()\n+          .setListField(DISABLED_REPLICAS_STR, new ArrayList<>(dataNodeConfig.getDisabledReplicas()));\n+      dataNodeConfig.getDiskConfigs().forEach((mountPath, diskConfig) -> {\n+        Map<String, String> diskProps = new HashMap<>();\n+        diskProps.put(DISK_STATE, diskConfig.getState() == HardwareState.AVAILABLE ? AVAILABLE_STR : UNAVAILABLE_STR);\n+        diskProps.put(DISK_CAPACITY_STR, Long.toString(diskConfig.getDiskCapacityInBytes()));\n+        StringBuilder replicasStrBuilder = new StringBuilder();\n+        diskConfig.getReplicaConfigs()\n+            .forEach((partitionName, replicaConfig) -> replicasStrBuilder.append(partitionName)\n+                .append(REPLICAS_STR_SEPARATOR)\n+                .append(replicaConfig.getReplicaCapacityInBytes())\n+                .append(REPLICAS_STR_SEPARATOR)\n+                .append(replicaConfig.getPartitionClass())\n+                .append(REPLICAS_DELIM_STR));\n+        diskProps.put(REPLICAS_STR, replicasStrBuilder.toString());\n+        instanceConfig.getRecord().setMapField(mountPath, diskProps);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "originalPosition": 208}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODc1MTYxMA==", "bodyText": "This comment might be out of scope but I wonder if node (frontend/server) lost connection to ZK and got reconnected, is there a mechanism to re-read all datanode configs in PropertyStore (and re-populate in-mem clustermap)?\nAs far as I know, InstanceConfig current has an async way to read all configs again (not ideal) but in next release of Helix, it's changed to a sync way, which guarantees all configs will be read again after reconnection.", "url": "https://github.com/linkedin/ambry/pull/1555#discussion_r448751610", "createdAt": "2020-07-02T05:08:05Z", "author": {"login": "jsjtzyy"}, "path": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/PropertyStoreToDataNodeConfigAdapter.java", "diffHunk": "@@ -0,0 +1,278 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *\n+ */\n+\n+package com.github.ambry.clustermap;\n+\n+import com.github.ambry.config.ClusterMapConfig;\n+import com.github.ambry.utils.Utils;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.Executors;\n+import org.apache.helix.AccessOption;\n+import org.apache.helix.store.HelixPropertyListener;\n+import org.apache.helix.store.HelixPropertyStore;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+import org.apache.zookeeper.data.Stat;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.github.ambry.clustermap.ClusterMapUtils.*;\n+\n+\n+/**\n+ * An implementation of {@link DataNodeConfigSource} that reads {@link ZNRecord}s stored under the\n+ * \"/DataNodeConfigs\" path in a {@link HelixPropertyStore}.\n+ */\n+public class PropertyStoreToDataNodeConfigAdapter implements DataNodeConfigSource {\n+  static final String CONFIG_PATH = \"/DataNodeConfigs\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(PropertyStoreToDataNodeConfigAdapter.class);\n+  private final HelixPropertyStore<ZNRecord> propertyStore;\n+  private final Converter converter;\n+  private final Executor eventExecutor;\n+\n+  /**\n+   * @param propertyStore the {@link HelixPropertyStore} instance to use to interact with zookeeper.\n+   * @param clusterMapConfig the {@link ClusterMapConfig} to use.\n+   * @param dcName the datacenter name for for the property store.\n+   */\n+  public PropertyStoreToDataNodeConfigAdapter(HelixPropertyStore<ZNRecord> propertyStore,\n+      ClusterMapConfig clusterMapConfig, String dcName) {\n+    this.propertyStore = propertyStore;\n+    this.converter = new Converter(clusterMapConfig.clusterMapDefaultPartitionClass, dcName);\n+    this.eventExecutor = Executors.newSingleThreadExecutor();\n+  }\n+\n+  @Override\n+  public void addDataNodeConfigChangeListener(DataNodeConfigChangeListener listener) throws Exception {\n+    Subscription subscription = new Subscription(listener);\n+    propertyStore.subscribe(CONFIG_PATH, subscription);\n+    subscription.start();\n+  }\n+\n+  @Override\n+  public boolean set(DataNodeConfig config) {\n+    ZNRecord record = converter.convert(config);\n+    String path = CONFIG_PATH + \"/\" + record.getId();\n+    return propertyStore.set(path, record, AccessOption.PERSISTENT);\n+  }\n+\n+  @Override\n+  public DataNodeConfig get(String instanceName) {\n+    String path = CONFIG_PATH + \"/\" + instanceName;\n+    ZNRecord record = propertyStore.get(path, new Stat(), AccessOption.PERSISTENT);\n+    return record != null ? converter.convert(record) : null;\n+  }\n+\n+  private class Subscription implements HelixPropertyListener {\n+    private final DataNodeConfigChangeListener listener;\n+    private final CompletableFuture<Void> initFuture = new CompletableFuture<>();\n+\n+    Subscription(DataNodeConfigChangeListener listener) {\n+      this.listener = listener;\n+    }\n+\n+    /**\n+     * Tell the background executor to perform any initialization needed. This should be called after registering this\n+     * listener using {@link HelixPropertyStore#subscribe}. This will ensure that the first event sent to\n+     * {@link #listener} will contain the entire set of current configs.\n+     * @throws Exception if there was an exception while making the initialization call.\n+     */\n+    void start() throws Exception {\n+      eventExecutor.execute(this::initializeIfNeeded);\n+      try {\n+        initFuture.get();\n+      } catch (ExecutionException e) {\n+        throw Utils.extractExecutionExceptionCause(e);\n+      }\n+    }\n+\n+    @Override\n+    public void onDataChange(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} changed\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataCreate(String path) {\n+      LOGGER.debug(\"DataNodeConfig path {} created\", path);\n+      onPathChange(path);\n+    }\n+\n+    @Override\n+    public void onDataDelete(String path) {\n+      // TODO handle node deletions dynamically. Doing so requires further work in the ClusterChangeHandler impl\n+      LOGGER.info(\"DataNodeConfig path {} deleted. This requires a restart to handle\", path);\n+    }\n+\n+    private void onPathChange(String path) {\n+      eventExecutor.execute(() -> updateOrInitialize(path));\n+    }\n+\n+    private Iterable<DataNodeConfig> lazyIterable(Collection<ZNRecord> records) {\n+      return () -> records.stream().map(converter::convert).filter(Objects::nonNull).iterator();\n+    }\n+\n+    /**\n+     * Either perform a full initialization or notify the listener about an incremental update to a path. This should be\n+     * called from the event executor thread.\n+     * @param changedPath the path that changed.\n+     */\n+    private synchronized void updateOrInitialize(String changedPath) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67404bee4d56348f4856b7a32fce14dd66159d20"}, "originalPosition": 140}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03f71bf46e4a463cb2721ee27dc411adc1ecfe9b", "author": {"user": {"login": "cgtz", "name": "Casey Getz"}}, "url": "https://github.com/linkedin/ambry/commit/03f71bf46e4a463cb2721ee27dc411adc1ecfe9b", "committedDate": "2020-07-13T15:36:13Z", "message": "Address final comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1114, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}