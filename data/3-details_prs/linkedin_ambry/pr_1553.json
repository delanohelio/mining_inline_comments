{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI4ODEyNzI4", "number": 1553, "title": "Checkpoint Azure compaction progress", "bodyText": "Add configurable limit to number of blobs purged in a partition each compaction round.", "createdAt": "2020-06-05T22:24:25Z", "url": "https://github.com/linkedin/ambry/pull/1553", "merged": true, "mergeCommit": {"oid": "b2ac9c83d97ec53fcd046c285e40673a918728ba"}, "closed": true, "closedAt": "2020-06-12T16:39:23Z", "author": {"login": "lightningrob"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcoaTi6AH2gAyNDI4ODEyNzI4OjdhZGNkY2E0NzgyYTQ1ZDAwNmNkZWYxZmJmMTViMTU3NTNhMjQzMTk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcqbvPKAH2gAyNDI4ODEyNzI4OjRiYzQwNDYyOGRlOTcwMjQxMWU4OTM5YzM0ZWQ5MGRkM2M0NTAxN2Q=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "7adcdca4782a45d006cdef1fbf15b15753a24319", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/7adcdca4782a45d006cdef1fbf15b15753a24319", "committedDate": "2020-06-05T22:21:56Z", "message": "Checkpoint Azure compaction progress"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d34e15d61539621f4a4c3c373d5196cd168671fd", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/d34e15d61539621f4a4c3c373d5196cd168671fd", "committedDate": "2020-06-08T04:28:24Z", "message": "Add configurable limit to number of blobs purged in a partition each compaction round."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI3MzI4MDc1", "url": "https://github.com/linkedin/ambry/pull/1553#pullrequestreview-427328075", "createdAt": "2020-06-09T16:16:07Z", "commit": {"oid": "d34e15d61539621f4a4c3c373d5196cd168671fd"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxNjoxNjowN1rOGhSPxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxNjoyOToyNFrOGhSxIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU1NTE0MA==", "bodyText": "retries here for retry-able error?", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437555140", "createdAt": "2020-06-09T16:16:07Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -257,17 +309,61 @@ int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageExce\n   }\n \n   /**\n-   * Returns the dead blob in the specified partition with the earliest expiration or deletion time.\n+   * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n-   * @param fieldName the field name to use (expiration or deletion time).\n-   * @return the {@link CloudBlobMetadata} for the dead blob, or NULL if none was found.\n-   * @throws CloudStorageException\n+   * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n+   * @throws BlobStorageException if the checkpoint blob\n+   */\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws BlobStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(64);\n+    boolean hasCheckpoint = azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d34e15d61539621f4a4c3c373d5196cd168671fd"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU1NjI2OA==", "bodyText": "why 64? Also maybe we can make it private static final", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437556268", "createdAt": "2020-06-09T16:17:57Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -257,17 +309,61 @@ int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageExce\n   }\n \n   /**\n-   * Returns the dead blob in the specified partition with the earliest expiration or deletion time.\n+   * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n-   * @param fieldName the field name to use (expiration or deletion time).\n-   * @return the {@link CloudBlobMetadata} for the dead blob, or NULL if none was found.\n-   * @throws CloudStorageException\n+   * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n+   * @throws BlobStorageException if the checkpoint blob\n+   */\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws BlobStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(64);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d34e15d61539621f4a4c3c373d5196cd168671fd"}, "originalPosition": 190}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU2MTEyOA==", "bodyText": "We should cache this information instead of doing a get always. Only when we dont see a cache for this partition, should we do getCompactionInProgress", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437561128", "createdAt": "2020-06-09T16:25:33Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -257,17 +309,61 @@ int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageExce\n   }\n \n   /**\n-   * Returns the dead blob in the specified partition with the earliest expiration or deletion time.\n+   * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n-   * @param fieldName the field name to use (expiration or deletion time).\n-   * @return the {@link CloudBlobMetadata} for the dead blob, or NULL if none was found.\n-   * @throws CloudStorageException\n+   * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n+   * @throws BlobStorageException if the checkpoint blob\n+   */\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws BlobStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(64);\n+    boolean hasCheckpoint = azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false);\n+    if (!hasCheckpoint) {\n+      return emptyCheckpoints;\n+    }\n+    try {\n+      // Payload format: {\"expirationTime\" : 12345, \"deletionTime\" : 67890}\n+      ObjectNode jsonNode = (ObjectNode) objectMapper.readTree(baos.toString());\n+      Map<String, Long> checkpoints = new HashMap<>();\n+      for (String fieldName : new String[]{CloudBlobMetadata.FIELD_DELETION_TIME,\n+          CloudBlobMetadata.FIELD_EXPIRATION_TIME}) {\n+        checkpoints.put(fieldName, jsonNode.has(fieldName) ? jsonNode.get(fieldName).longValue() : DEFAULT_TIME);\n+      }\n+      return checkpoints;\n+    } catch (IOException e) {\n+      logger.error(\"Could not retrieve compaction progress for {}\", partitionPath, e);\n+      azureMetrics.compactionProgressReadErrorCount.inc();\n+      return emptyCheckpoints;\n+    }\n+  }\n+\n+  /**\n+   * Update the compaction progress for a partition.\n+   * @param partitionPath the partition to update.\n+   * @param fieldName the compaction field (deletion or expiration time).\n+   * @param checkpointTime the updated progress time.\n    */\n-  public CloudBlobMetadata getOldestDeadlob(String partitionPath, String fieldName) throws CloudStorageException {\n-    // TODO: once we have Cosmos compaction table, can query that.\n-    List<CloudBlobMetadata> deadBlobs =\n-        requestAgent.doWithRetries(() -> getDeadBlobs(partitionPath, fieldName, 1, System.currentTimeMillis(), 1),\n-            \"GetDeadBlobs\", partitionPath);\n-    return deadBlobs.isEmpty() ? null : deadBlobs.get(0);\n+  boolean updateCompactionProgress(String partitionPath, String fieldName, long checkpointTime) {\n+    try {\n+      // load existing progress checkpoint.\n+      Map<String, Long> checkpoints = getCompactionProgress(partitionPath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d34e15d61539621f4a4c3c373d5196cd168671fd"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU2MTU4MA==", "bodyText": "Also maybe retry for retry-able error?", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437561580", "createdAt": "2020-06-09T16:26:10Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -257,17 +309,61 @@ int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageExce\n   }\n \n   /**\n-   * Returns the dead blob in the specified partition with the earliest expiration or deletion time.\n+   * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n-   * @param fieldName the field name to use (expiration or deletion time).\n-   * @return the {@link CloudBlobMetadata} for the dead blob, or NULL if none was found.\n-   * @throws CloudStorageException\n+   * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n+   * @throws BlobStorageException if the checkpoint blob\n+   */\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws BlobStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(64);\n+    boolean hasCheckpoint = azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false);\n+    if (!hasCheckpoint) {\n+      return emptyCheckpoints;\n+    }\n+    try {\n+      // Payload format: {\"expirationTime\" : 12345, \"deletionTime\" : 67890}\n+      ObjectNode jsonNode = (ObjectNode) objectMapper.readTree(baos.toString());\n+      Map<String, Long> checkpoints = new HashMap<>();\n+      for (String fieldName : new String[]{CloudBlobMetadata.FIELD_DELETION_TIME,\n+          CloudBlobMetadata.FIELD_EXPIRATION_TIME}) {\n+        checkpoints.put(fieldName, jsonNode.has(fieldName) ? jsonNode.get(fieldName).longValue() : DEFAULT_TIME);\n+      }\n+      return checkpoints;\n+    } catch (IOException e) {\n+      logger.error(\"Could not retrieve compaction progress for {}\", partitionPath, e);\n+      azureMetrics.compactionProgressReadErrorCount.inc();\n+      return emptyCheckpoints;\n+    }\n+  }\n+\n+  /**\n+   * Update the compaction progress for a partition.\n+   * @param partitionPath the partition to update.\n+   * @param fieldName the compaction field (deletion or expiration time).\n+   * @param checkpointTime the updated progress time.\n    */\n-  public CloudBlobMetadata getOldestDeadlob(String partitionPath, String fieldName) throws CloudStorageException {\n-    // TODO: once we have Cosmos compaction table, can query that.\n-    List<CloudBlobMetadata> deadBlobs =\n-        requestAgent.doWithRetries(() -> getDeadBlobs(partitionPath, fieldName, 1, System.currentTimeMillis(), 1),\n-            \"GetDeadBlobs\", partitionPath);\n-    return deadBlobs.isEmpty() ? null : deadBlobs.get(0);\n+  boolean updateCompactionProgress(String partitionPath, String fieldName, long checkpointTime) {\n+    try {\n+      // load existing progress checkpoint.\n+      Map<String, Long> checkpoints = getCompactionProgress(partitionPath);\n+      // Ensure we don't downgrade progress already recorded.\n+      if (checkpointTime <= checkpoints.getOrDefault(fieldName, DEFAULT_TIME)) {\n+        logger.info(\"Skipping update of compaction progress for {} because saved {} is more recent.\", partitionPath,\n+            fieldName);\n+        return false;\n+      }\n+      checkpoints.put(fieldName, Math.max(checkpointTime, checkpoints.get(fieldName)));\n+      String json = objectMapper.writeValueAsString(checkpoints);\n+      ByteArrayInputStream bais = new ByteArrayInputStream(json.getBytes());\n+      azureBlobDataAccessor.uploadFile(CHECKPOINT_CONTAINER, partitionPath, bais);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d34e15d61539621f4a4c3c373d5196cd168671fd"}, "originalPosition": 236}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU2MzY4MA==", "bodyText": "We don't need to compute compactionTime at the end of this method anymore.", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437563680", "createdAt": "2020-06-09T16:29:24Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -81,29 +103,39 @@ boolean isShuttingDown() {\n    * Purge the inactive blobs in the specified partition.\n    * @return the total number of blobs purged.\n    */\n-  public int compactPartition(String partitionPath) {\n+  public int compactPartition(String partitionPath) throws CloudStorageException {\n     if (isShuttingDown()) {\n-      logger.info(\"Skipping compaction due to shut down.\");\n+      logger.info(\"Skipping compaction of {} due to shut down.\", partitionPath);\n       return 0;\n     }\n \n+    Map<String, Long> checkpoints;\n+    try {\n+      checkpoints = getCompactionProgress(partitionPath);\n+    } catch (BlobStorageException | UncheckedIOException e) {\n+      // If checkpoint couldn't be read, skip and try later.\n+      throw new CloudStorageException(\"Compaction of \" + partitionPath + \" failed reading checkpoint\", e);\n+    }\n+\n     long now = System.currentTimeMillis();\n     long compactionStartTime = now;\n+    // FIXME: incorrect because time limit is for all partitions\n     long timeToQuit = now + compactionTimeLimitMs;\n     long queryEndTime = now - retentionPeriodMs;\n     long queryStartTime = now - TimeUnit.DAYS.toMillis(lookbackDays);\n-    Date queryStartDate = new Date(queryStartTime);\n     Date queryEndDate = new Date(queryEndTime);\n     int totalBlobsPurged = 0;\n-    logger.info(\"Compacting partition {} over time range {} - {}\", partitionPath, queryStartDate, queryEndDate);\n     try {\n+      long deletionStartTime = Math.max(queryStartTime, checkpoints.get(CloudBlobMetadata.FIELD_DELETION_TIME));\n       int numPurged =\n-          compactPartition(partitionPath, CloudBlobMetadata.FIELD_DELETION_TIME, queryStartTime, queryEndTime,\n+          compactPartition(partitionPath, CloudBlobMetadata.FIELD_DELETION_TIME, deletionStartTime, queryEndTime,\n               timeToQuit);\n       logger.info(\"Purged {} deleted blobs in partition {} up to {}\", numPurged, partitionPath, queryEndDate);\n       totalBlobsPurged += numPurged;\n-      numPurged = compactPartition(partitionPath, CloudBlobMetadata.FIELD_EXPIRATION_TIME, queryStartTime, queryEndTime,\n-          timeToQuit);\n+      long expirationStartTime = Math.max(queryStartTime, checkpoints.get(CloudBlobMetadata.FIELD_EXPIRATION_TIME));\n+      numPurged =\n+          compactPartition(partitionPath, CloudBlobMetadata.FIELD_EXPIRATION_TIME, expirationStartTime, queryEndTime,\n+              timeToQuit);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d34e15d61539621f4a4c3c373d5196cd168671fd"}, "originalPosition": 110}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI3MzU1MTM1", "url": "https://github.com/linkedin/ambry/pull/1553#pullrequestreview-427355135", "createdAt": "2020-06-09T16:49:00Z", "commit": {"oid": "d34e15d61539621f4a4c3c373d5196cd168671fd"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxNjo0OTowMVrOGhTfDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxNjo1MTo0MVrOGhTlgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU3NTQzOQ==", "bodyText": "javadocs need update.", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437575439", "createdAt": "2020-06-09T16:49:01Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -81,29 +103,39 @@ boolean isShuttingDown() {\n    * Purge the inactive blobs in the specified partition.\n    * @return the total number of blobs purged.\n    */\n-  public int compactPartition(String partitionPath) {\n+  public int compactPartition(String partitionPath) throws CloudStorageException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d34e15d61539621f4a4c3c373d5196cd168671fd"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU3NTc4MQ==", "bodyText": "javadocs of the constructor needs update.", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437575781", "createdAt": "2020-06-09T16:49:35Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -56,11 +74,15 @@ public AzureStorageCompactor(AzureBlobDataAccessor azureBlobDataAccessor, Cosmos\n     this.azureMetrics = azureMetrics;\n     this.retentionPeriodMs = TimeUnit.DAYS.toMillis(cloudConfig.cloudDeletedBlobRetentionDays);\n     this.queryLimit = cloudConfig.cloudBlobCompactionQueryLimit;\n+    this.purgeLimit = cloudConfig.cloudCompactionPurgeLimit;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d34e15d61539621f4a4c3c373d5196cd168671fd"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU3NzA5MQ==", "bodyText": "the compactionStartTime variable above is redundant now.", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437577091", "createdAt": "2020-06-09T16:51:41Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -81,29 +103,39 @@ boolean isShuttingDown() {\n    * Purge the inactive blobs in the specified partition.\n    * @return the total number of blobs purged.\n    */\n-  public int compactPartition(String partitionPath) {\n+  public int compactPartition(String partitionPath) throws CloudStorageException {\n     if (isShuttingDown()) {\n-      logger.info(\"Skipping compaction due to shut down.\");\n+      logger.info(\"Skipping compaction of {} due to shut down.\", partitionPath);\n       return 0;\n     }\n \n+    Map<String, Long> checkpoints;\n+    try {\n+      checkpoints = getCompactionProgress(partitionPath);\n+    } catch (BlobStorageException | UncheckedIOException e) {\n+      // If checkpoint couldn't be read, skip and try later.\n+      throw new CloudStorageException(\"Compaction of \" + partitionPath + \" failed reading checkpoint\", e);\n+    }\n+\n     long now = System.currentTimeMillis();\n     long compactionStartTime = now;\n+    // FIXME: incorrect because time limit is for all partitions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d34e15d61539621f4a4c3c373d5196cd168671fd"}, "originalPosition": 89}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI3MzkwMzQy", "url": "https://github.com/linkedin/ambry/pull/1553#pullrequestreview-427390342", "createdAt": "2020-06-09T17:33:36Z", "commit": {"oid": "d34e15d61539621f4a4c3c373d5196cd168671fd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxNzozMzozN1rOGhVHUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxNzozMzozN1rOGhVHUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYwMjEyOA==", "bodyText": "Not seeing any caller for this API except UT in the current diff. Will this be used in future PR?", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437602128", "createdAt": "2020-06-09T17:33:37Z", "author": {"login": "ssen-li"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -237,6 +237,22 @@ public boolean downloadFile(String containerName, String fileName, OutputStream\n     }\n   }\n \n+  /**\n+   * Delete a file from blob storage, if it exists.\n+   * @param containerName name of the container containing file to delete.\n+   * @param fileName name of the file to delete.\n+   * @return true if the file was deleted, otherwise false.\n+   * @throws BlobStorageException for any error on ABS side.\n+   */\n+  boolean deleteFile(String containerName, String fileName) throws BlobStorageException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d34e15d61539621f4a4c3c373d5196cd168671fd"}, "originalPosition": 11}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b550e038a4694e98e19c898d57d60bf094f0bac8", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/b550e038a4694e98e19c898d57d60bf094f0bac8", "committedDate": "2020-06-10T23:47:45Z", "message": "Address review comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9d457b806f23574e4c16d42a2270b162da3c7324", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/9d457b806f23574e4c16d42a2270b162da3c7324", "committedDate": "2020-06-11T00:52:45Z", "message": "Merge branch 'master' of github.com:linkedin/ambry into compaction-progress"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4NTI5ODk3", "url": "https://github.com/linkedin/ambry/pull/1553#pullrequestreview-428529897", "createdAt": "2020-06-11T01:18:45Z", "commit": {"oid": "9d457b806f23574e4c16d42a2270b162da3c7324"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e5601860ff562740068c4cf471b9667b2439b7bf", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/e5601860ff562740068c4cf471b9667b2439b7bf", "committedDate": "2020-06-11T07:40:11Z", "message": "Bug fix and cleanup"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI5MDk0NDI2", "url": "https://github.com/linkedin/ambry/pull/1553#pullrequestreview-429094426", "createdAt": "2020-06-11T16:40:54Z", "commit": {"oid": "e5601860ff562740068c4cf471b9667b2439b7bf"}, "state": "APPROVED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNjo0MDo1NVrOGily2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNzo1NzoyNlrOGiot5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODkyMzk5Mw==", "bodyText": "Since this is only used for integration tests right now, it may not matter too much, but would it be possible to delete the blob without making a call to check for existence first (and checking delete status code).", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r438923993", "createdAt": "2020-06-11T16:40:55Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -238,6 +238,22 @@ public boolean downloadFile(String containerName, String fileName, OutputStream\n     }\n   }\n \n+  /**\n+   * Delete a file from blob storage, if it exists.\n+   * @param containerName name of the container containing file to delete.\n+   * @param fileName name of the file to delete.\n+   * @return true if the file was deleted, otherwise false.\n+   * @throws BlobStorageException for any error on ABS side.\n+   */\n+  boolean deleteFile(String containerName, String fileName) throws BlobStorageException {\n+    BlockBlobClient blobClient = getBlockBlobClient(containerName, fileName, false);\n+    if (blobClient.exists()) {\n+      blobClient.delete();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5601860ff562740068c4cf471b9667b2439b7bf"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODkyNDg3Ng==", "bodyText": "I guess this is to handle cases when there are two metadata objects in the list with the same ID?", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r438924876", "createdAt": "2020-06-11T16:42:26Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureCloudDestination.java", "diffHunk": "@@ -221,7 +221,7 @@ public short undeleteBlob(BlobId blobId, short lifeVersion, CloudUpdateValidator\n     for (List<BlobId> batchOfBlobs : chunkedBlobIdList) {\n       metadataList.addAll(getBlobMetadataChunked(batchOfBlobs));\n     }\n-    return metadataList.stream().collect(Collectors.toMap(CloudBlobMetadata::getId, Function.identity()));\n+    return metadataList.stream().collect(Collectors.toMap(CloudBlobMetadata::getId, Function.identity(), (x, y) -> x));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5601860ff562740068c4cf471b9667b2439b7bf"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODk0NTIzNg==", "bodyText": "since references to this map get returned via different method calls, consider wrapping it in Collections.unmodifiableMap It also seems like it could be static.", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r438945236", "createdAt": "2020-06-11T17:18:37Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -56,11 +73,13 @@ public AzureStorageCompactor(AzureBlobDataAccessor azureBlobDataAccessor, Cosmos\n     this.azureMetrics = azureMetrics;\n     this.retentionPeriodMs = TimeUnit.DAYS.toMillis(cloudConfig.cloudDeletedBlobRetentionDays);\n     this.queryLimit = cloudConfig.cloudBlobCompactionQueryLimit;\n+    this.purgeLimit = cloudConfig.cloudCompactionPurgeLimit;\n     this.queryBucketDays = cloudConfig.cloudCompactionQueryBucketDays;\n     this.lookbackDays = cloudConfig.cloudCompactionLookbackDays;\n-    // TODO: change this\n-    compactionTimeLimitMs = TimeUnit.HOURS.toMillis(cloudConfig.cloudBlobCompactionIntervalHours);\n     requestAgent = new CloudRequestAgent(cloudConfig, vcrMetrics);\n+    emptyCheckpoints = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5601860ff562740068c4cf471b9667b2439b7bf"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODk2NDkzNw==", "bodyText": "Using the same ByteArrayOutputStream for all retries could cause issues if the first try partially wrote to BAOS and then the second try will just start appending to the same stream.\nInstead you could do something like this:\n    ByteArrayOutputStream baos = requestAgent.doWithRetries(() -> {\n      ByteArrayOutputStream os = new ByteArrayOutputStream(CHECKPOINT_BUFFER_SIZE);\n      boolean hasCheckpoint = azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, os, false);\n      return hasCheckpoint ? os : null;\n    }, \"Download compaction checkpoint\", partitionPath);", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r438964937", "createdAt": "2020-06-11T17:49:18Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -257,17 +289,67 @@ int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageExce\n   }\n \n   /**\n-   * Returns the dead blob in the specified partition with the earliest expiration or deletion time.\n+   * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n-   * @param fieldName the field name to use (expiration or deletion time).\n-   * @return the {@link CloudBlobMetadata} for the dead blob, or NULL if none was found.\n-   * @throws CloudStorageException\n+   * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n+   * @throws CloudStorageException if the operation fails.\n+   */\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws CloudStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(CHECKPOINT_BUFFER_SIZE);\n+    boolean hasCheckpoint = requestAgent.doWithRetries(\n+        () -> azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5601860ff562740068c4cf471b9667b2439b7bf"}, "originalPosition": 264}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODk3MTg3OA==", "bodyText": "For me, it seems cleaner for checkpoints to be a simple object (instead of a map) with getDeletionTimeCheckpoint and getFieldExpirationTime and toBytes method so the logic around serde/defaults can all be in one place.", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r438971878", "createdAt": "2020-06-11T17:57:26Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -257,17 +289,67 @@ int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageExce\n   }\n \n   /**\n-   * Returns the dead blob in the specified partition with the earliest expiration or deletion time.\n+   * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n-   * @param fieldName the field name to use (expiration or deletion time).\n-   * @return the {@link CloudBlobMetadata} for the dead blob, or NULL if none was found.\n-   * @throws CloudStorageException\n+   * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n+   * @throws CloudStorageException if the operation fails.\n+   */\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws CloudStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(CHECKPOINT_BUFFER_SIZE);\n+    boolean hasCheckpoint = requestAgent.doWithRetries(\n+        () -> azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false),\n+        \"Download compaction checkpoint\", partitionPath);\n+    if (!hasCheckpoint) {\n+      return emptyCheckpoints;\n+    }\n+    try {\n+      // Payload format: {\"expirationTime\" : 12345, \"deletionTime\" : 67890}\n+      ObjectNode jsonNode = (ObjectNode) objectMapper.readTree(baos.toString());\n+      Map<String, Long> checkpoints = new HashMap<>();\n+      for (String fieldName : new String[]{CloudBlobMetadata.FIELD_DELETION_TIME,\n+          CloudBlobMetadata.FIELD_EXPIRATION_TIME}) {\n+        checkpoints.put(fieldName, jsonNode.has(fieldName) ? jsonNode.get(fieldName).longValue() : DEFAULT_TIME);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5601860ff562740068c4cf471b9667b2439b7bf"}, "originalPosition": 275}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4bc404628de9702411e8939c34ed90dd3c45017d", "author": {"user": {"login": "shipkit-org", "name": "shipkit.org automated bot"}}, "url": "https://github.com/linkedin/ambry/commit/4bc404628de9702411e8939c34ed90dd3c45017d", "committedDate": "2020-06-12T05:09:56Z", "message": "Address review comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1108, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}