{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY0MzMxMDU0", "number": 1361, "title": "Use cosmos changefeed for replication from azure to on prem dc", "bodyText": "Use cosmos change feed for replication from azure to on-prem dc.\nCreate a new AzureFindToken to track replication progress.", "createdAt": "2020-01-17T21:44:21Z", "url": "https://github.com/linkedin/ambry/pull/1361", "merged": true, "mergeCommit": {"oid": "f17058a6af73a336d1074b124586c23e2ef221ef"}, "closed": true, "closedAt": "2020-02-18T00:43:06Z", "author": {"login": "ankagrawal"}, "timelineItems": {"totalCount": 63, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb9SL5sAFqTM0NzYyNjg3Ng==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcEaE9uABqjMwNDA1ODI2OTk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ3NjI2ODc2", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-347626876", "createdAt": "2020-01-23T21:41:00Z", "commit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QyMTo0MTowMFrOFhNkgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QyMjoxMzo0N1rOFhOZuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM2OTY2NA==", "bodyText": "Can remove \"destination\", possibly substitute \"replication\"", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r370369664", "createdAt": "2020-01-23T21:41:00Z", "author": {"login": "lightningrob"}, "path": "ambry-api/src/main/java/com.github.ambry/config/CloudConfig.java", "diffHunk": "@@ -20,6 +20,7 @@\n \n   public static final String CLOUD_IS_VCR = \"cloud.is.vcr\";\n   public static final String VIRTUAL_REPLICATOR_CLUSTER_FACTORY_CLASS = \"virtual.replicator.cluster.factory.class\";\n+  public static final String CLOUD_DESTINATION_TOKEN_FACTORY_CLASS = \"cloud.destination.token.factory\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM3MTEyMw==", "bodyText": "Generic utility should be in utility class (if one doesn't exist already).", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r370371123", "createdAt": "2020-01-23T21:44:23Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/AzureFindToken.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress in azure.\n+ */\n+public class AzureFindToken {\n+  private final String startContinuationToken;\n+  private final String endContinuationToken;\n+  private final int index;\n+  private final int totalItems;\n+  private final String azureTokenRequestId;\n+  private final short version;\n+\n+  public static short VERSION_0 = 0;\n+  public static short DEFAULT_VERSION = VERSION_0;\n+\n+  /**\n+   * Default constructor to create a {@link AzureFindToken} with uninitialized continuation token.\n+   */\n+  public AzureFindToken() {\n+    startContinuationToken = null;\n+    index = -1;\n+    endContinuationToken = null;\n+    totalItems = -1;\n+    azureTokenRequestId = null;\n+    version = DEFAULT_VERSION;\n+  }\n+\n+  /**\n+   * Create {@link AzureFindToken} from provided values.\n+   * @param startContinuationToken\n+   * @param endContinuationToken\n+   * @param index\n+   * @param totalItems\n+   * @param azureTokenRequestId\n+   */\n+  public AzureFindToken(String startContinuationToken, String endContinuationToken, int index, int totalItems,\n+      String azureTokenRequestId) {\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.azureTokenRequestId = azureTokenRequestId;\n+    this.version = DEFAULT_VERSION;\n+  }\n+\n+  /**\n+   * Constructor to create a {@link AzureFindToken} with specified token values and specified version.\n+   * @param startContinuationToken\n+   * @param endContinuationToken\n+   * @param index\n+   * @param totalItems\n+   * @param azureTokenRequestId\n+   * @param version\n+   */\n+  public AzureFindToken(String startContinuationToken, String endContinuationToken, int index, int totalItems,\n+      String azureTokenRequestId, short version) {\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.azureTokenRequestId = azureTokenRequestId;\n+    this.version = version;\n+  }\n+\n+  /**\n+   * Deserialize {@link AzureFindToken} object from input stream.\n+   * @param inputStream {@link DataOutputStream} to deserialize from.\n+   * @return {@link AzureFindToken} object.\n+   * @throws IOException\n+   */\n+  public static AzureFindToken fromBytes(DataInputStream inputStream) throws IOException {\n+    short version = inputStream.readShort();\n+    String startContinuationToken = extractStringFromStream(inputStream);\n+    String endContinuationToken = extractStringFromStream(inputStream);\n+    int index = inputStream.readInt();\n+    int totalItems = inputStream.readInt();\n+    String azureTokenRequestId = extractStringFromStream(inputStream);\n+    return new AzureFindToken(startContinuationToken, endContinuationToken, index, totalItems, azureTokenRequestId,\n+        version);\n+  }\n+\n+  /**\n+   * Extract string from the {@link DataInputStream}.\n+   * @param inputStream {@link DataInputStream} to extract String from.\n+   * @return extracted String from {@code inputStream}.\n+   * @throws IOException\n+   */\n+  private static String extractStringFromStream(DataInputStream inputStream) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM3MTY5OA==", "bodyText": "Any reason this can't live in the azure subpackage?", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r370371698", "createdAt": "2020-01-23T21:45:50Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/AzureFindToken.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress in azure.\n+ */\n+public class AzureFindToken {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM3MTk4Mg==", "bodyText": "Also, please document that it is based on Cosmos change feed.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r370371982", "createdAt": "2020-01-23T21:46:29Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/AzureFindToken.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress in azure.\n+ */\n+public class AzureFindToken {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM3MTY5OA=="}, "originalCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM3MjM2Ng==", "bodyText": "hashcode method won't do much unless you also override equals(Object) method.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r370372366", "createdAt": "2020-01-23T21:47:25Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/AzureFindToken.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress in azure.\n+ */\n+public class AzureFindToken {\n+  private final String startContinuationToken;\n+  private final String endContinuationToken;\n+  private final int index;\n+  private final int totalItems;\n+  private final String azureTokenRequestId;\n+  private final short version;\n+\n+  public static short VERSION_0 = 0;\n+  public static short DEFAULT_VERSION = VERSION_0;\n+\n+  /**\n+   * Default constructor to create a {@link AzureFindToken} with uninitialized continuation token.\n+   */\n+  public AzureFindToken() {\n+    startContinuationToken = null;\n+    index = -1;\n+    endContinuationToken = null;\n+    totalItems = -1;\n+    azureTokenRequestId = null;\n+    version = DEFAULT_VERSION;\n+  }\n+\n+  /**\n+   * Create {@link AzureFindToken} from provided values.\n+   * @param startContinuationToken\n+   * @param endContinuationToken\n+   * @param index\n+   * @param totalItems\n+   * @param azureTokenRequestId\n+   */\n+  public AzureFindToken(String startContinuationToken, String endContinuationToken, int index, int totalItems,\n+      String azureTokenRequestId) {\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.azureTokenRequestId = azureTokenRequestId;\n+    this.version = DEFAULT_VERSION;\n+  }\n+\n+  /**\n+   * Constructor to create a {@link AzureFindToken} with specified token values and specified version.\n+   * @param startContinuationToken\n+   * @param endContinuationToken\n+   * @param index\n+   * @param totalItems\n+   * @param azureTokenRequestId\n+   * @param version\n+   */\n+  public AzureFindToken(String startContinuationToken, String endContinuationToken, int index, int totalItems,\n+      String azureTokenRequestId, short version) {\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.azureTokenRequestId = azureTokenRequestId;\n+    this.version = version;\n+  }\n+\n+  /**\n+   * Deserialize {@link AzureFindToken} object from input stream.\n+   * @param inputStream {@link DataOutputStream} to deserialize from.\n+   * @return {@link AzureFindToken} object.\n+   * @throws IOException\n+   */\n+  public static AzureFindToken fromBytes(DataInputStream inputStream) throws IOException {\n+    short version = inputStream.readShort();\n+    String startContinuationToken = extractStringFromStream(inputStream);\n+    String endContinuationToken = extractStringFromStream(inputStream);\n+    int index = inputStream.readInt();\n+    int totalItems = inputStream.readInt();\n+    String azureTokenRequestId = extractStringFromStream(inputStream);\n+    return new AzureFindToken(startContinuationToken, endContinuationToken, index, totalItems, azureTokenRequestId,\n+        version);\n+  }\n+\n+  /**\n+   * Extract string from the {@link DataInputStream}.\n+   * @param inputStream {@link DataInputStream} to extract String from.\n+   * @return extracted String from {@code inputStream}.\n+   * @throws IOException\n+   */\n+  private static String extractStringFromStream(DataInputStream inputStream) throws IOException {\n+    int size = inputStream.readInt();\n+    byte[] bytes = new byte[size];\n+    inputStream.read(bytes);\n+    return new String(bytes);\n+  }\n+\n+  public byte[] toBytes() {\n+    byte[] buf = new byte[size()];\n+    ByteBuffer bufWrap = ByteBuffer.wrap(buf);\n+    bufWrap.putShort(version);\n+    bufWrap.putInt(startContinuationToken.length());\n+    bufWrap.put(startContinuationToken.getBytes());\n+    bufWrap.putInt(endContinuationToken.length());\n+    bufWrap.put(endContinuationToken.getBytes());\n+    bufWrap.putInt(index);\n+    bufWrap.putInt(totalItems);\n+    bufWrap.putInt(azureTokenRequestId.length());\n+    bufWrap.put(azureTokenRequestId.getBytes());\n+    return buf;\n+  }\n+\n+  public short getVersion() {\n+    return version;\n+  }\n+\n+  public int size() {\n+    return Short.BYTES + 5 * Integer.BYTES + startContinuationToken.length() + endContinuationToken.length()\n+        + azureTokenRequestId.length();\n+  }\n+\n+  public boolean equals(AzureFindToken azureFindToken) {\n+    return azureFindToken.getVersion() == version && Utils.checkNullableStringEquals(\n+        azureFindToken.getStartContinuationToken(), startContinuationToken) && Utils.checkNullableStringEquals(\n+        azureFindToken.getEndContinuationToken(), endContinuationToken) && azureFindToken.getTotalItems() == totalItems\n+        && azureFindToken.getIndex() == index && Utils.checkNullableStringEquals(\n+        azureFindToken.getAzureTokenRequestId(), azureTokenRequestId);\n+  }\n+\n+  /**\n+   * Return startContinuationToken of the current token.\n+   * @return startContinuationToken.\n+   */\n+  public String getStartContinuationToken() {\n+    return startContinuationToken;\n+  }\n+\n+  /**\n+   * Return endContinuationToken of the current token.\n+   * @return endContinuationToken.\n+   */\n+  public String getEndContinuationToken() {\n+    return endContinuationToken;\n+  }\n+\n+  /**\n+   * Return index of the current token.\n+   * @return index.\n+   */\n+  public int getIndex() {\n+    return index;\n+  }\n+\n+  /**\n+   * Return totalitems in the current token.\n+   * @return totalitems.\n+   */\n+  public int getTotalItems() {\n+    return totalItems;\n+  }\n+\n+  /**\n+   * Return azureTokenRequestId of the current token.\n+   * @return azureTokenRequestId.\n+   */\n+  public String getAzureTokenRequestId() {\n+    return azureTokenRequestId;\n+  }\n+\n+  @Override\n+  public int hashCode() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "originalPosition": 190}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM3NTQ3MA==", "bodyText": "Do we need a separate class for this?", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r370375470", "createdAt": "2020-01-23T21:54:58Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureCloudDestination.java", "diffHunk": "@@ -62,25 +59,18 @@\n   private static final Logger logger = LoggerFactory.getLogger(AzureCloudDestination.class);\n   private static final String THRESHOLD_PARAM = \"@threshold\";\n   private static final String LIMIT_PARAM = \"@limit\";\n-  private static final String TIME_SINCE_PARAM = \"@timesince\";\n   private static final String BATCH_ID_QUERY_TEMPLATE = \"SELECT * FROM c WHERE c.id IN (%s)\";\n   static final int ID_QUERY_BATCH_SIZE = 1000;\n   private static final String DEAD_BLOBS_QUERY_TEMPLATE =\n       \"SELECT TOP \" + LIMIT_PARAM + \" * FROM c WHERE (c.\" + CloudBlobMetadata.FIELD_DELETION_TIME + \" BETWEEN 1 AND \"\n           + THRESHOLD_PARAM + \")\" + \" OR (c.\" + CloudBlobMetadata.FIELD_EXPIRATION_TIME + \" BETWEEN 1 AND \"\n           + THRESHOLD_PARAM + \")\" + \" ORDER BY c.\" + CloudBlobMetadata.FIELD_UPLOAD_TIME + \" ASC\";\n-  // Note: ideally would like to order by uploadTime and id, but Cosmos doesn't allow without composite index.\n-  // It is unlikely (but not impossible) for two blobs in same partition to have the same uploadTime (would have to\n-  // be multiple VCR's uploading same partition).  We track the lastBlobId in the CloudFindToken and skip it if\n-  // is returned in successive queries.\n-  private static final String ENTRIES_SINCE_QUERY_TEMPLATE =\n-      \"SELECT TOP \" + LIMIT_PARAM + \" * FROM c WHERE c.\" + CosmosDataAccessor.COSMOS_LAST_UPDATED_COLUMN + \" >= \"\n-          + TIME_SINCE_PARAM + \" ORDER BY c.\" + CosmosDataAccessor.COSMOS_LAST_UPDATED_COLUMN + \" ASC\";\n   private static final String SEPARATOR = \"-\";\n   private static final int findSinceQueryLimit = 1000;\n   private final AzureBlobDataAccessor azureBlobDataAccessor;\n   private final AsyncDocumentClient asyncDocumentClient;\n   private final CosmosDataAccessor cosmosDataAccessor;\n+  private final CosmosChangeFeed cosmosChangeFeed;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM4MDQwNA==", "bodyText": "I find this class confusing, especially this getter method.  It definitely feels like a cache, so maybe the class name should say so.  (I saw you mentioned it in the javadoc.) I originally thought this class would be used to actually run the change feed queries as opposed to delegating that to the data accessor and holding the results.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r370380404", "createdAt": "2020-01-23T22:06:56Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeed.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.AzureFindToken;\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+\n+/**\n+ * Class to handle and cache cosmos change feed.\n+ */\n+public class CosmosChangeFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String azureRequestId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param azureRequestId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String azureRequestId,\n+        List<CloudBlobMetadata> fetchedEntries) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.azureRequestId = azureRequestId;\n+      this.fetchedEntries = fetchedEntries;\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the azure request id.\n+     * @return azure request id.\n+     */\n+    String getAzureRequestId() {\n+      return azureRequestId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+  }\n+\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeed} object.\n+   * @param cacheSize default number of cachedEntries for each partition.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   */\n+  public CosmosChangeFeed(int cacheSize, CosmosDataAccessor cosmosDataAccessor) {\n+    this.defaultCacheSize = cacheSize;\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+  }\n+\n+  /**\n+   * Get next set of change feed entries for the specified partition, after the {@code azureFindToken}.\n+   * The number of entries is capped by maxEntriesSize.\n+   * This method creates a cache for change feed entries. If the {@code azureFindToken} is not valid,\n+   * or if all the items in the cache are consumed, then it queries cosmos for new entries.\n+   * @param azureFindToken {@link AzureFindToken} after which the next entries have to be returned.\n+   * @param results {@link List} of {@link CloudBlobMetadata} objects which will be populated by new entries.\n+   * @param maxEntriesSize maximum size of all the blobs returned in {@code results}\n+   * @param partitionId Partition for which change feed entries have to be returned.\n+   * @return updated {@link AzureFindToken} after processing the next set of entries.\n+   */\n+  public AzureFindToken getNextEntriesAndToken(AzureFindToken azureFindToken, List<CloudBlobMetadata> results,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM4MTA4Mw==", "bodyText": "The last 3-4 lines have a lot of duplicate code, please simplify it.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r370381083", "createdAt": "2020-01-23T22:08:32Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeed.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.AzureFindToken;\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+\n+/**\n+ * Class to handle and cache cosmos change feed.\n+ */\n+public class CosmosChangeFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String azureRequestId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param azureRequestId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String azureRequestId,\n+        List<CloudBlobMetadata> fetchedEntries) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.azureRequestId = azureRequestId;\n+      this.fetchedEntries = fetchedEntries;\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the azure request id.\n+     * @return azure request id.\n+     */\n+    String getAzureRequestId() {\n+      return azureRequestId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+  }\n+\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeed} object.\n+   * @param cacheSize default number of cachedEntries for each partition.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   */\n+  public CosmosChangeFeed(int cacheSize, CosmosDataAccessor cosmosDataAccessor) {\n+    this.defaultCacheSize = cacheSize;\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+  }\n+\n+  /**\n+   * Get next set of change feed entries for the specified partition, after the {@code azureFindToken}.\n+   * The number of entries is capped by maxEntriesSize.\n+   * This method creates a cache for change feed entries. If the {@code azureFindToken} is not valid,\n+   * or if all the items in the cache are consumed, then it queries cosmos for new entries.\n+   * @param azureFindToken {@link AzureFindToken} after which the next entries have to be returned.\n+   * @param results {@link List} of {@link CloudBlobMetadata} objects which will be populated by new entries.\n+   * @param maxEntriesSize maximum size of all the blobs returned in {@code results}\n+   * @param partitionId Partition for which change feed entries have to be returned.\n+   * @return updated {@link AzureFindToken} after processing the next set of entries.\n+   */\n+  public AzureFindToken getNextEntriesAndToken(AzureFindToken azureFindToken, List<CloudBlobMetadata> results,\n+      long maxEntriesSize, String partitionId) throws DocumentClientException {\n+    int index = azureFindToken.getIndex();\n+    if (!changeFeedCache.containsKey(partitionId) || !isCacheValid(partitionId, azureFindToken)) {\n+      populateChangeFeedCache(partitionId, azureFindToken.getStartContinuationToken());\n+      index = 0;\n+    }\n+\n+    long resultSize = 0;\n+    while (true) {\n+      if (index < changeFeedCache.get(partitionId).getFetchedEntries().size()) {\n+        if (resultSize + changeFeedCache.get(partitionId).getFetchedEntries().get(index).getSize() < maxEntriesSize) {\n+          results.add(changeFeedCache.get(partitionId).getFetchedEntries().get(index));\n+          resultSize = resultSize + changeFeedCache.get(partitionId).getFetchedEntries().get(index).getSize();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM4MTY4MA==", "bodyText": "Don't you have an equals method that does the same check?", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r370381680", "createdAt": "2020-01-23T22:10:01Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeed.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.AzureFindToken;\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+\n+/**\n+ * Class to handle and cache cosmos change feed.\n+ */\n+public class CosmosChangeFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String azureRequestId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param azureRequestId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String azureRequestId,\n+        List<CloudBlobMetadata> fetchedEntries) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.azureRequestId = azureRequestId;\n+      this.fetchedEntries = fetchedEntries;\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the azure request id.\n+     * @return azure request id.\n+     */\n+    String getAzureRequestId() {\n+      return azureRequestId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+  }\n+\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeed} object.\n+   * @param cacheSize default number of cachedEntries for each partition.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   */\n+  public CosmosChangeFeed(int cacheSize, CosmosDataAccessor cosmosDataAccessor) {\n+    this.defaultCacheSize = cacheSize;\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+  }\n+\n+  /**\n+   * Get next set of change feed entries for the specified partition, after the {@code azureFindToken}.\n+   * The number of entries is capped by maxEntriesSize.\n+   * This method creates a cache for change feed entries. If the {@code azureFindToken} is not valid,\n+   * or if all the items in the cache are consumed, then it queries cosmos for new entries.\n+   * @param azureFindToken {@link AzureFindToken} after which the next entries have to be returned.\n+   * @param results {@link List} of {@link CloudBlobMetadata} objects which will be populated by new entries.\n+   * @param maxEntriesSize maximum size of all the blobs returned in {@code results}\n+   * @param partitionId Partition for which change feed entries have to be returned.\n+   * @return updated {@link AzureFindToken} after processing the next set of entries.\n+   */\n+  public AzureFindToken getNextEntriesAndToken(AzureFindToken azureFindToken, List<CloudBlobMetadata> results,\n+      long maxEntriesSize, String partitionId) throws DocumentClientException {\n+    int index = azureFindToken.getIndex();\n+    if (!changeFeedCache.containsKey(partitionId) || !isCacheValid(partitionId, azureFindToken)) {\n+      populateChangeFeedCache(partitionId, azureFindToken.getStartContinuationToken());\n+      index = 0;\n+    }\n+\n+    long resultSize = 0;\n+    while (true) {\n+      if (index < changeFeedCache.get(partitionId).getFetchedEntries().size()) {\n+        if (resultSize + changeFeedCache.get(partitionId).getFetchedEntries().get(index).getSize() < maxEntriesSize) {\n+          results.add(changeFeedCache.get(partitionId).getFetchedEntries().get(index));\n+          resultSize = resultSize + changeFeedCache.get(partitionId).getFetchedEntries().get(index).getSize();\n+          index++;\n+        } else {\n+          if (resultSize == 0) {\n+            results.add(changeFeedCache.get(partitionId).getFetchedEntries().get(index));\n+            index++;\n+          }\n+          break;\n+        }\n+      } else {\n+        populateChangeFeedCache(partitionId, azureFindToken.getEndContinuationToken());\n+        if (cacheEmpty(partitionId)) {\n+          // this means that there are no new changes\n+          break;\n+        }\n+        index = 0;\n+      }\n+    }\n+\n+    return new AzureFindToken(changeFeedCache.get(partitionId).getStartContinuationToken(),\n+        changeFeedCache.get(partitionId).getEndContinuationToken(), index,\n+        changeFeedCache.get(partitionId).getFetchedEntries().size(),\n+        changeFeedCache.get(partitionId).getAzureRequestId());\n+  }\n+\n+  /**\n+   * Check is the cache is valid for the {@code azureFindToken} provided.\n+   * @param partitionId partition of the {@code azureFindToken}.\n+   * @param azureFindToken {@link AzureFindToken} object.\n+   * @return true is cache is valid. false otherwise.\n+   */\n+  private boolean isCacheValid(String partitionId, AzureFindToken azureFindToken) {\n+    ChangeFeedCacheEntry changeFeedCacheEntry = changeFeedCache.get(partitionId);\n+    return Utils.checkNullableStringEquals(azureFindToken.getAzureTokenRequestId(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM4Mjk4Mg==", "bodyText": "Lines need formatting.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r370382982", "createdAt": "2020-01-23T22:13:00Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -155,6 +157,40 @@ void testConnectivity() {\n     }\n   }\n \n+  /**\n+   * Query cosmos change feed to get the next set of {@code CloudBlobMetadata} objects in specified {@code partitionPath}\n+   * after {@code requestContinationToken}, capped by specified {@code maxFeedSize} representing the max number of items to\n+   * be queried from the change feed.\n+   * @param requestContinationToken Continuation token after which change feed is requested.\n+   * @param maxFeedSize max item count to be requested in the feed query.\n+   * @param changeFeed {@link CloudBlobMetadata} {@code List} to be populated with the next set of entries returned by change feed query.\n+   * @param partitionPath partition for which the change feed is requested.\n+   * @return next continuation token.\n+   * @throws DocumentClientException\n+   */\n+  public String queryChangeFeed(String requestContinationToken, int maxFeedSize, List<CloudBlobMetadata> changeFeed,\n+      String partitionPath) throws DocumentClientException {\n+    ChangeFeedOptions changeFeedOptions = new ChangeFeedOptions();\n+    changeFeedOptions.setPartitionKey(new PartitionKey(partitionPath));\n+    changeFeedOptions.setMaxItemCount(maxFeedSize);\n+    if (requestContinationToken == null) {\n+      changeFeedOptions.setStartFromBeginning(true);\n+    } else {\n+      changeFeedOptions.setRequestContinuation(requestContinationToken);\n+    }\n+    try {\n+      FeedResponse<Document> feedResponse =\n+          asyncDocumentClient.queryDocumentChangeFeed(cosmosCollectionLink, changeFeedOptions).limit(1).toBlocking().single();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM4MzI5MQ==", "bodyText": "Is this a TODO?", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r370383291", "createdAt": "2020-01-23T22:13:47Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/CloudBlobStoreTest.java", "diffHunk": "@@ -297,33 +297,40 @@ public void testFindEntriesSince() throws Exception {\n     long blobSize = 200000;\n     int numBlobsFound = 5;\n     List<CloudBlobMetadata> metadataList = generateMetadataList(startTime, blobSize, numBlobsFound);\n-    when(dest.findEntriesSince(anyString(), any(CloudFindToken.class), anyLong())).thenReturn(metadataList);\n+    CloudFindToken cloudFindToken = new CloudFindToken(blobSize * numBlobsFound,\n+        new AzureFindToken(\"start\", \"end\", 0, numBlobsFound, UUID.randomUUID().toString()));\n+    //create a list of 10 blobs with total size less than maxSize, and return it as part of query ChangeFeed\n+    when(dest.findEntriesSince(anyString(), any(CloudFindToken.class), anyLong(), anyList())).thenReturn(\n+        cloudFindToken);\n     CloudFindToken startToken = new CloudFindToken();\n     FindInfo findInfo = store.findEntriesSince(startToken, maxTotalSize);\n-    assertEquals(numBlobsFound, findInfo.getMessageEntries().size());\n     CloudFindToken outputToken = (CloudFindToken) findInfo.getFindToken();\n-    assertEquals(startTime + numBlobsFound - 1, outputToken.getLastUpdateTime());\n     assertEquals(blobSize * numBlobsFound, outputToken.getBytesRead());\n-    assertEquals(Collections.singletonList(metadataList.get(numBlobsFound - 1).getId()),\n-        new ArrayList<String>(outputToken.getLastUpdateTimeReadBlobIds()));\n+    assertEquals(numBlobsFound, outputToken.getAzureFindToken().getTotalItems());\n+    assertEquals(0, outputToken.getAzureFindToken().getIndex());\n \n     // 2) call find with new token, return more data including lastBlob, verify token updated\n     startTime += 1000;\n     metadataList = generateMetadataList(startTime, blobSize, numBlobsFound);\n-    when(dest.findEntriesSince(anyString(), any(CloudFindToken.class), anyLong())).thenReturn(metadataList);\n+    cloudFindToken = new CloudFindToken(blobSize * 2 * numBlobsFound,\n+        new AzureFindToken(\"start2\", \"end2\", 0, numBlobsFound, UUID.randomUUID().toString()));\n+    when(dest.findEntriesSince(anyString(), any(CloudFindToken.class), anyLong(), anyList())).thenReturn(\n+        cloudFindToken);\n     findInfo = store.findEntriesSince(outputToken, maxTotalSize);\n     outputToken = (CloudFindToken) findInfo.getFindToken();\n-    assertEquals(startTime + numBlobsFound - 1, outputToken.getLastUpdateTime());\n     assertEquals(blobSize * 2 * numBlobsFound, outputToken.getBytesRead());\n-    assertEquals(Collections.singletonList(metadataList.get(numBlobsFound - 1).getId()),\n-        new ArrayList<String>(outputToken.getLastUpdateTimeReadBlobIds()));\n+    assertEquals(numBlobsFound, outputToken.getAzureFindToken().getTotalItems());\n+    assertEquals(0, outputToken.getAzureFindToken().getIndex());\n \n     // 3) call find with new token, no more data, verify token unchanged\n-    when(dest.findEntriesSince(anyString(), any(CloudFindToken.class), anyLong())).thenReturn(Collections.emptyList());\n+    metadataList = Collections.emptyList();\n+    when(dest.findEntriesSince(anyString(), any(CloudFindToken.class), anyLong(), anyList())).thenReturn(outputToken);\n     findInfo = store.findEntriesSince(outputToken, maxTotalSize);\n     assertTrue(findInfo.getMessageEntries().isEmpty());\n     FindToken finalToken = findInfo.getFindToken();\n     assertEquals(outputToken, finalToken);\n+\n+    // call with new find token, and add total blobs larger than maxSize, and see that all the blobs are exhausted only after 3 tries", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "originalPosition": 61}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUyNDQxOTA1", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-352441905", "createdAt": "2020-02-03T17:48:30Z", "commit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxNzo0ODozMFrOFk6LEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxNzo0ODozMFrOFk6LEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI0NjE2Mg==", "bodyText": "Use Utils.serializeString for all of the size prefixed string type things here. It handles edge cases around character encodings.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r374246162", "createdAt": "2020-02-03T17:48:30Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/AzureFindToken.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress in azure.\n+ */\n+public class AzureFindToken {\n+  private final String startContinuationToken;\n+  private final String endContinuationToken;\n+  private final int index;\n+  private final int totalItems;\n+  private final String azureTokenRequestId;\n+  private final short version;\n+\n+  public static short VERSION_0 = 0;\n+  public static short DEFAULT_VERSION = VERSION_0;\n+\n+  /**\n+   * Default constructor to create a {@link AzureFindToken} with uninitialized continuation token.\n+   */\n+  public AzureFindToken() {\n+    startContinuationToken = null;\n+    index = -1;\n+    endContinuationToken = null;\n+    totalItems = -1;\n+    azureTokenRequestId = null;\n+    version = DEFAULT_VERSION;\n+  }\n+\n+  /**\n+   * Create {@link AzureFindToken} from provided values.\n+   * @param startContinuationToken\n+   * @param endContinuationToken\n+   * @param index\n+   * @param totalItems\n+   * @param azureTokenRequestId\n+   */\n+  public AzureFindToken(String startContinuationToken, String endContinuationToken, int index, int totalItems,\n+      String azureTokenRequestId) {\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.azureTokenRequestId = azureTokenRequestId;\n+    this.version = DEFAULT_VERSION;\n+  }\n+\n+  /**\n+   * Constructor to create a {@link AzureFindToken} with specified token values and specified version.\n+   * @param startContinuationToken\n+   * @param endContinuationToken\n+   * @param index\n+   * @param totalItems\n+   * @param azureTokenRequestId\n+   * @param version\n+   */\n+  public AzureFindToken(String startContinuationToken, String endContinuationToken, int index, int totalItems,\n+      String azureTokenRequestId, short version) {\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.azureTokenRequestId = azureTokenRequestId;\n+    this.version = version;\n+  }\n+\n+  /**\n+   * Deserialize {@link AzureFindToken} object from input stream.\n+   * @param inputStream {@link DataOutputStream} to deserialize from.\n+   * @return {@link AzureFindToken} object.\n+   * @throws IOException\n+   */\n+  public static AzureFindToken fromBytes(DataInputStream inputStream) throws IOException {\n+    short version = inputStream.readShort();\n+    String startContinuationToken = extractStringFromStream(inputStream);\n+    String endContinuationToken = extractStringFromStream(inputStream);\n+    int index = inputStream.readInt();\n+    int totalItems = inputStream.readInt();\n+    String azureTokenRequestId = extractStringFromStream(inputStream);\n+    return new AzureFindToken(startContinuationToken, endContinuationToken, index, totalItems, azureTokenRequestId,\n+        version);\n+  }\n+\n+  /**\n+   * Extract string from the {@link DataInputStream}.\n+   * @param inputStream {@link DataInputStream} to extract String from.\n+   * @return extracted String from {@code inputStream}.\n+   * @throws IOException\n+   */\n+  private static String extractStringFromStream(DataInputStream inputStream) throws IOException {\n+    int size = inputStream.readInt();\n+    byte[] bytes = new byte[size];\n+    inputStream.read(bytes);\n+    return new String(bytes);\n+  }\n+\n+  public byte[] toBytes() {\n+    byte[] buf = new byte[size()];\n+    ByteBuffer bufWrap = ByteBuffer.wrap(buf);\n+    bufWrap.putShort(version);\n+    bufWrap.putInt(startContinuationToken.length());\n+    bufWrap.put(startContinuationToken.getBytes());\n+    bufWrap.putInt(endContinuationToken.length());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "originalPosition": 123}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUyNDQ2MDAz", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-352446003", "createdAt": "2020-02-03T17:55:08Z", "commit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxNzo1NTowOFrOFk6Xpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxNzo1NzowNFrOFk6bTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI0OTM4Mg==", "bodyText": "Let's call it CosmosChangeFeedToken? We can change the legacy token name to CosmosTimestampToken", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r374249382", "createdAt": "2020-02-03T17:55:08Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/AzureFindToken.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress in azure.\n+ */\n+public class AzureFindToken {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM3MTY5OA=="}, "originalCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI1MDMxOA==", "bodyText": "Maybe implement the general Object.equals method? You just need to add null handling and type checking to this method.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r374250318", "createdAt": "2020-02-03T17:57:04Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/AzureFindToken.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress in azure.\n+ */\n+public class AzureFindToken {\n+  private final String startContinuationToken;\n+  private final String endContinuationToken;\n+  private final int index;\n+  private final int totalItems;\n+  private final String azureTokenRequestId;\n+  private final short version;\n+\n+  public static short VERSION_0 = 0;\n+  public static short DEFAULT_VERSION = VERSION_0;\n+\n+  /**\n+   * Default constructor to create a {@link AzureFindToken} with uninitialized continuation token.\n+   */\n+  public AzureFindToken() {\n+    startContinuationToken = null;\n+    index = -1;\n+    endContinuationToken = null;\n+    totalItems = -1;\n+    azureTokenRequestId = null;\n+    version = DEFAULT_VERSION;\n+  }\n+\n+  /**\n+   * Create {@link AzureFindToken} from provided values.\n+   * @param startContinuationToken\n+   * @param endContinuationToken\n+   * @param index\n+   * @param totalItems\n+   * @param azureTokenRequestId\n+   */\n+  public AzureFindToken(String startContinuationToken, String endContinuationToken, int index, int totalItems,\n+      String azureTokenRequestId) {\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.azureTokenRequestId = azureTokenRequestId;\n+    this.version = DEFAULT_VERSION;\n+  }\n+\n+  /**\n+   * Constructor to create a {@link AzureFindToken} with specified token values and specified version.\n+   * @param startContinuationToken\n+   * @param endContinuationToken\n+   * @param index\n+   * @param totalItems\n+   * @param azureTokenRequestId\n+   * @param version\n+   */\n+  public AzureFindToken(String startContinuationToken, String endContinuationToken, int index, int totalItems,\n+      String azureTokenRequestId, short version) {\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.azureTokenRequestId = azureTokenRequestId;\n+    this.version = version;\n+  }\n+\n+  /**\n+   * Deserialize {@link AzureFindToken} object from input stream.\n+   * @param inputStream {@link DataOutputStream} to deserialize from.\n+   * @return {@link AzureFindToken} object.\n+   * @throws IOException\n+   */\n+  public static AzureFindToken fromBytes(DataInputStream inputStream) throws IOException {\n+    short version = inputStream.readShort();\n+    String startContinuationToken = extractStringFromStream(inputStream);\n+    String endContinuationToken = extractStringFromStream(inputStream);\n+    int index = inputStream.readInt();\n+    int totalItems = inputStream.readInt();\n+    String azureTokenRequestId = extractStringFromStream(inputStream);\n+    return new AzureFindToken(startContinuationToken, endContinuationToken, index, totalItems, azureTokenRequestId,\n+        version);\n+  }\n+\n+  /**\n+   * Extract string from the {@link DataInputStream}.\n+   * @param inputStream {@link DataInputStream} to extract String from.\n+   * @return extracted String from {@code inputStream}.\n+   * @throws IOException\n+   */\n+  private static String extractStringFromStream(DataInputStream inputStream) throws IOException {\n+    int size = inputStream.readInt();\n+    byte[] bytes = new byte[size];\n+    inputStream.read(bytes);\n+    return new String(bytes);\n+  }\n+\n+  public byte[] toBytes() {\n+    byte[] buf = new byte[size()];\n+    ByteBuffer bufWrap = ByteBuffer.wrap(buf);\n+    bufWrap.putShort(version);\n+    bufWrap.putInt(startContinuationToken.length());\n+    bufWrap.put(startContinuationToken.getBytes());\n+    bufWrap.putInt(endContinuationToken.length());\n+    bufWrap.put(endContinuationToken.getBytes());\n+    bufWrap.putInt(index);\n+    bufWrap.putInt(totalItems);\n+    bufWrap.putInt(azureTokenRequestId.length());\n+    bufWrap.put(azureTokenRequestId.getBytes());\n+    return buf;\n+  }\n+\n+  public short getVersion() {\n+    return version;\n+  }\n+\n+  public int size() {\n+    return Short.BYTES + 5 * Integer.BYTES + startContinuationToken.length() + endContinuationToken.length()\n+        + azureTokenRequestId.length();\n+  }\n+\n+  public boolean equals(AzureFindToken azureFindToken) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb"}, "originalPosition": 141}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "12a4c8c06288789028af6dd3db349a49e02c20fb", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/12a4c8c06288789028af6dd3db349a49e02c20fb", "committedDate": "2020-01-21T23:31:13Z", "message": "Tests for change feed implementation.\nAll tests passing."}, "afterCommit": {"oid": "6a5fef438fe9adc006bbe5a1ba05ac6a4ce54af8", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/6a5fef438fe9adc006bbe5a1ba05ac6a4ce54af8", "committedDate": "2020-02-05T01:13:08Z", "message": "rebase and rebase fixes"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c7e79dedddc328cc7750aeb3f1646d1ed46e0fab", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/c7e79dedddc328cc7750aeb3f1646d1ed46e0fab", "committedDate": "2020-02-05T17:26:05Z", "message": "Add back update time based token and its tests."}, "afterCommit": {"oid": "4ac9e051af2240bc31bf1c236f1daabe299d7481", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/4ac9e051af2240bc31bf1c236f1daabe299d7481", "committedDate": "2020-02-05T17:27:58Z", "message": "Add back update time based token and its tests."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d75d63e9ec781ba6d5cefba1eaf735fab0d8f841", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/d75d63e9ec781ba6d5cefba1eaf735fab0d8f841", "committedDate": "2020-02-05T22:25:53Z", "message": "cleanup"}, "afterCommit": {"oid": "048757f7467c8c1248242e0b402a88e38723e3de", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/048757f7467c8c1248242e0b402a88e38723e3de", "committedDate": "2020-02-06T02:29:15Z", "message": "Add missing javadocs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU1MzQ4NjM0", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-355348634", "createdAt": "2020-02-07T18:25:58Z", "commit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxODoyNTo1OFrOFnGE5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxOToxNjoyOFrOFnHeKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzODM0Mw==", "bodyText": "Minor: in azure -> using Cosmos change feed.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376538343", "createdAt": "2020-02-07T18:25:58Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedFindToken.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.replication.FindTokenType;\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress in azure.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzOTAwMw==", "bodyText": "Minor: no need to store this in class, just return it in getType methods.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376539003", "createdAt": "2020-02-07T18:27:28Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedFindToken.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.replication.FindTokenType;\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress in azure.\n+ */\n+public class CosmosChangeFeedFindToken implements FindToken {\n+  private final short version;\n+  private final long bytesRead;\n+  private final FindTokenType findTokenType = FindTokenType.CloudBased;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzOTY2OA==", "bodyText": "Can call the second constructor passing default version.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376539668", "createdAt": "2020-02-07T18:28:50Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedFindToken.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.replication.FindTokenType;\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress in azure.\n+ */\n+public class CosmosChangeFeedFindToken implements FindToken {\n+  private final short version;\n+  private final long bytesRead;\n+  private final FindTokenType findTokenType = FindTokenType.CloudBased;\n+  private final String startContinuationToken;\n+  private final String endContinuationToken;\n+  private final int index;\n+  private final int totalItems;\n+  private final String azureTokenRequestId;\n+\n+  private final static short VERSION_0 = 0;\n+  private final static short DEFAULT_VERSION = VERSION_0;\n+\n+  /**\n+   * Default constructor to create a {@link CosmosChangeFeedFindToken} with uninitialized continuation token.\n+   */\n+  public CosmosChangeFeedFindToken() {\n+    version = DEFAULT_VERSION;\n+    bytesRead = 0;\n+    startContinuationToken = \"\";\n+    index = -1;\n+    endContinuationToken = \"\";\n+    totalItems = -1;\n+    azureTokenRequestId = \"\";\n+  }\n+\n+  /**\n+   * Create {@link CosmosChangeFeedFindToken} from provided values.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from cosmos.\n+   * @param endContinuationToken end token from cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param azureTokenRequestId request id of the change feed query.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String azureTokenRequestId) {\n+    this.version = DEFAULT_VERSION;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU0MDUxOQ==", "bodyText": "Why is this needed in addition to getType()?", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376540519", "createdAt": "2020-02-07T18:30:42Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedFindToken.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.replication.FindTokenType;\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress in azure.\n+ */\n+public class CosmosChangeFeedFindToken implements FindToken {\n+  private final short version;\n+  private final long bytesRead;\n+  private final FindTokenType findTokenType = FindTokenType.CloudBased;\n+  private final String startContinuationToken;\n+  private final String endContinuationToken;\n+  private final int index;\n+  private final int totalItems;\n+  private final String azureTokenRequestId;\n+\n+  private final static short VERSION_0 = 0;\n+  private final static short DEFAULT_VERSION = VERSION_0;\n+\n+  /**\n+   * Default constructor to create a {@link CosmosChangeFeedFindToken} with uninitialized continuation token.\n+   */\n+  public CosmosChangeFeedFindToken() {\n+    version = DEFAULT_VERSION;\n+    bytesRead = 0;\n+    startContinuationToken = \"\";\n+    index = -1;\n+    endContinuationToken = \"\";\n+    totalItems = -1;\n+    azureTokenRequestId = \"\";\n+  }\n+\n+  /**\n+   * Create {@link CosmosChangeFeedFindToken} from provided values.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from cosmos.\n+   * @param endContinuationToken end token from cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param azureTokenRequestId request id of the change feed query.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String azureTokenRequestId) {\n+    this.version = DEFAULT_VERSION;\n+    this.bytesRead = bytesRead;\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.azureTokenRequestId = azureTokenRequestId;\n+  }\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedFindToken} with specified token values and specified version.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from cosmos.\n+   * @param endContinuationToken end token from cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param azureTokenRequestId request id of the change feed query.\n+   * @param version token version.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String azureTokenRequestId, short version) {\n+    this.version = version;\n+    this.bytesRead = bytesRead;\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.azureTokenRequestId = azureTokenRequestId;\n+  }\n+\n+  /**\n+   * Deserialize {@link CosmosChangeFeedFindToken} object from input stream.\n+   * @param inputStream {@link DataOutputStream} to deserialize from.\n+   * @return {@link CosmosChangeFeedFindToken} object.\n+   * @throws IOException\n+   */\n+  public static CosmosChangeFeedFindToken fromBytes(DataInputStream inputStream) throws IOException {\n+    CosmosChangeFeedFindToken cosmosChangeFeedFindToken;\n+    DataInputStream stream = new DataInputStream(inputStream);\n+    short version = stream.readShort();\n+    switch (version) {\n+      case VERSION_0:\n+        FindTokenType type = FindTokenType.values()[stream.readShort()];\n+        if (type != FindTokenType.CloudBased) {\n+          throw new IllegalArgumentException(\n+              String.format(\"Invalid token type %s found while deserialization. Expected %s.\", type,\n+                  FindTokenType.CloudBased));\n+        }\n+        long bytesRead = stream.readLong();\n+        String startContinuationToken = Utils.readIntString(inputStream);\n+        String endContinuationToken = Utils.readIntString(inputStream);\n+        int index = inputStream.readInt();\n+        int totalItems = inputStream.readInt();\n+        String azureTokenRequestId = Utils.readIntString(inputStream);\n+        cosmosChangeFeedFindToken =\n+            new CosmosChangeFeedFindToken(bytesRead, startContinuationToken, endContinuationToken, index, totalItems,\n+                azureTokenRequestId, version);\n+        break;\n+      default:\n+        throw new IllegalStateException(\"Unknown version of cloud token: \" + version);\n+    }\n+    return cosmosChangeFeedFindToken;\n+  }\n+\n+  /**\n+   * Serialize {@link CosmosChangeFeedFindToken} to byte array.\n+   * @return serialized byte array.\n+   */\n+  public byte[] toBytes() {\n+    byte[] buf = new byte[size()];\n+    ByteBuffer bufWrap = ByteBuffer.wrap(buf);\n+    bufWrap.putShort(getVersion());\n+    bufWrap.putShort((short) getType().ordinal());\n+    bufWrap.putLong(getBytesRead());\n+    Utils.serializeNullableString(bufWrap, startContinuationToken);\n+    Utils.serializeNullableString(bufWrap, endContinuationToken);\n+    bufWrap.putInt(index);\n+    bufWrap.putInt(totalItems);\n+    Utils.serializeNullableString(bufWrap, azureTokenRequestId);\n+    return buf;\n+  }\n+\n+  /**\n+   * Calculate size of the token.\n+   * @return size of the token.\n+   */\n+  public int size() {\n+    return 2 * Short.BYTES + Long.BYTES + 5 * Integer.BYTES + getNullableStringLength(startContinuationToken)\n+        + getNullableStringLength(endContinuationToken) + getNullableStringLength(azureTokenRequestId);\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    CosmosChangeFeedFindToken cosmosChangeFeedFindToken = (CosmosChangeFeedFindToken) o;\n+    return cosmosChangeFeedFindToken.getVersion() == getVersion()\n+        && cosmosChangeFeedFindToken.getBytesRead() == getBytesRead() && checkNullableStringEquals(\n+        cosmosChangeFeedFindToken.getStartContinuationToken(), startContinuationToken) && checkNullableStringEquals(\n+        cosmosChangeFeedFindToken.getEndContinuationToken(), endContinuationToken)\n+        && cosmosChangeFeedFindToken.getTotalItems() == totalItems && cosmosChangeFeedFindToken.getIndex() == index\n+        && checkNullableStringEquals(cosmosChangeFeedFindToken.getAzureTokenRequestId(), azureTokenRequestId);\n+  }\n+\n+  /**\n+   * Return startContinuationToken of the current token.\n+   * @return startContinuationToken.\n+   */\n+  public String getStartContinuationToken() {\n+    return startContinuationToken;\n+  }\n+\n+  public String getEndContinuationToken() {\n+    return endContinuationToken;\n+  }\n+\n+  /**\n+   * Return index of the current token.\n+   * @return index.\n+   */\n+  public int getIndex() {\n+    return index;\n+  }\n+\n+  /**\n+   * Return totalitems in the current token.\n+   * @return totalitems.\n+   */\n+  public int getTotalItems() {\n+    return totalItems;\n+  }\n+\n+  @Override\n+  public long getBytesRead() {\n+    return bytesRead;\n+  }\n+\n+  @Override\n+  public FindTokenType getType() {\n+    return findTokenType;\n+  }\n+\n+  @Override\n+  public short getVersion() {\n+    return version;\n+  }\n+\n+  public FindTokenType getFindTokenType() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 219}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU0MTU4OQ==", "bodyText": "Looks like the file needs formatting?", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376541589", "createdAt": "2020-02-07T18:33:00Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureReplicationFeed.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.replication.FindToken;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.List;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from azure, and a bookmark in form of {@link FindToken}.\n+ */\n+public interface AzureReplicationFeed {\n+\n+  /**\n+   * Populate the next set of {@link CloudBlobMetadata} objects in {@code nextEntries} of specified partition {@code partitionPath}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU0Mjc2NQ==", "bodyText": "Thinking out loud, wondering if we can return a pair struct holding new FindToken and entries list, rather than requiring caller to supply an empty non-null list.  (I wish Java had python's multivariate returns.)", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376542765", "createdAt": "2020-02-07T18:35:40Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureReplicationFeed.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.replication.FindToken;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.List;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from azure, and a bookmark in form of {@link FindToken}.\n+ */\n+public interface AzureReplicationFeed {\n+\n+  /**\n+   * Populate the next set of {@link CloudBlobMetadata} objects in {@code nextEntries} of specified partition {@code partitionPath}\n+   * from the specified {@link FindToken} such that total size of all blobs in the entries are less or equal to {@code maxTotalSizeOfEntries}.\n+   * This method should return at least one blob, if exists, after {@code curfindToken}, irrespective of {@code maxTotalSizeOfEntries} requirement.\n+   * @param curfindToken current {@link FindToken} object that acts as a bookmark to return blobs after.\n+   * @param nextEntries {@link List} to populate next {@link CloudBlobMetadata} objects in.\n+   * @param maxTotalSizeOfEntries max total size of all the {@link CloudBlobMetadata} objects returned.\n+   * @param partitionPath partition of the blobs.\n+   * @return Updated {@link FindToken} object which can act as a bookmark for subsequent requests.\n+   */\n+  FindToken getNextEntriesAndUpdatedToken(FindToken curfindToken, List<CloudBlobMetadata> nextEntries,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU0MzU5MQ==", "bodyText": "Minor: could make this a subclass of AzureReplicationFeed to reduce file clutter.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376543591", "createdAt": "2020-02-07T18:37:33Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureReplicationFeedType.java", "diffHunk": "@@ -0,0 +1,25 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+/**\n+ * Type of cloud replication feed.\n+ */\n+public enum AzureReplicationFeedType {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU0Mzk3NQ==", "bodyText": "Minor: please capitalize Cosmos and Azure in comments throughout.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376543975", "createdAt": "2020-02-07T18:38:20Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureReplicationFeedType.java", "diffHunk": "@@ -0,0 +1,25 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+/**\n+ * Type of cloud replication feed.\n+ */\n+public enum AzureReplicationFeedType {\n+  /** Replication feed is obtained from cosmos change feed api */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU0NTI2OQ==", "bodyText": "add javadoc comment that map is keyed by partitionId", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376545269", "createdAt": "2020-02-07T18:41:13Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from azure and corresponding {@link FindToken}\n+ * using cosmos change feed apis.\n+ */\n+public class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String azureRequestId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param azureRequestId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String azureRequestId,\n+        List<CloudBlobMetadata> fetchedEntries) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.azureRequestId = azureRequestId;\n+      this.fetchedEntries = fetchedEntries;\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the azure request id.\n+     * @return azure request id.\n+     */\n+    String getAzureRequestId() {\n+      return azureRequestId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+  }\n+\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU0ODI0MA==", "bodyText": "Is it necessary to check all the members?  I thought the requestId guaranteed it was from the same consumer.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376548240", "createdAt": "2020-02-07T18:47:30Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from azure and corresponding {@link FindToken}\n+ * using cosmos change feed apis.\n+ */\n+public class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String azureRequestId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param azureRequestId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String azureRequestId,\n+        List<CloudBlobMetadata> fetchedEntries) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.azureRequestId = azureRequestId;\n+      this.fetchedEntries = fetchedEntries;\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the azure request id.\n+     * @return azure request id.\n+     */\n+    String getAzureRequestId() {\n+      return azureRequestId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+  }\n+\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedBasedReplicationFeed} object.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   * @param azureMetrics{@link {@link AzureMetrics} object.\n+   */\n+  public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccessor, AzureMetrics azureMetrics) {\n+    this.defaultCacheSize = AzureCloudDestination.getFindSinceQueryLimit();\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.azureMetrics = azureMetrics;\n+  }\n+\n+  /**\n+   * Get next set of change feed entries for the specified partition, after the {@code cosmosChangeFeedFindToken}.\n+   * The number of entries is capped by maxEntriesSize.\n+   * This method creates a cache for change feed entries. If the {@code cosmosChangeFeedFindToken} is not valid,\n+   * or if all the items in the cache are consumed, then it queries cosmos for new entries.\n+   * @param cosmosChangeFeedFindToken {@link CosmosChangeFeedFindToken} after which the next entries have to be returned.\n+   * @param results {@link List} of {@link CloudBlobMetadata} objects which will be populated by new entries.\n+   * @param maxEntriesSize maximum size of all the blobs returned in {@code results}\n+   * @param partitionId Partition for which change feed entries have to be returned.\n+   * @return updated {@link CosmosChangeFeedFindToken} after processing the next set of entries.\n+   */\n+  public CosmosChangeFeedFindToken getNextEntriesAndToken(CosmosChangeFeedFindToken cosmosChangeFeedFindToken,\n+      List<CloudBlobMetadata> results, long maxEntriesSize, String partitionId) throws DocumentClientException {\n+    int index = cosmosChangeFeedFindToken.getIndex();\n+    if (!changeFeedCache.containsKey(partitionId) || !isCacheValid(partitionId, cosmosChangeFeedFindToken)) {\n+      populateChangeFeedCache(partitionId, cosmosChangeFeedFindToken.getStartContinuationToken());\n+      index = 0;\n+    }\n+\n+    long resultSize = 0;\n+    while (true) {\n+      if (index < changeFeedCache.get(partitionId).getFetchedEntries().size()) {\n+        if (resultSize + changeFeedCache.get(partitionId).getFetchedEntries().get(index).getSize() < maxEntriesSize\n+            || resultSize == 0) {\n+          results.add(changeFeedCache.get(partitionId).getFetchedEntries().get(index));\n+          resultSize = resultSize + changeFeedCache.get(partitionId).getFetchedEntries().get(index).getSize();\n+          index++;\n+        } else {\n+          break;\n+        }\n+      } else {\n+        populateChangeFeedCache(partitionId, cosmosChangeFeedFindToken.getEndContinuationToken());\n+        if (cacheEmpty(partitionId)) {\n+          // this means that there are no new changes\n+          break;\n+        }\n+        index = 0;\n+      }\n+    }\n+\n+    return new CosmosChangeFeedFindToken(cosmosChangeFeedFindToken.getBytesRead() + resultSize,\n+        changeFeedCache.get(partitionId).getStartContinuationToken(),\n+        changeFeedCache.get(partitionId).getEndContinuationToken(), index,\n+        changeFeedCache.get(partitionId).getFetchedEntries().size(),\n+        changeFeedCache.get(partitionId).getAzureRequestId(), cosmosChangeFeedFindToken.getVersion());\n+  }\n+\n+  @Override\n+  public CosmosChangeFeedFindToken getNextEntriesAndUpdatedToken(FindToken curfindToken,\n+      List<CloudBlobMetadata> nextEntries, long maxTotalSizeOfEntries, String partitionPath)\n+      throws DocumentClientException {\n+    return getNextEntriesAndToken((CosmosChangeFeedFindToken) curfindToken, nextEntries, maxTotalSizeOfEntries,\n+        partitionPath);\n+  }\n+\n+  /**\n+   * Check is the cache is valid for the {@code cosmosChangeFeedFindToken} provided.\n+   * @param partitionId partition of the {@code cosmosChangeFeedFindToken}.\n+   * @param cosmosChangeFeedFindToken {@link CosmosChangeFeedFindToken} object.\n+   * @return true is cache is valid. false otherwise.\n+   */\n+  private boolean isCacheValid(String partitionId, CosmosChangeFeedFindToken cosmosChangeFeedFindToken) {\n+    ChangeFeedCacheEntry changeFeedCacheEntry = changeFeedCache.get(partitionId);\n+    return Utils.checkNullableStringEquals(cosmosChangeFeedFindToken.getAzureTokenRequestId(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU1MDc2Ng==", "bodyText": "Please read changeFeedCache.get(partitionId).getFetchedEntries() into fetchedEntries list up front and reuse.\nThen you also won't need the cacheEmpty method since you can just check fetchedEntries.isEmpty().", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376550766", "createdAt": "2020-02-07T18:53:00Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from azure and corresponding {@link FindToken}\n+ * using cosmos change feed apis.\n+ */\n+public class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String azureRequestId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param azureRequestId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String azureRequestId,\n+        List<CloudBlobMetadata> fetchedEntries) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.azureRequestId = azureRequestId;\n+      this.fetchedEntries = fetchedEntries;\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the azure request id.\n+     * @return azure request id.\n+     */\n+    String getAzureRequestId() {\n+      return azureRequestId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+  }\n+\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedBasedReplicationFeed} object.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   * @param azureMetrics{@link {@link AzureMetrics} object.\n+   */\n+  public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccessor, AzureMetrics azureMetrics) {\n+    this.defaultCacheSize = AzureCloudDestination.getFindSinceQueryLimit();\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.azureMetrics = azureMetrics;\n+  }\n+\n+  /**\n+   * Get next set of change feed entries for the specified partition, after the {@code cosmosChangeFeedFindToken}.\n+   * The number of entries is capped by maxEntriesSize.\n+   * This method creates a cache for change feed entries. If the {@code cosmosChangeFeedFindToken} is not valid,\n+   * or if all the items in the cache are consumed, then it queries cosmos for new entries.\n+   * @param cosmosChangeFeedFindToken {@link CosmosChangeFeedFindToken} after which the next entries have to be returned.\n+   * @param results {@link List} of {@link CloudBlobMetadata} objects which will be populated by new entries.\n+   * @param maxEntriesSize maximum size of all the blobs returned in {@code results}\n+   * @param partitionId Partition for which change feed entries have to be returned.\n+   * @return updated {@link CosmosChangeFeedFindToken} after processing the next set of entries.\n+   */\n+  public CosmosChangeFeedFindToken getNextEntriesAndToken(CosmosChangeFeedFindToken cosmosChangeFeedFindToken,\n+      List<CloudBlobMetadata> results, long maxEntriesSize, String partitionId) throws DocumentClientException {\n+    int index = cosmosChangeFeedFindToken.getIndex();\n+    if (!changeFeedCache.containsKey(partitionId) || !isCacheValid(partitionId, cosmosChangeFeedFindToken)) {\n+      populateChangeFeedCache(partitionId, cosmosChangeFeedFindToken.getStartContinuationToken());\n+      index = 0;\n+    }\n+\n+    long resultSize = 0;\n+    while (true) {\n+      if (index < changeFeedCache.get(partitionId).getFetchedEntries().size()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU1MjA2Mg==", "bodyText": "If you are handling the case where partition assignment \"bounces back\" to a VCR node, then mention that in a comment.  But I would prefer to remove the cache entry when a partition is unassigned (if that is feasible).", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376552062", "createdAt": "2020-02-07T18:55:51Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from azure and corresponding {@link FindToken}\n+ * using cosmos change feed apis.\n+ */\n+public class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String azureRequestId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param azureRequestId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String azureRequestId,\n+        List<CloudBlobMetadata> fetchedEntries) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.azureRequestId = azureRequestId;\n+      this.fetchedEntries = fetchedEntries;\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the azure request id.\n+     * @return azure request id.\n+     */\n+    String getAzureRequestId() {\n+      return azureRequestId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+  }\n+\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedBasedReplicationFeed} object.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   * @param azureMetrics{@link {@link AzureMetrics} object.\n+   */\n+  public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccessor, AzureMetrics azureMetrics) {\n+    this.defaultCacheSize = AzureCloudDestination.getFindSinceQueryLimit();\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.azureMetrics = azureMetrics;\n+  }\n+\n+  /**\n+   * Get next set of change feed entries for the specified partition, after the {@code cosmosChangeFeedFindToken}.\n+   * The number of entries is capped by maxEntriesSize.\n+   * This method creates a cache for change feed entries. If the {@code cosmosChangeFeedFindToken} is not valid,\n+   * or if all the items in the cache are consumed, then it queries cosmos for new entries.\n+   * @param cosmosChangeFeedFindToken {@link CosmosChangeFeedFindToken} after which the next entries have to be returned.\n+   * @param results {@link List} of {@link CloudBlobMetadata} objects which will be populated by new entries.\n+   * @param maxEntriesSize maximum size of all the blobs returned in {@code results}\n+   * @param partitionId Partition for which change feed entries have to be returned.\n+   * @return updated {@link CosmosChangeFeedFindToken} after processing the next set of entries.\n+   */\n+  public CosmosChangeFeedFindToken getNextEntriesAndToken(CosmosChangeFeedFindToken cosmosChangeFeedFindToken,\n+      List<CloudBlobMetadata> results, long maxEntriesSize, String partitionId) throws DocumentClientException {\n+    int index = cosmosChangeFeedFindToken.getIndex();\n+    if (!changeFeedCache.containsKey(partitionId) || !isCacheValid(partitionId, cosmosChangeFeedFindToken)) {\n+      populateChangeFeedCache(partitionId, cosmosChangeFeedFindToken.getStartContinuationToken());\n+      index = 0;\n+    }\n+\n+    long resultSize = 0;\n+    while (true) {\n+      if (index < changeFeedCache.get(partitionId).getFetchedEntries().size()) {\n+        if (resultSize + changeFeedCache.get(partitionId).getFetchedEntries().get(index).getSize() < maxEntriesSize\n+            || resultSize == 0) {\n+          results.add(changeFeedCache.get(partitionId).getFetchedEntries().get(index));\n+          resultSize = resultSize + changeFeedCache.get(partitionId).getFetchedEntries().get(index).getSize();\n+          index++;\n+        } else {\n+          break;\n+        }\n+      } else {\n+        populateChangeFeedCache(partitionId, cosmosChangeFeedFindToken.getEndContinuationToken());\n+        if (cacheEmpty(partitionId)) {\n+          // this means that there are no new changes\n+          break;\n+        }\n+        index = 0;\n+      }\n+    }\n+\n+    return new CosmosChangeFeedFindToken(cosmosChangeFeedFindToken.getBytesRead() + resultSize,\n+        changeFeedCache.get(partitionId).getStartContinuationToken(),\n+        changeFeedCache.get(partitionId).getEndContinuationToken(), index,\n+        changeFeedCache.get(partitionId).getFetchedEntries().size(),\n+        changeFeedCache.get(partitionId).getAzureRequestId(), cosmosChangeFeedFindToken.getVersion());\n+  }\n+\n+  @Override\n+  public CosmosChangeFeedFindToken getNextEntriesAndUpdatedToken(FindToken curfindToken,\n+      List<CloudBlobMetadata> nextEntries, long maxTotalSizeOfEntries, String partitionPath)\n+      throws DocumentClientException {\n+    return getNextEntriesAndToken((CosmosChangeFeedFindToken) curfindToken, nextEntries, maxTotalSizeOfEntries,\n+        partitionPath);\n+  }\n+\n+  /**\n+   * Check is the cache is valid for the {@code cosmosChangeFeedFindToken} provided.\n+   * @param partitionId partition of the {@code cosmosChangeFeedFindToken}.\n+   * @param cosmosChangeFeedFindToken {@link CosmosChangeFeedFindToken} object.\n+   * @return true is cache is valid. false otherwise.\n+   */\n+  private boolean isCacheValid(String partitionId, CosmosChangeFeedFindToken cosmosChangeFeedFindToken) {\n+    ChangeFeedCacheEntry changeFeedCacheEntry = changeFeedCache.get(partitionId);\n+    return Utils.checkNullableStringEquals(cosmosChangeFeedFindToken.getAzureTokenRequestId(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU0ODI0MA=="}, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU1MjIzMQ==", "bodyText": "doubt this is needed (see above)", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376552231", "createdAt": "2020-02-07T18:56:11Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from azure and corresponding {@link FindToken}\n+ * using cosmos change feed apis.\n+ */\n+public class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String azureRequestId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param azureRequestId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String azureRequestId,\n+        List<CloudBlobMetadata> fetchedEntries) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.azureRequestId = azureRequestId;\n+      this.fetchedEntries = fetchedEntries;\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the azure request id.\n+     * @return azure request id.\n+     */\n+    String getAzureRequestId() {\n+      return azureRequestId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+  }\n+\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedBasedReplicationFeed} object.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   * @param azureMetrics{@link {@link AzureMetrics} object.\n+   */\n+  public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccessor, AzureMetrics azureMetrics) {\n+    this.defaultCacheSize = AzureCloudDestination.getFindSinceQueryLimit();\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.azureMetrics = azureMetrics;\n+  }\n+\n+  /**\n+   * Get next set of change feed entries for the specified partition, after the {@code cosmosChangeFeedFindToken}.\n+   * The number of entries is capped by maxEntriesSize.\n+   * This method creates a cache for change feed entries. If the {@code cosmosChangeFeedFindToken} is not valid,\n+   * or if all the items in the cache are consumed, then it queries cosmos for new entries.\n+   * @param cosmosChangeFeedFindToken {@link CosmosChangeFeedFindToken} after which the next entries have to be returned.\n+   * @param results {@link List} of {@link CloudBlobMetadata} objects which will be populated by new entries.\n+   * @param maxEntriesSize maximum size of all the blobs returned in {@code results}\n+   * @param partitionId Partition for which change feed entries have to be returned.\n+   * @return updated {@link CosmosChangeFeedFindToken} after processing the next set of entries.\n+   */\n+  public CosmosChangeFeedFindToken getNextEntriesAndToken(CosmosChangeFeedFindToken cosmosChangeFeedFindToken,\n+      List<CloudBlobMetadata> results, long maxEntriesSize, String partitionId) throws DocumentClientException {\n+    int index = cosmosChangeFeedFindToken.getIndex();\n+    if (!changeFeedCache.containsKey(partitionId) || !isCacheValid(partitionId, cosmosChangeFeedFindToken)) {\n+      populateChangeFeedCache(partitionId, cosmosChangeFeedFindToken.getStartContinuationToken());\n+      index = 0;\n+    }\n+\n+    long resultSize = 0;\n+    while (true) {\n+      if (index < changeFeedCache.get(partitionId).getFetchedEntries().size()) {\n+        if (resultSize + changeFeedCache.get(partitionId).getFetchedEntries().get(index).getSize() < maxEntriesSize\n+            || resultSize == 0) {\n+          results.add(changeFeedCache.get(partitionId).getFetchedEntries().get(index));\n+          resultSize = resultSize + changeFeedCache.get(partitionId).getFetchedEntries().get(index).getSize();\n+          index++;\n+        } else {\n+          break;\n+        }\n+      } else {\n+        populateChangeFeedCache(partitionId, cosmosChangeFeedFindToken.getEndContinuationToken());\n+        if (cacheEmpty(partitionId)) {\n+          // this means that there are no new changes\n+          break;\n+        }\n+        index = 0;\n+      }\n+    }\n+\n+    return new CosmosChangeFeedFindToken(cosmosChangeFeedFindToken.getBytesRead() + resultSize,\n+        changeFeedCache.get(partitionId).getStartContinuationToken(),\n+        changeFeedCache.get(partitionId).getEndContinuationToken(), index,\n+        changeFeedCache.get(partitionId).getFetchedEntries().size(),\n+        changeFeedCache.get(partitionId).getAzureRequestId(), cosmosChangeFeedFindToken.getVersion());\n+  }\n+\n+  @Override\n+  public CosmosChangeFeedFindToken getNextEntriesAndUpdatedToken(FindToken curfindToken,\n+      List<CloudBlobMetadata> nextEntries, long maxTotalSizeOfEntries, String partitionPath)\n+      throws DocumentClientException {\n+    return getNextEntriesAndToken((CosmosChangeFeedFindToken) curfindToken, nextEntries, maxTotalSizeOfEntries,\n+        partitionPath);\n+  }\n+\n+  /**\n+   * Check is the cache is valid for the {@code cosmosChangeFeedFindToken} provided.\n+   * @param partitionId partition of the {@code cosmosChangeFeedFindToken}.\n+   * @param cosmosChangeFeedFindToken {@link CosmosChangeFeedFindToken} object.\n+   * @return true is cache is valid. false otherwise.\n+   */\n+  private boolean isCacheValid(String partitionId, CosmosChangeFeedFindToken cosmosChangeFeedFindToken) {\n+    ChangeFeedCacheEntry changeFeedCacheEntry = changeFeedCache.get(partitionId);\n+    return Utils.checkNullableStringEquals(cosmosChangeFeedFindToken.getAzureTokenRequestId(),\n+        changeFeedCacheEntry.getAzureRequestId()) && Utils.checkNullableStringEquals(\n+        cosmosChangeFeedFindToken.getStartContinuationToken(), changeFeedCacheEntry.getStartContinuationToken())\n+        && Utils.checkNullableStringEquals(cosmosChangeFeedFindToken.getEndContinuationToken(),\n+        changeFeedCacheEntry.getEndContinuationToken())\n+        && cosmosChangeFeedFindToken.getTotalItems() == changeFeedCacheEntry.getFetchedEntries().size();\n+  }\n+\n+  private boolean cacheEmpty(String partitionId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU1NTU5MA==", "bodyText": "We shouldn't need to generate a new requestid on every call.  Can reuse the previous one if not first time.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376555590", "createdAt": "2020-02-07T19:03:28Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from azure and corresponding {@link FindToken}\n+ * using cosmos change feed apis.\n+ */\n+public class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String azureRequestId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param azureRequestId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String azureRequestId,\n+        List<CloudBlobMetadata> fetchedEntries) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.azureRequestId = azureRequestId;\n+      this.fetchedEntries = fetchedEntries;\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the azure request id.\n+     * @return azure request id.\n+     */\n+    String getAzureRequestId() {\n+      return azureRequestId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+  }\n+\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedBasedReplicationFeed} object.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   * @param azureMetrics{@link {@link AzureMetrics} object.\n+   */\n+  public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccessor, AzureMetrics azureMetrics) {\n+    this.defaultCacheSize = AzureCloudDestination.getFindSinceQueryLimit();\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.azureMetrics = azureMetrics;\n+  }\n+\n+  /**\n+   * Get next set of change feed entries for the specified partition, after the {@code cosmosChangeFeedFindToken}.\n+   * The number of entries is capped by maxEntriesSize.\n+   * This method creates a cache for change feed entries. If the {@code cosmosChangeFeedFindToken} is not valid,\n+   * or if all the items in the cache are consumed, then it queries cosmos for new entries.\n+   * @param cosmosChangeFeedFindToken {@link CosmosChangeFeedFindToken} after which the next entries have to be returned.\n+   * @param results {@link List} of {@link CloudBlobMetadata} objects which will be populated by new entries.\n+   * @param maxEntriesSize maximum size of all the blobs returned in {@code results}\n+   * @param partitionId Partition for which change feed entries have to be returned.\n+   * @return updated {@link CosmosChangeFeedFindToken} after processing the next set of entries.\n+   */\n+  public CosmosChangeFeedFindToken getNextEntriesAndToken(CosmosChangeFeedFindToken cosmosChangeFeedFindToken,\n+      List<CloudBlobMetadata> results, long maxEntriesSize, String partitionId) throws DocumentClientException {\n+    int index = cosmosChangeFeedFindToken.getIndex();\n+    if (!changeFeedCache.containsKey(partitionId) || !isCacheValid(partitionId, cosmosChangeFeedFindToken)) {\n+      populateChangeFeedCache(partitionId, cosmosChangeFeedFindToken.getStartContinuationToken());\n+      index = 0;\n+    }\n+\n+    long resultSize = 0;\n+    while (true) {\n+      if (index < changeFeedCache.get(partitionId).getFetchedEntries().size()) {\n+        if (resultSize + changeFeedCache.get(partitionId).getFetchedEntries().get(index).getSize() < maxEntriesSize\n+            || resultSize == 0) {\n+          results.add(changeFeedCache.get(partitionId).getFetchedEntries().get(index));\n+          resultSize = resultSize + changeFeedCache.get(partitionId).getFetchedEntries().get(index).getSize();\n+          index++;\n+        } else {\n+          break;\n+        }\n+      } else {\n+        populateChangeFeedCache(partitionId, cosmosChangeFeedFindToken.getEndContinuationToken());\n+        if (cacheEmpty(partitionId)) {\n+          // this means that there are no new changes\n+          break;\n+        }\n+        index = 0;\n+      }\n+    }\n+\n+    return new CosmosChangeFeedFindToken(cosmosChangeFeedFindToken.getBytesRead() + resultSize,\n+        changeFeedCache.get(partitionId).getStartContinuationToken(),\n+        changeFeedCache.get(partitionId).getEndContinuationToken(), index,\n+        changeFeedCache.get(partitionId).getFetchedEntries().size(),\n+        changeFeedCache.get(partitionId).getAzureRequestId(), cosmosChangeFeedFindToken.getVersion());\n+  }\n+\n+  @Override\n+  public CosmosChangeFeedFindToken getNextEntriesAndUpdatedToken(FindToken curfindToken,\n+      List<CloudBlobMetadata> nextEntries, long maxTotalSizeOfEntries, String partitionPath)\n+      throws DocumentClientException {\n+    return getNextEntriesAndToken((CosmosChangeFeedFindToken) curfindToken, nextEntries, maxTotalSizeOfEntries,\n+        partitionPath);\n+  }\n+\n+  /**\n+   * Check is the cache is valid for the {@code cosmosChangeFeedFindToken} provided.\n+   * @param partitionId partition of the {@code cosmosChangeFeedFindToken}.\n+   * @param cosmosChangeFeedFindToken {@link CosmosChangeFeedFindToken} object.\n+   * @return true is cache is valid. false otherwise.\n+   */\n+  private boolean isCacheValid(String partitionId, CosmosChangeFeedFindToken cosmosChangeFeedFindToken) {\n+    ChangeFeedCacheEntry changeFeedCacheEntry = changeFeedCache.get(partitionId);\n+    return Utils.checkNullableStringEquals(cosmosChangeFeedFindToken.getAzureTokenRequestId(),\n+        changeFeedCacheEntry.getAzureRequestId()) && Utils.checkNullableStringEquals(\n+        cosmosChangeFeedFindToken.getStartContinuationToken(), changeFeedCacheEntry.getStartContinuationToken())\n+        && Utils.checkNullableStringEquals(cosmosChangeFeedFindToken.getEndContinuationToken(),\n+        changeFeedCacheEntry.getEndContinuationToken())\n+        && cosmosChangeFeedFindToken.getTotalItems() == changeFeedCacheEntry.getFetchedEntries().size();\n+  }\n+\n+  private boolean cacheEmpty(String partitionId) {\n+    return changeFeedCache.get(partitionId).getFetchedEntries().size() == 0;\n+  }\n+\n+  /**\n+   * Populate change feed cache by querying cosmos for the next set of change feed entries after the specified request continuation token.\n+   * @param partitionId Partition for which the change feed cache needs to be populated.\n+   * @param startRequestContinuationToken request continuation token from which the change feed query needs to be made.\n+   */\n+  private void populateChangeFeedCache(String partitionId, String startRequestContinuationToken)\n+      throws DocumentClientException {\n+    List<CloudBlobMetadata> changeFeedEntries = new ArrayList<>(defaultCacheSize);\n+    String newRequestContinuationToken =\n+        cosmosDataAccessor.queryChangeFeed(startRequestContinuationToken, defaultCacheSize, changeFeedEntries,\n+            partitionId);\n+    ChangeFeedCacheEntry changeFeedCacheEntry =\n+        new ChangeFeedCacheEntry(startRequestContinuationToken, newRequestContinuationToken,\n+            UUID.randomUUID().toString(), changeFeedEntries);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU1NjExOA==", "bodyText": "Cool, I like this refactoring.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376556118", "createdAt": "2020-02-07T19:04:47Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosUpdateTimeBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.replication.FindToken;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import com.microsoft.azure.cosmosdb.SqlParameter;\n+import com.microsoft.azure.cosmosdb.SqlParameterCollection;\n+import com.microsoft.azure.cosmosdb.SqlQuerySpec;\n+import java.util.List;\n+import java.util.ListIterator;\n+import java.util.Set;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from azure and corresponding {@link FindToken}\n+ * using cosmos update time field.\n+ */\n+public class CosmosUpdateTimeBasedReplicationFeed implements AzureReplicationFeed {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU1ODU1Nw==", "bodyText": "Minor: I would pass cosmosDataAccessor, azureMetrics here explicitly rather than relying on constructor to initialize them beforehand.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376558557", "createdAt": "2020-02-07T19:10:20Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureCloudDestination.java", "diffHunk": "@@ -449,10 +416,27 @@ private static final CloudStorageException toCloudStorageException(String messag\n       retryDelayMs = ((DocumentClientException) e).getRetryAfterInMilliseconds();\n     }\n     // Everything is retryable except NOT_FOUND\n-    isRetryable = (statusCode != HttpConstants.StatusCodes.NOTFOUND);\n+    boolean isRetryable = (statusCode != HttpConstants.StatusCodes.NOTFOUND);\n     return new CloudStorageException(message, e, isRetryable, retryDelayMs);\n   }\n \n+  /**\n+   * Return corresponding {@link AzureReplicationFeed} object for specified {@link AzureReplicationFeedType}.\n+   * @param azureReplicationFeedType replication feed type.\n+   * @return {@link AzureReplicationFeed} object.\n+   */\n+  private AzureReplicationFeed getReplicationFeedObj(AzureReplicationFeedType azureReplicationFeedType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU1OTgyMA==", "bodyText": "good catch", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376559820", "createdAt": "2020-02-07T19:13:16Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudBlobMetadata.java", "diffHunk": "@@ -471,7 +471,6 @@ public void serialize(CloudBlobMetadata value, JsonGenerator gen, SerializerProv\n       if (value.encryptionOrigin != EncryptionOrigin.NONE) {\n         gen.writeStringField(FIELD_ENCRYPTION_ORIGIN, value.encryptionOrigin.toString());\n         if (value.encryptionOrigin == EncryptionOrigin.VCR) {\n-          gen.writeStringField(FIELD_ENCRYPTION_ORIGIN, value.encryptionOrigin.toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU2MTE5NQ==", "bodyText": "Objects.equals(str1, str2) does the same thing, no?", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r376561195", "createdAt": "2020-02-07T19:16:28Z", "author": {"login": "lightningrob"}, "path": "ambry-utils/src/main/java/com.github.ambry.utils/Utils.java", "diffHunk": "@@ -1152,6 +1152,19 @@ public static boolean isNullOrEmpty(String str) {\n     return str == null || str.isEmpty();\n   }\n \n+  /**\n+   * Compare two strings which can be null.\n+   * @param str1 {@link String} to compare.\n+   * @param str2 {@link String} to compare.\n+   * @return true if strings are equal. False otherwise.\n+   */\n+  public static boolean checkNullableStringEquals(String str1, String str2) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0f6fafb3ad8f197fe6cc54157eadb68a31530cb"}, "originalPosition": 10}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3MDA2ODk2", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-357006896", "createdAt": "2020-02-11T21:25:33Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMToyNTozM1rOFoZrcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMjoyNDoxNFrOFobYnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkwODA4Mw==", "bodyText": "minor: format this file please", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r377908083", "createdAt": "2020-02-11T21:25:33Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudBlobMetadata.java", "diffHunk": "@@ -471,7 +471,6 @@ public void serialize(CloudBlobMetadata value, JsonGenerator gen, SerializerProv\n       if (value.encryptionOrigin != EncryptionOrigin.NONE) {\n         gen.writeStringField(FIELD_ENCRYPTION_ORIGIN, value.encryptionOrigin.toString());\n         if (value.encryptionOrigin == EncryptionOrigin.VCR) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkxNDg0Nw==", "bodyText": "Sorry, I don't quite understand where nextEntries comes from.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r377914847", "createdAt": "2020-02-11T21:38:37Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudDestination.java", "diffHunk": "@@ -81,15 +82,16 @@ boolean uploadBlob(BlobId blobId, long inputLength, CloudBlobMetadata cloudBlobM\n   List<CloudBlobMetadata> getDeadBlobs(String partitionPath) throws CloudStorageException;\n \n   /**\n-   * Returns a sequenced list of blobs in the specified partition, ordered by update time starting from the\n-   * specified time.\n+   * Populates an ordered sequenced list of blobs in the specified partition in {@code nextEntries} {@link List}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkxNTU2Nw==", "bodyText": "minor: returns FindResult which contains both blob metadata and updated FindToken", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r377915567", "createdAt": "2020-02-11T21:40:05Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudDestination.java", "diffHunk": "@@ -81,15 +82,16 @@ boolean uploadBlob(BlobId blobId, long inputLength, CloudBlobMetadata cloudBlobM\n   List<CloudBlobMetadata> getDeadBlobs(String partitionPath) throws CloudStorageException;\n \n   /**\n-   * Returns a sequenced list of blobs in the specified partition, ordered by update time starting from the\n-   * specified time.\n+   * Populates an ordered sequenced list of blobs in the specified partition in {@code nextEntries} {@link List}.\n+   * Returns the updated {@link com.github.ambry.replication.FindToken}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkyMDI4MQ==", "bodyText": "nvm, I figure out what it means by reading AzureReplicationFeed interface.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r377920281", "createdAt": "2020-02-11T21:49:22Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudDestination.java", "diffHunk": "@@ -81,15 +82,16 @@ boolean uploadBlob(BlobId blobId, long inputLength, CloudBlobMetadata cloudBlobM\n   List<CloudBlobMetadata> getDeadBlobs(String partitionPath) throws CloudStorageException;\n \n   /**\n-   * Returns a sequenced list of blobs in the specified partition, ordered by update time starting from the\n-   * specified time.\n+   * Populates an ordered sequenced list of blobs in the specified partition in {@code nextEntries} {@link List}.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkxNDg0Nw=="}, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkyMzE2OA==", "bodyText": "can be package-private", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r377923168", "createdAt": "2020-02-11T21:55:23Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureCloudDestination.java", "diffHunk": "@@ -416,6 +376,14 @@ public boolean retrieveTokens(String partitionPath, String tokenFileName, Output\n     }\n   }\n \n+  /**\n+   * Return {@code findSinceQueryLimit}\n+   * @return value of {@code findSinceQueryLimit}\n+   */\n+  public static int getFindSinceQueryLimit() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkyOTUzMA==", "bodyText": "typo in getCreationTimestamp", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r377929530", "createdAt": "2020-02-11T22:09:22Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String cacheSessionId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+    private final String partitionId;\n+    private long creationTimestamp;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param cacheSessionId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String cacheSessionId,\n+        List<CloudBlobMetadata> fetchedEntries, String partitionId) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.cacheSessionId = cacheSessionId;\n+      this.fetchedEntries = fetchedEntries;\n+      this.partitionId = partitionId;\n+      this.creationTimestamp = System.currentTimeMillis();\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the Azure request id.\n+     * @return Azure request id.\n+     */\n+    String getCacheSessionId() {\n+      return cacheSessionId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+\n+    /**\n+     * Return the {@code partitionId}\n+     * @return {@code partitionId}\n+     */\n+    public String getPartitionId() {\n+      return partitionId;\n+    }\n+\n+    /**\n+     * Return the creation time stamp.\n+     * @return  {@code creationTimestamp}\n+     */\n+    public long getCreationimestamp() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkzMTUwNA==", "bodyText": "CACHE_VALID_DURATION_IN_MS seems to be a little better.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r377931504", "createdAt": "2020-02-11T22:13:58Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String cacheSessionId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+    private final String partitionId;\n+    private long creationTimestamp;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param cacheSessionId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String cacheSessionId,\n+        List<CloudBlobMetadata> fetchedEntries, String partitionId) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.cacheSessionId = cacheSessionId;\n+      this.fetchedEntries = fetchedEntries;\n+      this.partitionId = partitionId;\n+      this.creationTimestamp = System.currentTimeMillis();\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the Azure request id.\n+     * @return Azure request id.\n+     */\n+    String getCacheSessionId() {\n+      return cacheSessionId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+\n+    /**\n+     * Return the {@code partitionId}\n+     * @return {@code partitionId}\n+     */\n+    public String getPartitionId() {\n+      return partitionId;\n+    }\n+\n+    /**\n+     * Return the creation time stamp.\n+     * @return  {@code creationTimestamp}\n+     */\n+    public long getCreationimestamp() {\n+      return creationTimestamp;\n+    }\n+\n+    /**\n+     * Check if is this entry is expired. The condition for expiry uses {@code creationTimestamp}. This is good enough as\n+     * this means that the cached set of fetches entries hasn't been consumed within the invalidation duration.\n+     * @return true if this entry is expired. false otherwise.\n+     */\n+    boolean isExpired() {\n+      return creationTimestamp < System.currentTimeMillis() - CACHE_INVALIDATION_DURATION_IN_MILLIS;\n+    }\n+  }\n+\n+  // change feed cache by cache session id\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+  private final static long CACHE_INVALIDATION_DURATION_IN_MILLIS = 60 * 60 * 1000; //1 hour", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkzMjIzOA==", "bodyText": "I wonder why we choose CACHE_INVALIDATION_DURATION_IN_MILLIS as initial delay. Can we use a random initial delay?", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r377932238", "createdAt": "2020-02-11T22:15:43Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String cacheSessionId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+    private final String partitionId;\n+    private long creationTimestamp;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param cacheSessionId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String cacheSessionId,\n+        List<CloudBlobMetadata> fetchedEntries, String partitionId) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.cacheSessionId = cacheSessionId;\n+      this.fetchedEntries = fetchedEntries;\n+      this.partitionId = partitionId;\n+      this.creationTimestamp = System.currentTimeMillis();\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the Azure request id.\n+     * @return Azure request id.\n+     */\n+    String getCacheSessionId() {\n+      return cacheSessionId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+\n+    /**\n+     * Return the {@code partitionId}\n+     * @return {@code partitionId}\n+     */\n+    public String getPartitionId() {\n+      return partitionId;\n+    }\n+\n+    /**\n+     * Return the creation time stamp.\n+     * @return  {@code creationTimestamp}\n+     */\n+    public long getCreationimestamp() {\n+      return creationTimestamp;\n+    }\n+\n+    /**\n+     * Check if is this entry is expired. The condition for expiry uses {@code creationTimestamp}. This is good enough as\n+     * this means that the cached set of fetches entries hasn't been consumed within the invalidation duration.\n+     * @return true if this entry is expired. false otherwise.\n+     */\n+    boolean isExpired() {\n+      return creationTimestamp < System.currentTimeMillis() - CACHE_INVALIDATION_DURATION_IN_MILLIS;\n+    }\n+  }\n+\n+  // change feed cache by cache session id\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+  private final static long CACHE_INVALIDATION_DURATION_IN_MILLIS = 60 * 60 * 1000; //1 hour\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedBasedReplicationFeed} object.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   * @param azureMetrics{@link {@link AzureMetrics} object.\n+   */\n+  public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccessor, AzureMetrics azureMetrics) {\n+    this.defaultCacheSize = AzureCloudDestination.getFindSinceQueryLimit();\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.azureMetrics = azureMetrics;\n+    // schedule periodic invalidation of cache\n+    Utils.newScheduler(1, false)\n+        .scheduleAtFixedRate(() -> changeFeedCache.entrySet().removeIf(entry -> entry.getValue().isExpired()),\n+            CACHE_INVALIDATION_DURATION_IN_MILLIS, CACHE_INVALIDATION_DURATION_IN_MILLIS, TimeUnit.MILLISECONDS);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkzMjkxNw==", "bodyText": "minor: curfindToken -> curFindToken", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r377932917", "createdAt": "2020-02-11T22:17:19Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String cacheSessionId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+    private final String partitionId;\n+    private long creationTimestamp;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param cacheSessionId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String cacheSessionId,\n+        List<CloudBlobMetadata> fetchedEntries, String partitionId) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.cacheSessionId = cacheSessionId;\n+      this.fetchedEntries = fetchedEntries;\n+      this.partitionId = partitionId;\n+      this.creationTimestamp = System.currentTimeMillis();\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the Azure request id.\n+     * @return Azure request id.\n+     */\n+    String getCacheSessionId() {\n+      return cacheSessionId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+\n+    /**\n+     * Return the {@code partitionId}\n+     * @return {@code partitionId}\n+     */\n+    public String getPartitionId() {\n+      return partitionId;\n+    }\n+\n+    /**\n+     * Return the creation time stamp.\n+     * @return  {@code creationTimestamp}\n+     */\n+    public long getCreationimestamp() {\n+      return creationTimestamp;\n+    }\n+\n+    /**\n+     * Check if is this entry is expired. The condition for expiry uses {@code creationTimestamp}. This is good enough as\n+     * this means that the cached set of fetches entries hasn't been consumed within the invalidation duration.\n+     * @return true if this entry is expired. false otherwise.\n+     */\n+    boolean isExpired() {\n+      return creationTimestamp < System.currentTimeMillis() - CACHE_INVALIDATION_DURATION_IN_MILLIS;\n+    }\n+  }\n+\n+  // change feed cache by cache session id\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+  private final static long CACHE_INVALIDATION_DURATION_IN_MILLIS = 60 * 60 * 1000; //1 hour\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedBasedReplicationFeed} object.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   * @param azureMetrics{@link {@link AzureMetrics} object.\n+   */\n+  public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccessor, AzureMetrics azureMetrics) {\n+    this.defaultCacheSize = AzureCloudDestination.getFindSinceQueryLimit();\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.azureMetrics = azureMetrics;\n+    // schedule periodic invalidation of cache\n+    Utils.newScheduler(1, false)\n+        .scheduleAtFixedRate(() -> changeFeedCache.entrySet().removeIf(entry -> entry.getValue().isExpired()),\n+            CACHE_INVALIDATION_DURATION_IN_MILLIS, CACHE_INVALIDATION_DURATION_IN_MILLIS, TimeUnit.MILLISECONDS);\n+  }\n+\n+  /**\n+   * Get next set of change feed entries for the specified partition, after the {@code cosmosChangeFeedFindToken}.\n+   * The number of entries is capped by maxEntriesSize.\n+   * This method creates a cache for change feed entries. If the {@code cosmosChangeFeedFindToken} is not valid,\n+   * or if all the items in the cache are consumed, then it queries Cosmos for new entries.\n+   * @param cosmosChangeFeedFindToken {@link CosmosChangeFeedFindToken} after which the next entries have to be returned.\n+   * @param results {@link List} of {@link CloudBlobMetadata} objects which will be populated by new entries.\n+   * @param maxEntriesSize maximum size of all the blobs returned in {@code results}\n+   * @param partitionId Partition for which change feed entries have to be returned.\n+   * @return updated {@link CosmosChangeFeedFindToken} after processing the next set of entries.\n+   */\n+  private CosmosChangeFeedFindToken getNextEntriesAndToken(CosmosChangeFeedFindToken cosmosChangeFeedFindToken,\n+      List<CloudBlobMetadata> results, long maxEntriesSize, String partitionId) throws DocumentClientException {\n+    int index = cosmosChangeFeedFindToken.getIndex();\n+    String cacheSesionId = cosmosChangeFeedFindToken.getCacheSessionId();\n+    if (!changeFeedCache.containsKey(cacheSesionId) || !isCacheValid(partitionId, cosmosChangeFeedFindToken)) {\n+      // the cache may not be valid. So we cannot use session id\n+      cacheSesionId = populateChangeFeedCache(partitionId, cosmosChangeFeedFindToken.getStartContinuationToken());\n+      // invalidate the previous token's cache\n+      changeFeedCache.remove(cosmosChangeFeedFindToken.getCacheSessionId());\n+      index = 0;\n+    }\n+\n+    long resultSize = 0;\n+\n+    List<CloudBlobMetadata> fetchedEntries = changeFeedCache.get(cacheSesionId).getFetchedEntries();\n+    while (true) {\n+      if (index < fetchedEntries.size()) {\n+        if (resultSize + fetchedEntries.get(index).getSize() < maxEntriesSize || resultSize == 0) {\n+          results.add(fetchedEntries.get(index));\n+          resultSize = resultSize + fetchedEntries.get(index).getSize();\n+          index++;\n+        } else {\n+          break;\n+        }\n+      } else {\n+        // we can reuse the session id in this case, because we know that the cache ran out of new items.\n+        populateChangeFeedCache(partitionId, cosmosChangeFeedFindToken.getEndContinuationToken(),\n+            cosmosChangeFeedFindToken.getCacheSessionId());\n+        fetchedEntries = changeFeedCache.get(cacheSesionId).getFetchedEntries();\n+        if (fetchedEntries.isEmpty()) {\n+          // this means that there are no new changes\n+          break;\n+        }\n+        index = 0;\n+      }\n+    }\n+\n+    return new CosmosChangeFeedFindToken(cosmosChangeFeedFindToken.getBytesRead() + resultSize,\n+        changeFeedCache.get(cacheSesionId).getStartContinuationToken(),\n+        changeFeedCache.get(cacheSesionId).getEndContinuationToken(), index,\n+        changeFeedCache.get(cacheSesionId).getFetchedEntries().size(), cacheSesionId,\n+        cosmosChangeFeedFindToken.getVersion());\n+  }\n+\n+  @Override\n+  public FindResult getNextEntriesAndUpdatedToken(FindToken curfindToken, long maxTotalSizeOfEntries,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 200}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkzNjAzMQ==", "bodyText": "could you explain why here is 5 * Integer.BYTES ?", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r377936031", "createdAt": "2020-02-11T22:24:14Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedFindToken.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.replication.FindTokenType;\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress using Cosmos change feed.\n+ */\n+public class CosmosChangeFeedFindToken implements FindToken {\n+  private final short version;\n+  private final long bytesRead;\n+  private final FindTokenType type = FindTokenType.CloudBased;\n+  private final String startContinuationToken;\n+  private final String endContinuationToken;\n+  private final int index;\n+  private final int totalItems;\n+  private final String cacheSessionId;\n+\n+  private final static short VERSION_0 = 0;\n+  private final static short DEFAULT_VERSION = VERSION_0;\n+\n+  /**\n+   * Default constructor to create a {@link CosmosChangeFeedFindToken} with uninitialized continuation token.\n+   */\n+  public CosmosChangeFeedFindToken() {\n+    version = DEFAULT_VERSION;\n+    bytesRead = 0;\n+    startContinuationToken = \"\";\n+    index = -1;\n+    endContinuationToken = \"\";\n+    totalItems = -1;\n+    cacheSessionId = \"\";\n+  }\n+\n+  /**\n+   * Create {@link CosmosChangeFeedFindToken} from provided values.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId) {\n+    this(bytesRead, startContinuationToken, endContinuationToken, index, totalItems, cacheSessionId, DEFAULT_VERSION);\n+  }\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedFindToken} with specified token values and specified version.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   * @param version token version.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId, short version) {\n+    this.version = version;\n+    this.bytesRead = bytesRead;\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.cacheSessionId = cacheSessionId;\n+  }\n+\n+  /**\n+   * Deserialize {@link CosmosChangeFeedFindToken} object from input stream.\n+   * @param inputStream {@link DataOutputStream} to deserialize from.\n+   * @return {@link CosmosChangeFeedFindToken} object.\n+   * @throws IOException\n+   */\n+  public static CosmosChangeFeedFindToken fromBytes(DataInputStream inputStream) throws IOException {\n+    CosmosChangeFeedFindToken cosmosChangeFeedFindToken;\n+    DataInputStream stream = new DataInputStream(inputStream);\n+    short version = stream.readShort();\n+    switch (version) {\n+      case VERSION_0:\n+        FindTokenType type = FindTokenType.values()[stream.readShort()];\n+        if (type != FindTokenType.CloudBased) {\n+          throw new IllegalArgumentException(\n+              String.format(\"Invalid token type %s found while deserialization. Expected %s.\", type,\n+                  FindTokenType.CloudBased));\n+        }\n+        long bytesRead = stream.readLong();\n+        String startContinuationToken = Utils.readIntString(inputStream);\n+        String endContinuationToken = Utils.readIntString(inputStream);\n+        int index = inputStream.readInt();\n+        int totalItems = inputStream.readInt();\n+        String cacheSessionId = Utils.readIntString(inputStream);\n+        cosmosChangeFeedFindToken =\n+            new CosmosChangeFeedFindToken(bytesRead, startContinuationToken, endContinuationToken, index, totalItems,\n+                cacheSessionId, version);\n+        break;\n+      default:\n+        throw new IllegalStateException(\"Unknown version of cloud token: \" + version);\n+    }\n+    return cosmosChangeFeedFindToken;\n+  }\n+\n+  /**\n+   * Serialize {@link CosmosChangeFeedFindToken} to byte array.\n+   * @return serialized byte array.\n+   */\n+  public byte[] toBytes() {\n+    byte[] buf = new byte[size()];\n+    ByteBuffer bufWrap = ByteBuffer.wrap(buf);\n+    bufWrap.putShort(getVersion());\n+    bufWrap.putShort((short) getType().ordinal());\n+    bufWrap.putLong(getBytesRead());\n+    Utils.serializeNullableString(bufWrap, startContinuationToken);\n+    Utils.serializeNullableString(bufWrap, endContinuationToken);\n+    bufWrap.putInt(index);\n+    bufWrap.putInt(totalItems);\n+    Utils.serializeNullableString(bufWrap, cacheSessionId);\n+    return buf;\n+  }\n+\n+  /**\n+   * Calculate size of the token.\n+   * @return size of the token.\n+   */\n+  public int size() {\n+    return 2 * Short.BYTES + Long.BYTES + 5 * Integer.BYTES + getNullableStringLength(startContinuationToken)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 149}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3MjQ4MzE4", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-357248318", "createdAt": "2020-02-12T08:01:31Z", "commit": {"oid": "fc678e43ca2e10503ce4e74cbd8bf0925d48ec36"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwODowMTozMlrOFokuaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwODowMTozMlrOFokuaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA4OTA2NA==", "bodyText": "I don't see much benefit to the inner method.  We can roll that into this one.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r378089064", "createdAt": "2020-02-12T08:01:32Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -153,11 +154,13 @@ public CosmosChangeFeedFindToken getNextEntriesAndToken(CosmosChangeFeedFindToke\n   }\n \n   @Override\n-  public CosmosChangeFeedFindToken getNextEntriesAndUpdatedToken(FindToken curfindToken,\n-      List<CloudBlobMetadata> nextEntries, long maxTotalSizeOfEntries, String partitionPath)\n-      throws DocumentClientException {\n-    return getNextEntriesAndToken((CosmosChangeFeedFindToken) curfindToken, nextEntries, maxTotalSizeOfEntries,\n-        partitionPath);\n+  public FindResult getNextEntriesAndUpdatedToken(FindToken curfindToken, long maxTotalSizeOfEntries,\n+      String partitionPath) throws DocumentClientException {\n+    List<CloudBlobMetadata> nextEntries = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc678e43ca2e10503ce4e74cbd8bf0925d48ec36"}, "originalPosition": 28}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3MjU3OTU3", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-357257957", "createdAt": "2020-02-12T08:21:40Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwODoyMTo0MFrOFolMyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwODoyMTo0MFrOFolMyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA5Njg0Mg==", "bodyText": "update javadoc (nextEntries param removed)", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r378096842", "createdAt": "2020-02-12T08:21:40Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureReplicationFeed.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.List;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure, and a bookmark in form of {@link FindToken}.\n+ */\n+public interface AzureReplicationFeed {\n+\n+  enum FeedType {\n+    /** Replication feed is obtained from cosmos change feed api */\n+    COSMOS_CHANGE_FEED,\n+\n+    /** Replication feed is obtained from cosmos queries ordered by update time */\n+    COSMOS_UPDATE_TIME\n+  }\n+\n+  /**\n+   * Populate the next set of {@link CloudBlobMetadata} objects in {@code nextEntries} of specified partition {@code partitionPath}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3MjYxODY5", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-357261869", "createdAt": "2020-02-12T08:28:52Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwODoyODo1MlrOFolZtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwODoyODo1MlrOFolZtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODEwMDE0OA==", "bodyText": "add exception to javadoc", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r378100148", "createdAt": "2020-02-12T08:28:52Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureReplicationFeed.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.List;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure, and a bookmark in form of {@link FindToken}.\n+ */\n+public interface AzureReplicationFeed {\n+\n+  enum FeedType {\n+    /** Replication feed is obtained from cosmos change feed api */\n+    COSMOS_CHANGE_FEED,\n+\n+    /** Replication feed is obtained from cosmos queries ordered by update time */\n+    COSMOS_UPDATE_TIME\n+  }\n+\n+  /**\n+   * Populate the next set of {@link CloudBlobMetadata} objects in {@code nextEntries} of specified partition {@code partitionPath}\n+   * from the specified {@link FindToken} such that total size of all blobs in the entries are less or equal to {@code maxTotalSizeOfEntries}.\n+   * This method should return at least one blob, if exists, after {@code curfindToken}, irrespective of {@code maxTotalSizeOfEntries} requirement.\n+   * @param curfindToken current {@link FindToken} object that acts as a bookmark to return blobs after.\n+   * @param maxTotalSizeOfEntries max total size of all the {@link CloudBlobMetadata} objects returned.\n+   * @param partitionPath partition of the blobs.\n+   * @return {@link FindResult} instance that contains updated {@link FindToken} object which can act as a bookmark for\n+   * subsequent requests, and {@link List} of {@link CloudBlobMetadata} entries.\n+   */\n+  FindResult getNextEntriesAndUpdatedToken(FindToken curfindToken, long maxTotalSizeOfEntries, String partitionPath)\n+      throws DocumentClientException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NDU1MzIx", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358455321", "createdAt": "2020-02-13T18:34:44Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODozNDo0NFrOFpe_qw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODozNDo0NFrOFpe_qw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA0Mzc1NQ==", "bodyText": "Please be consistent on using getters (getVersion()) vs straight reference (index, totalItems).  I prefer the latter.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379043755", "createdAt": "2020-02-13T18:34:44Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedFindToken.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.replication.FindTokenType;\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress using Cosmos change feed.\n+ */\n+public class CosmosChangeFeedFindToken implements FindToken {\n+  private final short version;\n+  private final long bytesRead;\n+  private final FindTokenType type = FindTokenType.CloudBased;\n+  private final String startContinuationToken;\n+  private final String endContinuationToken;\n+  private final int index;\n+  private final int totalItems;\n+  private final String cacheSessionId;\n+\n+  private final static short VERSION_0 = 0;\n+  private final static short DEFAULT_VERSION = VERSION_0;\n+\n+  /**\n+   * Default constructor to create a {@link CosmosChangeFeedFindToken} with uninitialized continuation token.\n+   */\n+  public CosmosChangeFeedFindToken() {\n+    version = DEFAULT_VERSION;\n+    bytesRead = 0;\n+    startContinuationToken = \"\";\n+    index = -1;\n+    endContinuationToken = \"\";\n+    totalItems = -1;\n+    cacheSessionId = \"\";\n+  }\n+\n+  /**\n+   * Create {@link CosmosChangeFeedFindToken} from provided values.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId) {\n+    this(bytesRead, startContinuationToken, endContinuationToken, index, totalItems, cacheSessionId, DEFAULT_VERSION);\n+  }\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedFindToken} with specified token values and specified version.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   * @param version token version.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId, short version) {\n+    this.version = version;\n+    this.bytesRead = bytesRead;\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.cacheSessionId = cacheSessionId;\n+  }\n+\n+  /**\n+   * Deserialize {@link CosmosChangeFeedFindToken} object from input stream.\n+   * @param inputStream {@link DataOutputStream} to deserialize from.\n+   * @return {@link CosmosChangeFeedFindToken} object.\n+   * @throws IOException\n+   */\n+  public static CosmosChangeFeedFindToken fromBytes(DataInputStream inputStream) throws IOException {\n+    CosmosChangeFeedFindToken cosmosChangeFeedFindToken;\n+    DataInputStream stream = new DataInputStream(inputStream);\n+    short version = stream.readShort();\n+    switch (version) {\n+      case VERSION_0:\n+        FindTokenType type = FindTokenType.values()[stream.readShort()];\n+        if (type != FindTokenType.CloudBased) {\n+          throw new IllegalArgumentException(\n+              String.format(\"Invalid token type %s found while deserialization. Expected %s.\", type,\n+                  FindTokenType.CloudBased));\n+        }\n+        long bytesRead = stream.readLong();\n+        String startContinuationToken = Utils.readIntString(inputStream);\n+        String endContinuationToken = Utils.readIntString(inputStream);\n+        int index = inputStream.readInt();\n+        int totalItems = inputStream.readInt();\n+        String cacheSessionId = Utils.readIntString(inputStream);\n+        cosmosChangeFeedFindToken =\n+            new CosmosChangeFeedFindToken(bytesRead, startContinuationToken, endContinuationToken, index, totalItems,\n+                cacheSessionId, version);\n+        break;\n+      default:\n+        throw new IllegalStateException(\"Unknown version of cloud token: \" + version);\n+    }\n+    return cosmosChangeFeedFindToken;\n+  }\n+\n+  /**\n+   * Serialize {@link CosmosChangeFeedFindToken} to byte array.\n+   * @return serialized byte array.\n+   */\n+  public byte[] toBytes() {\n+    byte[] buf = new byte[size()];\n+    ByteBuffer bufWrap = ByteBuffer.wrap(buf);\n+    bufWrap.putShort(getVersion());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 133}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NDU2Mzcy", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358456372", "createdAt": "2020-02-13T18:36:16Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODozNjoxNlrOFpfCxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODozNjoxNlrOFpfCxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA0NDU1MQ==", "bodyText": "Minor: can simply say \"return new CosmosChangeFeedFindToken(...);", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379044551", "createdAt": "2020-02-13T18:36:16Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedFindToken.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.replication.FindTokenType;\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress using Cosmos change feed.\n+ */\n+public class CosmosChangeFeedFindToken implements FindToken {\n+  private final short version;\n+  private final long bytesRead;\n+  private final FindTokenType type = FindTokenType.CloudBased;\n+  private final String startContinuationToken;\n+  private final String endContinuationToken;\n+  private final int index;\n+  private final int totalItems;\n+  private final String cacheSessionId;\n+\n+  private final static short VERSION_0 = 0;\n+  private final static short DEFAULT_VERSION = VERSION_0;\n+\n+  /**\n+   * Default constructor to create a {@link CosmosChangeFeedFindToken} with uninitialized continuation token.\n+   */\n+  public CosmosChangeFeedFindToken() {\n+    version = DEFAULT_VERSION;\n+    bytesRead = 0;\n+    startContinuationToken = \"\";\n+    index = -1;\n+    endContinuationToken = \"\";\n+    totalItems = -1;\n+    cacheSessionId = \"\";\n+  }\n+\n+  /**\n+   * Create {@link CosmosChangeFeedFindToken} from provided values.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId) {\n+    this(bytesRead, startContinuationToken, endContinuationToken, index, totalItems, cacheSessionId, DEFAULT_VERSION);\n+  }\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedFindToken} with specified token values and specified version.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   * @param version token version.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId, short version) {\n+    this.version = version;\n+    this.bytesRead = bytesRead;\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.cacheSessionId = cacheSessionId;\n+  }\n+\n+  /**\n+   * Deserialize {@link CosmosChangeFeedFindToken} object from input stream.\n+   * @param inputStream {@link DataOutputStream} to deserialize from.\n+   * @return {@link CosmosChangeFeedFindToken} object.\n+   * @throws IOException\n+   */\n+  public static CosmosChangeFeedFindToken fromBytes(DataInputStream inputStream) throws IOException {\n+    CosmosChangeFeedFindToken cosmosChangeFeedFindToken;\n+    DataInputStream stream = new DataInputStream(inputStream);\n+    short version = stream.readShort();\n+    switch (version) {\n+      case VERSION_0:\n+        FindTokenType type = FindTokenType.values()[stream.readShort()];\n+        if (type != FindTokenType.CloudBased) {\n+          throw new IllegalArgumentException(\n+              String.format(\"Invalid token type %s found while deserialization. Expected %s.\", type,\n+                  FindTokenType.CloudBased));\n+        }\n+        long bytesRead = stream.readLong();\n+        String startContinuationToken = Utils.readIntString(inputStream);\n+        String endContinuationToken = Utils.readIntString(inputStream);\n+        int index = inputStream.readInt();\n+        int totalItems = inputStream.readInt();\n+        String cacheSessionId = Utils.readIntString(inputStream);\n+        cosmosChangeFeedFindToken =\n+            new CosmosChangeFeedFindToken(bytesRead, startContinuationToken, endContinuationToken, index, totalItems,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 117}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NDU3OTU5", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358457959", "createdAt": "2020-02-13T18:38:36Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODozODozNlrOFpfHyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODozODozNlrOFpfHyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA0NTgzNQ==", "bodyText": "Minor: inconsistency where in some places you say Utils.method1() and others you say method2() making use of static import.  Please be consistent.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379045835", "createdAt": "2020-02-13T18:38:36Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedFindToken.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.replication.FindTokenType;\n+import com.github.ambry.utils.Utils;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress using Cosmos change feed.\n+ */\n+public class CosmosChangeFeedFindToken implements FindToken {\n+  private final short version;\n+  private final long bytesRead;\n+  private final FindTokenType type = FindTokenType.CloudBased;\n+  private final String startContinuationToken;\n+  private final String endContinuationToken;\n+  private final int index;\n+  private final int totalItems;\n+  private final String cacheSessionId;\n+\n+  private final static short VERSION_0 = 0;\n+  private final static short DEFAULT_VERSION = VERSION_0;\n+\n+  /**\n+   * Default constructor to create a {@link CosmosChangeFeedFindToken} with uninitialized continuation token.\n+   */\n+  public CosmosChangeFeedFindToken() {\n+    version = DEFAULT_VERSION;\n+    bytesRead = 0;\n+    startContinuationToken = \"\";\n+    index = -1;\n+    endContinuationToken = \"\";\n+    totalItems = -1;\n+    cacheSessionId = \"\";\n+  }\n+\n+  /**\n+   * Create {@link CosmosChangeFeedFindToken} from provided values.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId) {\n+    this(bytesRead, startContinuationToken, endContinuationToken, index, totalItems, cacheSessionId, DEFAULT_VERSION);\n+  }\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedFindToken} with specified token values and specified version.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   * @param version token version.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId, short version) {\n+    this.version = version;\n+    this.bytesRead = bytesRead;\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.cacheSessionId = cacheSessionId;\n+  }\n+\n+  /**\n+   * Deserialize {@link CosmosChangeFeedFindToken} object from input stream.\n+   * @param inputStream {@link DataOutputStream} to deserialize from.\n+   * @return {@link CosmosChangeFeedFindToken} object.\n+   * @throws IOException\n+   */\n+  public static CosmosChangeFeedFindToken fromBytes(DataInputStream inputStream) throws IOException {\n+    CosmosChangeFeedFindToken cosmosChangeFeedFindToken;\n+    DataInputStream stream = new DataInputStream(inputStream);\n+    short version = stream.readShort();\n+    switch (version) {\n+      case VERSION_0:\n+        FindTokenType type = FindTokenType.values()[stream.readShort()];\n+        if (type != FindTokenType.CloudBased) {\n+          throw new IllegalArgumentException(\n+              String.format(\"Invalid token type %s found while deserialization. Expected %s.\", type,\n+                  FindTokenType.CloudBased));\n+        }\n+        long bytesRead = stream.readLong();\n+        String startContinuationToken = Utils.readIntString(inputStream);\n+        String endContinuationToken = Utils.readIntString(inputStream);\n+        int index = inputStream.readInt();\n+        int totalItems = inputStream.readInt();\n+        String cacheSessionId = Utils.readIntString(inputStream);\n+        cosmosChangeFeedFindToken =\n+            new CosmosChangeFeedFindToken(bytesRead, startContinuationToken, endContinuationToken, index, totalItems,\n+                cacheSessionId, version);\n+        break;\n+      default:\n+        throw new IllegalStateException(\"Unknown version of cloud token: \" + version);\n+    }\n+    return cosmosChangeFeedFindToken;\n+  }\n+\n+  /**\n+   * Serialize {@link CosmosChangeFeedFindToken} to byte array.\n+   * @return serialized byte array.\n+   */\n+  public byte[] toBytes() {\n+    byte[] buf = new byte[size()];\n+    ByteBuffer bufWrap = ByteBuffer.wrap(buf);\n+    bufWrap.putShort(getVersion());\n+    bufWrap.putShort((short) getType().ordinal());\n+    bufWrap.putLong(getBytesRead());\n+    Utils.serializeNullableString(bufWrap, startContinuationToken);\n+    Utils.serializeNullableString(bufWrap, endContinuationToken);\n+    bufWrap.putInt(index);\n+    bufWrap.putInt(totalItems);\n+    Utils.serializeNullableString(bufWrap, cacheSessionId);\n+    return buf;\n+  }\n+\n+  /**\n+   * Calculate size of the token.\n+   * @return size of the token.\n+   */\n+  public int size() {\n+    return 2 * Short.BYTES + Long.BYTES + 5 * Integer.BYTES + getNullableStringLength(startContinuationToken)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 149}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NDYwMTcw", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358460170", "createdAt": "2020-02-13T18:42:03Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODo0MjowM1rOFpfOsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODo0MjowM1rOFpfOsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA0NzYwMg==", "bodyText": "This will probably work for us, but if someone passed a high enough maxFeedSize to result in multiple feed pages, single() will throw an exception.  Maybe put a TODO/FIXME comment for now.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379047602", "createdAt": "2020-02-13T18:42:03Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -155,6 +157,44 @@ void testConnectivity() {\n     }\n   }\n \n+  /**\n+   * Query Cosmos change feed to get the next set of {@code CloudBlobMetadata} objects in specified {@code partitionPath}\n+   * after {@code requestContinationToken}, capped by specified {@code maxFeedSize} representing the max number of items to\n+   * be queried from the change feed.\n+   * @param requestContinationToken Continuation token after which change feed is requested.\n+   * @param maxFeedSize max item count to be requested in the feed query.\n+   * @param changeFeed {@link CloudBlobMetadata} {@code List} to be populated with the next set of entries returned by change feed query.\n+   * @param partitionPath partition for which the change feed is requested.\n+   * @return next continuation token.\n+   * @throws DocumentClientException\n+   */\n+  public String queryChangeFeed(String requestContinationToken, int maxFeedSize, List<CloudBlobMetadata> changeFeed,\n+      String partitionPath) throws DocumentClientException {\n+    ChangeFeedOptions changeFeedOptions = new ChangeFeedOptions();\n+    changeFeedOptions.setPartitionKey(new PartitionKey(partitionPath));\n+    changeFeedOptions.setMaxItemCount(maxFeedSize);\n+    if (requestContinationToken == null) {\n+      changeFeedOptions.setStartFromBeginning(true);\n+    } else {\n+      changeFeedOptions.setRequestContinuation(requestContinationToken);\n+    }\n+    try {\n+      FeedResponse<Document> feedResponse =\n+          asyncDocumentClient.queryDocumentChangeFeed(cosmosCollectionLink, changeFeedOptions)\n+              .limit(1)\n+              .toBlocking()\n+              .single();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NDYzOTM0", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358463934", "createdAt": "2020-02-13T18:47:46Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODo0Nzo0NlrOFpfabA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODo0Nzo0NlrOFpfabA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA1MDYwNA==", "bodyText": "Rather than casting in every line, cast once to cosmosFindToken and reuse.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379050604", "createdAt": "2020-02-13T18:47:46Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/azure/AzureCloudDestinationTest.java", "diffHunk": "@@ -393,6 +416,71 @@ public void testGetDeadBlobs() throws Exception {\n   /** Test findEntriesSince. */\n   @Test\n   public void testFindEntriesSince() throws Exception {\n+    switch (azureReplicationFeedType) {\n+      case COSMOS_CHANGE_FEED:\n+        testFindEntriesSinceUsingChangeFeed();\n+        break;\n+      case COSMOS_UPDATE_TIME:\n+        testFindEntriesSinceUsingUpdateTime();\n+        break;\n+    }\n+  }\n+\n+  /** Test findEntriesSince when cloud destination uses change feed based token. */\n+  private void testFindEntriesSinceUsingChangeFeed() throws Exception {\n+    long chunkSize = 110000;\n+    long maxTotalSize = 1000000; // between 9 and 10 chunks\n+    long startTime = System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1);\n+    int totalBlobs = 20;\n+\n+    // create metadata list where total size > maxTotalSize\n+    List<String> blobIdList = new ArrayList<>();\n+    List<CloudBlobMetadata> cloudBlobMetadataList = new ArrayList<>();\n+    for (int j = 0; j < totalBlobs; j++) {\n+      BlobId blobId = generateBlobId();\n+      blobIdList.add(blobId.getID());\n+      CloudBlobMetadata inputMetadata = new CloudBlobMetadata(blobId, creationTime, Utils.Infinite_Time, chunkSize,\n+          CloudBlobMetadata.EncryptionOrigin.NONE);\n+      inputMetadata.setUploadTime(startTime + j);\n+      cloudBlobMetadataList.add(inputMetadata);\n+    }\n+\n+    MockChangeFeedQuery mockChangeFeedQuery = new MockChangeFeedQuery();\n+    AzureReplicationFeed azureReplicationFeed = getReplicationFeedObj(mockChangeFeedQuery);\n+    FieldSetter.setField(azureDest, azureDest.getClass().getDeclaredField(\"azureReplicationFeed\"),\n+        azureReplicationFeed);\n+    cloudBlobMetadataList.stream().forEach(doc -> mockChangeFeedQuery.add(doc));\n+    FindToken findToken = new CosmosChangeFeedFindToken();\n+    // Run the query\n+    FindResult findResult = azureDest.findEntriesSince(blobId.getPartition().toPathString(), findToken, maxTotalSize);\n+    List<CloudBlobMetadata> firstResult = findResult.getMetadataList();\n+    findToken = findResult.getUpdatedFindToken();\n+    assertEquals(\"Did not get expected doc count\", maxTotalSize / chunkSize, firstResult.size());\n+\n+    assertEquals(\"Find token has wrong end continuation token\", ((CosmosChangeFeedFindToken) findToken).getIndex(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 161}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NDY1NjA1", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358465605", "createdAt": "2020-02-13T18:50:17Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODo1MDoxN1rOFpffmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODo1MDoxN1rOFpffmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA1MTkzMA==", "bodyText": "It seems overkill to run every test with both feed types when only one test depends on it.  Please remove and make the two below findEntriesSince methods public.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379051930", "createdAt": "2020-02-13T18:50:17Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/azure/AzureCloudDestinationTest.java", "diffHunk": "@@ -95,6 +98,26 @@\n   private long creationTime = System.currentTimeMillis();\n   private long deletionTime = creationTime + 10000;\n   private long expirationTime = Utils.Infinite_Time;\n+  private AzureReplicationFeed.FeedType azureReplicationFeedType;\n+\n+  /**\n+   * Parameterized constructor.\n+   * @param azureReplicationFeedType type of replication feed used by {@link AzureCloudDestination}\n+   */\n+  public AzureCloudDestinationTest(AzureReplicationFeed.FeedType azureReplicationFeedType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NDY1OTE1", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358465915", "createdAt": "2020-02-13T18:50:44Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODo1MDo0NFrOFpfgmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODo1MDo0NFrOFpfgmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA1MjE4NA==", "bodyText": "Remove test", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379052184", "createdAt": "2020-02-13T18:50:44Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/azure/AzureCloudDestinationTest.java", "diffHunk": "@@ -393,6 +416,71 @@ public void testGetDeadBlobs() throws Exception {\n   /** Test findEntriesSince. */\n   @Test\n   public void testFindEntriesSince() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 119}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NDY2MTQw", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358466140", "createdAt": "2020-02-13T18:51:05Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODo1MTowNlrOFpfhRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODo1MTowNlrOFpfhRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA1MjM1OQ==", "bodyText": "Make public @test", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379052359", "createdAt": "2020-02-13T18:51:06Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/azure/AzureCloudDestinationTest.java", "diffHunk": "@@ -393,6 +416,71 @@ public void testGetDeadBlobs() throws Exception {\n   /** Test findEntriesSince. */\n   @Test\n   public void testFindEntriesSince() throws Exception {\n+    switch (azureReplicationFeedType) {\n+      case COSMOS_CHANGE_FEED:\n+        testFindEntriesSinceUsingChangeFeed();\n+        break;\n+      case COSMOS_UPDATE_TIME:\n+        testFindEntriesSinceUsingUpdateTime();\n+        break;\n+    }\n+  }\n+\n+  /** Test findEntriesSince when cloud destination uses change feed based token. */\n+  private void testFindEntriesSinceUsingChangeFeed() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 131}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NDY2MzQ4", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358466348", "createdAt": "2020-02-13T18:51:25Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODo1MToyNVrOFpfh4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODo1MToyNVrOFpfh4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA1MjUxMg==", "bodyText": "Make public @test", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379052512", "createdAt": "2020-02-13T18:51:25Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/azure/AzureCloudDestinationTest.java", "diffHunk": "@@ -393,6 +416,71 @@ public void testGetDeadBlobs() throws Exception {\n   /** Test findEntriesSince. */\n   @Test\n   public void testFindEntriesSince() throws Exception {\n+    switch (azureReplicationFeedType) {\n+      case COSMOS_CHANGE_FEED:\n+        testFindEntriesSinceUsingChangeFeed();\n+        break;\n+      case COSMOS_UPDATE_TIME:\n+        testFindEntriesSinceUsingUpdateTime();\n+        break;\n+    }\n+  }\n+\n+  /** Test findEntriesSince when cloud destination uses change feed based token. */\n+  private void testFindEntriesSinceUsingChangeFeed() throws Exception {\n+    long chunkSize = 110000;\n+    long maxTotalSize = 1000000; // between 9 and 10 chunks\n+    long startTime = System.currentTimeMillis() - TimeUnit.DAYS.toMillis(1);\n+    int totalBlobs = 20;\n+\n+    // create metadata list where total size > maxTotalSize\n+    List<String> blobIdList = new ArrayList<>();\n+    List<CloudBlobMetadata> cloudBlobMetadataList = new ArrayList<>();\n+    for (int j = 0; j < totalBlobs; j++) {\n+      BlobId blobId = generateBlobId();\n+      blobIdList.add(blobId.getID());\n+      CloudBlobMetadata inputMetadata = new CloudBlobMetadata(blobId, creationTime, Utils.Infinite_Time, chunkSize,\n+          CloudBlobMetadata.EncryptionOrigin.NONE);\n+      inputMetadata.setUploadTime(startTime + j);\n+      cloudBlobMetadataList.add(inputMetadata);\n+    }\n+\n+    MockChangeFeedQuery mockChangeFeedQuery = new MockChangeFeedQuery();\n+    AzureReplicationFeed azureReplicationFeed = getReplicationFeedObj(mockChangeFeedQuery);\n+    FieldSetter.setField(azureDest, azureDest.getClass().getDeclaredField(\"azureReplicationFeed\"),\n+        azureReplicationFeed);\n+    cloudBlobMetadataList.stream().forEach(doc -> mockChangeFeedQuery.add(doc));\n+    FindToken findToken = new CosmosChangeFeedFindToken();\n+    // Run the query\n+    FindResult findResult = azureDest.findEntriesSince(blobId.getPartition().toPathString(), findToken, maxTotalSize);\n+    List<CloudBlobMetadata> firstResult = findResult.getMetadataList();\n+    findToken = findResult.getUpdatedFindToken();\n+    assertEquals(\"Did not get expected doc count\", maxTotalSize / chunkSize, firstResult.size());\n+\n+    assertEquals(\"Find token has wrong end continuation token\", ((CosmosChangeFeedFindToken) findToken).getIndex(),\n+        firstResult.size());\n+    assertEquals(\"Find token has wrong totalItems count\", ((CosmosChangeFeedFindToken) findToken).getTotalItems(),\n+        Math.min(blobIdList.size(), AzureCloudDestination.getFindSinceQueryLimit()));\n+    cloudBlobMetadataList = cloudBlobMetadataList.subList(firstResult.size(), cloudBlobMetadataList.size());\n+\n+    findResult = azureDest.findEntriesSince(blobId.getPartition().toPathString(), findToken, maxTotalSize);\n+    List<CloudBlobMetadata> secondResult = findResult.getMetadataList();\n+    findToken = findResult.getUpdatedFindToken();\n+\n+    assertEquals(\"Unexpected doc count\", maxTotalSize / chunkSize, secondResult.size());\n+    assertEquals(\"Unexpected first blobId\", blobIdList.get(firstResult.size()), secondResult.get(0).getId());\n+\n+    assertEquals(\"Find token has wrong totalItems count\", ((CosmosChangeFeedFindToken) findToken).getTotalItems(),\n+        Math.min(blobIdList.size(), AzureCloudDestination.getFindSinceQueryLimit()));\n+\n+    // Rerun with max size below blob size, and make sure it returns one result\n+    findResult = azureDest.findEntriesSince(blobId.getPartition().toPathString(), findToken, chunkSize - 1);\n+    List<CloudBlobMetadata> thirdResult = findResult.getMetadataList();\n+    assertEquals(\"Expected one result\", 1, thirdResult.size());\n+  }\n+\n+  /** Test findEntriesSince when cloud destination uses update time based token. */\n+  private void testFindEntriesSinceUsingUpdateTime() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 184}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NDY4ODU3", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358468857", "createdAt": "2020-02-13T18:54:58Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODo1NDo1OFrOFpfpyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxODo1NDo1OFrOFpfpyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA1NDUzOA==", "bodyText": "Same comment as in unit test: only testFindEntriesSince() really needs this and other tests can be time consuming.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379054538", "createdAt": "2020-02-13T18:54:58Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/azure/AzureIntegrationTest.java", "diffHunk": "@@ -73,6 +82,31 @@\n   private String propFileName = \"azure-test.properties\";\n   private String tokenFileName = \"replicaTokens\";\n \n+  /**\n+   * Parameterized constructor.\n+   * @param replicationCloudTokenFactory type of token factory used by {@link CloudDestination}\n+   */\n+  public AzureIntegrationTest(String replicationCloudTokenFactory) throws ReflectiveOperationException {\n+    super();\n+    this.replicationCloudTokenFactory = replicationCloudTokenFactory;\n+    Properties properties = new Properties();\n+    properties.setProperty(\"replication.cloud.token.factory\", replicationCloudTokenFactory);\n+    VerifiableProperties verifiableProperties = new VerifiableProperties(properties);\n+    ReplicationConfig replicationConfig = new ReplicationConfig(verifiableProperties);\n+    findTokenFactory =\n+        new FindTokenHelper(null, replicationConfig).getFindTokenFactoryFromReplicaType(ReplicaType.CLOUD_BACKED);\n+  }\n+\n+  /**\n+   * static method to generate parameters.\n+   * @return {@link Collection} of parameters.\n+   */\n+  @Parameterized.Parameters\n+  public static List<Object[]> input() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 81}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NDc3OTc3", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358477977", "createdAt": "2020-02-13T19:08:53Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxOTowODo1M1rOFpgE-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxOTowODo1M1rOFpgE-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA2MTQ5OA==", "bodyText": "The else clause has no test coverage.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379061498", "createdAt": "2020-02-13T19:08:53Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,256 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String cacheSessionId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+    private final String partitionId;\n+    private long creationTimestamp;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param cacheSessionId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String cacheSessionId,\n+        List<CloudBlobMetadata> fetchedEntries, String partitionId) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.cacheSessionId = cacheSessionId;\n+      this.fetchedEntries = fetchedEntries;\n+      this.partitionId = partitionId;\n+      this.creationTimestamp = System.currentTimeMillis();\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the Azure request id.\n+     * @return Azure request id.\n+     */\n+    String getCacheSessionId() {\n+      return cacheSessionId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+\n+    /**\n+     * Return the {@code partitionId}\n+     * @return {@code partitionId}\n+     */\n+    public String getPartitionId() {\n+      return partitionId;\n+    }\n+\n+    /**\n+     * Return the creation time stamp.\n+     * @return  {@code creationTimestamp}\n+     */\n+    public long getCreationimestamp() {\n+      return creationTimestamp;\n+    }\n+\n+    /**\n+     * Check if is this entry is expired. The condition for expiry uses {@code creationTimestamp}. This is good enough as\n+     * this means that the cached set of fetches entries hasn't been consumed within the invalidation duration.\n+     * @return true if this entry is expired. false otherwise.\n+     */\n+    boolean isExpired() {\n+      return creationTimestamp < System.currentTimeMillis() - CACHE_INVALIDATION_DURATION_IN_MILLIS;\n+    }\n+  }\n+\n+  // change feed cache by cache session id\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+  private final static long CACHE_INVALIDATION_DURATION_IN_MILLIS = 60 * 60 * 1000; //1 hour\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedBasedReplicationFeed} object.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   * @param azureMetrics{@link {@link AzureMetrics} object.\n+   */\n+  public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccessor, AzureMetrics azureMetrics) {\n+    this.defaultCacheSize = AzureCloudDestination.getFindSinceQueryLimit();\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.azureMetrics = azureMetrics;\n+    // schedule periodic invalidation of cache\n+    Utils.newScheduler(1, false)\n+        .scheduleAtFixedRate(() -> changeFeedCache.entrySet().removeIf(entry -> entry.getValue().isExpired()),\n+            CACHE_INVALIDATION_DURATION_IN_MILLIS, CACHE_INVALIDATION_DURATION_IN_MILLIS, TimeUnit.MILLISECONDS);\n+  }\n+\n+  /**\n+   * Get next set of change feed entries for the specified partition, after the {@code cosmosChangeFeedFindToken}.\n+   * The number of entries is capped by maxEntriesSize.\n+   * This method creates a cache for change feed entries. If the {@code cosmosChangeFeedFindToken} is not valid,\n+   * or if all the items in the cache are consumed, then it queries Cosmos for new entries.\n+   * @param cosmosChangeFeedFindToken {@link CosmosChangeFeedFindToken} after which the next entries have to be returned.\n+   * @param results {@link List} of {@link CloudBlobMetadata} objects which will be populated by new entries.\n+   * @param maxEntriesSize maximum size of all the blobs returned in {@code results}\n+   * @param partitionId Partition for which change feed entries have to be returned.\n+   * @return updated {@link CosmosChangeFeedFindToken} after processing the next set of entries.\n+   */\n+  private CosmosChangeFeedFindToken getNextEntriesAndToken(CosmosChangeFeedFindToken cosmosChangeFeedFindToken,\n+      List<CloudBlobMetadata> results, long maxEntriesSize, String partitionId) throws DocumentClientException {\n+    int index = cosmosChangeFeedFindToken.getIndex();\n+    String cacheSesionId = cosmosChangeFeedFindToken.getCacheSessionId();\n+    if (!changeFeedCache.containsKey(cacheSesionId) || !isCacheValid(partitionId, cosmosChangeFeedFindToken)) {\n+      // the cache may not be valid. So we cannot use session id\n+      cacheSesionId = populateChangeFeedCache(partitionId, cosmosChangeFeedFindToken.getStartContinuationToken());\n+      // invalidate the previous token's cache\n+      changeFeedCache.remove(cosmosChangeFeedFindToken.getCacheSessionId());\n+      index = 0;\n+    }\n+\n+    long resultSize = 0;\n+\n+    List<CloudBlobMetadata> fetchedEntries = changeFeedCache.get(cacheSesionId).getFetchedEntries();\n+    while (true) {\n+      if (index < fetchedEntries.size()) {\n+        if (resultSize + fetchedEntries.get(index).getSize() < maxEntriesSize || resultSize == 0) {\n+          results.add(fetchedEntries.get(index));\n+          resultSize = resultSize + fetchedEntries.get(index).getSize();\n+          index++;\n+        } else {\n+          break;\n+        }\n+      } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 179}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NDgxMzQx", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358481341", "createdAt": "2020-02-13T19:14:12Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxOToxNDoxMlrOFpgPMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxOToxNDoxMlrOFpgPMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA2NDExNQ==", "bodyText": "Because of this override, CosmosDataAccessor.queryChangeFeed() has no test coverage.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379064115", "createdAt": "2020-02-13T19:14:12Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/azure/MockChangeFeedQuery.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.microsoft.azure.cosmosdb.rx.AsyncDocumentClient;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.mockito.Mockito.*;\n+\n+\n+public class MockChangeFeedQuery extends CosmosDataAccessor {\n+  private final Map<String, String> continuationTokenToBlobIdMap = new HashMap<>();\n+  private final Map<String, String> blobIdToContinuationTokenMap = new HashMap<>();\n+  private final Map<String, CloudBlobMetadata> blobIdToMetadataMap = new HashMap<>();\n+  private int continuationTokenCounter = -1;\n+\n+  MockChangeFeedQuery() {\n+    super(mock(AsyncDocumentClient.class), \"\", mock(AzureMetrics.class));\n+  }\n+\n+  /**\n+   * Add a blobid to the change feed.\n+   * @param cloudBlobMetadata {@link CloudBlobMetadata} to add.\n+   */\n+  void add(CloudBlobMetadata cloudBlobMetadata) {\n+    String blobId = cloudBlobMetadata.getId();\n+    blobIdToMetadataMap.put(blobId, cloudBlobMetadata);\n+    if (blobIdToContinuationTokenMap.containsKey(blobId)) {\n+      continuationTokenToBlobIdMap.put(blobIdToContinuationTokenMap.get(blobId), null);\n+    }\n+    continuationTokenToBlobIdMap.put(Integer.toString(++continuationTokenCounter), blobId);\n+    blobIdToContinuationTokenMap.put(blobId, Integer.toString(continuationTokenCounter));\n+  }\n+\n+  public String queryChangeFeed(String requestContinuationToken, int maxFeedSize, List<CloudBlobMetadata> changeFeed,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NDgyMTQ3", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358482147", "createdAt": "2020-02-13T19:15:35Z", "commit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxOToxNTozNVrOFpgRuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxOToxNTozNVrOFpgRuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA2NDc2MA==", "bodyText": "Method has no test coverage.  Can you add test case to CosmosDataAccessorTest?", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379064760", "createdAt": "2020-02-13T19:15:35Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -155,6 +157,44 @@ void testConnectivity() {\n     }\n   }\n \n+  /**\n+   * Query Cosmos change feed to get the next set of {@code CloudBlobMetadata} objects in specified {@code partitionPath}\n+   * after {@code requestContinationToken}, capped by specified {@code maxFeedSize} representing the max number of items to\n+   * be queried from the change feed.\n+   * @param requestContinationToken Continuation token after which change feed is requested.\n+   * @param maxFeedSize max item count to be requested in the feed query.\n+   * @param changeFeed {@link CloudBlobMetadata} {@code List} to be populated with the next set of entries returned by change feed query.\n+   * @param partitionPath partition for which the change feed is requested.\n+   * @return next continuation token.\n+   * @throws DocumentClientException\n+   */\n+  public String queryChangeFeed(String requestContinationToken, int maxFeedSize, List<CloudBlobMetadata> changeFeed,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e7f972396e1f0f4873a79739c0f3cb1445778e2"}, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NTUxMDUy", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358551052", "createdAt": "2020-02-13T21:07:07Z", "commit": {"oid": "256d7caa1a1b5798084c6f342f4584a6f91346b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMTowNzowN1rOFpjj1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMTowNzowN1rOFpjj1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTExODU0OQ==", "bodyText": "Newline missing", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379118549", "createdAt": "2020-02-13T21:07:07Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -138,27 +138,30 @@ public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccesso\n     // schedule periodic invalidation of cache\n     Utils.newScheduler(1, false)\n         .scheduleAtFixedRate(() -> changeFeedCache.entrySet().removeIf(entry -> entry.getValue().isExpired()),\n-            CACHE_INVALIDATION_DURATION_IN_MILLIS, CACHE_INVALIDATION_DURATION_IN_MILLIS, TimeUnit.MILLISECONDS);\n+            CACHE_VALID_DURATION_IN_MS, CACHE_VALID_DURATION_IN_MS, TimeUnit.MILLISECONDS);\n   }\n \n+  @Override\n   /**\n-   * Get next set of change feed entries for the specified partition, after the {@code cosmosChangeFeedFindToken}.\n+   * Get next set of change feed entries for the specified partition, after the {@code curFindToken}.\n    * The number of entries is capped by maxEntriesSize.\n-   * This method creates a cache for change feed entries. If the {@code cosmosChangeFeedFindToken} is not valid,\n+   * This method creates a cache for change feed entries. If the {@code curFindToken} is not valid,\n    * or if all the items in the cache are consumed, then it queries Cosmos for new entries.\n-   * @param cosmosChangeFeedFindToken {@link CosmosChangeFeedFindToken} after which the next entries have to be returned.\n-   * @param results {@link List} of {@link CloudBlobMetadata} objects which will be populated by new entries.\n-   * @param maxEntriesSize maximum size of all the blobs returned in {@code results}\n-   * @param partitionId Partition for which change feed entries have to be returned.\n-   * @return updated {@link CosmosChangeFeedFindToken} after processing the next set of entries.\n-   */\n-  private CosmosChangeFeedFindToken getNextEntriesAndToken(CosmosChangeFeedFindToken cosmosChangeFeedFindToken,\n-      List<CloudBlobMetadata> results, long maxEntriesSize, String partitionId) throws DocumentClientException {\n+   * @param curFindToken {@link FindToken} after which the next entries have to be returned.\n+   * @param maxTotalSizeOfEntries maximum size of all the blobs returned.\n+   * @param partitionPath Partition for which change feed entries have to be returned.\n+   * @return {@link FindResult} instance that contains updated {@link FindToken} object which can act as a bookmark for\n+   * subsequent requests, and {@link List} of {@link CloudBlobMetadata} entries.\n+   * @throws {@link DocumentClientException}.\n+   */ public FindResult getNextEntriesAndUpdatedToken(FindToken curFindToken, long maxTotalSizeOfEntries,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "256d7caa1a1b5798084c6f342f4584a6f91346b4"}, "originalPosition": 57}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NTUzMzA0", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358553304", "createdAt": "2020-02-13T21:10:57Z", "commit": {"oid": "256d7caa1a1b5798084c6f342f4584a6f91346b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMToxMDo1N1rOFpjqtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMToxMDo1N1rOFpjqtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTEyMDMxMA==", "bodyText": "Need Test annotation in these two methods?", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379120310", "createdAt": "2020-02-13T21:10:57Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/azure/AzureCloudDestinationTest.java", "diffHunk": "@@ -413,21 +392,8 @@ public void testGetDeadBlobs() throws Exception {\n     assertEquals(1, azureMetrics.deadBlobsQueryTime.getCount());\n   }\n \n-  /** Test findEntriesSince. */\n-  @Test\n-  public void testFindEntriesSince() throws Exception {\n-    switch (azureReplicationFeedType) {\n-      case COSMOS_CHANGE_FEED:\n-        testFindEntriesSinceUsingChangeFeed();\n-        break;\n-      case COSMOS_UPDATE_TIME:\n-        testFindEntriesSinceUsingUpdateTime();\n-        break;\n-    }\n-  }\n-\n   /** Test findEntriesSince when cloud destination uses change feed based token. */\n-  private void testFindEntriesSinceUsingChangeFeed() throws Exception {\n+  public void testFindEntriesSinceUsingChangeFeed() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "256d7caa1a1b5798084c6f342f4584a6f91346b4"}, "originalPosition": 102}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NjkzNzM4", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358693738", "createdAt": "2020-02-14T03:27:58Z", "commit": {"oid": "746572261bfae9864566f6f59adb26afff96a8f0"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMzoyNzo1OFrOFpqrfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMzo0Nzo1NlrOFpq5Jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIzNTE5Nw==", "bodyText": "this class can be static, since it doesn't use any fields from the outer class", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379235197", "createdAt": "2020-02-14T03:27:58Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "746572261bfae9864566f6f59adb26afff96a8f0"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIzNTMwNw==", "bodyText": "make upper case if this is meant to be a constant", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379235307", "createdAt": "2020-02-14T03:28:34Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureCloudDestination.java", "diffHunk": "@@ -65,28 +62,19 @@\n   private static final Logger logger = LoggerFactory.getLogger(AzureCloudDestination.class);\n   private static final String THRESHOLD_PARAM = \"@threshold\";\n   private static final String LIMIT_PARAM = \"@limit\";\n-  private static final String TIME_SINCE_PARAM = \"@timesince\";\n   private static final String BATCH_ID_QUERY_TEMPLATE = \"SELECT * FROM c WHERE c.id IN (%s)\";\n   static final int ID_QUERY_BATCH_SIZE = 1000;\n+  private static final int findSinceQueryLimit = 1000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "746572261bfae9864566f6f59adb26afff96a8f0"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIzNTQ5Ng==", "bodyText": "cacheSessionId", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379235496", "createdAt": "2020-02-14T03:29:49Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String cacheSessionId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+    private final String partitionId;\n+    private long creationTimestamp;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param cacheSessionId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String cacheSessionId,\n+        List<CloudBlobMetadata> fetchedEntries, String partitionId) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.cacheSessionId = cacheSessionId;\n+      this.fetchedEntries = fetchedEntries;\n+      this.partitionId = partitionId;\n+      this.creationTimestamp = System.currentTimeMillis();\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the Azure request id.\n+     * @return Azure request id.\n+     */\n+    String getCacheSessionId() {\n+      return cacheSessionId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+\n+    /**\n+     * Return the {@code partitionId}\n+     * @return {@code partitionId}\n+     */\n+    public String getPartitionId() {\n+      return partitionId;\n+    }\n+\n+    /**\n+     * Return the creation time stamp.\n+     * @return  {@code creationTimestamp}\n+     */\n+    public long getCreationTimestamp() {\n+      return creationTimestamp;\n+    }\n+\n+    /**\n+     * Check if is this entry is expired. The condition for expiry uses {@code creationTimestamp}. This is good enough as\n+     * this means that the cached set of fetches entries hasn't been consumed within the invalidation duration.\n+     * @return true if this entry is expired. false otherwise.\n+     */\n+    boolean isExpired() {\n+      return creationTimestamp < System.currentTimeMillis() - CACHE_VALID_DURATION_IN_MS;\n+    }\n+  }\n+\n+  // change feed cache by cache session id\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+  private final static long CACHE_VALID_DURATION_IN_MS = 60 * 60 * 1000; //1 hour\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedBasedReplicationFeed} object.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   * @param azureMetrics{@link {@link AzureMetrics} object.\n+   */\n+  public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccessor, AzureMetrics azureMetrics) {\n+    this.defaultCacheSize = AzureCloudDestination.getFindSinceQueryLimit();\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.azureMetrics = azureMetrics;\n+    // schedule periodic invalidation of cache\n+    Utils.newScheduler(1, false)\n+        .scheduleAtFixedRate(() -> changeFeedCache.entrySet().removeIf(entry -> entry.getValue().isExpired()),\n+            CACHE_VALID_DURATION_IN_MS, CACHE_VALID_DURATION_IN_MS, TimeUnit.MILLISECONDS);\n+  }\n+\n+  /**\n+   * Get next set of change feed entries for the specified partition, after the {@code curFindToken}.\n+   * The number of entries is capped by maxEntriesSize.\n+   * This method creates a cache for change feed entries. If the {@code curFindToken} is not valid,\n+   * or if all the items in the cache are consumed, then it queries Cosmos for new entries.\n+   * @param curFindToken {@link FindToken} after which the next entries have to be returned.\n+   * @param maxTotalSizeOfEntries maximum size of all the blobs returned.\n+   * @param partitionPath Partition for which change feed entries have to be returned.\n+   * @return {@link FindResult} instance that contains updated {@link FindToken} object which can act as a bookmark for\n+   * subsequent requests, and {@link List} of {@link CloudBlobMetadata} entries.\n+   * @throws {@link DocumentClientException}.\n+   */\n+  @Override\n+  public FindResult getNextEntriesAndUpdatedToken(FindToken curFindToken, long maxTotalSizeOfEntries,\n+      String partitionPath) throws DocumentClientException {\n+    List<CloudBlobMetadata> nextEntries = new ArrayList<>();\n+    CosmosChangeFeedFindToken cosmosChangeFeedFindToken = (CosmosChangeFeedFindToken) curFindToken;\n+    int index = cosmosChangeFeedFindToken.getIndex();\n+    String cacheSesionId = cosmosChangeFeedFindToken.getCacheSessionId();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "746572261bfae9864566f6f59adb26afff96a8f0"}, "originalPosition": 162}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIzNTc2OA==", "bodyText": "you can use TimeUnit.HOURS.toMillis(1) here", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379235768", "createdAt": "2020-02-14T03:31:20Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String cacheSessionId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+    private final String partitionId;\n+    private long creationTimestamp;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param cacheSessionId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String cacheSessionId,\n+        List<CloudBlobMetadata> fetchedEntries, String partitionId) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.cacheSessionId = cacheSessionId;\n+      this.fetchedEntries = fetchedEntries;\n+      this.partitionId = partitionId;\n+      this.creationTimestamp = System.currentTimeMillis();\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the Azure request id.\n+     * @return Azure request id.\n+     */\n+    String getCacheSessionId() {\n+      return cacheSessionId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+\n+    /**\n+     * Return the {@code partitionId}\n+     * @return {@code partitionId}\n+     */\n+    public String getPartitionId() {\n+      return partitionId;\n+    }\n+\n+    /**\n+     * Return the creation time stamp.\n+     * @return  {@code creationTimestamp}\n+     */\n+    public long getCreationTimestamp() {\n+      return creationTimestamp;\n+    }\n+\n+    /**\n+     * Check if is this entry is expired. The condition for expiry uses {@code creationTimestamp}. This is good enough as\n+     * this means that the cached set of fetches entries hasn't been consumed within the invalidation duration.\n+     * @return true if this entry is expired. false otherwise.\n+     */\n+    boolean isExpired() {\n+      return creationTimestamp < System.currentTimeMillis() - CACHE_VALID_DURATION_IN_MS;\n+    }\n+  }\n+\n+  // change feed cache by cache session id\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+  private final static long CACHE_VALID_DURATION_IN_MS = 60 * 60 * 1000; //1 hour", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "746572261bfae9864566f6f59adb26afff96a8f0"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIzODY5NQ==", "bodyText": "Since you are not using daemon threads and the executor is not shutdown, this may block the jvm process from ending in some cases.\nThe server/vcr does have a shared scheduler that could be passed into here, if its not too cumbersome to get it all the way here. Alternatively you could make ReplicationFeed closable and shutdown the ExecutorService yourself.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379238695", "createdAt": "2020-02-14T03:47:56Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String cacheSessionId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+    private final String partitionId;\n+    private long creationTimestamp;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param cacheSessionId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String cacheSessionId,\n+        List<CloudBlobMetadata> fetchedEntries, String partitionId) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.cacheSessionId = cacheSessionId;\n+      this.fetchedEntries = fetchedEntries;\n+      this.partitionId = partitionId;\n+      this.creationTimestamp = System.currentTimeMillis();\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the Azure request id.\n+     * @return Azure request id.\n+     */\n+    String getCacheSessionId() {\n+      return cacheSessionId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+\n+    /**\n+     * Return the {@code partitionId}\n+     * @return {@code partitionId}\n+     */\n+    public String getPartitionId() {\n+      return partitionId;\n+    }\n+\n+    /**\n+     * Return the creation time stamp.\n+     * @return  {@code creationTimestamp}\n+     */\n+    public long getCreationTimestamp() {\n+      return creationTimestamp;\n+    }\n+\n+    /**\n+     * Check if is this entry is expired. The condition for expiry uses {@code creationTimestamp}. This is good enough as\n+     * this means that the cached set of fetches entries hasn't been consumed within the invalidation duration.\n+     * @return true if this entry is expired. false otherwise.\n+     */\n+    boolean isExpired() {\n+      return creationTimestamp < System.currentTimeMillis() - CACHE_VALID_DURATION_IN_MS;\n+    }\n+  }\n+\n+  // change feed cache by cache session id\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+  private final static long CACHE_VALID_DURATION_IN_MS = 60 * 60 * 1000; //1 hour\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedBasedReplicationFeed} object.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   * @param azureMetrics{@link {@link AzureMetrics} object.\n+   */\n+  public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccessor, AzureMetrics azureMetrics) {\n+    this.defaultCacheSize = AzureCloudDestination.getFindSinceQueryLimit();\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.azureMetrics = azureMetrics;\n+    // schedule periodic invalidation of cache\n+    Utils.newScheduler(1, false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "746572261bfae9864566f6f59adb26afff96a8f0"}, "originalPosition": 139}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NzExMzE5", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358711319", "createdAt": "2020-02-14T04:51:40Z", "commit": {"oid": "05b647952a32686529b175f2abf679a8ef785444"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwNDo1MTo0MFrOFprk-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwNTozNjo0OFrOFpsDoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI0OTkxMg==", "bodyText": "if the populateChangeFeedCache methods returned the CacheEntry objects they added, you could get rid of a lot of the duplicate get calls to the cache in getNextEntriesAndUpdatedToken. It also may help to avoid some edge cases where the background cache eviction thread could remove items from the cache while the getNextEntriesAndUpdatedToken is executing (causing NPEs).", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379249912", "createdAt": "2020-02-14T04:51:40Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String cacheSessionId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+    private final String partitionId;\n+    private long creationTimestamp;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param cacheSessionId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String cacheSessionId,\n+        List<CloudBlobMetadata> fetchedEntries, String partitionId) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.cacheSessionId = cacheSessionId;\n+      this.fetchedEntries = fetchedEntries;\n+      this.partitionId = partitionId;\n+      this.creationTimestamp = System.currentTimeMillis();\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the Azure request id.\n+     * @return Azure request id.\n+     */\n+    String getCacheSessionId() {\n+      return cacheSessionId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+\n+    /**\n+     * Return the {@code partitionId}\n+     * @return {@code partitionId}\n+     */\n+    public String getPartitionId() {\n+      return partitionId;\n+    }\n+\n+    /**\n+     * Return the creation time stamp.\n+     * @return  {@code creationTimestamp}\n+     */\n+    public long getCreationTimestamp() {\n+      return creationTimestamp;\n+    }\n+\n+    /**\n+     * Check if is this entry is expired. The condition for expiry uses {@code creationTimestamp}. This is good enough as\n+     * this means that the cached set of fetches entries hasn't been consumed within the invalidation duration.\n+     * @return true if this entry is expired. false otherwise.\n+     */\n+    boolean isExpired() {\n+      return creationTimestamp < System.currentTimeMillis() - CACHE_VALID_DURATION_IN_MS;\n+    }\n+  }\n+\n+  // change feed cache by cache session id\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+  private final static long CACHE_VALID_DURATION_IN_MS = 60 * 60 * 1000; //1 hour\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedBasedReplicationFeed} object.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   * @param azureMetrics{@link {@link AzureMetrics} object.\n+   */\n+  public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccessor, AzureMetrics azureMetrics) {\n+    this.defaultCacheSize = AzureCloudDestination.getFindSinceQueryLimit();\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.azureMetrics = azureMetrics;\n+    // schedule periodic invalidation of cache\n+    Utils.newScheduler(1, false)\n+        .scheduleAtFixedRate(() -> changeFeedCache.entrySet().removeIf(entry -> entry.getValue().isExpired()),\n+            CACHE_VALID_DURATION_IN_MS, CACHE_VALID_DURATION_IN_MS, TimeUnit.MILLISECONDS);\n+  }\n+\n+  /**\n+   * Get next set of change feed entries for the specified partition, after the {@code curFindToken}.\n+   * The number of entries is capped by maxEntriesSize.\n+   * This method creates a cache for change feed entries. If the {@code curFindToken} is not valid,\n+   * or if all the items in the cache are consumed, then it queries Cosmos for new entries.\n+   * @param curFindToken {@link FindToken} after which the next entries have to be returned.\n+   * @param maxTotalSizeOfEntries maximum size of all the blobs returned.\n+   * @param partitionPath Partition for which change feed entries have to be returned.\n+   * @return {@link FindResult} instance that contains updated {@link FindToken} object which can act as a bookmark for\n+   * subsequent requests, and {@link List} of {@link CloudBlobMetadata} entries.\n+   * @throws {@link DocumentClientException}.\n+   */\n+  @Override\n+  public FindResult getNextEntriesAndUpdatedToken(FindToken curFindToken, long maxTotalSizeOfEntries,\n+      String partitionPath) throws DocumentClientException {\n+    List<CloudBlobMetadata> nextEntries = new ArrayList<>();\n+    CosmosChangeFeedFindToken cosmosChangeFeedFindToken = (CosmosChangeFeedFindToken) curFindToken;\n+    int index = cosmosChangeFeedFindToken.getIndex();\n+    String cacheSesionId = cosmosChangeFeedFindToken.getCacheSessionId();\n+    if (!changeFeedCache.containsKey(cacheSesionId) || !isCacheValid(partitionPath, cosmosChangeFeedFindToken)) {\n+      // the cache may not be valid. So we cannot use session id\n+      cacheSesionId = populateChangeFeedCache(partitionPath, cosmosChangeFeedFindToken.getStartContinuationToken());\n+      // invalidate the previous token's cache\n+      changeFeedCache.remove(cosmosChangeFeedFindToken.getCacheSessionId());\n+      index = 0;\n+    }\n+\n+    long resultSize = 0;\n+\n+    List<CloudBlobMetadata> fetchedEntries = changeFeedCache.get(cacheSesionId).getFetchedEntries();\n+    while (true) {\n+      if (index < fetchedEntries.size()) {\n+        if (resultSize + fetchedEntries.get(index).getSize() < maxTotalSizeOfEntries || resultSize == 0) {\n+          nextEntries.add(fetchedEntries.get(index));\n+          resultSize = resultSize + fetchedEntries.get(index).getSize();\n+          index++;\n+        } else {\n+          break;\n+        }\n+      } else {\n+        // we can reuse the session id in this case, because we know that the cache ran out of new items.\n+        populateChangeFeedCache(partitionPath, cosmosChangeFeedFindToken.getEndContinuationToken(),\n+            cosmosChangeFeedFindToken.getCacheSessionId());\n+        fetchedEntries = changeFeedCache.get(cacheSesionId).getFetchedEntries();\n+        if (fetchedEntries.isEmpty()) {\n+          // this means that there are no new changes\n+          break;\n+        }\n+        index = 0;\n+      }\n+    }\n+\n+    FindToken updatedToken = new CosmosChangeFeedFindToken(cosmosChangeFeedFindToken.getBytesRead() + resultSize,\n+        changeFeedCache.get(cacheSesionId).getStartContinuationToken(),\n+        changeFeedCache.get(cacheSesionId).getEndContinuationToken(), index,\n+        changeFeedCache.get(cacheSesionId).getFetchedEntries().size(), cacheSesionId,\n+        cosmosChangeFeedFindToken.getVersion());\n+    return new FindResult(nextEntries, updatedToken);\n+  }\n+\n+  /**\n+   * Check is the cache is valid for the {@code cosmosChangeFeedFindToken} provided.\n+   * @param partitionId partition of the {@code cosmosChangeFeedFindToken}.\n+   * @param cosmosChangeFeedFindToken {@link CosmosChangeFeedFindToken} object.\n+   * @return true is cache is valid. false otherwise.\n+   */\n+  private boolean isCacheValid(String partitionId, CosmosChangeFeedFindToken cosmosChangeFeedFindToken) {\n+    ChangeFeedCacheEntry changeFeedCacheEntry = changeFeedCache.get(cosmosChangeFeedFindToken.getCacheSessionId());\n+    return Objects.equals(cosmosChangeFeedFindToken.getCacheSessionId(), changeFeedCacheEntry.getCacheSessionId())\n+        && Objects.equals(cosmosChangeFeedFindToken.getStartContinuationToken(),\n+        changeFeedCacheEntry.getStartContinuationToken()) && Objects.equals(\n+        cosmosChangeFeedFindToken.getEndContinuationToken(), changeFeedCacheEntry.getEndContinuationToken())\n+        && cosmosChangeFeedFindToken.getTotalItems() == changeFeedCacheEntry.getFetchedEntries().size()\n+        && Objects.equals(partitionId, changeFeedCacheEntry.getPartitionId());\n+  }\n+\n+  /**\n+   * Populate change feed cache by querying Cosmos for the next set of change feed entries after the specified request\n+   * continuation token. Also generate a new session id for the cache.\n+   * @param partitionId Partition for which the change feed cache needs to be populated.\n+   * @param startRequestContinuationToken request continuation token from which the change feed query needs to be made.\n+   * @return new cache session id.\n+   */\n+  private String populateChangeFeedCache(String partitionId, String startRequestContinuationToken)\n+      throws DocumentClientException {\n+    String cacheSessionId = UUID.randomUUID().toString();\n+    populateChangeFeedCache(partitionId, startRequestContinuationToken, cacheSessionId);\n+    return cacheSessionId;\n+  }\n+\n+  /**\n+   * Populate change feed cache by querying Cosmos for the next set of change feed entries after the specified request continuation token.\n+   * @param partitionId Partition for which the change feed cache needs to be populated.\n+   * @param startRequestContinuationToken request continuation token from which the change feed query needs to be made.\n+   * @param cacheSessionId cacheSessionId to use in the cache.\n+   */\n+  private void populateChangeFeedCache(String partitionId, String startRequestContinuationToken, String cacheSessionId)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05b647952a32686529b175f2abf679a8ef785444"}, "originalPosition": 240}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI1MjEwMA==", "bodyText": "I would recommend trying to minimize the number of changeFeedCache.get calls since the background thread can change the state of the map from underneath.\nAssuming populateChangeFeedCache returns a CacheEntry object and isCacheValid takes in the CacheEntry instead of getting again, you could do something like:\nCacheEntry cacheEntry = changeFeedCache.get(sessionId);\nif (cacheEntry == null || !isCacheValid(cacheEntry, partitionPath, cosmosChangeFeedFindToken)) {\n  cacheEntry = populateChangeFeed(...);\n  ...\n}\n...\n    } else {\n        cacheEntry = populateChangeFeedCache(partitionPath, cosmosChangeFeedFindToken.getEndContinuationToken(),\n            cosmosChangeFeedFindToken.getCacheSessionId());", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379252100", "createdAt": "2020-02-14T05:05:10Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String cacheSessionId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+    private final String partitionId;\n+    private long creationTimestamp;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param cacheSessionId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String cacheSessionId,\n+        List<CloudBlobMetadata> fetchedEntries, String partitionId) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.cacheSessionId = cacheSessionId;\n+      this.fetchedEntries = fetchedEntries;\n+      this.partitionId = partitionId;\n+      this.creationTimestamp = System.currentTimeMillis();\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the Azure request id.\n+     * @return Azure request id.\n+     */\n+    String getCacheSessionId() {\n+      return cacheSessionId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+\n+    /**\n+     * Return the {@code partitionId}\n+     * @return {@code partitionId}\n+     */\n+    public String getPartitionId() {\n+      return partitionId;\n+    }\n+\n+    /**\n+     * Return the creation time stamp.\n+     * @return  {@code creationTimestamp}\n+     */\n+    public long getCreationTimestamp() {\n+      return creationTimestamp;\n+    }\n+\n+    /**\n+     * Check if is this entry is expired. The condition for expiry uses {@code creationTimestamp}. This is good enough as\n+     * this means that the cached set of fetches entries hasn't been consumed within the invalidation duration.\n+     * @return true if this entry is expired. false otherwise.\n+     */\n+    boolean isExpired() {\n+      return creationTimestamp < System.currentTimeMillis() - CACHE_VALID_DURATION_IN_MS;\n+    }\n+  }\n+\n+  // change feed cache by cache session id\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+  private final static long CACHE_VALID_DURATION_IN_MS = 60 * 60 * 1000; //1 hour\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedBasedReplicationFeed} object.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   * @param azureMetrics{@link {@link AzureMetrics} object.\n+   */\n+  public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccessor, AzureMetrics azureMetrics) {\n+    this.defaultCacheSize = AzureCloudDestination.getFindSinceQueryLimit();\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.azureMetrics = azureMetrics;\n+    // schedule periodic invalidation of cache\n+    Utils.newScheduler(1, false)\n+        .scheduleAtFixedRate(() -> changeFeedCache.entrySet().removeIf(entry -> entry.getValue().isExpired()),\n+            CACHE_VALID_DURATION_IN_MS, CACHE_VALID_DURATION_IN_MS, TimeUnit.MILLISECONDS);\n+  }\n+\n+  /**\n+   * Get next set of change feed entries for the specified partition, after the {@code curFindToken}.\n+   * The number of entries is capped by maxEntriesSize.\n+   * This method creates a cache for change feed entries. If the {@code curFindToken} is not valid,\n+   * or if all the items in the cache are consumed, then it queries Cosmos for new entries.\n+   * @param curFindToken {@link FindToken} after which the next entries have to be returned.\n+   * @param maxTotalSizeOfEntries maximum size of all the blobs returned.\n+   * @param partitionPath Partition for which change feed entries have to be returned.\n+   * @return {@link FindResult} instance that contains updated {@link FindToken} object which can act as a bookmark for\n+   * subsequent requests, and {@link List} of {@link CloudBlobMetadata} entries.\n+   * @throws {@link DocumentClientException}.\n+   */\n+  @Override\n+  public FindResult getNextEntriesAndUpdatedToken(FindToken curFindToken, long maxTotalSizeOfEntries,\n+      String partitionPath) throws DocumentClientException {\n+    List<CloudBlobMetadata> nextEntries = new ArrayList<>();\n+    CosmosChangeFeedFindToken cosmosChangeFeedFindToken = (CosmosChangeFeedFindToken) curFindToken;\n+    int index = cosmosChangeFeedFindToken.getIndex();\n+    String cacheSesionId = cosmosChangeFeedFindToken.getCacheSessionId();\n+    if (!changeFeedCache.containsKey(cacheSesionId) || !isCacheValid(partitionPath, cosmosChangeFeedFindToken)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05b647952a32686529b175f2abf679a8ef785444"}, "originalPosition": 163}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI1NDYzMA==", "bodyText": "add @Override", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379254630", "createdAt": "2020-02-14T05:19:57Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedFindToken.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.replication.FindTokenType;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress using Cosmos change feed.\n+ */\n+public class CosmosChangeFeedFindToken implements FindToken {\n+  private final short version;\n+  private final long bytesRead;\n+  private final FindTokenType type = FindTokenType.CloudBased;\n+  private final String startContinuationToken;\n+  private final String endContinuationToken;\n+  private final int index;\n+  private final int totalItems;\n+  private final String cacheSessionId;\n+\n+  private final static short VERSION_0 = 0;\n+  private final static short DEFAULT_VERSION = VERSION_0;\n+\n+  /**\n+   * Default constructor to create a {@link CosmosChangeFeedFindToken} with uninitialized continuation token.\n+   */\n+  public CosmosChangeFeedFindToken() {\n+    version = DEFAULT_VERSION;\n+    bytesRead = 0;\n+    startContinuationToken = \"\";\n+    index = -1;\n+    endContinuationToken = \"\";\n+    totalItems = -1;\n+    cacheSessionId = \"\";\n+  }\n+\n+  /**\n+   * Create {@link CosmosChangeFeedFindToken} from provided values.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId) {\n+    this(bytesRead, startContinuationToken, endContinuationToken, index, totalItems, cacheSessionId, DEFAULT_VERSION);\n+  }\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedFindToken} with specified token values and specified version.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   * @param version token version.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId, short version) {\n+    this.version = version;\n+    this.bytesRead = bytesRead;\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.cacheSessionId = cacheSessionId;\n+  }\n+\n+  /**\n+   * Deserialize {@link CosmosChangeFeedFindToken} object from input stream.\n+   * @param inputStream {@link DataOutputStream} to deserialize from.\n+   * @return {@link CosmosChangeFeedFindToken} object.\n+   * @throws IOException\n+   */\n+  public static CosmosChangeFeedFindToken fromBytes(DataInputStream inputStream) throws IOException {\n+    DataInputStream stream = new DataInputStream(inputStream);\n+    short version = stream.readShort();\n+    switch (version) {\n+      case VERSION_0:\n+        FindTokenType type = FindTokenType.values()[stream.readShort()];\n+        if (type != FindTokenType.CloudBased) {\n+          throw new IllegalArgumentException(\n+              String.format(\"Invalid token type %s found while deserialization. Expected %s.\", type,\n+                  FindTokenType.CloudBased));\n+        }\n+        long bytesRead = stream.readLong();\n+        String startContinuationToken = readIntString(inputStream);\n+        String endContinuationToken = readIntString(inputStream);\n+        int index = inputStream.readInt();\n+        int totalItems = inputStream.readInt();\n+        String cacheSessionId = readIntString(inputStream);\n+        return new CosmosChangeFeedFindToken(bytesRead, startContinuationToken, endContinuationToken, index, totalItems,\n+            cacheSessionId, version);\n+      default:\n+        throw new IllegalStateException(\"Unknown version of cloud token: \" + version);\n+    }\n+  }\n+\n+  /**\n+   * Serialize {@link CosmosChangeFeedFindToken} to byte array.\n+   * @return serialized byte array.\n+   */\n+  public byte[] toBytes() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05b647952a32686529b175f2abf679a8ef785444"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI1NTE0Nw==", "bodyText": "add @Override", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379255147", "createdAt": "2020-02-14T05:22:27Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedFindToken.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.replication.FindTokenType;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress using Cosmos change feed.\n+ */\n+public class CosmosChangeFeedFindToken implements FindToken {\n+  private final short version;\n+  private final long bytesRead;\n+  private final FindTokenType type = FindTokenType.CloudBased;\n+  private final String startContinuationToken;\n+  private final String endContinuationToken;\n+  private final int index;\n+  private final int totalItems;\n+  private final String cacheSessionId;\n+\n+  private final static short VERSION_0 = 0;\n+  private final static short DEFAULT_VERSION = VERSION_0;\n+\n+  /**\n+   * Default constructor to create a {@link CosmosChangeFeedFindToken} with uninitialized continuation token.\n+   */\n+  public CosmosChangeFeedFindToken() {\n+    version = DEFAULT_VERSION;\n+    bytesRead = 0;\n+    startContinuationToken = \"\";\n+    index = -1;\n+    endContinuationToken = \"\";\n+    totalItems = -1;\n+    cacheSessionId = \"\";\n+  }\n+\n+  /**\n+   * Create {@link CosmosChangeFeedFindToken} from provided values.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId) {\n+    this(bytesRead, startContinuationToken, endContinuationToken, index, totalItems, cacheSessionId, DEFAULT_VERSION);\n+  }\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedFindToken} with specified token values and specified version.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   * @param version token version.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId, short version) {\n+    this.version = version;\n+    this.bytesRead = bytesRead;\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.cacheSessionId = cacheSessionId;\n+  }\n+\n+  /**\n+   * Deserialize {@link CosmosChangeFeedFindToken} object from input stream.\n+   * @param inputStream {@link DataOutputStream} to deserialize from.\n+   * @return {@link CosmosChangeFeedFindToken} object.\n+   * @throws IOException\n+   */\n+  public static CosmosChangeFeedFindToken fromBytes(DataInputStream inputStream) throws IOException {\n+    DataInputStream stream = new DataInputStream(inputStream);\n+    short version = stream.readShort();\n+    switch (version) {\n+      case VERSION_0:\n+        FindTokenType type = FindTokenType.values()[stream.readShort()];\n+        if (type != FindTokenType.CloudBased) {\n+          throw new IllegalArgumentException(\n+              String.format(\"Invalid token type %s found while deserialization. Expected %s.\", type,\n+                  FindTokenType.CloudBased));\n+        }\n+        long bytesRead = stream.readLong();\n+        String startContinuationToken = readIntString(inputStream);\n+        String endContinuationToken = readIntString(inputStream);\n+        int index = inputStream.readInt();\n+        int totalItems = inputStream.readInt();\n+        String cacheSessionId = readIntString(inputStream);\n+        return new CosmosChangeFeedFindToken(bytesRead, startContinuationToken, endContinuationToken, index, totalItems,\n+            cacheSessionId, version);\n+      default:\n+        throw new IllegalStateException(\"Unknown version of cloud token: \" + version);\n+    }\n+  }\n+\n+  /**\n+   * Serialize {@link CosmosChangeFeedFindToken} to byte array.\n+   * @return serialized byte array.\n+   */\n+  public byte[] toBytes() {\n+    byte[] buf = new byte[size()];\n+    ByteBuffer bufWrap = ByteBuffer.wrap(buf);\n+    bufWrap.putShort(version);\n+    bufWrap.putShort((short) type.ordinal());\n+    bufWrap.putLong(bytesRead);\n+    serializeNullableString(bufWrap, startContinuationToken);\n+    serializeNullableString(bufWrap, endContinuationToken);\n+    bufWrap.putInt(index);\n+    bufWrap.putInt(totalItems);\n+    serializeNullableString(bufWrap, cacheSessionId);\n+    return buf;\n+  }\n+\n+  /**\n+   * Calculate size of the token.\n+   * @return size of the token.\n+   */\n+  public int size() {\n+    return 2 * Short.BYTES + Long.BYTES + 5 * Integer.BYTES + getNullableStringLength(startContinuationToken)\n+        + getNullableStringLength(endContinuationToken) + getNullableStringLength(cacheSessionId);\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    CosmosChangeFeedFindToken cosmosChangeFeedFindToken = (CosmosChangeFeedFindToken) o;\n+    return cosmosChangeFeedFindToken.getVersion() == version && cosmosChangeFeedFindToken.getBytesRead() == bytesRead\n+        && Objects.equals(cosmosChangeFeedFindToken.getStartContinuationToken(), startContinuationToken)\n+        && Objects.equals(cosmosChangeFeedFindToken.getEndContinuationToken(), endContinuationToken)\n+        && cosmosChangeFeedFindToken.getTotalItems() == totalItems && cosmosChangeFeedFindToken.getIndex() == index\n+        && Objects.equals(cosmosChangeFeedFindToken.getCacheSessionId(), cacheSessionId);\n+  }\n+\n+  /**\n+   * Return startContinuationToken of the current token.\n+   * @return startContinuationToken.\n+   */\n+  public String getStartContinuationToken() {\n+    return startContinuationToken;\n+  }\n+\n+  public String getEndContinuationToken() {\n+    return endContinuationToken;\n+  }\n+\n+  /**\n+   * Return index of the current token.\n+   * @return index.\n+   */\n+  public int getIndex() {\n+    return index;\n+  }\n+\n+  /**\n+   * Return totalitems in the current token.\n+   * @return totalitems.\n+   */\n+  public int getTotalItems() {\n+    return totalItems;\n+  }\n+\n+  @Override\n+  public long getBytesRead() {\n+    return bytesRead;\n+  }\n+\n+  @Override\n+  public short getVersion() {\n+    return version;\n+  }\n+\n+  public FindTokenType getType() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05b647952a32686529b175f2abf679a8ef785444"}, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI1NjgyOQ==", "bodyText": "Instead of constructing an intermediate list you could also:\nfeedResponse.getResults().stream().map(doc -> createMetadataFromDocument(doc)).forEach(changeFeed::add)", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379256829", "createdAt": "2020-02-14T05:31:40Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -155,6 +156,46 @@ void testConnectivity() {\n     }\n   }\n \n+  /**\n+   * Query Cosmos change feed to get the next set of {@code CloudBlobMetadata} objects in specified {@code partitionPath}\n+   * after {@code requestContinationToken}, capped by specified {@code maxFeedSize} representing the max number of items to\n+   * be queried from the change feed.\n+   * @param requestContinuationToken Continuation token after which change feed is requested.\n+   * @param maxFeedSize max item count to be requested in the feed query.\n+   * @param changeFeed {@link CloudBlobMetadata} {@code List} to be populated with the next set of entries returned by change feed query.\n+   * @param partitionPath partition for which the change feed is requested.\n+   * @param timer the {@link Timer} to use to record query time (excluding waiting).\n+   * @return next continuation token.\n+   * @throws DocumentClientException\n+   */\n+  public String queryChangeFeed(String requestContinuationToken, int maxFeedSize, List<CloudBlobMetadata> changeFeed,\n+      String partitionPath, Timer timer) throws DocumentClientException {\n+    azureMetrics.changeFeedQueryCount.inc();\n+    ChangeFeedOptions changeFeedOptions = new ChangeFeedOptions();\n+    changeFeedOptions.setPartitionKey(new PartitionKey(partitionPath));\n+    changeFeedOptions.setMaxItemCount(maxFeedSize);\n+    if (Utils.isNullOrEmpty(requestContinuationToken)) {\n+      changeFeedOptions.setStartFromBeginning(true);\n+    } else {\n+      changeFeedOptions.setRequestContinuation(requestContinuationToken);\n+    }\n+    try {\n+      FeedResponse<Document> feedResponse = executeCosmosChangeFeedQuery(changeFeedOptions, timer);\n+      changeFeed.addAll(\n+          feedResponse.getResults().stream().map(doc -> createMetadataFromDocument(doc)).collect(Collectors.toList()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05b647952a32686529b175f2abf679a8ef785444"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI1Nzc2MA==", "bodyText": "to address this, could you use toIterable() and have this method return Iterable<FeedResponse<Document>>?", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379257760", "createdAt": "2020-02-14T05:36:48Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -194,6 +235,43 @@ private CloudBlobMetadata createMetadataFromDocument(Document document) {\n     return resourceResponse;\n   }\n \n+  /**\n+   * Utility method to call Cosmos document query method and record the query time.\n+   * @param sqlQuerySpec the DocumentDB query to execute.\n+   * @param feedOptions {@link FeedOptions} object specifying the options associated with the method.\n+   * @param timer the {@link Timer} to use to record query time (excluding waiting).\n+   * @return {@link BlockingObservable} object containing the query response.\n+   */\n+  private BlockingObservable<FeedResponse<Document>> executeCosmosQuery(SqlQuerySpec sqlQuerySpec,\n+      FeedOptions feedOptions, Timer timer) {\n+    Timer.Context operationTimer = timer.time();\n+    try {\n+      return asyncDocumentClient.queryDocuments(cosmosCollectionLink, sqlQuerySpec, feedOptions).toBlocking();\n+    } finally {\n+      operationTimer.stop();\n+    }\n+  }\n+\n+  /**\n+   * Utility method to call Cosmos change feed query method and record the query time.\n+   * @param changeFeedOptions {@link ChangeFeedOptions} object specifying the options associated with the method.\n+   * @param timer the {@link Timer} to use to record query time (excluding waiting).\n+   * @return {@link FeedResponse} object representing the query response.\n+   */\n+  private FeedResponse<Document> executeCosmosChangeFeedQuery(ChangeFeedOptions changeFeedOptions, Timer timer) {\n+    Timer.Context operationTimer = timer.time();\n+    try {\n+      // FIXME: Using single() for the observable returned by toBlocking() works for now. But if a high enough maxFeedSize", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05b647952a32686529b175f2abf679a8ef785444"}, "originalPosition": 109}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NzU3MDk0", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-358757094", "createdAt": "2020-02-14T07:40:59Z", "commit": {"oid": "05b647952a32686529b175f2abf679a8ef785444"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwNzo0MDo1OVrOFpt1rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwODoxMjo0MlrOFpubqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI4Njk1OA==", "bodyText": "done.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379286958", "createdAt": "2020-02-14T07:40:59Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureCloudDestination.java", "diffHunk": "@@ -65,28 +62,19 @@\n   private static final Logger logger = LoggerFactory.getLogger(AzureCloudDestination.class);\n   private static final String THRESHOLD_PARAM = \"@threshold\";\n   private static final String LIMIT_PARAM = \"@limit\";\n-  private static final String TIME_SINCE_PARAM = \"@timesince\";\n   private static final String BATCH_ID_QUERY_TEMPLATE = \"SELECT * FROM c WHERE c.id IN (%s)\";\n   static final int ID_QUERY_BATCH_SIZE = 1000;\n+  private static final int findSinceQueryLimit = 1000;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIzNTMwNw=="}, "originalCommit": {"oid": "746572261bfae9864566f6f59adb26afff96a8f0"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5MTE5MQ==", "bodyText": "done.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379291191", "createdAt": "2020-02-14T07:55:15Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIzNTE5Nw=="}, "originalCommit": {"oid": "746572261bfae9864566f6f59adb26afff96a8f0"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5MTUxNA==", "bodyText": "nice.. done.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379291514", "createdAt": "2020-02-14T07:56:13Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String cacheSessionId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+    private final String partitionId;\n+    private long creationTimestamp;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param cacheSessionId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String cacheSessionId,\n+        List<CloudBlobMetadata> fetchedEntries, String partitionId) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.cacheSessionId = cacheSessionId;\n+      this.fetchedEntries = fetchedEntries;\n+      this.partitionId = partitionId;\n+      this.creationTimestamp = System.currentTimeMillis();\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the Azure request id.\n+     * @return Azure request id.\n+     */\n+    String getCacheSessionId() {\n+      return cacheSessionId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+\n+    /**\n+     * Return the {@code partitionId}\n+     * @return {@code partitionId}\n+     */\n+    public String getPartitionId() {\n+      return partitionId;\n+    }\n+\n+    /**\n+     * Return the creation time stamp.\n+     * @return  {@code creationTimestamp}\n+     */\n+    public long getCreationTimestamp() {\n+      return creationTimestamp;\n+    }\n+\n+    /**\n+     * Check if is this entry is expired. The condition for expiry uses {@code creationTimestamp}. This is good enough as\n+     * this means that the cached set of fetches entries hasn't been consumed within the invalidation duration.\n+     * @return true if this entry is expired. false otherwise.\n+     */\n+    boolean isExpired() {\n+      return creationTimestamp < System.currentTimeMillis() - CACHE_VALID_DURATION_IN_MS;\n+    }\n+  }\n+\n+  // change feed cache by cache session id\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+  private final static long CACHE_VALID_DURATION_IN_MS = 60 * 60 * 1000; //1 hour", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIzNTc2OA=="}, "originalCommit": {"oid": "746572261bfae9864566f6f59adb26afff96a8f0"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5NDQ1Mg==", "bodyText": "Implemented a closeable interface.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379294452", "createdAt": "2020-02-14T08:05:53Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String cacheSessionId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+    private final String partitionId;\n+    private long creationTimestamp;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param cacheSessionId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String cacheSessionId,\n+        List<CloudBlobMetadata> fetchedEntries, String partitionId) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.cacheSessionId = cacheSessionId;\n+      this.fetchedEntries = fetchedEntries;\n+      this.partitionId = partitionId;\n+      this.creationTimestamp = System.currentTimeMillis();\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the Azure request id.\n+     * @return Azure request id.\n+     */\n+    String getCacheSessionId() {\n+      return cacheSessionId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+\n+    /**\n+     * Return the {@code partitionId}\n+     * @return {@code partitionId}\n+     */\n+    public String getPartitionId() {\n+      return partitionId;\n+    }\n+\n+    /**\n+     * Return the creation time stamp.\n+     * @return  {@code creationTimestamp}\n+     */\n+    public long getCreationTimestamp() {\n+      return creationTimestamp;\n+    }\n+\n+    /**\n+     * Check if is this entry is expired. The condition for expiry uses {@code creationTimestamp}. This is good enough as\n+     * this means that the cached set of fetches entries hasn't been consumed within the invalidation duration.\n+     * @return true if this entry is expired. false otherwise.\n+     */\n+    boolean isExpired() {\n+      return creationTimestamp < System.currentTimeMillis() - CACHE_VALID_DURATION_IN_MS;\n+    }\n+  }\n+\n+  // change feed cache by cache session id\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+  private final static long CACHE_VALID_DURATION_IN_MS = 60 * 60 * 1000; //1 hour\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedBasedReplicationFeed} object.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   * @param azureMetrics{@link {@link AzureMetrics} object.\n+   */\n+  public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccessor, AzureMetrics azureMetrics) {\n+    this.defaultCacheSize = AzureCloudDestination.getFindSinceQueryLimit();\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.azureMetrics = azureMetrics;\n+    // schedule periodic invalidation of cache\n+    Utils.newScheduler(1, false)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIzODY5NQ=="}, "originalCommit": {"oid": "746572261bfae9864566f6f59adb26afff96a8f0"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5NTEzOQ==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379295139", "createdAt": "2020-02-14T08:08:00Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.cloud.CloudBlobMetadata;\n+import com.github.ambry.cloud.FindResult;\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.DocumentClientException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+\n+\n+/**\n+ * The replication feed that provides next list of blobs to replicate from Azure and corresponding {@link FindToken}\n+ * using Cosmos change feed apis.\n+ */\n+public final class CosmosChangeFeedBasedReplicationFeed implements AzureReplicationFeed {\n+\n+  /**\n+   * Class representing change feed cache for each partition.\n+   */\n+  class ChangeFeedCacheEntry {\n+    private final String startContinuationToken;\n+    private final String endContinuationToken;\n+    private final String cacheSessionId;\n+    private final List<CloudBlobMetadata> fetchedEntries;\n+    private final String partitionId;\n+    private long creationTimestamp;\n+\n+    /**\n+     * Constructor for {@link ChangeFeedCacheEntry}.\n+     * @param startContinuationToken start continuation token from where the cached entries are stored.\n+     * @param endContinuationToken end continuation token after all the cached items are consumed.\n+     * @param cacheSessionId a random UUID which uniquely identifies each cached info.\n+     * @param fetchedEntries {@link List} of cached {@link CloudBlobMetadata} objects.\n+     */\n+    ChangeFeedCacheEntry(String startContinuationToken, String endContinuationToken, String cacheSessionId,\n+        List<CloudBlobMetadata> fetchedEntries, String partitionId) {\n+      this.startContinuationToken = startContinuationToken;\n+      this.endContinuationToken = endContinuationToken;\n+      this.cacheSessionId = cacheSessionId;\n+      this.fetchedEntries = fetchedEntries;\n+      this.partitionId = partitionId;\n+      this.creationTimestamp = System.currentTimeMillis();\n+    }\n+\n+    /**\n+     * Return start continuation token.\n+     * @return start continuation token.\n+     */\n+    String getStartContinuationToken() {\n+      return startContinuationToken;\n+    }\n+\n+    /**\n+     * Return the end continuation token.\n+     * @return end continuation token.\n+     */\n+    String getEndContinuationToken() {\n+      return endContinuationToken;\n+    }\n+\n+    /**\n+     * Return the Azure request id.\n+     * @return Azure request id.\n+     */\n+    String getCacheSessionId() {\n+      return cacheSessionId;\n+    }\n+\n+    /**\n+     * Return the fetch entries list.\n+     * @return {@link List} of {@link CloudBlobMetadata} entries.\n+     */\n+    List<CloudBlobMetadata> getFetchedEntries() {\n+      return fetchedEntries;\n+    }\n+\n+    /**\n+     * Return the {@code partitionId}\n+     * @return {@code partitionId}\n+     */\n+    public String getPartitionId() {\n+      return partitionId;\n+    }\n+\n+    /**\n+     * Return the creation time stamp.\n+     * @return  {@code creationTimestamp}\n+     */\n+    public long getCreationTimestamp() {\n+      return creationTimestamp;\n+    }\n+\n+    /**\n+     * Check if is this entry is expired. The condition for expiry uses {@code creationTimestamp}. This is good enough as\n+     * this means that the cached set of fetches entries hasn't been consumed within the invalidation duration.\n+     * @return true if this entry is expired. false otherwise.\n+     */\n+    boolean isExpired() {\n+      return creationTimestamp < System.currentTimeMillis() - CACHE_VALID_DURATION_IN_MS;\n+    }\n+  }\n+\n+  // change feed cache by cache session id\n+  private final ConcurrentHashMap<String, ChangeFeedCacheEntry> changeFeedCache;\n+  private final int defaultCacheSize;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final AzureMetrics azureMetrics;\n+  private final static long CACHE_VALID_DURATION_IN_MS = 60 * 60 * 1000; //1 hour\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedBasedReplicationFeed} object.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object.\n+   * @param azureMetrics{@link {@link AzureMetrics} object.\n+   */\n+  public CosmosChangeFeedBasedReplicationFeed(CosmosDataAccessor cosmosDataAccessor, AzureMetrics azureMetrics) {\n+    this.defaultCacheSize = AzureCloudDestination.getFindSinceQueryLimit();\n+    changeFeedCache = new ConcurrentHashMap<>();\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.azureMetrics = azureMetrics;\n+    // schedule periodic invalidation of cache\n+    Utils.newScheduler(1, false)\n+        .scheduleAtFixedRate(() -> changeFeedCache.entrySet().removeIf(entry -> entry.getValue().isExpired()),\n+            CACHE_VALID_DURATION_IN_MS, CACHE_VALID_DURATION_IN_MS, TimeUnit.MILLISECONDS);\n+  }\n+\n+  /**\n+   * Get next set of change feed entries for the specified partition, after the {@code curFindToken}.\n+   * The number of entries is capped by maxEntriesSize.\n+   * This method creates a cache for change feed entries. If the {@code curFindToken} is not valid,\n+   * or if all the items in the cache are consumed, then it queries Cosmos for new entries.\n+   * @param curFindToken {@link FindToken} after which the next entries have to be returned.\n+   * @param maxTotalSizeOfEntries maximum size of all the blobs returned.\n+   * @param partitionPath Partition for which change feed entries have to be returned.\n+   * @return {@link FindResult} instance that contains updated {@link FindToken} object which can act as a bookmark for\n+   * subsequent requests, and {@link List} of {@link CloudBlobMetadata} entries.\n+   * @throws {@link DocumentClientException}.\n+   */\n+  @Override\n+  public FindResult getNextEntriesAndUpdatedToken(FindToken curFindToken, long maxTotalSizeOfEntries,\n+      String partitionPath) throws DocumentClientException {\n+    List<CloudBlobMetadata> nextEntries = new ArrayList<>();\n+    CosmosChangeFeedFindToken cosmosChangeFeedFindToken = (CosmosChangeFeedFindToken) curFindToken;\n+    int index = cosmosChangeFeedFindToken.getIndex();\n+    String cacheSesionId = cosmosChangeFeedFindToken.getCacheSessionId();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIzNTQ5Ng=="}, "originalCommit": {"oid": "746572261bfae9864566f6f59adb26afff96a8f0"}, "originalPosition": 162}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5NTY0NA==", "bodyText": "done.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379295644", "createdAt": "2020-02-14T08:09:42Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedFindToken.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.replication.FindTokenType;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress using Cosmos change feed.\n+ */\n+public class CosmosChangeFeedFindToken implements FindToken {\n+  private final short version;\n+  private final long bytesRead;\n+  private final FindTokenType type = FindTokenType.CloudBased;\n+  private final String startContinuationToken;\n+  private final String endContinuationToken;\n+  private final int index;\n+  private final int totalItems;\n+  private final String cacheSessionId;\n+\n+  private final static short VERSION_0 = 0;\n+  private final static short DEFAULT_VERSION = VERSION_0;\n+\n+  /**\n+   * Default constructor to create a {@link CosmosChangeFeedFindToken} with uninitialized continuation token.\n+   */\n+  public CosmosChangeFeedFindToken() {\n+    version = DEFAULT_VERSION;\n+    bytesRead = 0;\n+    startContinuationToken = \"\";\n+    index = -1;\n+    endContinuationToken = \"\";\n+    totalItems = -1;\n+    cacheSessionId = \"\";\n+  }\n+\n+  /**\n+   * Create {@link CosmosChangeFeedFindToken} from provided values.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId) {\n+    this(bytesRead, startContinuationToken, endContinuationToken, index, totalItems, cacheSessionId, DEFAULT_VERSION);\n+  }\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedFindToken} with specified token values and specified version.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   * @param version token version.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId, short version) {\n+    this.version = version;\n+    this.bytesRead = bytesRead;\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.cacheSessionId = cacheSessionId;\n+  }\n+\n+  /**\n+   * Deserialize {@link CosmosChangeFeedFindToken} object from input stream.\n+   * @param inputStream {@link DataOutputStream} to deserialize from.\n+   * @return {@link CosmosChangeFeedFindToken} object.\n+   * @throws IOException\n+   */\n+  public static CosmosChangeFeedFindToken fromBytes(DataInputStream inputStream) throws IOException {\n+    DataInputStream stream = new DataInputStream(inputStream);\n+    short version = stream.readShort();\n+    switch (version) {\n+      case VERSION_0:\n+        FindTokenType type = FindTokenType.values()[stream.readShort()];\n+        if (type != FindTokenType.CloudBased) {\n+          throw new IllegalArgumentException(\n+              String.format(\"Invalid token type %s found while deserialization. Expected %s.\", type,\n+                  FindTokenType.CloudBased));\n+        }\n+        long bytesRead = stream.readLong();\n+        String startContinuationToken = readIntString(inputStream);\n+        String endContinuationToken = readIntString(inputStream);\n+        int index = inputStream.readInt();\n+        int totalItems = inputStream.readInt();\n+        String cacheSessionId = readIntString(inputStream);\n+        return new CosmosChangeFeedFindToken(bytesRead, startContinuationToken, endContinuationToken, index, totalItems,\n+            cacheSessionId, version);\n+      default:\n+        throw new IllegalStateException(\"Unknown version of cloud token: \" + version);\n+    }\n+  }\n+\n+  /**\n+   * Serialize {@link CosmosChangeFeedFindToken} to byte array.\n+   * @return serialized byte array.\n+   */\n+  public byte[] toBytes() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI1NDYzMA=="}, "originalCommit": {"oid": "05b647952a32686529b175f2abf679a8ef785444"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5NTgxMg==", "bodyText": "done.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379295812", "createdAt": "2020-02-14T08:10:13Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedFindToken.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/**\n+ * Copyright 2019 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.replication.FindToken;\n+import com.github.ambry.replication.FindTokenType;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+\n+import static com.github.ambry.utils.Utils.*;\n+\n+\n+/**\n+ * Class representing the replication token to track replication progress using Cosmos change feed.\n+ */\n+public class CosmosChangeFeedFindToken implements FindToken {\n+  private final short version;\n+  private final long bytesRead;\n+  private final FindTokenType type = FindTokenType.CloudBased;\n+  private final String startContinuationToken;\n+  private final String endContinuationToken;\n+  private final int index;\n+  private final int totalItems;\n+  private final String cacheSessionId;\n+\n+  private final static short VERSION_0 = 0;\n+  private final static short DEFAULT_VERSION = VERSION_0;\n+\n+  /**\n+   * Default constructor to create a {@link CosmosChangeFeedFindToken} with uninitialized continuation token.\n+   */\n+  public CosmosChangeFeedFindToken() {\n+    version = DEFAULT_VERSION;\n+    bytesRead = 0;\n+    startContinuationToken = \"\";\n+    index = -1;\n+    endContinuationToken = \"\";\n+    totalItems = -1;\n+    cacheSessionId = \"\";\n+  }\n+\n+  /**\n+   * Create {@link CosmosChangeFeedFindToken} from provided values.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId) {\n+    this(bytesRead, startContinuationToken, endContinuationToken, index, totalItems, cacheSessionId, DEFAULT_VERSION);\n+  }\n+\n+  /**\n+   * Constructor to create a {@link CosmosChangeFeedFindToken} with specified token values and specified version.\n+   * @param bytesRead bytes read by remote so far.\n+   * @param startContinuationToken start token from Cosmos.\n+   * @param endContinuationToken end token from Cosmos.\n+   * @param index index in cache upto which items are consumed by remote.\n+   * @param totalItems total number of items in cache.\n+   * @param cacheSessionId request id of the change feed query.\n+   * @param version token version.\n+   */\n+  public CosmosChangeFeedFindToken(long bytesRead, String startContinuationToken, String endContinuationToken,\n+      int index, int totalItems, String cacheSessionId, short version) {\n+    this.version = version;\n+    this.bytesRead = bytesRead;\n+    this.startContinuationToken = startContinuationToken;\n+    this.endContinuationToken = endContinuationToken;\n+    this.index = index;\n+    this.totalItems = totalItems;\n+    this.cacheSessionId = cacheSessionId;\n+  }\n+\n+  /**\n+   * Deserialize {@link CosmosChangeFeedFindToken} object from input stream.\n+   * @param inputStream {@link DataOutputStream} to deserialize from.\n+   * @return {@link CosmosChangeFeedFindToken} object.\n+   * @throws IOException\n+   */\n+  public static CosmosChangeFeedFindToken fromBytes(DataInputStream inputStream) throws IOException {\n+    DataInputStream stream = new DataInputStream(inputStream);\n+    short version = stream.readShort();\n+    switch (version) {\n+      case VERSION_0:\n+        FindTokenType type = FindTokenType.values()[stream.readShort()];\n+        if (type != FindTokenType.CloudBased) {\n+          throw new IllegalArgumentException(\n+              String.format(\"Invalid token type %s found while deserialization. Expected %s.\", type,\n+                  FindTokenType.CloudBased));\n+        }\n+        long bytesRead = stream.readLong();\n+        String startContinuationToken = readIntString(inputStream);\n+        String endContinuationToken = readIntString(inputStream);\n+        int index = inputStream.readInt();\n+        int totalItems = inputStream.readInt();\n+        String cacheSessionId = readIntString(inputStream);\n+        return new CosmosChangeFeedFindToken(bytesRead, startContinuationToken, endContinuationToken, index, totalItems,\n+            cacheSessionId, version);\n+      default:\n+        throw new IllegalStateException(\"Unknown version of cloud token: \" + version);\n+    }\n+  }\n+\n+  /**\n+   * Serialize {@link CosmosChangeFeedFindToken} to byte array.\n+   * @return serialized byte array.\n+   */\n+  public byte[] toBytes() {\n+    byte[] buf = new byte[size()];\n+    ByteBuffer bufWrap = ByteBuffer.wrap(buf);\n+    bufWrap.putShort(version);\n+    bufWrap.putShort((short) type.ordinal());\n+    bufWrap.putLong(bytesRead);\n+    serializeNullableString(bufWrap, startContinuationToken);\n+    serializeNullableString(bufWrap, endContinuationToken);\n+    bufWrap.putInt(index);\n+    bufWrap.putInt(totalItems);\n+    serializeNullableString(bufWrap, cacheSessionId);\n+    return buf;\n+  }\n+\n+  /**\n+   * Calculate size of the token.\n+   * @return size of the token.\n+   */\n+  public int size() {\n+    return 2 * Short.BYTES + Long.BYTES + 5 * Integer.BYTES + getNullableStringLength(startContinuationToken)\n+        + getNullableStringLength(endContinuationToken) + getNullableStringLength(cacheSessionId);\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    CosmosChangeFeedFindToken cosmosChangeFeedFindToken = (CosmosChangeFeedFindToken) o;\n+    return cosmosChangeFeedFindToken.getVersion() == version && cosmosChangeFeedFindToken.getBytesRead() == bytesRead\n+        && Objects.equals(cosmosChangeFeedFindToken.getStartContinuationToken(), startContinuationToken)\n+        && Objects.equals(cosmosChangeFeedFindToken.getEndContinuationToken(), endContinuationToken)\n+        && cosmosChangeFeedFindToken.getTotalItems() == totalItems && cosmosChangeFeedFindToken.getIndex() == index\n+        && Objects.equals(cosmosChangeFeedFindToken.getCacheSessionId(), cacheSessionId);\n+  }\n+\n+  /**\n+   * Return startContinuationToken of the current token.\n+   * @return startContinuationToken.\n+   */\n+  public String getStartContinuationToken() {\n+    return startContinuationToken;\n+  }\n+\n+  public String getEndContinuationToken() {\n+    return endContinuationToken;\n+  }\n+\n+  /**\n+   * Return index of the current token.\n+   * @return index.\n+   */\n+  public int getIndex() {\n+    return index;\n+  }\n+\n+  /**\n+   * Return totalitems in the current token.\n+   * @return totalitems.\n+   */\n+  public int getTotalItems() {\n+    return totalItems;\n+  }\n+\n+  @Override\n+  public long getBytesRead() {\n+    return bytesRead;\n+  }\n+\n+  @Override\n+  public short getVersion() {\n+    return version;\n+  }\n+\n+  public FindTokenType getType() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI1NTE0Nw=="}, "originalCommit": {"oid": "05b647952a32686529b175f2abf679a8ef785444"}, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5NjY4MA==", "bodyText": "done.", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379296680", "createdAt": "2020-02-14T08:12:42Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -155,6 +156,46 @@ void testConnectivity() {\n     }\n   }\n \n+  /**\n+   * Query Cosmos change feed to get the next set of {@code CloudBlobMetadata} objects in specified {@code partitionPath}\n+   * after {@code requestContinationToken}, capped by specified {@code maxFeedSize} representing the max number of items to\n+   * be queried from the change feed.\n+   * @param requestContinuationToken Continuation token after which change feed is requested.\n+   * @param maxFeedSize max item count to be requested in the feed query.\n+   * @param changeFeed {@link CloudBlobMetadata} {@code List} to be populated with the next set of entries returned by change feed query.\n+   * @param partitionPath partition for which the change feed is requested.\n+   * @param timer the {@link Timer} to use to record query time (excluding waiting).\n+   * @return next continuation token.\n+   * @throws DocumentClientException\n+   */\n+  public String queryChangeFeed(String requestContinuationToken, int maxFeedSize, List<CloudBlobMetadata> changeFeed,\n+      String partitionPath, Timer timer) throws DocumentClientException {\n+    azureMetrics.changeFeedQueryCount.inc();\n+    ChangeFeedOptions changeFeedOptions = new ChangeFeedOptions();\n+    changeFeedOptions.setPartitionKey(new PartitionKey(partitionPath));\n+    changeFeedOptions.setMaxItemCount(maxFeedSize);\n+    if (Utils.isNullOrEmpty(requestContinuationToken)) {\n+      changeFeedOptions.setStartFromBeginning(true);\n+    } else {\n+      changeFeedOptions.setRequestContinuation(requestContinuationToken);\n+    }\n+    try {\n+      FeedResponse<Document> feedResponse = executeCosmosChangeFeedQuery(changeFeedOptions, timer);\n+      changeFeed.addAll(\n+          feedResponse.getResults().stream().map(doc -> createMetadataFromDocument(doc)).collect(Collectors.toList()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI1NjgyOQ=="}, "originalCommit": {"oid": "05b647952a32686529b175f2abf679a8ef785444"}, "originalPosition": 62}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU5MTU1ODAw", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-359155800", "createdAt": "2020-02-14T19:05:10Z", "commit": {"oid": "66bbab57bbf072e2f05b69b833f1f73d12902b4f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU5MTU4NzA2", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-359158706", "createdAt": "2020-02-14T19:10:20Z", "commit": {"oid": "66bbab57bbf072e2f05b69b833f1f73d12902b4f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOToxMDoyMFrOFqAzgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOToxMDoyMFrOFqAzgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU5NzY5OQ==", "bodyText": "Is the close called from anywhere?", "url": "https://github.com/linkedin/ambry/pull/1361#discussion_r379597699", "createdAt": "2020-02-14T19:10:20Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosChangeFeedBasedReplicationFeed.java", "diffHunk": "@@ -194,13 +197,18 @@ public FindResult getNextEntriesAndUpdatedToken(FindToken curFindToken, long max\n     }\n \n     FindToken updatedToken = new CosmosChangeFeedFindToken(cosmosChangeFeedFindToken.getBytesRead() + resultSize,\n-        changeFeedCache.get(cacheSesionId).getStartContinuationToken(),\n-        changeFeedCache.get(cacheSesionId).getEndContinuationToken(), index,\n-        changeFeedCache.get(cacheSesionId).getFetchedEntries().size(), cacheSesionId,\n+        changeFeedCache.get(cacheSessionId).getStartContinuationToken(),\n+        changeFeedCache.get(cacheSessionId).getEndContinuationToken(), index,\n+        changeFeedCache.get(cacheSessionId).getFetchedEntries().size(), cacheSessionId,\n         cosmosChangeFeedFindToken.getVersion());\n     return new FindResult(nextEntries, updatedToken);\n   }\n \n+  @Override\n+  public void close() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66bbab57bbf072e2f05b69b833f1f73d12902b4f"}, "originalPosition": 99}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU5Mjg3NDQ0", "url": "https://github.com/linkedin/ambry/pull/1361#pullrequestreview-359287444", "createdAt": "2020-02-15T00:39:57Z", "commit": {"oid": "0f4091e3fb6e31b15f31efc12a346a422a6b6fef"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "43a4debf7cbd80b45bdd1039d5f484d5916ce495", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/43a4debf7cbd80b45bdd1039d5f484d5916ce495", "committedDate": "2020-02-15T01:36:55Z", "message": "Add clouddestinationtoken"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a17b5d7297bfbb08b4dbf753b89ecafb0b817002", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/a17b5d7297bfbb08b4dbf753b89ecafb0b817002", "committedDate": "2020-02-15T01:36:55Z", "message": "Remove factory for cloud destination tokens.\nAdd Change feed cache."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0f31478db8f952dbab3ea0c3622e1b1b240d8f7e", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/0f31478db8f952dbab3ea0c3622e1b1b240d8f7e", "committedDate": "2020-02-15T01:36:55Z", "message": "Rename AzureCloudDestinationToken to AzureFindToken. Move AzureFindToken to cloud module."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e4ee9707ba7118670a99f0e4beea07795bc5caab", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/e4ee9707ba7118670a99f0e4beea07795bc5caab", "committedDate": "2020-02-15T01:36:55Z", "message": "End to end production code."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "207744864756b835b803a9697e5f9ba099cd8198", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/207744864756b835b803a9697e5f9ba099cd8198", "committedDate": "2020-02-15T01:36:55Z", "message": "start test code and bug fix in prod code."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bd649d8b56c4db1e8a0cc65e7878767aac474e6c", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/bd649d8b56c4db1e8a0cc65e7878767aac474e6c", "committedDate": "2020-02-15T01:36:55Z", "message": "Tests for change feed implementation.\nAll tests passing."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cb6ba109e1b52a4a41462dc973a23abef1b56a57", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/cb6ba109e1b52a4a41462dc973a23abef1b56a57", "committedDate": "2020-02-15T01:36:55Z", "message": "Change defaults in azure find token to empty string.\nAll tests passing."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "691556742adad90f3145ecf48e69df2db00cbfa1", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/691556742adad90f3145ecf48e69df2db00cbfa1", "committedDate": "2020-02-15T01:36:55Z", "message": "Add support for multiple azure replication feed types.\nMake azure find token type configurable\nAddress review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "360bdaf0dcb06ea5b97d2036e587b41e5a16eb67", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/360bdaf0dcb06ea5b97d2036e587b41e5a16eb67", "committedDate": "2020-02-15T01:36:55Z", "message": "rebase and rebase fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "31d1bc6557ab65e07a3c3de2313e097eb698c96a", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/31d1bc6557ab65e07a3c3de2313e097eb698c96a", "committedDate": "2020-02-15T01:36:55Z", "message": "Add back update time based token and its tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "673f318da92c40424c9a5beb2a5b009375f4f1ce", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/673f318da92c40424c9a5beb2a5b009375f4f1ce", "committedDate": "2020-02-15T01:36:55Z", "message": "Run azure cloud destination test with both change feed and update time tokens"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ec99d116c7748cf0ce15526578d9de26e180e82", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/2ec99d116c7748cf0ce15526578d9de26e180e82", "committedDate": "2020-02-15T01:36:55Z", "message": "cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a1012b40139dd54f7bb5288e79d320c8a940f33d", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/a1012b40139dd54f7bb5288e79d320c8a940f33d", "committedDate": "2020-02-15T01:36:55Z", "message": "Add missing javadocs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "335667e97b947c473aa2880e15436c21d0ee97f4", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/335667e97b947c473aa2880e15436c21d0ee97f4", "committedDate": "2020-02-15T01:36:55Z", "message": "Make latch based cloud destination support both replication feeds"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ebed07fc202412ec26bb501aedc698ba6f02cb5f", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/ebed07fc202412ec26bb501aedc698ba6f02cb5f", "committedDate": "2020-02-15T01:36:55Z", "message": "Add parameterized tests for all findEntriesSince tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8e00126bd712b26963d3c38f73509ea42947b209", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/8e00126bd712b26963d3c38f73509ea42947b209", "committedDate": "2020-02-15T01:36:55Z", "message": "Fix a bug in serialization of CloudBlobMetadata\nAdd token type based parameterized test to AzureIntegrationTest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "97ce889077bb70216d5ce816963d9b24c3157b1f", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/97ce889077bb70216d5ce816963d9b24c3157b1f", "committedDate": "2020-02-15T01:36:55Z", "message": "Use correct Utils apis"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4a8f911402c943ebbcded5d14a6aeeccb8fdb229", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/4a8f911402c943ebbcded5d14a6aeeccb8fdb229", "committedDate": "2020-02-15T01:36:55Z", "message": "Remove cosmoschangefeedcache class and merge it into cosmoschangedfeedbasedreplicationfeed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e440e6e4a6bc30cd9bfa059c09299245e2e1e53a", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/e440e6e4a6bc30cd9bfa059c09299245e2e1e53a", "committedDate": "2020-02-15T01:36:55Z", "message": "Add review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b4dd454dc7ff59687b8395217d62142325cb374f", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/b4dd454dc7ff59687b8395217d62142325cb374f", "committedDate": "2020-02-15T01:36:55Z", "message": "Use FindResult instead of passing around lists"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a9c947178146a44707f8a0eaa49d65258b0c0437", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/a9c947178146a44707f8a0eaa49d65258b0c0437", "committedDate": "2020-02-15T01:36:55Z", "message": "Rename azureRequestId to cacheSessionId. Make changefeed cache keyed by session id so that multiple replicas of the same partition pulling from same vcr can be handled."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0294dbc3134f92f8ed4dff8290eae447fc085034", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/0294dbc3134f92f8ed4dff8290eae447fc085034", "committedDate": "2020-02-15T01:36:55Z", "message": "Fix unit test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b96b1a7bc711b271854592571f561e45fd2c472f", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/b96b1a7bc711b271854592571f561e45fd2c472f", "committedDate": "2020-02-15T01:36:55Z", "message": "Make CosmosChangeFeedBasedReplicationFeed class final as we are creating a thread in its constructor"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3295e5445c227c64764cacb3abb1ee14f46e6435", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/3295e5445c227c64764cacb3abb1ee14f46e6435", "committedDate": "2020-02-15T01:36:55Z", "message": "Address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "580bf8e820b45fef10de1febee899f3cae329e1c", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/580bf8e820b45fef10de1febee899f3cae329e1c", "committedDate": "2020-02-15T01:36:55Z", "message": "Fix annotations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5af19aa5cea33f7677a256fddfb4eb86baf8a279", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/5af19aa5cea33f7677a256fddfb4eb86baf8a279", "committedDate": "2020-02-15T01:36:55Z", "message": "Add metrics for changefeedquery. Remove Parameterized tests. Add tests for queryChangeFeed."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "71e72523d7bc4c665ed11765578fd22ff83c5525", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/71e72523d7bc4c665ed11765578fd22ff83c5525", "committedDate": "2020-02-15T01:36:55Z", "message": "Make CosmosChangeFeedFindTokenFactory as default for replication from cloud."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "43e8fb047b66e85a9fa383aabae8e49799951b05", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/43e8fb047b66e85a9fa383aabae8e49799951b05", "committedDate": "2020-02-15T01:36:55Z", "message": "Address Casey's review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a48a02eb4f3814357ace4adf16765e7f5d860b2a", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/a48a02eb4f3814357ace4adf16765e7f5d860b2a", "committedDate": "2020-02-15T01:36:55Z", "message": "Minimize cache operations in cosmos change feed cache to reason about concurrency issues in a better way."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eb495d06578d03620c4cb6695531102e7edaf317", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/eb495d06578d03620c4cb6695531102e7edaf317", "committedDate": "2020-02-15T01:36:55Z", "message": "Make CloudDestination and AzureReplicationFeed extend closeable interface for cleanup."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "248434a7f18e816e1de7757cdcf78cde1dad8ba7", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/248434a7f18e816e1de7757cdcf78cde1dad8ba7", "committedDate": "2020-02-15T01:28:53Z", "message": "Make CloudDestination and AzureReplicationFeed extend closeable interface for cleanup."}, "afterCommit": {"oid": "eb495d06578d03620c4cb6695531102e7edaf317", "author": {"user": {"login": "ankagrawal", "name": "Ankur Agrawal"}}, "url": "https://github.com/linkedin/ambry/commit/eb495d06578d03620c4cb6695531102e7edaf317", "committedDate": "2020-02-15T01:36:55Z", "message": "Make CloudDestination and AzureReplicationFeed extend closeable interface for cleanup."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1559, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}