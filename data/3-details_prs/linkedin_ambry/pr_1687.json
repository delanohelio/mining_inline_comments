{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE1ODMzNjUw", "number": 1687, "title": "[DISK_IO]Prefetch BlobAll for GetChunk request to reduce disk IOs", "bodyText": "Prefetch BlobAll for GetChunk request, which only needs BlobData, to reduce disk IOs.\nRefacotr CloudMessageReadSet to use ByteBuf as content store.", "createdAt": "2020-11-05T06:39:16Z", "url": "https://github.com/linkedin/ambry/pull/1687", "merged": true, "mergeCommit": {"oid": "3c0cbb4f4c2ffad1d7bbffe6f066ac43958927a5"}, "closed": true, "closedAt": "2020-11-17T23:56:51Z", "author": {"login": "zzmao"}, "timelineItems": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdZcg0kAH2gAyNTE1ODMzNjUwOmI5ZTkwNTA4MWI0ODc1YjVmYzBhNGYxNzhjN2Q3OWY1YmM4MThlZjk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABddiiJ3gFqTUzMjkwODg5NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "b9e905081b4875b5fc0a4f178c7d79f5bc818ef9", "author": {"user": {"login": "zzmao", "name": "Ze Mao"}}, "url": "https://github.com/linkedin/ambry/commit/b9e905081b4875b5fc0a4f178c7d79f5bc818ef9", "committedDate": "2020-11-05T06:38:32Z", "message": "Prefetch BlobAll for GetChunk request to reduce disk IOs\n\n1. Prefetch BlobAll for GetChunk request, which only needs BlobData, to reduce disk IOs.\n2. Refacotr CloudMessageReadSet to use ByteBuf as content store."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ef6b044d17f520c8aeb4fa12feccc8fa6e40f385", "author": {"user": {"login": "zzmao", "name": "Ze Mao"}}, "url": "https://github.com/linkedin/ambry/commit/ef6b044d17f520c8aeb4fa12feccc8fa6e40f385", "committedDate": "2020-11-05T19:12:43Z", "message": "Fix encrpption key issue"}, "afterCommit": {"oid": "929d5fc04988b206ce3a89d407620c56fa62cd6c", "author": {"user": {"login": "zzmao", "name": "Ze Mao"}}, "url": "https://github.com/linkedin/ambry/commit/929d5fc04988b206ce3a89d407620c56fa62cd6c", "committedDate": "2020-11-05T22:53:15Z", "message": "Fix encrpption key issue."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0949f241d1d2531bcd7fe0042479c9f6a8bcf909", "author": {"user": {"login": "zzmao", "name": "Ze Mao"}}, "url": "https://github.com/linkedin/ambry/commit/0949f241d1d2531bcd7fe0042479c9f6a8bcf909", "committedDate": "2020-11-05T23:25:33Z", "message": "Fix encrpption key issue"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "929d5fc04988b206ce3a89d407620c56fa62cd6c", "author": {"user": {"login": "zzmao", "name": "Ze Mao"}}, "url": "https://github.com/linkedin/ambry/commit/929d5fc04988b206ce3a89d407620c56fa62cd6c", "committedDate": "2020-11-05T22:53:15Z", "message": "Fix encrpption key issue."}, "afterCommit": {"oid": "0949f241d1d2531bcd7fe0042479c9f6a8bcf909", "author": {"user": {"login": "zzmao", "name": "Ze Mao"}}, "url": "https://github.com/linkedin/ambry/commit/0949f241d1d2531bcd7fe0042479c9f6a8bcf909", "committedDate": "2020-11-05T23:25:33Z", "message": "Fix encrpption key issue"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1NTU3MzY3", "url": "https://github.com/linkedin/ambry/pull/1687#pullrequestreview-525557367", "createdAt": "2020-11-06T23:39:38Z", "commit": {"oid": "0949f241d1d2531bcd7fe0042479c9f6a8bcf909"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQyMzozOTozOFrOHvAtew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQyMzo0MzowM1rOHvAwig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA1Njc2Mw==", "bodyText": "Can the flag == Blob case on line 203 be removed now?", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r519056763", "createdAt": "2020-11-06T23:39:38Z", "author": {"login": "cgtz"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);\n+            InputStream is = new ByteBufInputStream(blobAll);\n+\n+            MessageHeader_Format headerFormat = parseHeaderAndVerifyStoreKey(is, i);\n+\n+            MessageMetadata messageMetadata = null;\n+            if (headerFormat.hasEncryptionKeyRecord()) {\n+              // If encryption key exists, MessageMetadata with encryption key is needed.\n+              ByteBuf duplicatedByteBuf = blobAll.duplicate();\n+              duplicatedByteBuf.readerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset());\n+              duplicatedByteBuf.writerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset()\n+                  + headerFormat.getBlobEncryptionKeyRecordSize());\n+              messageMetadata =\n+                  new MessageMetadata(deserializeBlobEncryptionKey(new ByteBufInputStream(duplicatedByteBuf)));\n+            }\n+            messageMetadataList.add(messageMetadata);\n+            sendInfoList.add(i,\n+                new SendInfo(headerFormat.getBlobRecordRelativeOffset(), headerFormat.getBlobRecordSize()));\n+            totalSizeToWrite += headerFormat.getBlobRecordSize();\n+\n+            // Adjust underlying ByteBuf reader and writer index.\n+            blobAll.readerIndex(headerFormat.getBlobRecordRelativeOffset());\n+            blobAll.writerIndex((int) (headerFormat.getBlobRecordRelativeOffset() + headerFormat.getBlobRecordSize()));\n+          }\n         } else {\n-          long startTime = SystemTime.getInstance().milliseconds();\n           BufferedInputStream bufferedInputStream =\n               new BufferedInputStream(new MessageReadSetIndexInputStream(readSet, i, 0),\n                   BUFFERED_INPUT_STREAM_BUFFER_SIZE);\n-          // read and verify header version\n-          byte[] headerVersionBytes = new byte[Version_Field_Size_In_Bytes];\n-          bufferedInputStream.read(headerVersionBytes, 0, Version_Field_Size_In_Bytes);\n-          short version = ByteBuffer.wrap(headerVersionBytes).getShort();\n-          if (!isValidHeaderVersion(version)) {\n-            throw new MessageFormatException(\n-                \"Version not known while reading message - version \" + version + \", StoreKey \" + readSet.getKeyAt(i),\n-                MessageFormatErrorCodes.Unknown_Format_Version);\n-          }\n-          logger.trace(\"Calculate offsets, read and verify header version time: {}\",\n-              SystemTime.getInstance().milliseconds() - startTime);\n-\n-          // read and verify header\n-          startTime = SystemTime.getInstance().milliseconds();\n-          byte[] headerBytes = new byte[getHeaderSizeForVersion(version)];\n-          bufferedInputStream.read(headerBytes, Version_Field_Size_In_Bytes,\n-              headerBytes.length - Version_Field_Size_In_Bytes);\n-\n-          ByteBuffer header = ByteBuffer.wrap(headerBytes);\n-          header.putShort(version);\n-          header.rewind();\n-          MessageHeader_Format headerFormat = getMessageHeader(version, header);\n-          headerFormat.verifyHeader();\n-          logger.trace(\"Calculate offsets, read and verify header time: {}\",\n-              SystemTime.getInstance().milliseconds() - startTime);\n-\n-          // read and verify storeKey\n-          startTime = SystemTime.getInstance().milliseconds();\n-          StoreKey storeKey = storeKeyFactory.getStoreKey(new DataInputStream(bufferedInputStream));\n-          if (storeKey.compareTo(readSet.getKeyAt(i)) != 0) {\n-            throw new MessageFormatException(\n-                \"Id mismatch between metadata and store - metadataId \" + readSet.getKeyAt(i) + \" storeId \" + storeKey,\n-                MessageFormatErrorCodes.Store_Key_Id_MisMatch);\n-          }\n-          logger.trace(\"Calculate offsets, read and verify storeKey time: {}\",\n-              SystemTime.getInstance().milliseconds() - startTime);\n \n-          startTime = SystemTime.getInstance().milliseconds();\n+          MessageHeader_Format headerFormat = parseHeaderAndVerifyStoreKey(bufferedInputStream, i);\n+\n+          long startTime = SystemTime.getInstance().milliseconds();\n           if (flag == MessageFormatFlags.BlobProperties) {\n             sendInfoList.add(i, new SendInfo(headerFormat.getBlobPropertiesRecordRelativeOffset(),\n                 headerFormat.getBlobPropertiesRecordSize()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0949f241d1d2531bcd7fe0042479c9f6a8bcf909"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA1NzU0Ng==", "bodyText": "extra println here", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r519057546", "createdAt": "2020-11-06T23:43:03Z", "author": {"login": "cgtz"}, "path": "ambry-server/src/integration-test/java/com/github/ambry/server/ServerTestUtil.java", "diffHunk": "@@ -2066,6 +2066,7 @@ static void endToEndReplicationWithMultiNodeSinglePartitionTest(String routerDat\n       channel3.disconnect();\n     } catch (Exception e) {\n       e.printStackTrace();\n+      System.out.println(e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0949f241d1d2531bcd7fe0042479c9f6a8bcf909"}, "originalPosition": 4}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f", "author": {"user": {"login": "zzmao", "name": "Ze Mao"}}, "url": "https://github.com/linkedin/ambry/commit/f1cbe85926b9c32f6bf885798d4560af579bdc9f", "committedDate": "2020-11-09T18:33:57Z", "message": "Address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2NTk4MzA4", "url": "https://github.com/linkedin/ambry/pull/1687#pullrequestreview-526598308", "createdAt": "2020-11-09T19:46:41Z", "commit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo0Njo0MVrOHv-9JA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOTo1NjozM1rOHv_Trw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NjU4MA==", "bodyText": "nit: we can use a more descriptive name than just is, something like 'blobInputStream.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520076580", "createdAt": "2020-11-09T19:46:41Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);\n+            InputStream is = new ByteBufInputStream(blobAll);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NjkyNg==", "bodyText": "you can do duplicatedByteBuf.setIndex(readerIndex, writerIndex) in just one statement.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520076926", "createdAt": "2020-11-09T19:47:16Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);\n+            InputStream is = new ByteBufInputStream(blobAll);\n+\n+            MessageHeader_Format headerFormat = parseHeaderAndVerifyStoreKey(is, i);\n+\n+            MessageMetadata messageMetadata = null;\n+            if (headerFormat.hasEncryptionKeyRecord()) {\n+              // If encryption key exists, MessageMetadata with encryption key is needed.\n+              ByteBuf duplicatedByteBuf = blobAll.duplicate();\n+              duplicatedByteBuf.readerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset());\n+              duplicatedByteBuf.writerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA3NzAxNA==", "bodyText": "same above", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520077014", "createdAt": "2020-11-09T19:47:26Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);\n+            InputStream is = new ByteBufInputStream(blobAll);\n+\n+            MessageHeader_Format headerFormat = parseHeaderAndVerifyStoreKey(is, i);\n+\n+            MessageMetadata messageMetadata = null;\n+            if (headerFormat.hasEncryptionKeyRecord()) {\n+              // If encryption key exists, MessageMetadata with encryption key is needed.\n+              ByteBuf duplicatedByteBuf = blobAll.duplicate();\n+              duplicatedByteBuf.readerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset());\n+              duplicatedByteBuf.writerIndex(headerFormat.getBlobEncryptionKeyRecordRelativeOffset()\n+                  + headerFormat.getBlobEncryptionKeyRecordSize());\n+              messageMetadata =\n+                  new MessageMetadata(deserializeBlobEncryptionKey(new ByteBufInputStream(duplicatedByteBuf)));\n+            }\n+            messageMetadataList.add(messageMetadata);\n+            sendInfoList.add(i,\n+                new SendInfo(headerFormat.getBlobRecordRelativeOffset(), headerFormat.getBlobRecordSize()));\n+            totalSizeToWrite += headerFormat.getBlobRecordSize();\n+\n+            // Adjust underlying ByteBuf reader and writer index.\n+            blobAll.readerIndex(headerFormat.getBlobRecordRelativeOffset());\n+            blobAll.writerIndex((int) (headerFormat.getBlobRecordRelativeOffset() + headerFormat.getBlobRecordSize()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA4MjM1MQ==", "bodyText": "I am a bit worried about the using prefetchedData here, without adding it to the dataFromReadSet list.\nIf there is any exception while parsing the header, this ByteBuf will be leaked. We should probably add the prefetchedData ByteBuf to the dataFromReadSet list before using it to parse the header.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520082351", "createdAt": "2020-11-09T19:56:33Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-messageformat/src/main/java/com/github/ambry/messageformat/MessageFormatSend.java", "diffHunk": "@@ -118,57 +119,49 @@ private void fetchDataFromReadSet() throws MessageFormatException {\n       logger.trace(\"Calculate offsets of messages for one partition, MessageFormatFlag : {} number of messages : {}\",\n           flag, messageCount);\n       for (int i = 0; i < messageCount; i++) {\n-        if (flag == MessageFormatFlags.All) {\n+        if (flag == MessageFormatFlags.All || flag == MessageFormatFlags.Blob) {\n           // just copy over the total size and use relative offset to be 0\n           // We do not have to check any version in this case as we dont\n           // have to read any data to deserialize anything.\n-          sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n-          messageMetadataList.add(i, null);\n-          totalSizeToWrite += readSet.sizeInBytes(i);\n           readSet.doPrefetch(i, 0, readSet.sizeInBytes(i));\n+\n+          if (flag == MessageFormatFlags.All) {\n+            sendInfoList.add(i, new SendInfo(0, readSet.sizeInBytes(i)));\n+            messageMetadataList.add(i, null);\n+            totalSizeToWrite += readSet.sizeInBytes(i);\n+          } else if (flag == MessageFormatFlags.Blob) {\n+            ByteBuf blobAll = readSet.getPrefetchedData(i);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1cbe85926b9c32f6bf885798d4560af579bdc9f"}, "originalPosition": 27}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "73b05e80323c27bc58e5ad746fd77bf64c7e12bd", "author": {"user": {"login": "zzmao", "name": "Ze Mao"}}, "url": "https://github.com/linkedin/ambry/commit/73b05e80323c27bc58e5ad746fd77bf64c7e12bd", "committedDate": "2020-11-09T20:23:06Z", "message": "address justin comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7116d6d0fc1ae4cc2ad720f641d1426353f03ae3", "author": {"user": {"login": "zzmao", "name": "Ze Mao"}}, "url": "https://github.com/linkedin/ambry/commit/7116d6d0fc1ae4cc2ad720f641d1426353f03ae3", "committedDate": "2020-11-09T22:06:15Z", "message": "address expection caused buf leak issue"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2NzIzODU5", "url": "https://github.com/linkedin/ambry/pull/1687#pullrequestreview-526723859", "createdAt": "2020-11-09T23:01:05Z", "commit": {"oid": "7116d6d0fc1ae4cc2ad720f641d1426353f03ae3"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMzowMTowNVrOHwFB1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQwMToyOTozMlrOHwINbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE3NjA4NA==", "bodyText": "why go extra step for this, we can just do\n      ByteBuf outputBuf = blobReadInfo.getBlobContent().duplicate();\n      long sizeToRead = Math.min(maxSize, blobReadInfo.getBlobSize() - relativeOffset);\n      outputBuf.setIndex((int) (relativeOffset), (int)(relativeOffset + sizeToRead));\n      written = channel.write(outputBuf.nioBuffer());", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520176084", "createdAt": "2020-11-09T23:01:05Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudMessageReadSet.java", "diffHunk": "@@ -56,14 +54,15 @@ public long writeTo(int index, WritableByteChannel channel, long relativeOffset,\n     try {\n       // TODO: Need to refactor the code to avoid prefetching blobs for BlobInfo request,\n       // or at least to prefetch only the header (requires CloudDestination enhancement)\n-      if (!blobReadInfo.isPrefetched()) {\n-        blobReadInfo.prefetchBlob(blobStore);\n+      if (!blobReadInfo.isBlobDownloaded()) {\n+        blobReadInfo.downloadBlob(blobStore);\n       }\n-      ByteBuffer outputBuffer = blobReadInfo.getPrefetchedBuffer();\n+      ByteBuf outputBuf = blobReadInfo.getBlobContent().duplicate();\n       long sizeToRead = Math.min(maxSize, blobReadInfo.getBlobSize() - relativeOffset);\n-      outputBuffer.limit((int) (relativeOffset + sizeToRead));\n-      outputBuffer.position((int) (relativeOffset));\n-      written = channel.write(outputBuffer);\n+      byte[] array = new byte[(int) sizeToRead];\n+      outputBuf.readerIndex((int) (relativeOffset));\n+      outputBuf.readBytes(array);\n+      written = channel.write(ByteBuffer.wrap(array));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7116d6d0fc1ae4cc2ad720f641d1426353f03ae3"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIyNTI4NA==", "bodyText": "nit: byteBuf.setIndex.", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520225284", "createdAt": "2020-11-10T01:20:33Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudMessageReadSet.java", "diffHunk": "@@ -98,10 +97,12 @@ public StoreKey getKeyAt(int index) {\n   public void doPrefetch(int index, long relativeOffset, long size) throws IOException {\n     BlobReadInfo blobReadInfo = blobReadInfoList.get(index);\n     try {\n-      if (!blobReadInfo.isPrefetched()) {\n-        blobReadInfo.prefetchBlob(blobStore);\n+      if (!blobReadInfo.isBlobDownloaded()) {\n+        blobReadInfo.downloadBlob(blobStore);\n       }\n-      blobReadInfo.setPositionAndSize(relativeOffset, size);\n+      ByteBuf byteBuf = blobReadInfoList.get(index).getBlobContent();\n+      byteBuf.readerIndex((int) (relativeOffset));\n+      byteBuf.writerIndex((int) (relativeOffset + size));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7116d6d0fc1ae4cc2ad720f641d1426353f03ae3"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIyODIwNw==", "bodyText": "is it safe to use netty ByteBuf in CloudMessageReadSet? @ankagrawal  what is the cloud network implementation? does it use http2? socket server?", "url": "https://github.com/linkedin/ambry/pull/1687#discussion_r520228207", "createdAt": "2020-11-10T01:29:32Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudMessageReadSet.java", "diffHunk": "@@ -140,81 +131,38 @@ private void validateIndex(int index) {\n   static class BlobReadInfo {\n     private final CloudBlobMetadata blobMetadata;\n     private final BlobId blobId;\n-    private ByteBuffer prefetchedBuffer;\n-    private int position = -1;\n-    private int size = -1;\n-    private boolean isPrefetched;\n+    private ByteBuf blobContent = null;\n \n     public BlobReadInfo(CloudBlobMetadata blobMetadata, BlobId blobId) {\n       this.blobMetadata = blobMetadata;\n       this.blobId = blobId;\n-      this.isPrefetched = false;\n     }\n \n     /**\n-     * Prefetch the {@code blob} from {@code CloudDestination} and put it in {@code prefetchedBuffer}\n+     * Download the {@code blob} from {@code CloudDestination} and put it in {@code blobContent}\n      * @param blobStore {@code CloudBlobStore} implementation representing the cloud from which download will happen.\n      * @throws StoreException if blob cloud not be downloaded\n      */\n-    public void prefetchBlob(CloudBlobStore blobStore) throws StoreException {\n+    public void downloadBlob(CloudBlobStore blobStore) throws StoreException {\n       // Casting blobsize to int, as blobs are chunked in Ambry, and chunk size is 4/8MB.\n       // However, if in future, if very large size of blobs are allowed, then prefetching logic should be changed.\n-      prefetchedBuffer = ByteBuffer.allocate((int) blobMetadata.getSize());\n-      ByteBufferOutputStream outputStream = new ByteBufferOutputStream(prefetchedBuffer);\n-      blobStore.downloadBlob(blobMetadata, blobId, outputStream);\n-      prefetchedBuffer.flip();\n-      isPrefetched = true;\n-    }\n-\n-    /**\n-     * Donwload the blob from the {@code CloudDestination} to the {@code OutputStream}\n-     * @param blobStore {@code CloudBlobStore} implementation representing the cloud from which download will happen.\n-     * @param outputStream OutputStream to download the blob to\n-     * @throws StoreException if blob download fails.\n-     */\n-    public void downloadBlob(CloudBlobStore blobStore, OutputStream outputStream) throws StoreException {\n+      blobContent = PooledByteBufAllocator.DEFAULT.ioBuffer((int) blobMetadata.getSize());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7116d6d0fc1ae4cc2ad720f641d1426353f03ae3"}, "originalPosition": 115}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "94421398e0b141422d26f0bcd579d9eb97ca8f29", "author": {"user": {"login": "zzmao", "name": "Ze Mao"}}, "url": "https://github.com/linkedin/ambry/commit/94421398e0b141422d26f0bcd579d9eb97ca8f29", "committedDate": "2020-11-17T19:14:41Z", "message": "address nit comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMyOTAzNzA4", "url": "https://github.com/linkedin/ambry/pull/1687#pullrequestreview-532903708", "createdAt": "2020-11-17T23:42:43Z", "commit": {"oid": "94421398e0b141422d26f0bcd579d9eb97ca8f29"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMyOTA4ODk1", "url": "https://github.com/linkedin/ambry/pull/1687#pullrequestreview-532908895", "createdAt": "2020-11-17T23:55:07Z", "commit": {"oid": "94421398e0b141422d26f0bcd579d9eb97ca8f29"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 984, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}