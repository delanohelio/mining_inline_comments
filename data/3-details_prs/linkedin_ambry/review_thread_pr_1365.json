{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY2NjMwOTIy", "number": 1365, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxODozOToyNVrODbVFXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQwNDo1NTozOVrODc227A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5OTgzNTgxOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxODozOToyNVrOFiwKIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQwMjoyMjo0MFrOFj_0yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTk4NDkyOQ==", "bodyText": "Lets make this final. Also add the comment about batch size <= 256 here. Along with that add additional comment that once added to AzureCloudConfig, we should validate this is <= 256.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r371984929", "createdAt": "2020-01-28T18:39:25Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -55,12 +63,15 @@\n   private static final Logger logger = LoggerFactory.getLogger(AzureBlobDataAccessor.class);\n   private static final String SEPARATOR = \"-\";\n   private final BlobServiceClient storageClient;\n+  private final BlobBatchClient blobBatchClient;\n   private final Configuration storageConfiguration;\n   private final AzureMetrics azureMetrics;\n   private final String clusterName;\n   // Containers known to exist in the storage account\n   private final Set<String> knownContainers = ConcurrentHashMap.newKeySet();\n   private ProxyOptions proxyOptions;\n+  // TODO: add to AzureCloudConfig\n+  private int purgeBatchSize = 100;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzI5MDE4Nw==", "bodyText": "Added to config class and moved comment there.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r373290187", "createdAt": "2020-01-31T02:22:40Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -55,12 +63,15 @@\n   private static final Logger logger = LoggerFactory.getLogger(AzureBlobDataAccessor.class);\n   private static final String SEPARATOR = \"-\";\n   private final BlobServiceClient storageClient;\n+  private final BlobBatchClient blobBatchClient;\n   private final Configuration storageConfiguration;\n   private final AzureMetrics azureMetrics;\n   private final String clusterName;\n   // Containers known to exist in the storage account\n   private final Set<String> knownContainers = ConcurrentHashMap.newKeySet();\n   private ProxyOptions proxyOptions;\n+  // TODO: add to AzureCloudConfig\n+  private int purgeBatchSize = 100;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTk4NDkyOQ=="}, "originalCommit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5OTgzNzMxOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxODozOTo1N1rOFiwLHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxODozOTo1N1rOFiwLHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTk4NTE4MA==", "bodyText": "I think we should move this comment to where we define the purgeBatchSize variable. Also see more comments about purgeBatchSize above.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r371985180", "createdAt": "2020-01-28T18:39:57Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +317,47 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    // Per docs.microsoft.com/en-us/rest/api/storageservices/blob-batch, must use batch size <= 256", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5OTg1MjM4OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxODo0NTowMlrOFiwU5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxODo0NTowMlrOFiwU5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTk4NzY4NQ==", "bodyText": "minor-XXS: maybe rename \"someBlobs\" to \"batchofBlobs\"", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r371987685", "createdAt": "2020-01-28T18:45:02Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +317,47 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    // Per docs.microsoft.com/en-us/rest/api/storageservices/blob-batch, must use batch size <= 256\n+    List<CloudBlobMetadata> deletedBlobs = new ArrayList<>();\n+    List<List<CloudBlobMetadata>> partitionedLists = Lists.partition(blobMetadataList, purgeBatchSize);\n+    for (List<CloudBlobMetadata> someBlobs : partitionedLists) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI5OTk0NzQ0OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQxOToxNToyNFrOFixQfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQwMjoyMzozNFrOFj_1YA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAwMjk0MQ==", "bodyText": "We should make the Duration.ofHours(1) configurable. Atleast final private for now.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r372002941", "createdAt": "2020-01-28T19:15:24Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +317,47 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    // Per docs.microsoft.com/en-us/rest/api/storageservices/blob-batch, must use batch size <= 256\n+    List<CloudBlobMetadata> deletedBlobs = new ArrayList<>();\n+    List<List<CloudBlobMetadata>> partitionedLists = Lists.partition(blobMetadataList, purgeBatchSize);\n+    for (List<CloudBlobMetadata> someBlobs : partitionedLists) {\n+      BlobBatch blobBatch = blobBatchClient.getBlobBatch();\n+      List<Response<Void>> responseList = new ArrayList<>();\n+      for (CloudBlobMetadata blobMetadata : someBlobs) {\n+        String containerName = getAzureContainerName(blobMetadata);\n+        String blobName = getAzureBlobName(blobMetadata);\n+        responseList.add(blobBatch.deleteBlob(containerName, blobName));\n+      }\n+      blobBatchClient.submitBatchWithResponse(blobBatch, false, Duration.ofHours(1), Context.NONE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzI5MDMzNg==", "bodyText": "Made private final and changed to 1 minute.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r373290336", "createdAt": "2020-01-31T02:23:34Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +317,47 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    // Per docs.microsoft.com/en-us/rest/api/storageservices/blob-batch, must use batch size <= 256\n+    List<CloudBlobMetadata> deletedBlobs = new ArrayList<>();\n+    List<List<CloudBlobMetadata>> partitionedLists = Lists.partition(blobMetadataList, purgeBatchSize);\n+    for (List<CloudBlobMetadata> someBlobs : partitionedLists) {\n+      BlobBatch blobBatch = blobBatchClient.getBlobBatch();\n+      List<Response<Void>> responseList = new ArrayList<>();\n+      for (CloudBlobMetadata blobMetadata : someBlobs) {\n+        String containerName = getAzureContainerName(blobMetadata);\n+        String blobName = getAzureBlobName(blobMetadata);\n+        responseList.add(blobBatch.deleteBlob(containerName, blobName));\n+      }\n+      blobBatchClient.submitBatchWithResponse(blobBatch, false, Duration.ofHours(1), Context.NONE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAwMjk0MQ=="}, "originalCommit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMDcwNjYwOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOFQyMzo1NDo0NVrOFi4rPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQwMjowNDo0NFrOFj_nqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjEyNDQ3OQ==", "bodyText": "can the statuscode returned by exception be one of OK, ACCEPTED, NOT_FOUND or GONE? If not, then maybe we can move the try catch block to include switch as well.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r372124479", "createdAt": "2020-01-28T23:54:45Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +317,47 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    // Per docs.microsoft.com/en-us/rest/api/storageservices/blob-batch, must use batch size <= 256\n+    List<CloudBlobMetadata> deletedBlobs = new ArrayList<>();\n+    List<List<CloudBlobMetadata>> partitionedLists = Lists.partition(blobMetadataList, purgeBatchSize);\n+    for (List<CloudBlobMetadata> someBlobs : partitionedLists) {\n+      BlobBatch blobBatch = blobBatchClient.getBlobBatch();\n+      List<Response<Void>> responseList = new ArrayList<>();\n+      for (CloudBlobMetadata blobMetadata : someBlobs) {\n+        String containerName = getAzureContainerName(blobMetadata);\n+        String blobName = getAzureBlobName(blobMetadata);\n+        responseList.add(blobBatch.deleteBlob(containerName, blobName));\n+      }\n+      blobBatchClient.submitBatchWithResponse(blobBatch, false, Duration.ofHours(1), Context.NONE);\n+      for (int j = 0; j < responseList.size(); j++) {\n+        Response<Void> response = responseList.get(j);\n+        CloudBlobMetadata blobMetadata = someBlobs.get(j);\n+        // Note: Response.getStatusCode() throws exception on any error.\n+        int statusCode;\n+        try {\n+          statusCode = response.getStatusCode();\n+        } catch (BlobStorageException bex) {\n+          statusCode = bex.getStatusCode();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzI4NjgyNw==", "bodyText": "They throw exception if the http status is not the \"expected\" value, which in this case is Accepted.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r373286827", "createdAt": "2020-01-31T02:04:44Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +317,47 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    // Per docs.microsoft.com/en-us/rest/api/storageservices/blob-batch, must use batch size <= 256\n+    List<CloudBlobMetadata> deletedBlobs = new ArrayList<>();\n+    List<List<CloudBlobMetadata>> partitionedLists = Lists.partition(blobMetadataList, purgeBatchSize);\n+    for (List<CloudBlobMetadata> someBlobs : partitionedLists) {\n+      BlobBatch blobBatch = blobBatchClient.getBlobBatch();\n+      List<Response<Void>> responseList = new ArrayList<>();\n+      for (CloudBlobMetadata blobMetadata : someBlobs) {\n+        String containerName = getAzureContainerName(blobMetadata);\n+        String blobName = getAzureBlobName(blobMetadata);\n+        responseList.add(blobBatch.deleteBlob(containerName, blobName));\n+      }\n+      blobBatchClient.submitBatchWithResponse(blobBatch, false, Duration.ofHours(1), Context.NONE);\n+      for (int j = 0; j < responseList.size(); j++) {\n+        Response<Void> response = responseList.get(j);\n+        CloudBlobMetadata blobMetadata = someBlobs.get(j);\n+        // Note: Response.getStatusCode() throws exception on any error.\n+        int statusCode;\n+        try {\n+          statusCode = response.getStatusCode();\n+        } catch (BlobStorageException bex) {\n+          statusCode = bex.getStatusCode();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjEyNDQ3OQ=="}, "originalCommit": {"oid": "5b0993ec9974e5b4c75db3e2d6291b0e74f5a55d"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDkzMTcyOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzo1ODoxOVrOFka9hA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzo1ODoxOVrOFka9hA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzczNDc4OA==", "bodyText": "make static?", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r373734788", "createdAt": "2020-01-31T23:58:19Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -55,12 +62,15 @@\n   private static final Logger logger = LoggerFactory.getLogger(AzureBlobDataAccessor.class);\n   private static final String SEPARATOR = \"-\";\n   private final BlobServiceClient storageClient;\n+  private final BlobBatchClient blobBatchClient;\n   private final Configuration storageConfiguration;\n   private final AzureMetrics azureMetrics;\n   private final String clusterName;\n   // Containers known to exist in the storage account\n   private final Set<String> knownContainers = ConcurrentHashMap.newKeySet();\n   private ProxyOptions proxyOptions;\n+  private final int purgeBatchSize;\n+  private final int batchPurgeTimeoutSec = 60;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNDIyMzUzOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxNjo1MTozOFrOFk4ZPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxOTo0OTozNFrOFk9xdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIxNzAyMw==", "bodyText": "So, the caller of this method will know to retry on the blobs that are not in the returned list?", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374217023", "createdAt": "2020-02-03T16:51:38Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +318,46 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    List<CloudBlobMetadata> deletedBlobs = new ArrayList<>();\n+    List<List<CloudBlobMetadata>> partitionedLists = Lists.partition(blobMetadataList, purgeBatchSize);\n+    for (List<CloudBlobMetadata> batchOfBlobs : partitionedLists) {\n+      BlobBatch blobBatch = blobBatchClient.getBlobBatch();\n+      List<Response<Void>> responseList = new ArrayList<>();\n+      for (CloudBlobMetadata blobMetadata : batchOfBlobs) {\n+        String containerName = getAzureContainerName(blobMetadata);\n+        String blobName = getAzureBlobName(blobMetadata);\n+        responseList.add(blobBatch.deleteBlob(containerName, blobName));\n+      }\n+      blobBatchClient.submitBatchWithResponse(blobBatch, false, Duration.ofSeconds(batchPurgeTimeoutSec), Context.NONE);\n+      for (int j = 0; j < responseList.size(); j++) {\n+        Response<Void> response = responseList.get(j);\n+        CloudBlobMetadata blobMetadata = batchOfBlobs.get(j);\n+        // Note: Response.getStatusCode() throws exception on any error.\n+        int statusCode;\n+        try {\n+          statusCode = response.getStatusCode();\n+        } catch (BlobStorageException bex) {\n+          statusCode = bex.getStatusCode();\n+        }\n+        switch (statusCode) {\n+          case HttpURLConnection.HTTP_OK:\n+          case HttpURLConnection.HTTP_ACCEPTED:\n+          case HttpURLConnection.HTTP_NOT_FOUND:\n+          case HttpURLConnection.HTTP_GONE:\n+            // blob was deleted or already gone\n+            deletedBlobs.add(blobMetadata);\n+            break;\n+          default:\n+            logger.error(\"Deleting blob {} got status {}\", blobMetadata.getId(), statusCode);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDMwNTE0Mw==", "bodyText": "Right.  They will be returned by the Cosmos query on the next compaction run (since we are not deleting those records).", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374305143", "createdAt": "2020-02-03T19:49:34Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +318,46 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    List<CloudBlobMetadata> deletedBlobs = new ArrayList<>();\n+    List<List<CloudBlobMetadata>> partitionedLists = Lists.partition(blobMetadataList, purgeBatchSize);\n+    for (List<CloudBlobMetadata> batchOfBlobs : partitionedLists) {\n+      BlobBatch blobBatch = blobBatchClient.getBlobBatch();\n+      List<Response<Void>> responseList = new ArrayList<>();\n+      for (CloudBlobMetadata blobMetadata : batchOfBlobs) {\n+        String containerName = getAzureContainerName(blobMetadata);\n+        String blobName = getAzureBlobName(blobMetadata);\n+        responseList.add(blobBatch.deleteBlob(containerName, blobName));\n+      }\n+      blobBatchClient.submitBatchWithResponse(blobBatch, false, Duration.ofSeconds(batchPurgeTimeoutSec), Context.NONE);\n+      for (int j = 0; j < responseList.size(); j++) {\n+        Response<Void> response = responseList.get(j);\n+        CloudBlobMetadata blobMetadata = batchOfBlobs.get(j);\n+        // Note: Response.getStatusCode() throws exception on any error.\n+        int statusCode;\n+        try {\n+          statusCode = response.getStatusCode();\n+        } catch (BlobStorageException bex) {\n+          statusCode = bex.getStatusCode();\n+        }\n+        switch (statusCode) {\n+          case HttpURLConnection.HTTP_OK:\n+          case HttpURLConnection.HTTP_ACCEPTED:\n+          case HttpURLConnection.HTTP_NOT_FOUND:\n+          case HttpURLConnection.HTTP_GONE:\n+            // blob was deleted or already gone\n+            deletedBlobs.add(blobMetadata);\n+            break;\n+          default:\n+            logger.error(\"Deleting blob {} got status {}\", blobMetadata.getId(), statusCode);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIxNzAyMw=="}, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNDI0MjQxOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxNjo1NzowN1rOFk4lKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxOTo1NjoxMVrOFk9-Tw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIyMDA3Mw==", "bodyText": "Apparently, the client will throw a general RuntimeException (looking at the code its IllegalStateException) and not BlobStorageException if it times out. We should make sure that the callers are prepared to handle that. It may be worth documenting in the javadocs too.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374220073", "createdAt": "2020-02-03T16:57:07Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +318,46 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    List<CloudBlobMetadata> deletedBlobs = new ArrayList<>();\n+    List<List<CloudBlobMetadata>> partitionedLists = Lists.partition(blobMetadataList, purgeBatchSize);\n+    for (List<CloudBlobMetadata> batchOfBlobs : partitionedLists) {\n+      BlobBatch blobBatch = blobBatchClient.getBlobBatch();\n+      List<Response<Void>> responseList = new ArrayList<>();\n+      for (CloudBlobMetadata blobMetadata : batchOfBlobs) {\n+        String containerName = getAzureContainerName(blobMetadata);\n+        String blobName = getAzureBlobName(blobMetadata);\n+        responseList.add(blobBatch.deleteBlob(containerName, blobName));\n+      }\n+      blobBatchClient.submitBatchWithResponse(blobBatch, false, Duration.ofSeconds(batchPurgeTimeoutSec), Context.NONE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDMwODQzMQ==", "bodyText": "Good point.  I  think it's okay to let the RuntimeException be thrown, but will add to the javadoc.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374308431", "createdAt": "2020-02-03T19:56:11Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -302,13 +318,46 @@ public boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value)\n   /**\n    * Permanently delete the specified blobs in Azure storage.\n    * @param blobMetadataList the list of {@link CloudBlobMetadata} referencing the blobs to purge.\n-   * @return the number of blobs successfully purged.\n+   * @return list of {@link CloudBlobMetadata} referencing the blobs successfully purged.\n    * @throws BlobStorageException if the purge operation fails.\n    */\n-  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n-    // TODO: use batch api to delete all\n-    // https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/storage/azure-storage-blob-batch\n-    throw new UnsupportedOperationException(\"Not yet implemented\");\n+  public List<CloudBlobMetadata> purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws BlobStorageException {\n+\n+    List<CloudBlobMetadata> deletedBlobs = new ArrayList<>();\n+    List<List<CloudBlobMetadata>> partitionedLists = Lists.partition(blobMetadataList, purgeBatchSize);\n+    for (List<CloudBlobMetadata> batchOfBlobs : partitionedLists) {\n+      BlobBatch blobBatch = blobBatchClient.getBlobBatch();\n+      List<Response<Void>> responseList = new ArrayList<>();\n+      for (CloudBlobMetadata blobMetadata : batchOfBlobs) {\n+        String containerName = getAzureContainerName(blobMetadata);\n+        String blobName = getAzureBlobName(blobMetadata);\n+        responseList.add(blobBatch.deleteBlob(containerName, blobName));\n+      }\n+      blobBatchClient.submitBatchWithResponse(blobBatch, false, Duration.ofSeconds(batchPurgeTimeoutSec), Context.NONE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIyMDA3Mw=="}, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNDMxNTU1OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureCloudDestination.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxNzoxOTowNVrOFk5SSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QyMDo0MDo0OVrOFk_Olw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIzMTYyNQ==", "bodyText": "I just want to confirm my understanding of the compaction logic: In the case where deletedBlobs.size() < blobMetadataList.size(), we rely on the fact that not deleting the record from cosmos will allow the dead blobs query to return the same blob ID in future compaction iterations?", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374231625", "createdAt": "2020-02-03T17:19:05Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureCloudDestination.java", "diffHunk": "@@ -349,51 +352,48 @@ private boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value\n   }\n \n   @Override\n-  public boolean purgeBlob(CloudBlobMetadata blobMetadata) throws CloudStorageException {\n-    String blobId = blobMetadata.getId();\n-    String blobFileName = azureBlobDataAccessor.getAzureBlobName(blobMetadata);\n-    String containerName = azureBlobDataAccessor.getAzureContainerName(blobMetadata);\n-    String partitionPath = blobMetadata.getPartitionId();\n-    azureMetrics.blobDeleteRequestCount.inc();\n+  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageException {\n+    if (blobMetadataList.isEmpty()) {\n+      return 0;\n+    }\n+    azureMetrics.blobDeleteRequestCount.inc(blobMetadataList.size());\n     Timer.Context deleteTimer = azureMetrics.blobDeletionTime.time();\n     try {\n-      // delete blob from storage\n-      boolean deletionDone = azureBlobDataAccessor.deleteFile(containerName, blobFileName);\n-\n-      // Delete the document too\n-      try {\n-        cosmosDataAccessor.deleteMetadata(blobMetadata);\n-        deletionDone = true;\n-        logger.debug(\"Purged blob {} from partition {}.\", blobId, partitionPath);\n-      } catch (DocumentClientException dex) {\n-        if (dex.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {\n-          logger.warn(\"Could not find metadata for blob {} to delete\", blobId);\n-        } else {\n-          throw dex;\n-        }\n+      List<CloudBlobMetadata> deletedBlobs = azureBlobDataAccessor.purgeBlobs(blobMetadataList);\n+      azureMetrics.blobDeletedCount.inc(deletedBlobs.size());\n+      azureMetrics.blobDeleteErrorCount.inc(blobMetadataList.size() - deletedBlobs.size());\n+\n+      // Remove them from Cosmos too\n+      for (CloudBlobMetadata blobMetadata : deletedBlobs) {\n+        deleteFromCosmos(blobMetadata);\n       }\n-      azureMetrics.blobDeletedCount.inc(deletionDone ? 1 : 0);\n-      return deletionDone;\n-    } catch (Exception e) {\n-      azureMetrics.blobDeleteErrorCount.inc();\n-      String error = (e instanceof DocumentClientException) ? \"Failed to delete metadata document for blob \" + blobId\n-          : \"Failed to delete blob \" + blobId + \", storage path: \" + containerName + \"/\" + blobFileName;\n-      throw toCloudStorageException(error, e);\n+      return deletedBlobs.size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDMyODk4Mw==", "bodyText": "Right.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374328983", "createdAt": "2020-02-03T20:40:49Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureCloudDestination.java", "diffHunk": "@@ -349,51 +352,48 @@ private boolean updateBlobMetadata(BlobId blobId, String fieldName, Object value\n   }\n \n   @Override\n-  public boolean purgeBlob(CloudBlobMetadata blobMetadata) throws CloudStorageException {\n-    String blobId = blobMetadata.getId();\n-    String blobFileName = azureBlobDataAccessor.getAzureBlobName(blobMetadata);\n-    String containerName = azureBlobDataAccessor.getAzureContainerName(blobMetadata);\n-    String partitionPath = blobMetadata.getPartitionId();\n-    azureMetrics.blobDeleteRequestCount.inc();\n+  public int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageException {\n+    if (blobMetadataList.isEmpty()) {\n+      return 0;\n+    }\n+    azureMetrics.blobDeleteRequestCount.inc(blobMetadataList.size());\n     Timer.Context deleteTimer = azureMetrics.blobDeletionTime.time();\n     try {\n-      // delete blob from storage\n-      boolean deletionDone = azureBlobDataAccessor.deleteFile(containerName, blobFileName);\n-\n-      // Delete the document too\n-      try {\n-        cosmosDataAccessor.deleteMetadata(blobMetadata);\n-        deletionDone = true;\n-        logger.debug(\"Purged blob {} from partition {}.\", blobId, partitionPath);\n-      } catch (DocumentClientException dex) {\n-        if (dex.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {\n-          logger.warn(\"Could not find metadata for blob {} to delete\", blobId);\n-        } else {\n-          throw dex;\n-        }\n+      List<CloudBlobMetadata> deletedBlobs = azureBlobDataAccessor.purgeBlobs(blobMetadataList);\n+      azureMetrics.blobDeletedCount.inc(deletedBlobs.size());\n+      azureMetrics.blobDeleteErrorCount.inc(blobMetadataList.size() - deletedBlobs.size());\n+\n+      // Remove them from Cosmos too\n+      for (CloudBlobMetadata blobMetadata : deletedBlobs) {\n+        deleteFromCosmos(blobMetadata);\n       }\n-      azureMetrics.blobDeletedCount.inc(deletionDone ? 1 : 0);\n-      return deletionDone;\n-    } catch (Exception e) {\n-      azureMetrics.blobDeleteErrorCount.inc();\n-      String error = (e instanceof DocumentClientException) ? \"Failed to delete metadata document for blob \" + blobId\n-          : \"Failed to delete blob \" + blobId + \", storage path: \" + containerName + \"/\" + blobFileName;\n-      throw toCloudStorageException(error, e);\n+      return deletedBlobs.size();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIzMTYyNQ=="}, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNDMyODc0OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxNzoyMzoxN1rOFk5aUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QyMDo0MTo0N1rOFk_QIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIzMzY4Mw==", "bodyText": "If we want to use guava directly in Ambry (I'm not against this since it has a lot of useful things), it might be good to explicitly reference the desired version in our build files. Interestingly, our dependencies seem to transitively bring in 3 versions of guava.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374233683", "createdAt": "2020-02-03T17:23:17Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -32,12 +36,15 @@\n import com.github.ambry.commons.BlobId;\n import com.github.ambry.config.CloudConfig;\n import com.github.ambry.utils.Utils;\n+import com.google.common.collect.Lists;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIzNTkxNA==", "bodyText": "If we decide not to use guava directly we can add a partition method to Utils.java (the impl is pretty simple), or just use List::subList", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374235914", "createdAt": "2020-02-03T17:27:56Z", "author": {"login": "cgtz"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -32,12 +36,15 @@\n import com.github.ambry.commons.BlobId;\n import com.github.ambry.config.CloudConfig;\n import com.github.ambry.utils.Utils;\n+import com.google.common.collect.Lists;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIzMzY4Mw=="}, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDMyOTM3OA==", "bodyText": "Added Utils.partitionList() and refactored AzureCloudDestination.getBlobMetadata() to use it as well.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374329378", "createdAt": "2020-02-03T20:41:47Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -32,12 +36,15 @@\n import com.github.ambry.commons.BlobId;\n import com.github.ambry.config.CloudConfig;\n import com.github.ambry.utils.Utils;\n+import com.google.common.collect.Lists;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIzMzY4Mw=="}, "originalCommit": {"oid": "0a174038358ca7cbeeb6389145131970c7e9bd5f"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNTg1NTE2OnYy", "diffSide": "RIGHT", "path": "ambry-utils/src/main/java/com.github.ambry.utils/Utils.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQwNDo1NTozOVrOFlIFgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjozOTowNVrOFll_nA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDQ3NDExNA==", "bodyText": "could this loop be simplified to:\nfor (int start = 0; start < inputList.size(); start += batchSize) {\n  int end = Math.min(start + batchSize, inputList.size());\n  partitionedLists.add(inputList.subList(start, end));\n}\n\nI may be missing an edge case here though", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374474114", "createdAt": "2020-02-04T04:55:39Z", "author": {"login": "cgtz"}, "path": "ambry-utils/src/main/java/com.github.ambry.utils/Utils.java", "diffHunk": "@@ -912,6 +913,30 @@ public static DataInputStream createDataInputStreamFromBuffer(Object buffer, boo\n         .collect(Collectors.toCollection(collectionFactory));\n   }\n \n+  /**\n+   * Partition the input list into a List of smaller sublists, each one limited to the specified batch size.\n+   * Method inspired by the Guava utility Lists.partition(List<T> list, int size).\n+   * @param inputList the input list to partition.\n+   * @param batchSize the maximum size of the returned sublists.\n+   * @return the partitioned list of sublists.\n+   */\n+  public static <T> List<List<T>> partitionList(List<T> inputList, int batchSize) {\n+    Objects.requireNonNull(inputList, \"Input list cannot be null\");\n+    if (batchSize < 1) {\n+      throw new IllegalArgumentException(\"Invalid batchSize: \" + batchSize);\n+    }\n+    List<List<T>> partitionedLists = new ArrayList<>();\n+    for (int j = 0; j < inputList.size() / batchSize + 1; j++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73c5d5935acc0bf9f0fcae9da577c951a676c69d"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0ODM2Mw==", "bodyText": "So I was thinking about this partition thing and it seems that we don't really need a full list and just need an iterable that we can do a for loop on. Something like this could work, I think:\n  public static <T> Iterable<List<T>> partition(List<T> list, int batchSize) {\n    return () -> new Iterator<List<T>>() {\n      int nextStart = 0;\n\n      @Override\n      public boolean hasNext() {\n        return nextStart < list.size();\n      }\n\n      @Override\n      public List<T> next() {\n        if (!hasNext()) {\n          throw new NoSuchElementException();\n        }\n        int start = nextStart;\n        nextStart += batchSize;\n        return list.subList(start, Math.min(start + batchSize, list.size()));\n      }\n    };\n  }", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374748363", "createdAt": "2020-02-04T15:40:13Z", "author": {"login": "cgtz"}, "path": "ambry-utils/src/main/java/com.github.ambry.utils/Utils.java", "diffHunk": "@@ -912,6 +913,30 @@ public static DataInputStream createDataInputStreamFromBuffer(Object buffer, boo\n         .collect(Collectors.toCollection(collectionFactory));\n   }\n \n+  /**\n+   * Partition the input list into a List of smaller sublists, each one limited to the specified batch size.\n+   * Method inspired by the Guava utility Lists.partition(List<T> list, int size).\n+   * @param inputList the input list to partition.\n+   * @param batchSize the maximum size of the returned sublists.\n+   * @return the partitioned list of sublists.\n+   */\n+  public static <T> List<List<T>> partitionList(List<T> inputList, int batchSize) {\n+    Objects.requireNonNull(inputList, \"Input list cannot be null\");\n+    if (batchSize < 1) {\n+      throw new IllegalArgumentException(\"Invalid batchSize: \" + batchSize);\n+    }\n+    List<List<T>> partitionedLists = new ArrayList<>();\n+    for (int j = 0; j < inputList.size() / batchSize + 1; j++) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDQ3NDExNA=="}, "originalCommit": {"oid": "73c5d5935acc0bf9f0fcae9da577c951a676c69d"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2MDUxNw==", "bodyText": "I'm not sure this is a big improvement.  I prefer to mimic the guava utility.  Once we upgrade to a newer JDK, we can probably retire the utility and use the Collections one.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374960517", "createdAt": "2020-02-04T22:31:17Z", "author": {"login": "lightningrob"}, "path": "ambry-utils/src/main/java/com.github.ambry.utils/Utils.java", "diffHunk": "@@ -912,6 +913,30 @@ public static DataInputStream createDataInputStreamFromBuffer(Object buffer, boo\n         .collect(Collectors.toCollection(collectionFactory));\n   }\n \n+  /**\n+   * Partition the input list into a List of smaller sublists, each one limited to the specified batch size.\n+   * Method inspired by the Guava utility Lists.partition(List<T> list, int size).\n+   * @param inputList the input list to partition.\n+   * @param batchSize the maximum size of the returned sublists.\n+   * @return the partitioned list of sublists.\n+   */\n+  public static <T> List<List<T>> partitionList(List<T> inputList, int batchSize) {\n+    Objects.requireNonNull(inputList, \"Input list cannot be null\");\n+    if (batchSize < 1) {\n+      throw new IllegalArgumentException(\"Invalid batchSize: \" + batchSize);\n+    }\n+    List<List<T>> partitionedLists = new ArrayList<>();\n+    for (int j = 0; j < inputList.size() / batchSize + 1; j++) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDQ3NDExNA=="}, "originalCommit": {"oid": "73c5d5935acc0bf9f0fcae9da577c951a676c69d"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2NDEyNA==", "bodyText": "I made the simplification you suggested.", "url": "https://github.com/linkedin/ambry/pull/1365#discussion_r374964124", "createdAt": "2020-02-04T22:39:05Z", "author": {"login": "lightningrob"}, "path": "ambry-utils/src/main/java/com.github.ambry.utils/Utils.java", "diffHunk": "@@ -912,6 +913,30 @@ public static DataInputStream createDataInputStreamFromBuffer(Object buffer, boo\n         .collect(Collectors.toCollection(collectionFactory));\n   }\n \n+  /**\n+   * Partition the input list into a List of smaller sublists, each one limited to the specified batch size.\n+   * Method inspired by the Guava utility Lists.partition(List<T> list, int size).\n+   * @param inputList the input list to partition.\n+   * @param batchSize the maximum size of the returned sublists.\n+   * @return the partitioned list of sublists.\n+   */\n+  public static <T> List<List<T>> partitionList(List<T> inputList, int batchSize) {\n+    Objects.requireNonNull(inputList, \"Input list cannot be null\");\n+    if (batchSize < 1) {\n+      throw new IllegalArgumentException(\"Invalid batchSize: \" + batchSize);\n+    }\n+    List<List<T>> partitionedLists = new ArrayList<>();\n+    for (int j = 0; j < inputList.size() / batchSize + 1; j++) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDQ3NDExNA=="}, "originalCommit": {"oid": "73c5d5935acc0bf9f0fcae9da577c951a676c69d"}, "originalPosition": 25}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1663, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}