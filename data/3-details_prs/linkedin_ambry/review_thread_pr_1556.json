{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMxMzgyNzY5", "number": 1556, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwMToxNzoxMlrOEDrYdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQyMToxNToyMFrOEEDz9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMjkxOTU2OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwMToxNzoxMlrOGg1gXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxODo1ODozNlrOGhYGTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA4NDI1Mw==", "bodyText": "Is the idea to wait until all the threads are done before assigning any new work?  That won't be optimal since a long running partition can keep the other threads idle for a while.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437084253", "createdAt": "2020-06-09T01:17:12Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +106,80 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (true) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        totalBlobsPurged += executorCompletionService.take().get();\n+        compactionInProgress--;\n+        compactedPartitionCount++;\n+        if (System.currentTimeMillis() >= timeToQuit) {\n+          logger.info(\"Compaction terminated due to time limit exceeded.\");\n+          break;\n+        }\n+        if (isShuttingDown()) {\n+          logger.info(\"Compaction terminated due to shut down.\");\n+          break;\n+        }\n+        if (partitionSnapshot.isEmpty()) {\n+          break;\n+        }\n+      }\n+      while (compactionInProgress > 0) {\n+        totalBlobsPurged += executorCompletionService.take().get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0320d1b6d51326803c2c81dc582f84d19b25c691"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzEwODIwNA==", "bodyText": "No.. the idea is not to wait until all threads are done. The idea to wait for any one of the threads to return, and then assign it new work immediately. executorCompletionService.take() retrieves and removes the Future representing the next completed task, waiting if none are yet present. Calling .get() on the future just returns the result.\nSo basically first we assign work to all threads. Then wait for any single thread to complete its work in the executorCompletionService.take().get() call. And then we loop over to assign work to this thread.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437108204", "createdAt": "2020-06-09T02:50:07Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +106,80 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (true) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        totalBlobsPurged += executorCompletionService.take().get();\n+        compactionInProgress--;\n+        compactedPartitionCount++;\n+        if (System.currentTimeMillis() >= timeToQuit) {\n+          logger.info(\"Compaction terminated due to time limit exceeded.\");\n+          break;\n+        }\n+        if (isShuttingDown()) {\n+          logger.info(\"Compaction terminated due to shut down.\");\n+          break;\n+        }\n+        if (partitionSnapshot.isEmpty()) {\n+          break;\n+        }\n+      }\n+      while (compactionInProgress > 0) {\n+        totalBlobsPurged += executorCompletionService.take().get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA4NDI1Mw=="}, "originalCommit": {"oid": "0320d1b6d51326803c2c81dc582f84d19b25c691"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzExMTM5Ng==", "bodyText": "Note that the final while loop while (compactionInProgress > 0) does loop through all the threads. This happens when all the partitions have been taken care of, and we have come out of the initial while(true) loop. Here we are just making sure that all the threads get completed before ending the compaction run.\nThis loop also takes care of the case where we encounter a compaction shutdown. In that case too, we break out of the while(true) loop, and wait for all the threads to complete work, before releasing the done latch.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437111396", "createdAt": "2020-06-09T03:02:27Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +106,80 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (true) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        totalBlobsPurged += executorCompletionService.take().get();\n+        compactionInProgress--;\n+        compactedPartitionCount++;\n+        if (System.currentTimeMillis() >= timeToQuit) {\n+          logger.info(\"Compaction terminated due to time limit exceeded.\");\n+          break;\n+        }\n+        if (isShuttingDown()) {\n+          logger.info(\"Compaction terminated due to shut down.\");\n+          break;\n+        }\n+        if (partitionSnapshot.isEmpty()) {\n+          break;\n+        }\n+      }\n+      while (compactionInProgress > 0) {\n+        totalBlobsPurged += executorCompletionService.take().get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA4NDI1Mw=="}, "originalCommit": {"oid": "0320d1b6d51326803c2c81dc582f84d19b25c691"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY1MTAyMQ==", "bodyText": "Okay, I see that now.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437651021", "createdAt": "2020-06-09T18:58:36Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +106,80 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (true) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        totalBlobsPurged += executorCompletionService.take().get();\n+        compactionInProgress--;\n+        compactedPartitionCount++;\n+        if (System.currentTimeMillis() >= timeToQuit) {\n+          logger.info(\"Compaction terminated due to time limit exceeded.\");\n+          break;\n+        }\n+        if (isShuttingDown()) {\n+          logger.info(\"Compaction terminated due to shut down.\");\n+          break;\n+        }\n+        if (partitionSnapshot.isEmpty()) {\n+          break;\n+        }\n+      }\n+      while (compactionInProgress > 0) {\n+        totalBlobsPurged += executorCompletionService.take().get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA4NDI1Mw=="}, "originalCommit": {"oid": "0320d1b6d51326803c2c81dc582f84d19b25c691"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMjkyMTUzOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwMToxODoxOVrOGg1hgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwMjo0NTozN1rOGg25jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA4NDU0NQ==", "bodyText": "This could be: while (!partitionSnapshot.isEmpty())", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437084545", "createdAt": "2020-06-09T01:18:19Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +106,80 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (true) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0320d1b6d51326803c2c81dc582f84d19b25c691"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzEwNzA4Ng==", "bodyText": "yes.. will fix this.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437107086", "createdAt": "2020-06-09T02:45:37Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +106,80 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (true) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA4NDU0NQ=="}, "originalCommit": {"oid": "0320d1b6d51326803c2c81dc582f84d19b25c691"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyNjQyNzYwOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxODo1MjowNVrOGhX3lA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxOToyMjoyOFrOGhY4Pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY0NzI1Mg==", "bodyText": "Typo: reassigned", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437647252", "createdAt": "2020-06-09T18:52:05Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +106,77 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (!partitionSnapshot.isEmpty()) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        totalBlobsPurged += executorCompletionService.take().get();\n+        compactionInProgress--;\n+        compactedPartitionCount++;\n+        if (System.currentTimeMillis() >= timeToQuit) {\n+          logger.info(\"Compaction terminated due to time limit exceeded.\");\n+          break;\n+        }\n+        if (isShuttingDown()) {\n+          logger.info(\"Compaction terminated due to shut down.\");\n+          break;\n+        }\n+      }\n+      while (compactionInProgress > 0) {\n+        totalBlobsPurged += executorCompletionService.take().get();\n+        compactionInProgress--;\n+        compactedPartitionCount++;\n+      }\n+      doneLatch.get().countDown();\n+    } catch (Throwable th) {\n+      logger.error(\"Hit exception running compaction task\", th);\n+    } finally {\n+      long compactionTime = (System.currentTimeMillis() - compactionStartTime) / 1000;\n+      logger.info(\"Purged {} blobs in {} partitions taking {} seconds\", totalBlobsPurged, compactedPartitionCount,\n+          compactionTime);\n+    }\n+    return totalBlobsPurged;\n+  }\n+\n+  /**\n+   * Purge the inactive blobs in the specified partitions.\n+   * @param partition the {@link PartitionId} to compact.\n+   * @return the total number of blobs purged in the partition.\n+   */\n+  private int compactPartition(PartitionId partition) {\n     if (isShuttingDown()) {\n       logger.info(\"Skipping compaction due to shut down.\");\n       return 0;\n     }\n \n-    // TODO: adjust count when compaction uses multiple threads\n-    doneLatch.set(new CountDownLatch(1));\n-\n-    Set<PartitionId> partitionsSnapshot = new HashSet<>(partitions);\n-    logger.info(\"Beginning dead blob compaction for {} partitions\", partitions.size());\n-    long now = System.currentTimeMillis();\n-    long compactionStartTime = now;\n-    long timeToQuit = now + compactionTimeLimitMs;\n-    int totalBlobsPurged = 0;\n-    for (PartitionId partitionId : partitionsSnapshot) {\n-      String partitionPath = partitionId.toPathString();\n-      if (!partitions.contains(partitionId)) {\n-        // Looks like partition was reassigned since the loop started, so skip it\n-        continue;\n-      }\n+    logger.info(\"Beginning dead blob compaction for partition {}\", partition);\n \n-      try {\n-        totalBlobsPurged += cloudDestination.compactPartition(partitionPath);\n-      } catch (CloudStorageException ex) {\n-        logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n-        vcrMetrics.compactionFailureCount.inc();\n-      }\n+    String partitionPath = partition.toPathString();\n+    if (!partitions.contains(partition)) {\n+      // Looks like partition was reassigned since the loop started, so skip it\n+      logger.warn(\"Skipping compaction of Partition {} as the partition was reassgined\", partition);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3178926f4ba04ef42c320ddc7fadf05d9961c8e"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2MzgwNg==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437663806", "createdAt": "2020-06-09T19:22:28Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +106,77 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (!partitionSnapshot.isEmpty()) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        totalBlobsPurged += executorCompletionService.take().get();\n+        compactionInProgress--;\n+        compactedPartitionCount++;\n+        if (System.currentTimeMillis() >= timeToQuit) {\n+          logger.info(\"Compaction terminated due to time limit exceeded.\");\n+          break;\n+        }\n+        if (isShuttingDown()) {\n+          logger.info(\"Compaction terminated due to shut down.\");\n+          break;\n+        }\n+      }\n+      while (compactionInProgress > 0) {\n+        totalBlobsPurged += executorCompletionService.take().get();\n+        compactionInProgress--;\n+        compactedPartitionCount++;\n+      }\n+      doneLatch.get().countDown();\n+    } catch (Throwable th) {\n+      logger.error(\"Hit exception running compaction task\", th);\n+    } finally {\n+      long compactionTime = (System.currentTimeMillis() - compactionStartTime) / 1000;\n+      logger.info(\"Purged {} blobs in {} partitions taking {} seconds\", totalBlobsPurged, compactedPartitionCount,\n+          compactionTime);\n+    }\n+    return totalBlobsPurged;\n+  }\n+\n+  /**\n+   * Purge the inactive blobs in the specified partitions.\n+   * @param partition the {@link PartitionId} to compact.\n+   * @return the total number of blobs purged in the partition.\n+   */\n+  private int compactPartition(PartitionId partition) {\n     if (isShuttingDown()) {\n       logger.info(\"Skipping compaction due to shut down.\");\n       return 0;\n     }\n \n-    // TODO: adjust count when compaction uses multiple threads\n-    doneLatch.set(new CountDownLatch(1));\n-\n-    Set<PartitionId> partitionsSnapshot = new HashSet<>(partitions);\n-    logger.info(\"Beginning dead blob compaction for {} partitions\", partitions.size());\n-    long now = System.currentTimeMillis();\n-    long compactionStartTime = now;\n-    long timeToQuit = now + compactionTimeLimitMs;\n-    int totalBlobsPurged = 0;\n-    for (PartitionId partitionId : partitionsSnapshot) {\n-      String partitionPath = partitionId.toPathString();\n-      if (!partitions.contains(partitionId)) {\n-        // Looks like partition was reassigned since the loop started, so skip it\n-        continue;\n-      }\n+    logger.info(\"Beginning dead blob compaction for partition {}\", partition);\n \n-      try {\n-        totalBlobsPurged += cloudDestination.compactPartition(partitionPath);\n-      } catch (CloudStorageException ex) {\n-        logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n-        vcrMetrics.compactionFailureCount.inc();\n-      }\n+    String partitionPath = partition.toPathString();\n+    if (!partitions.contains(partition)) {\n+      // Looks like partition was reassigned since the loop started, so skip it\n+      logger.warn(\"Skipping compaction of Partition {} as the partition was reassgined\", partition);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY0NzI1Mg=="}, "originalCommit": {"oid": "a3178926f4ba04ef42c320ddc7fadf05d9961c8e"}, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyNjQ0NjMyOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxODo1NzowNVrOGhYDFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxOTo0NToxMlrOGhZnLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY1MDE5Nw==", "bodyText": "Could we let compactPartition throw exception on error, and catch it here in the get() call?  Then we can increment compactedPartitionCount only in success case.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437650197", "createdAt": "2020-06-09T18:57:05Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +106,77 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (!partitionSnapshot.isEmpty()) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        totalBlobsPurged += executorCompletionService.take().get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3178926f4ba04ef42c320ddc7fadf05d9961c8e"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY3NTgyMQ==", "bodyText": "done.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437675821", "createdAt": "2020-06-09T19:45:12Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +106,77 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (!partitionSnapshot.isEmpty()) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        totalBlobsPurged += executorCompletionService.take().get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY1MDE5Nw=="}, "originalCommit": {"oid": "a3178926f4ba04ef42c320ddc7fadf05d9961c8e"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyNjg3NDg1OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/CloudStorageCompactorTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQyMDo1OTozMVrOGhcQcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQyMTo1MDoxOVrOGhdrdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcxOTE1Mg==", "bodyText": "nit: remove a partition from map", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437719152", "createdAt": "2020-06-09T20:59:31Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/CloudStorageCompactorTest.java", "diffHunk": "@@ -65,27 +64,23 @@ public void testCompactPartitions() throws Exception {\n     verify(mockDest, times(0)).purgeBlobs(any());\n     */\n \n-    // add 2 partitions to map\n-    int partition1 = 101, partition2 = 102;\n-    long compactionEndTime = System.currentTimeMillis() - TimeUnit.DAYS.toMillis(CloudConfig.DEFAULT_RETENTION_DAYS);\n-    String partitionPath1 = String.valueOf(partition1), partitionPath2 = String.valueOf(partition2);\n+    // add 100 partitions to map\n     String defaultClass = MockClusterMap.DEFAULT_PARTITION_CLASS;\n-    partitionMap.put(new MockPartitionId(partition1, defaultClass), null);\n-    partitionMap.put(new MockPartitionId(partition2, defaultClass), null);\n-\n-    when(mockDest.compactPartition(eq(partitionPath1))).thenReturn(pageSize);\n-    when(mockDest.compactPartition(eq(partitionPath2))).thenReturn(pageSize * 2);\n-    assertEquals(pageSize * 3, compactor.compactPartitions());\n+    for (int i = 0; i < 100; i++) {\n+      partitionMap.put(new MockPartitionId(i, defaultClass), null);\n+      when(mockDest.compactPartition(eq(Integer.toString(i)))).thenReturn(pageSize);\n+    }\n \n+    assertEquals(pageSize * 100, compactor.compactPartitions());\n \n-    // remove partition2 from map\n-    partitionMap.remove(new MockPartitionId(partition2, defaultClass));\n-    assertEquals(pageSize, compactor.compactPartitions());\n+    // remove a from map", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbd1e9f1619f2a2c33380e1a761239d764fe8b5c"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc0MjQ1NA==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437742454", "createdAt": "2020-06-09T21:50:19Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/CloudStorageCompactorTest.java", "diffHunk": "@@ -65,27 +64,23 @@ public void testCompactPartitions() throws Exception {\n     verify(mockDest, times(0)).purgeBlobs(any());\n     */\n \n-    // add 2 partitions to map\n-    int partition1 = 101, partition2 = 102;\n-    long compactionEndTime = System.currentTimeMillis() - TimeUnit.DAYS.toMillis(CloudConfig.DEFAULT_RETENTION_DAYS);\n-    String partitionPath1 = String.valueOf(partition1), partitionPath2 = String.valueOf(partition2);\n+    // add 100 partitions to map\n     String defaultClass = MockClusterMap.DEFAULT_PARTITION_CLASS;\n-    partitionMap.put(new MockPartitionId(partition1, defaultClass), null);\n-    partitionMap.put(new MockPartitionId(partition2, defaultClass), null);\n-\n-    when(mockDest.compactPartition(eq(partitionPath1))).thenReturn(pageSize);\n-    when(mockDest.compactPartition(eq(partitionPath2))).thenReturn(pageSize * 2);\n-    assertEquals(pageSize * 3, compactor.compactPartitions());\n+    for (int i = 0; i < 100; i++) {\n+      partitionMap.put(new MockPartitionId(i, defaultClass), null);\n+      when(mockDest.compactPartition(eq(Integer.toString(i)))).thenReturn(pageSize);\n+    }\n \n+    assertEquals(pageSize * 100, compactor.compactPartitions());\n \n-    // remove partition2 from map\n-    partitionMap.remove(new MockPartitionId(partition2, defaultClass));\n-    assertEquals(pageSize, compactor.compactPartitions());\n+    // remove a from map", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcxOTE1Mg=="}, "originalCommit": {"oid": "fbd1e9f1619f2a2c33380e1a761239d764fe8b5c"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyNjkyMjE1OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQyMToxNToyMFrOGhcu9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNToxNjowNVrOGhlKBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcyNjk2NA==", "bodyText": "I wonder if exception occurs in this method before we call executorCompletionService.take().get(), what will happen? Looks like it may not be captured at line 130, line 147. If it is captured by line 153 catch (Throwable th), will the whole batch compaction be terminated? What about some outstanding threads that are already assigned compaction task?", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437726964", "createdAt": "2020-06-09T21:15:20Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +107,84 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n-    if (isShuttingDown()) {\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (!partitionSnapshot.isEmpty()) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        try {\n+          totalBlobsPurged += executorCompletionService.take().get();\n+          compactedPartitionCount++;\n+        } catch (ExecutionException ex) {\n+          vcrMetrics.compactionFailureCount.inc();\n+        }\n+        compactionInProgress--;\n+        if (System.currentTimeMillis() >= timeToQuit) {\n+          logger.info(\"Compaction terminated due to time limit exceeded.\");\n+          break;\n+        }\n+        if (isShutDown()) {\n+          logger.info(\"Compaction terminated due to shut down.\");\n+          break;\n+        }\n+      }\n+      while (compactionInProgress > 0) {\n+        try {\n+          totalBlobsPurged += executorCompletionService.take().get();\n+          compactedPartitionCount++;\n+        } catch (ExecutionException ex) {\n+          vcrMetrics.compactionFailureCount.inc();\n+        }\n+        compactionInProgress--;\n+      }\n+      doneLatch.get().countDown();\n+    } catch (Throwable th) {\n+      logger.error(\"Hit exception running compaction task\", th);\n+    } finally {\n+      long compactionTime = (System.currentTimeMillis() - compactionStartTime) / 1000;\n+      logger.info(\"Purged {} blobs in {} partitions taking {} seconds\", totalBlobsPurged, compactedPartitionCount,\n+          compactionTime);\n+    }\n+    return totalBlobsPurged;\n+  }\n+\n+  /**\n+   * Purge the inactive blobs in the specified partitions.\n+   * @param partition the {@link PartitionId} to compact.\n+   * @return the total number of blobs purged in the partition.\n+   */\n+  private int compactPartition(PartitionId partition) throws CloudStorageException {\n+    if (isShutDown()) {\n       logger.info(\"Skipping compaction due to shut down.\");\n       return 0;\n     }\n \n-    // TODO: adjust count when compaction uses multiple threads\n-    doneLatch.set(new CountDownLatch(1));\n-\n-    Set<PartitionId> partitionsSnapshot = new HashSet<>(partitions);\n-    logger.info(\"Beginning dead blob compaction for {} partitions\", partitions.size());\n-    long now = System.currentTimeMillis();\n-    long compactionStartTime = now;\n-    long timeToQuit = now + compactionTimeLimitMs;\n-    int totalBlobsPurged = 0;\n-    for (PartitionId partitionId : partitionsSnapshot) {\n-      String partitionPath = partitionId.toPathString();\n-      if (!partitions.contains(partitionId)) {\n-        // Looks like partition was reassigned since the loop started, so skip it\n-        continue;\n-      }\n+    logger.info(\"Beginning dead blob compaction for partition {}\", partition);\n \n-      try {\n-        totalBlobsPurged += cloudDestination.compactPartition(partitionPath);\n-      } catch (CloudStorageException ex) {\n-        logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n-        vcrMetrics.compactionFailureCount.inc();\n-      }\n+    String partitionPath = partition.toPathString();\n+    if (!partitions.contains(partition)) {\n+      // Looks like partition was reassigned since the loop started, so skip it\n+      logger.warn(\"Skipping compaction of Partition {} as the partition was reassigned\", partition);\n+      return 0;\n+    }\n \n-      if (System.currentTimeMillis() >= timeToQuit) {\n-        logger.info(\"Compaction terminated due to time limit exceeded.\");\n-        break;\n-      }\n-      if (isShuttingDown()) {\n-        logger.info(\"Compaction terminated due to shut down.\");\n-        break;\n-      }\n+    try {\n+      return cloudDestination.compactPartition(partitionPath);\n+    } catch (CloudStorageException ex) {\n+      logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n+      throw ex;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbd1e9f1619f2a2c33380e1a761239d764fe8b5c"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc0MjAyNQ==", "bodyText": "If exception would happen in compactPartition(), it would be thrown only when take().get() is called. It would have been ignored silently if there was no call to take().get.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437742025", "createdAt": "2020-06-09T21:49:22Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +107,84 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n-    if (isShuttingDown()) {\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (!partitionSnapshot.isEmpty()) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        try {\n+          totalBlobsPurged += executorCompletionService.take().get();\n+          compactedPartitionCount++;\n+        } catch (ExecutionException ex) {\n+          vcrMetrics.compactionFailureCount.inc();\n+        }\n+        compactionInProgress--;\n+        if (System.currentTimeMillis() >= timeToQuit) {\n+          logger.info(\"Compaction terminated due to time limit exceeded.\");\n+          break;\n+        }\n+        if (isShutDown()) {\n+          logger.info(\"Compaction terminated due to shut down.\");\n+          break;\n+        }\n+      }\n+      while (compactionInProgress > 0) {\n+        try {\n+          totalBlobsPurged += executorCompletionService.take().get();\n+          compactedPartitionCount++;\n+        } catch (ExecutionException ex) {\n+          vcrMetrics.compactionFailureCount.inc();\n+        }\n+        compactionInProgress--;\n+      }\n+      doneLatch.get().countDown();\n+    } catch (Throwable th) {\n+      logger.error(\"Hit exception running compaction task\", th);\n+    } finally {\n+      long compactionTime = (System.currentTimeMillis() - compactionStartTime) / 1000;\n+      logger.info(\"Purged {} blobs in {} partitions taking {} seconds\", totalBlobsPurged, compactedPartitionCount,\n+          compactionTime);\n+    }\n+    return totalBlobsPurged;\n+  }\n+\n+  /**\n+   * Purge the inactive blobs in the specified partitions.\n+   * @param partition the {@link PartitionId} to compact.\n+   * @return the total number of blobs purged in the partition.\n+   */\n+  private int compactPartition(PartitionId partition) throws CloudStorageException {\n+    if (isShutDown()) {\n       logger.info(\"Skipping compaction due to shut down.\");\n       return 0;\n     }\n \n-    // TODO: adjust count when compaction uses multiple threads\n-    doneLatch.set(new CountDownLatch(1));\n-\n-    Set<PartitionId> partitionsSnapshot = new HashSet<>(partitions);\n-    logger.info(\"Beginning dead blob compaction for {} partitions\", partitions.size());\n-    long now = System.currentTimeMillis();\n-    long compactionStartTime = now;\n-    long timeToQuit = now + compactionTimeLimitMs;\n-    int totalBlobsPurged = 0;\n-    for (PartitionId partitionId : partitionsSnapshot) {\n-      String partitionPath = partitionId.toPathString();\n-      if (!partitions.contains(partitionId)) {\n-        // Looks like partition was reassigned since the loop started, so skip it\n-        continue;\n-      }\n+    logger.info(\"Beginning dead blob compaction for partition {}\", partition);\n \n-      try {\n-        totalBlobsPurged += cloudDestination.compactPartition(partitionPath);\n-      } catch (CloudStorageException ex) {\n-        logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n-        vcrMetrics.compactionFailureCount.inc();\n-      }\n+    String partitionPath = partition.toPathString();\n+    if (!partitions.contains(partition)) {\n+      // Looks like partition was reassigned since the loop started, so skip it\n+      logger.warn(\"Skipping compaction of Partition {} as the partition was reassigned\", partition);\n+      return 0;\n+    }\n \n-      if (System.currentTimeMillis() >= timeToQuit) {\n-        logger.info(\"Compaction terminated due to time limit exceeded.\");\n-        break;\n-      }\n-      if (isShuttingDown()) {\n-        logger.info(\"Compaction terminated due to shut down.\");\n-        break;\n-      }\n+    try {\n+      return cloudDestination.compactPartition(partitionPath);\n+    } catch (CloudStorageException ex) {\n+      logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n+      throw ex;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcyNjk2NA=="}, "originalCommit": {"oid": "fbd1e9f1619f2a2c33380e1a761239d764fe8b5c"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc4NjcxOA==", "bodyText": "I am ok with it as I don't see any issue regarding ignoring exception silently.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437786718", "createdAt": "2020-06-10T00:00:11Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +107,84 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n-    if (isShuttingDown()) {\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (!partitionSnapshot.isEmpty()) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        try {\n+          totalBlobsPurged += executorCompletionService.take().get();\n+          compactedPartitionCount++;\n+        } catch (ExecutionException ex) {\n+          vcrMetrics.compactionFailureCount.inc();\n+        }\n+        compactionInProgress--;\n+        if (System.currentTimeMillis() >= timeToQuit) {\n+          logger.info(\"Compaction terminated due to time limit exceeded.\");\n+          break;\n+        }\n+        if (isShutDown()) {\n+          logger.info(\"Compaction terminated due to shut down.\");\n+          break;\n+        }\n+      }\n+      while (compactionInProgress > 0) {\n+        try {\n+          totalBlobsPurged += executorCompletionService.take().get();\n+          compactedPartitionCount++;\n+        } catch (ExecutionException ex) {\n+          vcrMetrics.compactionFailureCount.inc();\n+        }\n+        compactionInProgress--;\n+      }\n+      doneLatch.get().countDown();\n+    } catch (Throwable th) {\n+      logger.error(\"Hit exception running compaction task\", th);\n+    } finally {\n+      long compactionTime = (System.currentTimeMillis() - compactionStartTime) / 1000;\n+      logger.info(\"Purged {} blobs in {} partitions taking {} seconds\", totalBlobsPurged, compactedPartitionCount,\n+          compactionTime);\n+    }\n+    return totalBlobsPurged;\n+  }\n+\n+  /**\n+   * Purge the inactive blobs in the specified partitions.\n+   * @param partition the {@link PartitionId} to compact.\n+   * @return the total number of blobs purged in the partition.\n+   */\n+  private int compactPartition(PartitionId partition) throws CloudStorageException {\n+    if (isShutDown()) {\n       logger.info(\"Skipping compaction due to shut down.\");\n       return 0;\n     }\n \n-    // TODO: adjust count when compaction uses multiple threads\n-    doneLatch.set(new CountDownLatch(1));\n-\n-    Set<PartitionId> partitionsSnapshot = new HashSet<>(partitions);\n-    logger.info(\"Beginning dead blob compaction for {} partitions\", partitions.size());\n-    long now = System.currentTimeMillis();\n-    long compactionStartTime = now;\n-    long timeToQuit = now + compactionTimeLimitMs;\n-    int totalBlobsPurged = 0;\n-    for (PartitionId partitionId : partitionsSnapshot) {\n-      String partitionPath = partitionId.toPathString();\n-      if (!partitions.contains(partitionId)) {\n-        // Looks like partition was reassigned since the loop started, so skip it\n-        continue;\n-      }\n+    logger.info(\"Beginning dead blob compaction for partition {}\", partition);\n \n-      try {\n-        totalBlobsPurged += cloudDestination.compactPartition(partitionPath);\n-      } catch (CloudStorageException ex) {\n-        logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n-        vcrMetrics.compactionFailureCount.inc();\n-      }\n+    String partitionPath = partition.toPathString();\n+    if (!partitions.contains(partition)) {\n+      // Looks like partition was reassigned since the loop started, so skip it\n+      logger.warn(\"Skipping compaction of Partition {} as the partition was reassigned\", partition);\n+      return 0;\n+    }\n \n-      if (System.currentTimeMillis() >= timeToQuit) {\n-        logger.info(\"Compaction terminated due to time limit exceeded.\");\n-        break;\n-      }\n-      if (isShuttingDown()) {\n-        logger.info(\"Compaction terminated due to shut down.\");\n-        break;\n-      }\n+    try {\n+      return cloudDestination.compactPartition(partitionPath);\n+    } catch (CloudStorageException ex) {\n+      logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n+      throw ex;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcyNjk2NA=="}, "originalCommit": {"oid": "fbd1e9f1619f2a2c33380e1a761239d764fe8b5c"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgwNTE3Mw==", "bodyText": "thanks.. one thing I wanted to note is that this code will actually never ignore the exception silently.\nThe way ExecutorCompletionService::take  works is that if any thread throws exception, it keeps the exception in its state without throwing out. But then when we call take() it looks at the state of the thread, and if the thread has returned some value, then it gives out the value with .get() call or if the thread has thrown exception, then it throws that exception on the take() call.\nSo in our code, we will never miss any exception, as we make sure to drain all the threads before shutdown.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437805173", "createdAt": "2020-06-10T01:10:56Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +107,84 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n-    if (isShuttingDown()) {\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (!partitionSnapshot.isEmpty()) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        try {\n+          totalBlobsPurged += executorCompletionService.take().get();\n+          compactedPartitionCount++;\n+        } catch (ExecutionException ex) {\n+          vcrMetrics.compactionFailureCount.inc();\n+        }\n+        compactionInProgress--;\n+        if (System.currentTimeMillis() >= timeToQuit) {\n+          logger.info(\"Compaction terminated due to time limit exceeded.\");\n+          break;\n+        }\n+        if (isShutDown()) {\n+          logger.info(\"Compaction terminated due to shut down.\");\n+          break;\n+        }\n+      }\n+      while (compactionInProgress > 0) {\n+        try {\n+          totalBlobsPurged += executorCompletionService.take().get();\n+          compactedPartitionCount++;\n+        } catch (ExecutionException ex) {\n+          vcrMetrics.compactionFailureCount.inc();\n+        }\n+        compactionInProgress--;\n+      }\n+      doneLatch.get().countDown();\n+    } catch (Throwable th) {\n+      logger.error(\"Hit exception running compaction task\", th);\n+    } finally {\n+      long compactionTime = (System.currentTimeMillis() - compactionStartTime) / 1000;\n+      logger.info(\"Purged {} blobs in {} partitions taking {} seconds\", totalBlobsPurged, compactedPartitionCount,\n+          compactionTime);\n+    }\n+    return totalBlobsPurged;\n+  }\n+\n+  /**\n+   * Purge the inactive blobs in the specified partitions.\n+   * @param partition the {@link PartitionId} to compact.\n+   * @return the total number of blobs purged in the partition.\n+   */\n+  private int compactPartition(PartitionId partition) throws CloudStorageException {\n+    if (isShutDown()) {\n       logger.info(\"Skipping compaction due to shut down.\");\n       return 0;\n     }\n \n-    // TODO: adjust count when compaction uses multiple threads\n-    doneLatch.set(new CountDownLatch(1));\n-\n-    Set<PartitionId> partitionsSnapshot = new HashSet<>(partitions);\n-    logger.info(\"Beginning dead blob compaction for {} partitions\", partitions.size());\n-    long now = System.currentTimeMillis();\n-    long compactionStartTime = now;\n-    long timeToQuit = now + compactionTimeLimitMs;\n-    int totalBlobsPurged = 0;\n-    for (PartitionId partitionId : partitionsSnapshot) {\n-      String partitionPath = partitionId.toPathString();\n-      if (!partitions.contains(partitionId)) {\n-        // Looks like partition was reassigned since the loop started, so skip it\n-        continue;\n-      }\n+    logger.info(\"Beginning dead blob compaction for partition {}\", partition);\n \n-      try {\n-        totalBlobsPurged += cloudDestination.compactPartition(partitionPath);\n-      } catch (CloudStorageException ex) {\n-        logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n-        vcrMetrics.compactionFailureCount.inc();\n-      }\n+    String partitionPath = partition.toPathString();\n+    if (!partitions.contains(partition)) {\n+      // Looks like partition was reassigned since the loop started, so skip it\n+      logger.warn(\"Skipping compaction of Partition {} as the partition was reassigned\", partition);\n+      return 0;\n+    }\n \n-      if (System.currentTimeMillis() >= timeToQuit) {\n-        logger.info(\"Compaction terminated due to time limit exceeded.\");\n-        break;\n-      }\n-      if (isShuttingDown()) {\n-        logger.info(\"Compaction terminated due to shut down.\");\n-        break;\n-      }\n+    try {\n+      return cloudDestination.compactPartition(partitionPath);\n+    } catch (CloudStorageException ex) {\n+      logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n+      throw ex;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcyNjk2NA=="}, "originalCommit": {"oid": "fbd1e9f1619f2a2c33380e1a761239d764fe8b5c"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0MjE3Mg==", "bodyText": "Thanks for clarification. You mentioned \"The way ExecutorCompletionService::take works is that if any thread throws exception, it keeps the exception in its state without throwing out.\".\nI would like to know if this also applies to executorCompletionService.submit() and executor.execute() inside ExecutorCompletionService. If I understand correctly, once the task is submitted, executor.execute() may happen before ExecutorCompletionService::take and at that moment, is any occurred exception still held internally (without being thrown out)?  Correct me if I am wrong, thanks.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437842172", "createdAt": "2020-06-10T03:40:38Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +107,84 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n-    if (isShuttingDown()) {\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (!partitionSnapshot.isEmpty()) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        try {\n+          totalBlobsPurged += executorCompletionService.take().get();\n+          compactedPartitionCount++;\n+        } catch (ExecutionException ex) {\n+          vcrMetrics.compactionFailureCount.inc();\n+        }\n+        compactionInProgress--;\n+        if (System.currentTimeMillis() >= timeToQuit) {\n+          logger.info(\"Compaction terminated due to time limit exceeded.\");\n+          break;\n+        }\n+        if (isShutDown()) {\n+          logger.info(\"Compaction terminated due to shut down.\");\n+          break;\n+        }\n+      }\n+      while (compactionInProgress > 0) {\n+        try {\n+          totalBlobsPurged += executorCompletionService.take().get();\n+          compactedPartitionCount++;\n+        } catch (ExecutionException ex) {\n+          vcrMetrics.compactionFailureCount.inc();\n+        }\n+        compactionInProgress--;\n+      }\n+      doneLatch.get().countDown();\n+    } catch (Throwable th) {\n+      logger.error(\"Hit exception running compaction task\", th);\n+    } finally {\n+      long compactionTime = (System.currentTimeMillis() - compactionStartTime) / 1000;\n+      logger.info(\"Purged {} blobs in {} partitions taking {} seconds\", totalBlobsPurged, compactedPartitionCount,\n+          compactionTime);\n+    }\n+    return totalBlobsPurged;\n+  }\n+\n+  /**\n+   * Purge the inactive blobs in the specified partitions.\n+   * @param partition the {@link PartitionId} to compact.\n+   * @return the total number of blobs purged in the partition.\n+   */\n+  private int compactPartition(PartitionId partition) throws CloudStorageException {\n+    if (isShutDown()) {\n       logger.info(\"Skipping compaction due to shut down.\");\n       return 0;\n     }\n \n-    // TODO: adjust count when compaction uses multiple threads\n-    doneLatch.set(new CountDownLatch(1));\n-\n-    Set<PartitionId> partitionsSnapshot = new HashSet<>(partitions);\n-    logger.info(\"Beginning dead blob compaction for {} partitions\", partitions.size());\n-    long now = System.currentTimeMillis();\n-    long compactionStartTime = now;\n-    long timeToQuit = now + compactionTimeLimitMs;\n-    int totalBlobsPurged = 0;\n-    for (PartitionId partitionId : partitionsSnapshot) {\n-      String partitionPath = partitionId.toPathString();\n-      if (!partitions.contains(partitionId)) {\n-        // Looks like partition was reassigned since the loop started, so skip it\n-        continue;\n-      }\n+    logger.info(\"Beginning dead blob compaction for partition {}\", partition);\n \n-      try {\n-        totalBlobsPurged += cloudDestination.compactPartition(partitionPath);\n-      } catch (CloudStorageException ex) {\n-        logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n-        vcrMetrics.compactionFailureCount.inc();\n-      }\n+    String partitionPath = partition.toPathString();\n+    if (!partitions.contains(partition)) {\n+      // Looks like partition was reassigned since the loop started, so skip it\n+      logger.warn(\"Skipping compaction of Partition {} as the partition was reassigned\", partition);\n+      return 0;\n+    }\n \n-      if (System.currentTimeMillis() >= timeToQuit) {\n-        logger.info(\"Compaction terminated due to time limit exceeded.\");\n-        break;\n-      }\n-      if (isShuttingDown()) {\n-        logger.info(\"Compaction terminated due to shut down.\");\n-        break;\n-      }\n+    try {\n+      return cloudDestination.compactPartition(partitionPath);\n+    } catch (CloudStorageException ex) {\n+      logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n+      throw ex;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcyNjk2NA=="}, "originalCommit": {"oid": "fbd1e9f1619f2a2c33380e1a761239d764fe8b5c"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2MjEwMw==", "bodyText": "Your understanding is right. Once the task is submitted, executor.execute() may happen anytime. However, if the user task that runs inside the execute() throws an exception, execute doesn't immediately throw the exception. It catches all the exception (including RunTimeException) and keeps it in its internal state. When take().get() is called, the Future::get method retrieves the outcome of the user task, figures out if the outcome of the task is an exception, and either throws the exception, or returns the result (outcome).\nSo until the get method is called no outcome (either exception or result) of user task is reported.\nFor more reference, do take a look at the ThreadPoolExecutor::runWorker and FutureTask::report methods of java executor library.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437862103", "createdAt": "2020-06-10T05:05:25Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +107,84 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n-    if (isShuttingDown()) {\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (!partitionSnapshot.isEmpty()) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        try {\n+          totalBlobsPurged += executorCompletionService.take().get();\n+          compactedPartitionCount++;\n+        } catch (ExecutionException ex) {\n+          vcrMetrics.compactionFailureCount.inc();\n+        }\n+        compactionInProgress--;\n+        if (System.currentTimeMillis() >= timeToQuit) {\n+          logger.info(\"Compaction terminated due to time limit exceeded.\");\n+          break;\n+        }\n+        if (isShutDown()) {\n+          logger.info(\"Compaction terminated due to shut down.\");\n+          break;\n+        }\n+      }\n+      while (compactionInProgress > 0) {\n+        try {\n+          totalBlobsPurged += executorCompletionService.take().get();\n+          compactedPartitionCount++;\n+        } catch (ExecutionException ex) {\n+          vcrMetrics.compactionFailureCount.inc();\n+        }\n+        compactionInProgress--;\n+      }\n+      doneLatch.get().countDown();\n+    } catch (Throwable th) {\n+      logger.error(\"Hit exception running compaction task\", th);\n+    } finally {\n+      long compactionTime = (System.currentTimeMillis() - compactionStartTime) / 1000;\n+      logger.info(\"Purged {} blobs in {} partitions taking {} seconds\", totalBlobsPurged, compactedPartitionCount,\n+          compactionTime);\n+    }\n+    return totalBlobsPurged;\n+  }\n+\n+  /**\n+   * Purge the inactive blobs in the specified partitions.\n+   * @param partition the {@link PartitionId} to compact.\n+   * @return the total number of blobs purged in the partition.\n+   */\n+  private int compactPartition(PartitionId partition) throws CloudStorageException {\n+    if (isShutDown()) {\n       logger.info(\"Skipping compaction due to shut down.\");\n       return 0;\n     }\n \n-    // TODO: adjust count when compaction uses multiple threads\n-    doneLatch.set(new CountDownLatch(1));\n-\n-    Set<PartitionId> partitionsSnapshot = new HashSet<>(partitions);\n-    logger.info(\"Beginning dead blob compaction for {} partitions\", partitions.size());\n-    long now = System.currentTimeMillis();\n-    long compactionStartTime = now;\n-    long timeToQuit = now + compactionTimeLimitMs;\n-    int totalBlobsPurged = 0;\n-    for (PartitionId partitionId : partitionsSnapshot) {\n-      String partitionPath = partitionId.toPathString();\n-      if (!partitions.contains(partitionId)) {\n-        // Looks like partition was reassigned since the loop started, so skip it\n-        continue;\n-      }\n+    logger.info(\"Beginning dead blob compaction for partition {}\", partition);\n \n-      try {\n-        totalBlobsPurged += cloudDestination.compactPartition(partitionPath);\n-      } catch (CloudStorageException ex) {\n-        logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n-        vcrMetrics.compactionFailureCount.inc();\n-      }\n+    String partitionPath = partition.toPathString();\n+    if (!partitions.contains(partition)) {\n+      // Looks like partition was reassigned since the loop started, so skip it\n+      logger.warn(\"Skipping compaction of Partition {} as the partition was reassigned\", partition);\n+      return 0;\n+    }\n \n-      if (System.currentTimeMillis() >= timeToQuit) {\n-        logger.info(\"Compaction terminated due to time limit exceeded.\");\n-        break;\n-      }\n-      if (isShuttingDown()) {\n-        logger.info(\"Compaction terminated due to shut down.\");\n-        break;\n-      }\n+    try {\n+      return cloudDestination.compactPartition(partitionPath);\n+    } catch (CloudStorageException ex) {\n+      logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n+      throw ex;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcyNjk2NA=="}, "originalCommit": {"oid": "fbd1e9f1619f2a2c33380e1a761239d764fe8b5c"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2NDk2NA==", "bodyText": "Thanks for thorough explanation, this clarifies everything. I am asking this because I had some issues with ScheduledExecutorService due to didn't catch exception correctly.", "url": "https://github.com/linkedin/ambry/pull/1556#discussion_r437864964", "createdAt": "2020-06-10T05:16:05Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -103,47 +107,84 @@ public int compactPartitions() {\n       logger.info(\"Skipping compaction as no partitions are assigned.\");\n       return 0;\n     }\n-    if (isShuttingDown()) {\n+    List<PartitionId> partitionSnapshot = new ArrayList<>(partitions);\n+    long compactionStartTime = System.currentTimeMillis();\n+    long timeToQuit = System.currentTimeMillis() + compactionTimeLimitMs;\n+    int compactionInProgress = 0;\n+    doneLatch.set(new CountDownLatch(1));\n+    int totalBlobsPurged = 0;\n+    int compactedPartitionCount = 0;\n+    try {\n+      while (!partitionSnapshot.isEmpty()) {\n+        while (compactionInProgress < numThreads) {\n+          if (partitionSnapshot.isEmpty()) {\n+            break;\n+          }\n+          PartitionId partitionId = partitionSnapshot.remove(0);\n+          executorCompletionService.submit(() -> compactPartition(partitionId));\n+          compactionInProgress++;\n+        }\n+        try {\n+          totalBlobsPurged += executorCompletionService.take().get();\n+          compactedPartitionCount++;\n+        } catch (ExecutionException ex) {\n+          vcrMetrics.compactionFailureCount.inc();\n+        }\n+        compactionInProgress--;\n+        if (System.currentTimeMillis() >= timeToQuit) {\n+          logger.info(\"Compaction terminated due to time limit exceeded.\");\n+          break;\n+        }\n+        if (isShutDown()) {\n+          logger.info(\"Compaction terminated due to shut down.\");\n+          break;\n+        }\n+      }\n+      while (compactionInProgress > 0) {\n+        try {\n+          totalBlobsPurged += executorCompletionService.take().get();\n+          compactedPartitionCount++;\n+        } catch (ExecutionException ex) {\n+          vcrMetrics.compactionFailureCount.inc();\n+        }\n+        compactionInProgress--;\n+      }\n+      doneLatch.get().countDown();\n+    } catch (Throwable th) {\n+      logger.error(\"Hit exception running compaction task\", th);\n+    } finally {\n+      long compactionTime = (System.currentTimeMillis() - compactionStartTime) / 1000;\n+      logger.info(\"Purged {} blobs in {} partitions taking {} seconds\", totalBlobsPurged, compactedPartitionCount,\n+          compactionTime);\n+    }\n+    return totalBlobsPurged;\n+  }\n+\n+  /**\n+   * Purge the inactive blobs in the specified partitions.\n+   * @param partition the {@link PartitionId} to compact.\n+   * @return the total number of blobs purged in the partition.\n+   */\n+  private int compactPartition(PartitionId partition) throws CloudStorageException {\n+    if (isShutDown()) {\n       logger.info(\"Skipping compaction due to shut down.\");\n       return 0;\n     }\n \n-    // TODO: adjust count when compaction uses multiple threads\n-    doneLatch.set(new CountDownLatch(1));\n-\n-    Set<PartitionId> partitionsSnapshot = new HashSet<>(partitions);\n-    logger.info(\"Beginning dead blob compaction for {} partitions\", partitions.size());\n-    long now = System.currentTimeMillis();\n-    long compactionStartTime = now;\n-    long timeToQuit = now + compactionTimeLimitMs;\n-    int totalBlobsPurged = 0;\n-    for (PartitionId partitionId : partitionsSnapshot) {\n-      String partitionPath = partitionId.toPathString();\n-      if (!partitions.contains(partitionId)) {\n-        // Looks like partition was reassigned since the loop started, so skip it\n-        continue;\n-      }\n+    logger.info(\"Beginning dead blob compaction for partition {}\", partition);\n \n-      try {\n-        totalBlobsPurged += cloudDestination.compactPartition(partitionPath);\n-      } catch (CloudStorageException ex) {\n-        logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n-        vcrMetrics.compactionFailureCount.inc();\n-      }\n+    String partitionPath = partition.toPathString();\n+    if (!partitions.contains(partition)) {\n+      // Looks like partition was reassigned since the loop started, so skip it\n+      logger.warn(\"Skipping compaction of Partition {} as the partition was reassigned\", partition);\n+      return 0;\n+    }\n \n-      if (System.currentTimeMillis() >= timeToQuit) {\n-        logger.info(\"Compaction terminated due to time limit exceeded.\");\n-        break;\n-      }\n-      if (isShuttingDown()) {\n-        logger.info(\"Compaction terminated due to shut down.\");\n-        break;\n-      }\n+    try {\n+      return cloudDestination.compactPartition(partitionPath);\n+    } catch (CloudStorageException ex) {\n+      logger.error(\"Compaction failed for partition {}\", partitionPath, ex);\n+      throw ex;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcyNjk2NA=="}, "originalCommit": {"oid": "fbd1e9f1619f2a2c33380e1a761239d764fe8b5c"}, "originalPosition": 185}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1247, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}