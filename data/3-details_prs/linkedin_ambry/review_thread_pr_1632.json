{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk0NDA1NTU1", "number": 1632, "reviewThreads": {"totalCount": 32, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QyMzowMjo0M1rOEqDD0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMDozOTowNVrOEsxYEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTI1Nzc2OnYy", "diffSide": "RIGHT", "path": "ambry-account/src/test/java/com/github/ambry/account/AccountTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QyMzowMjo0M1rOHcEEIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNzo1OToyNlrOHd_Leg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE4ODc3MA==", "bodyText": "containerIdSet is never used.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r499188770", "createdAt": "2020-10-03T23:02:43Z", "author": {"login": "jsjtzyy"}, "path": "ambry-account/src/test/java/com/github/ambry/account/AccountTestUtils.java", "diffHunk": "@@ -111,33 +113,11 @@ static void generateRefAccounts(Map<Short, Account> idToRefAccountMap,\n       Map<Short, Container> idToContainers = new HashMap<>();\n       List<Container> containers = new ArrayList<>();\n       Set<Short> containerIdSet = new HashSet<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIwNTg4Mg==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501205882", "createdAt": "2020-10-07T17:59:26Z", "author": {"login": "ankagrawal"}, "path": "ambry-account/src/test/java/com/github/ambry/account/AccountTestUtils.java", "diffHunk": "@@ -111,33 +113,11 @@ static void generateRefAccounts(Map<Short, Account> idToRefAccountMap,\n       Map<Short, Container> idToContainers = new HashMap<>();\n       List<Container> containers = new ArrayList<>();\n       Set<Short> containerIdSet = new HashSet<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE4ODc3MA=="}, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTI1Nzg2OnYy", "diffSide": "RIGHT", "path": "ambry-account/src/test/java/com/github/ambry/account/AccountTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QyMzowMjo0NlrOHcEELA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxODowMDoxNFrOHd_NPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE4ODc4MA==", "bodyText": "nit: use method reference, change containerBuilder -> containerBuilder.build() to ContainerBuilder::build.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r499188780", "createdAt": "2020-10-03T23:02:46Z", "author": {"login": "jsjtzyy"}, "path": "ambry-account/src/test/java/com/github/ambry/account/AccountTestUtils.java", "diffHunk": "@@ -111,33 +113,11 @@ static void generateRefAccounts(Map<Short, Account> idToRefAccountMap,\n       Map<Short, Container> idToContainers = new HashMap<>();\n       List<Container> containers = new ArrayList<>();\n       Set<Short> containerIdSet = new HashSet<>();\n-      for (int j = 0; j < containerCountPerAccount; j++) {\n-        short containerId = Utils.getRandomShort(random);\n-        if (!containerIdSet.add(containerId)) {\n-          j--;\n-          continue;\n-        }\n-        String containerName = UUID.randomUUID().toString();\n-        Container.ContainerStatus containerStatus =\n-            random.nextBoolean() ? Container.ContainerStatus.ACTIVE : Container.ContainerStatus.INACTIVE;\n-        String containerDescription = UUID.randomUUID().toString();\n-        boolean containerCaching = random.nextBoolean();\n-        boolean containerEncryption = random.nextBoolean();\n-        boolean containerPreviousEncryption = containerEncryption || random.nextBoolean();\n-        boolean mediaScanDisabled = random.nextBoolean();\n-        String replicationPolicy = TestUtils.getRandomString(10);\n-        boolean ttlRequired = random.nextBoolean();\n-        Container container = new ContainerBuilder(containerId, containerName, containerStatus, containerDescription,\n-            accountId).setEncrypted(containerEncryption)\n-            .setPreviouslyEncrypted(containerPreviousEncryption)\n-            .setCacheable(containerCaching)\n-            .setMediaScanDisabled(mediaScanDisabled)\n-            .setReplicationPolicy(replicationPolicy)\n-            .setTtlRequired(ttlRequired)\n-            .build();\n-        containers.add(container);\n-        idToContainers.put(containerId, container);\n-      }\n+      List<ContainerBuilder> containerBuilders = generateContainerBuilders(containerCountPerAccount, accountId);\n+\n+      containers.addAll(\n+          containerBuilders.stream().map(containerBuilder -> containerBuilder.build()).collect(Collectors.toList()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIwNjMzMg==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501206332", "createdAt": "2020-10-07T18:00:14Z", "author": {"login": "ankagrawal"}, "path": "ambry-account/src/test/java/com/github/ambry/account/AccountTestUtils.java", "diffHunk": "@@ -111,33 +113,11 @@ static void generateRefAccounts(Map<Short, Account> idToRefAccountMap,\n       Map<Short, Container> idToContainers = new HashMap<>();\n       List<Container> containers = new ArrayList<>();\n       Set<Short> containerIdSet = new HashSet<>();\n-      for (int j = 0; j < containerCountPerAccount; j++) {\n-        short containerId = Utils.getRandomShort(random);\n-        if (!containerIdSet.add(containerId)) {\n-          j--;\n-          continue;\n-        }\n-        String containerName = UUID.randomUUID().toString();\n-        Container.ContainerStatus containerStatus =\n-            random.nextBoolean() ? Container.ContainerStatus.ACTIVE : Container.ContainerStatus.INACTIVE;\n-        String containerDescription = UUID.randomUUID().toString();\n-        boolean containerCaching = random.nextBoolean();\n-        boolean containerEncryption = random.nextBoolean();\n-        boolean containerPreviousEncryption = containerEncryption || random.nextBoolean();\n-        boolean mediaScanDisabled = random.nextBoolean();\n-        String replicationPolicy = TestUtils.getRandomString(10);\n-        boolean ttlRequired = random.nextBoolean();\n-        Container container = new ContainerBuilder(containerId, containerName, containerStatus, containerDescription,\n-            accountId).setEncrypted(containerEncryption)\n-            .setPreviouslyEncrypted(containerPreviousEncryption)\n-            .setCacheable(containerCaching)\n-            .setMediaScanDisabled(mediaScanDisabled)\n-            .setReplicationPolicy(replicationPolicy)\n-            .setTtlRequired(ttlRequired)\n-            .build();\n-        containers.add(container);\n-        idToContainers.put(containerId, container);\n-      }\n+      List<ContainerBuilder> containerBuilders = generateContainerBuilders(containerCountPerAccount, accountId);\n+\n+      containers.addAll(\n+          containerBuilders.stream().map(containerBuilder -> containerBuilder.build()).collect(Collectors.toList()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE4ODc4MA=="}, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTI2Mjc0OnYy", "diffSide": "RIGHT", "path": "ambry-account/src/test/java/com/github/ambry/account/AccountTestUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QyMzoxMzoxMlrOHcEGhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxODowMjoxNVrOHd_SHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE4OTM4Mg==", "bodyText": "In my humble opinion, this method is for particular test case and is not general enough to justify a helper method in AccountTestUtils. (Also it has duplicate code with generateRefAccounts). Can we move this piece of logic to its caller testGetDeprecatedContainers() in HelixAccountServiceTest.java?", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r499189382", "createdAt": "2020-10-03T23:13:12Z", "author": {"login": "jsjtzyy"}, "path": "ambry-account/src/test/java/com/github/ambry/account/AccountTestUtils.java", "diffHunk": "@@ -146,4 +126,87 @@ static void generateRefAccounts(Map<Short, Account> idToRefAccountMap,\n     }\n     assertEquals(\"Wrong number of generated accounts\", accountCount, idToRefAccountMap.size());\n   }\n+\n+  /**\n+   * Randomly generates a collection of {@link Account}s, which do not have the same id or name with needed container status.\n+   * @param idToRefAccountMap A map from id to {@link Account} to populate with the generated {@link Account}s.\n+   * @param idToRefContainerMap A map from name to {@link Account} to populate with the generated {@link Account}s.\n+   * @param accountIdSet A set of ids that could not be used to generate {@link Account}s.\n+   * @param accountCount The number of {@link Account}s to generate.\n+   * @param timestamp timestamp for delete trigger time.\n+   * @throws Exception\n+   */\n+  static void generateRefAccountsForDeprecationTest(Map<Short, Account> idToRefAccountMap,\n+      Map<Short, Map<Short, Container>> idToRefContainerMap, Set<Short> accountIdSet, int accountCount, long timestamp)\n+      throws Exception {\n+    idToRefAccountMap.clear();\n+    idToRefContainerMap.clear();\n+    for (int i = 0; i < accountCount; i++) {\n+      short accountId = Utils.getRandomShort(random);\n+      if (!accountIdSet.add(accountId)) {\n+        i--;\n+        continue;\n+      }\n+      String accountName = UUID.randomUUID().toString();\n+      Account.AccountStatus accountStatus =\n+          random.nextBoolean() ? Account.AccountStatus.ACTIVE : Account.AccountStatus.INACTIVE;\n+      Map<Short, Container> idToContainers = new HashMap<>();\n+      List<Container> containers = new ArrayList<>();\n+      Set<Short> containerIdSet = new HashSet<>();\n+      List<ContainerBuilder> containerBuilders = generateContainerBuilders(4, accountId);\n+      containerBuilders.get(0).setStatus(Container.ContainerStatus.ACTIVE);\n+      containerBuilders.get(1).setStatus(Container.ContainerStatus.INACTIVE);\n+      containerBuilders.get(2).setStatus(Container.ContainerStatus.DELETE_IN_PROGRESS);\n+      containerBuilders.get(2).setDeleteTriggerTime(timestamp);\n+      containerBuilders.get(3).setStatus(Container.ContainerStatus.DELETE_IN_PROGRESS);\n+      containerBuilders.get(3).setDeleteTriggerTime(timestamp + 10000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIwNzU4Mw==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501207583", "createdAt": "2020-10-07T18:02:15Z", "author": {"login": "ankagrawal"}, "path": "ambry-account/src/test/java/com/github/ambry/account/AccountTestUtils.java", "diffHunk": "@@ -146,4 +126,87 @@ static void generateRefAccounts(Map<Short, Account> idToRefAccountMap,\n     }\n     assertEquals(\"Wrong number of generated accounts\", accountCount, idToRefAccountMap.size());\n   }\n+\n+  /**\n+   * Randomly generates a collection of {@link Account}s, which do not have the same id or name with needed container status.\n+   * @param idToRefAccountMap A map from id to {@link Account} to populate with the generated {@link Account}s.\n+   * @param idToRefContainerMap A map from name to {@link Account} to populate with the generated {@link Account}s.\n+   * @param accountIdSet A set of ids that could not be used to generate {@link Account}s.\n+   * @param accountCount The number of {@link Account}s to generate.\n+   * @param timestamp timestamp for delete trigger time.\n+   * @throws Exception\n+   */\n+  static void generateRefAccountsForDeprecationTest(Map<Short, Account> idToRefAccountMap,\n+      Map<Short, Map<Short, Container>> idToRefContainerMap, Set<Short> accountIdSet, int accountCount, long timestamp)\n+      throws Exception {\n+    idToRefAccountMap.clear();\n+    idToRefContainerMap.clear();\n+    for (int i = 0; i < accountCount; i++) {\n+      short accountId = Utils.getRandomShort(random);\n+      if (!accountIdSet.add(accountId)) {\n+        i--;\n+        continue;\n+      }\n+      String accountName = UUID.randomUUID().toString();\n+      Account.AccountStatus accountStatus =\n+          random.nextBoolean() ? Account.AccountStatus.ACTIVE : Account.AccountStatus.INACTIVE;\n+      Map<Short, Container> idToContainers = new HashMap<>();\n+      List<Container> containers = new ArrayList<>();\n+      Set<Short> containerIdSet = new HashSet<>();\n+      List<ContainerBuilder> containerBuilders = generateContainerBuilders(4, accountId);\n+      containerBuilders.get(0).setStatus(Container.ContainerStatus.ACTIVE);\n+      containerBuilders.get(1).setStatus(Container.ContainerStatus.INACTIVE);\n+      containerBuilders.get(2).setStatus(Container.ContainerStatus.DELETE_IN_PROGRESS);\n+      containerBuilders.get(2).setDeleteTriggerTime(timestamp);\n+      containerBuilders.get(3).setStatus(Container.ContainerStatus.DELETE_IN_PROGRESS);\n+      containerBuilders.get(3).setDeleteTriggerTime(timestamp + 10000);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE4OTM4Mg=="}, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTI2NDA3OnYy", "diffSide": "RIGHT", "path": "ambry-account/src/test/java/com/github/ambry/account/AccountTestUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QyMzoxNTo0M1rOHcEHIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxODowNDo0NFrOHd_XlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE4OTUzOA==", "bodyText": "If we decide to remove generateRefAccountsForDeprecationTest() method, we can return a list of containers instead of ContainerBuilder in this method.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r499189538", "createdAt": "2020-10-03T23:15:43Z", "author": {"login": "jsjtzyy"}, "path": "ambry-account/src/test/java/com/github/ambry/account/AccountTestUtils.java", "diffHunk": "@@ -146,4 +126,87 @@ static void generateRefAccounts(Map<Short, Account> idToRefAccountMap,\n     }\n     assertEquals(\"Wrong number of generated accounts\", accountCount, idToRefAccountMap.size());\n   }\n+\n+  /**\n+   * Randomly generates a collection of {@link Account}s, which do not have the same id or name with needed container status.\n+   * @param idToRefAccountMap A map from id to {@link Account} to populate with the generated {@link Account}s.\n+   * @param idToRefContainerMap A map from name to {@link Account} to populate with the generated {@link Account}s.\n+   * @param accountIdSet A set of ids that could not be used to generate {@link Account}s.\n+   * @param accountCount The number of {@link Account}s to generate.\n+   * @param timestamp timestamp for delete trigger time.\n+   * @throws Exception\n+   */\n+  static void generateRefAccountsForDeprecationTest(Map<Short, Account> idToRefAccountMap,\n+      Map<Short, Map<Short, Container>> idToRefContainerMap, Set<Short> accountIdSet, int accountCount, long timestamp)\n+      throws Exception {\n+    idToRefAccountMap.clear();\n+    idToRefContainerMap.clear();\n+    for (int i = 0; i < accountCount; i++) {\n+      short accountId = Utils.getRandomShort(random);\n+      if (!accountIdSet.add(accountId)) {\n+        i--;\n+        continue;\n+      }\n+      String accountName = UUID.randomUUID().toString();\n+      Account.AccountStatus accountStatus =\n+          random.nextBoolean() ? Account.AccountStatus.ACTIVE : Account.AccountStatus.INACTIVE;\n+      Map<Short, Container> idToContainers = new HashMap<>();\n+      List<Container> containers = new ArrayList<>();\n+      Set<Short> containerIdSet = new HashSet<>();\n+      List<ContainerBuilder> containerBuilders = generateContainerBuilders(4, accountId);\n+      containerBuilders.get(0).setStatus(Container.ContainerStatus.ACTIVE);\n+      containerBuilders.get(1).setStatus(Container.ContainerStatus.INACTIVE);\n+      containerBuilders.get(2).setStatus(Container.ContainerStatus.DELETE_IN_PROGRESS);\n+      containerBuilders.get(2).setDeleteTriggerTime(timestamp);\n+      containerBuilders.get(3).setStatus(Container.ContainerStatus.DELETE_IN_PROGRESS);\n+      containerBuilders.get(3).setDeleteTriggerTime(timestamp + 10000);\n+\n+      containers.addAll(\n+          containerBuilders.stream().map(containerBuilder -> containerBuilder.build()).collect(Collectors.toList()));\n+      idToContainers = containers.stream().collect(Collectors.toMap(Container::getId, Function.identity()));\n+      Account account = new AccountBuilder(accountId, accountName, accountStatus).containers(containers).build();\n+      assertEquals(\"Wrong number of generated containers for the account\", 4, account.getAllContainers().size());\n+      idToRefAccountMap.put(accountId, account);\n+      idToRefContainerMap.put(accountId, idToContainers);\n+    }\n+    assertEquals(\"Wrong number of generated accounts\", accountCount, idToRefAccountMap.size());\n+  }\n+\n+  /**\n+   * Generate {@link ContainerBuilder}s for specified {@code accountId}.\n+   * @param numContainers number of {@link ContainerBuilder}s to generate.\n+   * @param accountId accountId for container.\n+   * @return {@link List} of {@link ContainerBuilder}s.\n+   */\n+  private static List<ContainerBuilder> generateContainerBuilders(int numContainers, short accountId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIwODk4MQ==", "bodyText": "This method is also being used by AccountTestUtils.generateRefAccounts . The idea to create this method is that it can be used as a common method for tests that need to create a certain number of containers. This will generate the required number of ContainerBuilders, and individual tests can then modify the builders if they need to. Note that Container objects are immutable, hence I am returning ContainerBuilder here.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501208981", "createdAt": "2020-10-07T18:04:44Z", "author": {"login": "ankagrawal"}, "path": "ambry-account/src/test/java/com/github/ambry/account/AccountTestUtils.java", "diffHunk": "@@ -146,4 +126,87 @@ static void generateRefAccounts(Map<Short, Account> idToRefAccountMap,\n     }\n     assertEquals(\"Wrong number of generated accounts\", accountCount, idToRefAccountMap.size());\n   }\n+\n+  /**\n+   * Randomly generates a collection of {@link Account}s, which do not have the same id or name with needed container status.\n+   * @param idToRefAccountMap A map from id to {@link Account} to populate with the generated {@link Account}s.\n+   * @param idToRefContainerMap A map from name to {@link Account} to populate with the generated {@link Account}s.\n+   * @param accountIdSet A set of ids that could not be used to generate {@link Account}s.\n+   * @param accountCount The number of {@link Account}s to generate.\n+   * @param timestamp timestamp for delete trigger time.\n+   * @throws Exception\n+   */\n+  static void generateRefAccountsForDeprecationTest(Map<Short, Account> idToRefAccountMap,\n+      Map<Short, Map<Short, Container>> idToRefContainerMap, Set<Short> accountIdSet, int accountCount, long timestamp)\n+      throws Exception {\n+    idToRefAccountMap.clear();\n+    idToRefContainerMap.clear();\n+    for (int i = 0; i < accountCount; i++) {\n+      short accountId = Utils.getRandomShort(random);\n+      if (!accountIdSet.add(accountId)) {\n+        i--;\n+        continue;\n+      }\n+      String accountName = UUID.randomUUID().toString();\n+      Account.AccountStatus accountStatus =\n+          random.nextBoolean() ? Account.AccountStatus.ACTIVE : Account.AccountStatus.INACTIVE;\n+      Map<Short, Container> idToContainers = new HashMap<>();\n+      List<Container> containers = new ArrayList<>();\n+      Set<Short> containerIdSet = new HashSet<>();\n+      List<ContainerBuilder> containerBuilders = generateContainerBuilders(4, accountId);\n+      containerBuilders.get(0).setStatus(Container.ContainerStatus.ACTIVE);\n+      containerBuilders.get(1).setStatus(Container.ContainerStatus.INACTIVE);\n+      containerBuilders.get(2).setStatus(Container.ContainerStatus.DELETE_IN_PROGRESS);\n+      containerBuilders.get(2).setDeleteTriggerTime(timestamp);\n+      containerBuilders.get(3).setStatus(Container.ContainerStatus.DELETE_IN_PROGRESS);\n+      containerBuilders.get(3).setDeleteTriggerTime(timestamp + 10000);\n+\n+      containers.addAll(\n+          containerBuilders.stream().map(containerBuilder -> containerBuilder.build()).collect(Collectors.toList()));\n+      idToContainers = containers.stream().collect(Collectors.toMap(Container::getId, Function.identity()));\n+      Account account = new AccountBuilder(accountId, accountName, accountStatus).containers(containers).build();\n+      assertEquals(\"Wrong number of generated containers for the account\", 4, account.getAllContainers().size());\n+      idToRefAccountMap.put(accountId, account);\n+      idToRefContainerMap.put(accountId, idToContainers);\n+    }\n+    assertEquals(\"Wrong number of generated accounts\", accountCount, idToRefAccountMap.size());\n+  }\n+\n+  /**\n+   * Generate {@link ContainerBuilder}s for specified {@code accountId}.\n+   * @param numContainers number of {@link ContainerBuilder}s to generate.\n+   * @param accountId accountId for container.\n+   * @return {@link List} of {@link ContainerBuilder}s.\n+   */\n+  private static List<ContainerBuilder> generateContainerBuilders(int numContainers, short accountId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE4OTUzOA=="}, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTQ2MDc0OnYy", "diffSide": "RIGHT", "path": "ambry-api/src/main/java/com/github/ambry/account/AccountService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQwNjo1Nzo0MVrOHcFjXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxOTo1NToyM1rOHeDOQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTIxMzE0OQ==", "bodyText": "This can be simplified as:\ndeprecatedContainers.addAll(getContainersByStatus(Container.ContainerStatus.INACTIVE));", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r499213149", "createdAt": "2020-10-04T06:57:41Z", "author": {"login": "jsjtzyy"}, "path": "ambry-api/src/main/java/com/github/ambry/account/AccountService.java", "diffHunk": "@@ -122,6 +123,23 @@\n     return selectedContainers;\n   }\n \n+  /**\n+   * @return {@link Set} of {@link Container}s ready for deletion.\n+   */\n+  default Set<Container> getDeprecatedContainers(long containerDeletionRetentionDays) {\n+    Set<Container> deprecatedContainers = new HashSet<>();\n+    getContainersByStatus(Container.ContainerStatus.DELETE_IN_PROGRESS).forEach((container) -> {\n+      if (container.getDeleteTriggerTime() + TimeUnit.DAYS.toMillis(containerDeletionRetentionDays)\n+          <= System.currentTimeMillis()) {\n+        deprecatedContainers.add(container);\n+      }\n+    });\n+    getContainersByStatus(Container.ContainerStatus.INACTIVE).forEach((container) -> {\n+      deprecatedContainers.add(container);\n+    });", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3MjEyOQ==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501272129", "createdAt": "2020-10-07T19:55:23Z", "author": {"login": "ankagrawal"}, "path": "ambry-api/src/main/java/com/github/ambry/account/AccountService.java", "diffHunk": "@@ -122,6 +123,23 @@\n     return selectedContainers;\n   }\n \n+  /**\n+   * @return {@link Set} of {@link Container}s ready for deletion.\n+   */\n+  default Set<Container> getDeprecatedContainers(long containerDeletionRetentionDays) {\n+    Set<Container> deprecatedContainers = new HashSet<>();\n+    getContainersByStatus(Container.ContainerStatus.DELETE_IN_PROGRESS).forEach((container) -> {\n+      if (container.getDeleteTriggerTime() + TimeUnit.DAYS.toMillis(containerDeletionRetentionDays)\n+          <= System.currentTimeMillis()) {\n+        deprecatedContainers.add(container);\n+      }\n+    });\n+    getContainersByStatus(Container.ContainerStatus.INACTIVE).forEach((container) -> {\n+      deprecatedContainers.add(container);\n+    });", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTIxMzE0OQ=="}, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjAyMzIwOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/HelixVcrCluster.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQyMDozODoyNFrOHcKBxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxODozMTo0OFrOHeAczg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4NjQ2OA==", "bodyText": "String.format(...) is unnecessary.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r499286468", "createdAt": "2020-10-04T20:38:24Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/HelixVcrCluster.java", "diffHunk": "@@ -146,11 +162,31 @@ public void participate() throws Exception {\n         cloudConfig.vcrClusterZkConnectString);\n     VcrStateModelFactory stateModelFactory = Utils.getObj(cloudConfig.vcrHelixStateModelFactoryClass, this);\n     manager.getStateMachineEngine().registerStateModelFactory(stateModelFactory.getStateModelName(), stateModelFactory);\n+    registerContainerDeletionSyncTask(manager.getStateMachineEngine());\n     manager.connect();\n     helixAdmin = manager.getClusterManagmentTool();\n     logger.info(\"Participated in HelixVcrCluster successfully.\");\n   }\n \n+  /**\n+   * Register {@link CloudContainerDeletionSyncTask}s to sync deleted container information from account service to VCR.\n+   * @param engine the {@link StateMachineEngine} to register the task state model.\n+   */\n+  private void registerContainerDeletionSyncTask(StateMachineEngine engine) {\n+    Map<String, TaskFactory> taskFactoryMap = new HashMap<>();\n+    taskFactoryMap.put(String.format(\"%s\", CloudContainerDeletionSyncTask.class.getSimpleName()), new TaskFactory() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIyNjcwMg==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501226702", "createdAt": "2020-10-07T18:31:48Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/HelixVcrCluster.java", "diffHunk": "@@ -146,11 +162,31 @@ public void participate() throws Exception {\n         cloudConfig.vcrClusterZkConnectString);\n     VcrStateModelFactory stateModelFactory = Utils.getObj(cloudConfig.vcrHelixStateModelFactoryClass, this);\n     manager.getStateMachineEngine().registerStateModelFactory(stateModelFactory.getStateModelName(), stateModelFactory);\n+    registerContainerDeletionSyncTask(manager.getStateMachineEngine());\n     manager.connect();\n     helixAdmin = manager.getClusterManagmentTool();\n     logger.info(\"Participated in HelixVcrCluster successfully.\");\n   }\n \n+  /**\n+   * Register {@link CloudContainerDeletionSyncTask}s to sync deleted container information from account service to VCR.\n+   * @param engine the {@link StateMachineEngine} to register the task state model.\n+   */\n+  private void registerContainerDeletionSyncTask(StateMachineEngine engine) {\n+    Map<String, TaskFactory> taskFactoryMap = new HashMap<>();\n+    taskFactoryMap.put(String.format(\"%s\", CloudContainerDeletionSyncTask.class.getSimpleName()), new TaskFactory() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4NjQ2OA=="}, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjAyNTMxOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/HelixVcrCluster.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQyMDo0MToyNVrOHcKC0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMDozMzoyMVrOHeEdHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4NjczNw==", "bodyText": "I know it is legacy code but, if possible, I would like to move ContainerDeletionRetentionDays to accountServiceConfig rather than storeConfig. Thus, the retention days would be within accountService and we don't have to pass the parameters to getDeprecatedContainers(). What do you think?", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r499286737", "createdAt": "2020-10-04T20:41:25Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/HelixVcrCluster.java", "diffHunk": "@@ -146,11 +162,31 @@ public void participate() throws Exception {\n         cloudConfig.vcrClusterZkConnectString);\n     VcrStateModelFactory stateModelFactory = Utils.getObj(cloudConfig.vcrHelixStateModelFactoryClass, this);\n     manager.getStateMachineEngine().registerStateModelFactory(stateModelFactory.getStateModelName(), stateModelFactory);\n+    registerContainerDeletionSyncTask(manager.getStateMachineEngine());\n     manager.connect();\n     helixAdmin = manager.getClusterManagmentTool();\n     logger.info(\"Participated in HelixVcrCluster successfully.\");\n   }\n \n+  /**\n+   * Register {@link CloudContainerDeletionSyncTask}s to sync deleted container information from account service to VCR.\n+   * @param engine the {@link StateMachineEngine} to register the task state model.\n+   */\n+  private void registerContainerDeletionSyncTask(StateMachineEngine engine) {\n+    Map<String, TaskFactory> taskFactoryMap = new HashMap<>();\n+    taskFactoryMap.put(String.format(\"%s\", CloudContainerDeletionSyncTask.class.getSimpleName()), new TaskFactory() {\n+      @Override\n+      public Task createNewTask(TaskCallbackContext context) {\n+        return new CloudContainerDeletionSyncTask(accountService, storeConfig.storeContainerDeletionRetentionDays,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIyOTc5NA==", "bodyText": "Makes sense. But there doesn't seem to be any AccountServiceConfig class. I see that there are HelixAccountServiceConfig and MysqlAccountServiceConfig classes. We can either move this config to both the classes (but code duplication) or we can create a new class for this (but it will contain only one config for now). We also leave it as is for now. I don't have any preference for any of the options. Do let me know what would be the best course of action here.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501229794", "createdAt": "2020-10-07T18:37:08Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/HelixVcrCluster.java", "diffHunk": "@@ -146,11 +162,31 @@ public void participate() throws Exception {\n         cloudConfig.vcrClusterZkConnectString);\n     VcrStateModelFactory stateModelFactory = Utils.getObj(cloudConfig.vcrHelixStateModelFactoryClass, this);\n     manager.getStateMachineEngine().registerStateModelFactory(stateModelFactory.getStateModelName(), stateModelFactory);\n+    registerContainerDeletionSyncTask(manager.getStateMachineEngine());\n     manager.connect();\n     helixAdmin = manager.getClusterManagmentTool();\n     logger.info(\"Participated in HelixVcrCluster successfully.\");\n   }\n \n+  /**\n+   * Register {@link CloudContainerDeletionSyncTask}s to sync deleted container information from account service to VCR.\n+   * @param engine the {@link StateMachineEngine} to register the task state model.\n+   */\n+  private void registerContainerDeletionSyncTask(StateMachineEngine engine) {\n+    Map<String, TaskFactory> taskFactoryMap = new HashMap<>();\n+    taskFactoryMap.put(String.format(\"%s\", CloudContainerDeletionSyncTask.class.getSimpleName()), new TaskFactory() {\n+      @Override\n+      public Task createNewTask(TaskCallbackContext context) {\n+        return new CloudContainerDeletionSyncTask(accountService, storeConfig.storeContainerDeletionRetentionDays,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4NjczNw=="}, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI5MjMxNw==", "bodyText": "After rebasing the code, I see that we do have a AccountServiceConfig class that has been recently created. I would still want to defer this change to a future PR. The reason is that containerDeletionRetentionDelays config is also being used in BlobStoreCompactor class. In order to pass AccountServiceConfig to this class, I will need to change a lot of other classes that are unrelated to this change.\nI have created a ticket https://jira01.corp.linkedin.com:8443/browse/AMBRY-7592 for this which I have assigned to the current sprint.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501292317", "createdAt": "2020-10-07T20:33:21Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/HelixVcrCluster.java", "diffHunk": "@@ -146,11 +162,31 @@ public void participate() throws Exception {\n         cloudConfig.vcrClusterZkConnectString);\n     VcrStateModelFactory stateModelFactory = Utils.getObj(cloudConfig.vcrHelixStateModelFactoryClass, this);\n     manager.getStateMachineEngine().registerStateModelFactory(stateModelFactory.getStateModelName(), stateModelFactory);\n+    registerContainerDeletionSyncTask(manager.getStateMachineEngine());\n     manager.connect();\n     helixAdmin = manager.getClusterManagmentTool();\n     logger.info(\"Participated in HelixVcrCluster successfully.\");\n   }\n \n+  /**\n+   * Register {@link CloudContainerDeletionSyncTask}s to sync deleted container information from account service to VCR.\n+   * @param engine the {@link StateMachineEngine} to register the task state model.\n+   */\n+  private void registerContainerDeletionSyncTask(StateMachineEngine engine) {\n+    Map<String, TaskFactory> taskFactoryMap = new HashMap<>();\n+    taskFactoryMap.put(String.format(\"%s\", CloudContainerDeletionSyncTask.class.getSimpleName()), new TaskFactory() {\n+      @Override\n+      public Task createNewTask(TaskCallbackContext context) {\n+        return new CloudContainerDeletionSyncTask(accountService, storeConfig.storeContainerDeletionRetentionDays,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4NjczNw=="}, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjAzMDI4OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/StaticVcrClusterFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQyMDo0OTowN1rOHcKFYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxODozNzoyOFrOHeAptQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4NzM5Mw==", "bodyText": "These newly added parameters will be used in future PR?", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r499287393", "createdAt": "2020-10-04T20:49:07Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/StaticVcrClusterFactory.java", "diffHunk": "@@ -30,7 +32,8 @@\n   private final ClusterMap clusterMap;\n   private VirtualReplicatorCluster virtualReplicatorCluster;\n \n-  public StaticVcrClusterFactory(CloudConfig cloudConfig, ClusterMapConfig clusterMapConfig, ClusterMap clusterMap) {\n+  public StaticVcrClusterFactory(CloudConfig cloudConfig, ClusterMapConfig clusterMapConfig, ClusterMap clusterMap,\n+      AccountService accountService, StoreConfig storeConfig, CloudDestination cloudDestination) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIzMDAwNQ==", "bodyText": "Yes.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501230005", "createdAt": "2020-10-07T18:37:28Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/StaticVcrClusterFactory.java", "diffHunk": "@@ -30,7 +32,8 @@\n   private final ClusterMap clusterMap;\n   private VirtualReplicatorCluster virtualReplicatorCluster;\n \n-  public StaticVcrClusterFactory(CloudConfig cloudConfig, ClusterMapConfig clusterMapConfig, ClusterMap clusterMap) {\n+  public StaticVcrClusterFactory(CloudConfig cloudConfig, ClusterMapConfig clusterMapConfig, ClusterMap clusterMap,\n+      AccountService accountService, StoreConfig storeConfig, CloudDestination cloudDestination) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4NzM5Mw=="}, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjA0NTQzOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQyMToxMjoxM1rOHcKNAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxOToyNzozNlrOHeCUsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4OTM0NA==", "bodyText": "There is no return of this method, we can update the java doc", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r499289344", "createdAt": "2020-10-04T21:12:13Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -204,16 +202,11 @@ void compact(CompactionDetails details, byte[] bundleReadBuffer) throws IOExcept\n   private void getDeprecatedContainers() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI1NzM5NQ==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501257395", "createdAt": "2020-10-07T19:27:36Z", "author": {"login": "ankagrawal"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -204,16 +202,11 @@ void compact(CompactionDetails details, byte[] bundleReadBuffer) throws IOExcept\n   private void getDeprecatedContainers() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI4OTM0NA=="}, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjYwNjU4OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudContainerDeletionSyncTask.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNjozMToxN1rOHcPEaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxODoxNDo1NFrOHd_urA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM2OTA2NQ==", "bodyText": "minor: can you update doc for other two param?", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r499369065", "createdAt": "2020-10-05T06:31:17Z", "author": {"login": "SophieGuo410"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudContainerDeletionSyncTask.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.account.AccountService;\n+import com.github.ambry.account.Container;\n+import java.util.Set;\n+import org.apache.helix.task.Task;\n+import org.apache.helix.task.TaskResult;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Helix task to get the deleted containers information from {@link com.github.ambry.account.AccountService} and update\n+ * it in CosmosDb DeleteContainers table.\n+ */\n+public class CloudContainerDeletionSyncTask implements Task {\n+  private static final Logger logger = LoggerFactory.getLogger(CloudContainerDeletionSyncTask.class);\n+  private final AccountService accountService;\n+  private final long containerDeletionRetentionDays;\n+  private final CloudDestination cloudDestination;\n+\n+  /**\n+   * Constructor for {@link CloudContainerDeletionSyncTask}.\n+   * @param accountService {@link AccountService} object.\n+   */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIxNDg5Mg==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501214892", "createdAt": "2020-10-07T18:14:54Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudContainerDeletionSyncTask.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.account.AccountService;\n+import com.github.ambry.account.Container;\n+import java.util.Set;\n+import org.apache.helix.task.Task;\n+import org.apache.helix.task.TaskResult;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Helix task to get the deleted containers information from {@link com.github.ambry.account.AccountService} and update\n+ * it in CosmosDb DeleteContainers table.\n+ */\n+public class CloudContainerDeletionSyncTask implements Task {\n+  private static final Logger logger = LoggerFactory.getLogger(CloudContainerDeletionSyncTask.class);\n+  private final AccountService accountService;\n+  private final long containerDeletionRetentionDays;\n+  private final CloudDestination cloudDestination;\n+\n+  /**\n+   * Constructor for {@link CloudContainerDeletionSyncTask}.\n+   * @param accountService {@link AccountService} object.\n+   */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM2OTA2NQ=="}, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjY2MjQzOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureContainerCompactor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNjo1NTowOVrOHcPk8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxODozODoyMlrOHeArqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM3NzM5NA==", "bodyText": "Do we need the cloudConfig and vcrMetrics in this class?", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r499377394", "createdAt": "2020-10-05T06:55:09Z", "author": {"login": "SophieGuo410"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureContainerCompactor.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.azure.storage.blob.models.BlobErrorCode;\n+import com.azure.storage.blob.models.BlobStorageException;\n+import com.github.ambry.account.Container;\n+import com.github.ambry.cloud.CloudRequestAgent;\n+import com.github.ambry.cloud.CloudStorageException;\n+import com.github.ambry.cloud.VcrMetrics;\n+import com.github.ambry.config.CloudConfig;\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class that compacts containers in the Azure cloud by purging blobs of deleted containers from\n+ * ABS and Cosmos.\n+ */\n+public class AzureContainerCompactor {\n+  static final String CONTAINER_DELETION_CHECKPOINT_FILE = \"container-deletion-checkpoint\";\n+  private static final Logger logger = LoggerFactory.getLogger(AzureContainerCompactor.class);\n+\n+  private final AzureBlobDataAccessor azureBlobDataAccessor;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final CloudConfig cloudConfig;\n+  private final VcrMetrics vcrMetrics;\n+  private final AzureMetrics azureMetrics;\n+  private final CloudRequestAgent requestAgent;\n+\n+  /**\n+   * Constructor for {@link AzureContainerCompactor}.\n+   * @param azureBlobDataAccessor {@link AzureBlobDataAccessor} object to access Azure Blob Store.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object to access CosmosDb.\n+   * @param cloudConfig {@link CloudConfig} object.\n+   * @param vcrMetrics {@link VcrMetrics} object.\n+   * @param azureMetrics {@link AzureMetrics} object.\n+   */\n+  public AzureContainerCompactor(AzureBlobDataAccessor azureBlobDataAccessor, CosmosDataAccessor cosmosDataAccessor,\n+      CloudConfig cloudConfig, VcrMetrics vcrMetrics, AzureMetrics azureMetrics) {\n+    this.azureBlobDataAccessor = azureBlobDataAccessor;\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.cloudConfig = cloudConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIzMDUwNQ==", "bodyText": "This is for the next PR where I have implemented the actual compaction logic in this class.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501230505", "createdAt": "2020-10-07T18:38:22Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureContainerCompactor.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.azure.storage.blob.models.BlobErrorCode;\n+import com.azure.storage.blob.models.BlobStorageException;\n+import com.github.ambry.account.Container;\n+import com.github.ambry.cloud.CloudRequestAgent;\n+import com.github.ambry.cloud.CloudStorageException;\n+import com.github.ambry.cloud.VcrMetrics;\n+import com.github.ambry.config.CloudConfig;\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class that compacts containers in the Azure cloud by purging blobs of deleted containers from\n+ * ABS and Cosmos.\n+ */\n+public class AzureContainerCompactor {\n+  static final String CONTAINER_DELETION_CHECKPOINT_FILE = \"container-deletion-checkpoint\";\n+  private static final Logger logger = LoggerFactory.getLogger(AzureContainerCompactor.class);\n+\n+  private final AzureBlobDataAccessor azureBlobDataAccessor;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final CloudConfig cloudConfig;\n+  private final VcrMetrics vcrMetrics;\n+  private final AzureMetrics azureMetrics;\n+  private final CloudRequestAgent requestAgent;\n+\n+  /**\n+   * Constructor for {@link AzureContainerCompactor}.\n+   * @param azureBlobDataAccessor {@link AzureBlobDataAccessor} object to access Azure Blob Store.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object to access CosmosDb.\n+   * @param cloudConfig {@link CloudConfig} object.\n+   * @param vcrMetrics {@link VcrMetrics} object.\n+   * @param azureMetrics {@link AzureMetrics} object.\n+   */\n+  public AzureContainerCompactor(AzureBlobDataAccessor azureBlobDataAccessor, CosmosDataAccessor cosmosDataAccessor,\n+      CloudConfig cloudConfig, VcrMetrics vcrMetrics, AzureMetrics azureMetrics) {\n+    this.azureBlobDataAccessor = azureBlobDataAccessor;\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.cloudConfig = cloudConfig;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM3NzM5NA=="}, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjY2OTEzOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureContainerCompactor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNjo1Nzo0OFrOHcPo9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxODo0MzowOFrOHeA2AA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM3ODQyMw==", "bodyText": "partitionPath is null, is this expected?", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r499378423", "createdAt": "2020-10-05T06:57:48Z", "author": {"login": "SophieGuo410"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureContainerCompactor.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.azure.storage.blob.models.BlobErrorCode;\n+import com.azure.storage.blob.models.BlobStorageException;\n+import com.github.ambry.account.Container;\n+import com.github.ambry.cloud.CloudRequestAgent;\n+import com.github.ambry.cloud.CloudStorageException;\n+import com.github.ambry.cloud.VcrMetrics;\n+import com.github.ambry.config.CloudConfig;\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class that compacts containers in the Azure cloud by purging blobs of deleted containers from\n+ * ABS and Cosmos.\n+ */\n+public class AzureContainerCompactor {\n+  static final String CONTAINER_DELETION_CHECKPOINT_FILE = \"container-deletion-checkpoint\";\n+  private static final Logger logger = LoggerFactory.getLogger(AzureContainerCompactor.class);\n+\n+  private final AzureBlobDataAccessor azureBlobDataAccessor;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final CloudConfig cloudConfig;\n+  private final VcrMetrics vcrMetrics;\n+  private final AzureMetrics azureMetrics;\n+  private final CloudRequestAgent requestAgent;\n+\n+  /**\n+   * Constructor for {@link AzureContainerCompactor}.\n+   * @param azureBlobDataAccessor {@link AzureBlobDataAccessor} object to access Azure Blob Store.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object to access CosmosDb.\n+   * @param cloudConfig {@link CloudConfig} object.\n+   * @param vcrMetrics {@link VcrMetrics} object.\n+   * @param azureMetrics {@link AzureMetrics} object.\n+   */\n+  public AzureContainerCompactor(AzureBlobDataAccessor azureBlobDataAccessor, CosmosDataAccessor cosmosDataAccessor,\n+      CloudConfig cloudConfig, VcrMetrics vcrMetrics, AzureMetrics azureMetrics) {\n+    this.azureBlobDataAccessor = azureBlobDataAccessor;\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.cloudConfig = cloudConfig;\n+    this.vcrMetrics = vcrMetrics;\n+    this.azureMetrics = azureMetrics;\n+    requestAgent = new CloudRequestAgent(cloudConfig, vcrMetrics);\n+  }\n+\n+  /**\n+   * Update newly deleted containers from {@code deletedContainers} to CosmosDb since last checkpoint.\n+   * @param deletedContainers {@link Set} of deleted {@link Container}s.\n+   * @throws CloudStorageException in case of any error.\n+   */\n+  public void updateDeletedContainers(Set<Container> deletedContainers) throws CloudStorageException {\n+    if (deletedContainers.isEmpty()) {\n+      logger.info(\"Got empty set to update deleted containers. Skipping update deleted containers to cloud.\");\n+      return;\n+    }\n+    long lastUpdatedContainerTimestamp = getLatestContainerDeletionTime();\n+    long newLastUpdateContainerTimestamp = requestAgent.doWithRetries(() -> cosmosDataAccessor.updateDeletedContainers(\n+        deletedContainers.stream()\n+            .filter(container -> container.getDeleteTriggerTime() >= lastUpdatedContainerTimestamp)\n+            .collect(Collectors.toSet())), \"updateDeletedContainer\", null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIzMzE1Mg==", "bodyText": "Yes its expected because update of deprecated container happens for all partitions.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501233152", "createdAt": "2020-10-07T18:43:08Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureContainerCompactor.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.azure.storage.blob.models.BlobErrorCode;\n+import com.azure.storage.blob.models.BlobStorageException;\n+import com.github.ambry.account.Container;\n+import com.github.ambry.cloud.CloudRequestAgent;\n+import com.github.ambry.cloud.CloudStorageException;\n+import com.github.ambry.cloud.VcrMetrics;\n+import com.github.ambry.config.CloudConfig;\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class that compacts containers in the Azure cloud by purging blobs of deleted containers from\n+ * ABS and Cosmos.\n+ */\n+public class AzureContainerCompactor {\n+  static final String CONTAINER_DELETION_CHECKPOINT_FILE = \"container-deletion-checkpoint\";\n+  private static final Logger logger = LoggerFactory.getLogger(AzureContainerCompactor.class);\n+\n+  private final AzureBlobDataAccessor azureBlobDataAccessor;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final CloudConfig cloudConfig;\n+  private final VcrMetrics vcrMetrics;\n+  private final AzureMetrics azureMetrics;\n+  private final CloudRequestAgent requestAgent;\n+\n+  /**\n+   * Constructor for {@link AzureContainerCompactor}.\n+   * @param azureBlobDataAccessor {@link AzureBlobDataAccessor} object to access Azure Blob Store.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object to access CosmosDb.\n+   * @param cloudConfig {@link CloudConfig} object.\n+   * @param vcrMetrics {@link VcrMetrics} object.\n+   * @param azureMetrics {@link AzureMetrics} object.\n+   */\n+  public AzureContainerCompactor(AzureBlobDataAccessor azureBlobDataAccessor, CosmosDataAccessor cosmosDataAccessor,\n+      CloudConfig cloudConfig, VcrMetrics vcrMetrics, AzureMetrics azureMetrics) {\n+    this.azureBlobDataAccessor = azureBlobDataAccessor;\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.cloudConfig = cloudConfig;\n+    this.vcrMetrics = vcrMetrics;\n+    this.azureMetrics = azureMetrics;\n+    requestAgent = new CloudRequestAgent(cloudConfig, vcrMetrics);\n+  }\n+\n+  /**\n+   * Update newly deleted containers from {@code deletedContainers} to CosmosDb since last checkpoint.\n+   * @param deletedContainers {@link Set} of deleted {@link Container}s.\n+   * @throws CloudStorageException in case of any error.\n+   */\n+  public void updateDeletedContainers(Set<Container> deletedContainers) throws CloudStorageException {\n+    if (deletedContainers.isEmpty()) {\n+      logger.info(\"Got empty set to update deleted containers. Skipping update deleted containers to cloud.\");\n+      return;\n+    }\n+    long lastUpdatedContainerTimestamp = getLatestContainerDeletionTime();\n+    long newLastUpdateContainerTimestamp = requestAgent.doWithRetries(() -> cosmosDataAccessor.updateDeletedContainers(\n+        deletedContainers.stream()\n+            .filter(container -> container.getDeleteTriggerTime() >= lastUpdatedContainerTimestamp)\n+            .collect(Collectors.toSet())), \"updateDeletedContainer\", null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM3ODQyMw=="}, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjcwMTc5OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureContainerCompactor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzoxMDowMVrOHcP8MQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzoxMDowMVrOHcP8MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM4MzM0NQ==", "bodyText": "There's a situation that a container deletion has been canceled within the retention days so deleteTriggerTime will be back to 0, will you consider to sync that in the future implementation?", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r499383345", "createdAt": "2020-10-05T07:10:01Z", "author": {"login": "SophieGuo410"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureContainerCompactor.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.azure.storage.blob.models.BlobErrorCode;\n+import com.azure.storage.blob.models.BlobStorageException;\n+import com.github.ambry.account.Container;\n+import com.github.ambry.cloud.CloudRequestAgent;\n+import com.github.ambry.cloud.CloudStorageException;\n+import com.github.ambry.cloud.VcrMetrics;\n+import com.github.ambry.config.CloudConfig;\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Class that compacts containers in the Azure cloud by purging blobs of deleted containers from\n+ * ABS and Cosmos.\n+ */\n+public class AzureContainerCompactor {\n+  static final String CONTAINER_DELETION_CHECKPOINT_FILE = \"container-deletion-checkpoint\";\n+  private static final Logger logger = LoggerFactory.getLogger(AzureContainerCompactor.class);\n+\n+  private final AzureBlobDataAccessor azureBlobDataAccessor;\n+  private final CosmosDataAccessor cosmosDataAccessor;\n+  private final CloudConfig cloudConfig;\n+  private final VcrMetrics vcrMetrics;\n+  private final AzureMetrics azureMetrics;\n+  private final CloudRequestAgent requestAgent;\n+\n+  /**\n+   * Constructor for {@link AzureContainerCompactor}.\n+   * @param azureBlobDataAccessor {@link AzureBlobDataAccessor} object to access Azure Blob Store.\n+   * @param cosmosDataAccessor {@link CosmosDataAccessor} object to access CosmosDb.\n+   * @param cloudConfig {@link CloudConfig} object.\n+   * @param vcrMetrics {@link VcrMetrics} object.\n+   * @param azureMetrics {@link AzureMetrics} object.\n+   */\n+  public AzureContainerCompactor(AzureBlobDataAccessor azureBlobDataAccessor, CosmosDataAccessor cosmosDataAccessor,\n+      CloudConfig cloudConfig, VcrMetrics vcrMetrics, AzureMetrics azureMetrics) {\n+    this.azureBlobDataAccessor = azureBlobDataAccessor;\n+    this.cosmosDataAccessor = cosmosDataAccessor;\n+    this.cloudConfig = cloudConfig;\n+    this.vcrMetrics = vcrMetrics;\n+    this.azureMetrics = azureMetrics;\n+    requestAgent = new CloudRequestAgent(cloudConfig, vcrMetrics);\n+  }\n+\n+  /**\n+   * Update newly deleted containers from {@code deletedContainers} to CosmosDb since last checkpoint.\n+   * @param deletedContainers {@link Set} of deleted {@link Container}s.\n+   * @throws CloudStorageException in case of any error.\n+   */\n+  public void updateDeletedContainers(Set<Container> deletedContainers) throws CloudStorageException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf1bb0d24e5e1568cb2e3fb658a5a820080b0c7c"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMzcyMDUxOnYy", "diffSide": "RIGHT", "path": "ambry-api/src/main/java/com/github/ambry/account/AccountService.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxNzo1MDoyMlrOHdTP0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxNzo1MDoyMlrOHdTP0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ4NjA5Nw==", "bodyText": "I think we don't need this method in the interface as it is a specific case of getContainersByStatus().\nFor sure, the logic about retention days should not be in the interface.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r500486097", "createdAt": "2020-10-06T17:50:22Z", "author": {"login": "lightningrob"}, "path": "ambry-api/src/main/java/com/github/ambry/account/AccountService.java", "diffHunk": "@@ -122,6 +123,23 @@\n     return selectedContainers;\n   }\n \n+  /**\n+   * @return {@link Set} of {@link Container}s ready for deletion.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMzcyMzU1OnYy", "diffSide": "RIGHT", "path": "ambry-api/src/main/java/com/github/ambry/account/Container.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxNzo1MToxM1rOHdTRzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxNzo1MToxM1rOHdTRzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ4NjYwNQ==", "bodyText": "This change was made in a previous PR.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r500486605", "createdAt": "2020-10-06T17:51:13Z", "author": {"login": "lightningrob"}, "path": "ambry-api/src/main/java/com/github/ambry/account/Container.java", "diffHunk": "@@ -493,7 +493,7 @@ static Container fromJson(JSONObject json, short parentAccountId) throws JSONExc\n    * @return The metadata of the container.\n    * @throws JSONException If fails to compose metadata.\n    */\n-  JSONObject toJson() throws JSONException {\n+  public JSONObject toJson() throws JSONException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMzcyNzI1OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudContainerCompactor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxNzo1MjoxMFrOHdTUHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxODowNzoyM1rOHd_d0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ4NzE5Ng==", "bodyText": "Minor: would change deleted to \"deprecated\" or \"inactive\"", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r500487196", "createdAt": "2020-10-06T17:52:10Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudContainerCompactor.java", "diffHunk": "@@ -0,0 +1,22 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+\n+/**\n+ * Class that runs scheduled or on-demand compaction of blobs of deleted containers.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIxMDU3OA==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501210578", "createdAt": "2020-10-07T18:07:23Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudContainerCompactor.java", "diffHunk": "@@ -0,0 +1,22 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+\n+/**\n+ * Class that runs scheduled or on-demand compaction of blobs of deleted containers.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ4NzE5Ng=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMzc1MDk4OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudDestination.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxNzo1ODoyM1rOHdTjLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQyMDoxNDoxM1rOHfXYBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ5MTA1Mg==", "bodyText": "It's not obvious what this method does.  I also wonder if this method belongs in the interface, since it seems fairly specific to the Azure impl.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r500491052", "createdAt": "2020-10-06T17:58:23Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudDestination.java", "diffHunk": "@@ -137,4 +139,13 @@ boolean retrieveTokens(String partitionPath, String tokenFileName, OutputStream\n    * Halt any compactions in progress.\n    */\n   void stopCompaction();\n+\n+  /**\n+   * Update the deleted {@link Container}s to cloud.\n+   * @param deletedContainers {@link Set} of deleted {@link Container}s.\n+   * @param partitionIds {@link Collection} of cloud partition ids from where the container needs to be deleted.\n+   * @throws {@link CloudStorageException} if updating fails.\n+   */\n+  void updateDeletedContainers(Set<Container> deletedContainers, Collection<String> partitionIds)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIyMzY0Mg==", "bodyText": "I should have named this better. Changed the name and comments. This method deprecates the specified containers for specified partitions in cloud.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501223642", "createdAt": "2020-10-07T18:26:32Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudDestination.java", "diffHunk": "@@ -137,4 +139,13 @@ boolean retrieveTokens(String partitionPath, String tokenFileName, OutputStream\n    * Halt any compactions in progress.\n    */\n   void stopCompaction();\n+\n+  /**\n+   * Update the deleted {@link Container}s to cloud.\n+   * @param deletedContainers {@link Set} of deleted {@link Container}s.\n+   * @param partitionIds {@link Collection} of cloud partition ids from where the container needs to be deleted.\n+   * @throws {@link CloudStorageException} if updating fails.\n+   */\n+  void updateDeletedContainers(Set<Container> deletedContainers, Collection<String> partitionIds)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ5MTA1Mg=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU3NTQwNw==", "bodyText": "True, but the fact that the method takes set of partitions points to the Azure implementation that starts with the complete set and then tracks when each one is done.  Maybe you could argue that any implementation would need to do something similar, but I find the second parameter a bit of a code smell.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r502575407", "createdAt": "2020-10-09T17:29:26Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudDestination.java", "diffHunk": "@@ -137,4 +139,13 @@ boolean retrieveTokens(String partitionPath, String tokenFileName, OutputStream\n    * Halt any compactions in progress.\n    */\n   void stopCompaction();\n+\n+  /**\n+   * Update the deleted {@link Container}s to cloud.\n+   * @param deletedContainers {@link Set} of deleted {@link Container}s.\n+   * @param partitionIds {@link Collection} of cloud partition ids from where the container needs to be deleted.\n+   * @throws {@link CloudStorageException} if updating fails.\n+   */\n+  void updateDeletedContainers(Set<Container> deletedContainers, Collection<String> partitionIds)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ5MTA1Mg=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY1MDg4Ng==", "bodyText": "I have fixed the signatures and implementation to address this concern.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r502650886", "createdAt": "2020-10-09T20:14:13Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudDestination.java", "diffHunk": "@@ -137,4 +139,13 @@ boolean retrieveTokens(String partitionPath, String tokenFileName, OutputStream\n    * Halt any compactions in progress.\n    */\n   void stopCompaction();\n+\n+  /**\n+   * Update the deleted {@link Container}s to cloud.\n+   * @param deletedContainers {@link Set} of deleted {@link Container}s.\n+   * @param partitionIds {@link Collection} of cloud partition ids from where the container needs to be deleted.\n+   * @throws {@link CloudStorageException} if updating fails.\n+   */\n+  void updateDeletedContainers(Set<Container> deletedContainers, Collection<String> partitionIds)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ5MTA1Mg=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMzc1Njk5OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudRequestAgent.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxNzo1OTo1OVrOHdTnEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxODoyNzo0NFrOHeATnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ5MjA1MA==", "bodyText": "The word \"failed\" need to be there either way.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r500492050", "createdAt": "2020-10-06T17:59:59Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudRequestAgent.java", "diffHunk": "@@ -55,7 +56,8 @@ public CloudRequestAgent(CloudConfig cloudConfig, VcrMetrics vcrMetrics) {\n     }\n     // Line should never be reached\n     throw new CloudStorageException(\n-        actionName + \" failed partition \" + partitionPath + \" made \" + attempts + \" attempts\");\n+        actionName + (!Utils.isNullOrEmpty(partitionPath) ? (\" failed partition \" + partitionPath) : \"\") + \" made \" + attempts", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIyNDM0OA==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501224348", "createdAt": "2020-10-07T18:27:44Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudRequestAgent.java", "diffHunk": "@@ -55,7 +56,8 @@ public CloudRequestAgent(CloudConfig cloudConfig, VcrMetrics vcrMetrics) {\n     }\n     // Line should never be reached\n     throw new CloudStorageException(\n-        actionName + \" failed partition \" + partitionPath + \" made \" + attempts + \" attempts\");\n+        actionName + (!Utils.isNullOrEmpty(partitionPath) ? (\" failed partition \" + partitionPath) : \"\") + \" made \" + attempts", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ5MjA1MA=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMzc2OTE0OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/ContainerDeletionEntry.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxODowMzoxNVrOHdTufA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxODoyOTo0MlrOHeAX_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ5Mzk0OA==", "bodyText": "Please use different variable names to distinguish.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r500493948", "createdAt": "2020-10-06T18:03:15Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/ContainerDeletionEntry.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.account.Container;\n+import com.github.ambry.clustermap.ClusterMap;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.json.JSONObject;\n+\n+\n+/**\n+ * Class representing container deletion status in cloud.\n+ */\n+public class ContainerDeletionEntry {\n+\n+  static final String VERSION_KEY = \"version\";\n+  static final String CONTAINER_ID_KEY = \"containerId\";\n+  static final String ACCOUNT_ID_KEY = \"accountId\";\n+  static final String CONTAINER_DELETE_TRIGGER_TIME_KEY = \"deleteTriggerTime\";\n+  static final String IS_DELETED_KEY = \"isDeleted\";\n+  static final String DELETE_PENDING_PARTITIONS_KEY = \"deletePendingPartitions\";\n+\n+  private static short JSON_VERSION_1 = 1;\n+\n+  private final short version;\n+  private final short containerId;\n+  private final short accountId;\n+  private final Set<String> deletePendingPartitions;\n+  private final long deleteTriggerTimestamp;\n+  private boolean isDeleted;\n+\n+  /**\n+   * Constructor for {@link ContainerDeletionEntry}.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   */\n+  public ContainerDeletionEntry(short containerId, short accountId, long deleteTriggerTimestamp) {\n+    this.containerId = containerId;\n+    this.accountId = accountId;\n+    this.deleteTriggerTimestamp = deleteTriggerTimestamp;\n+    this.isDeleted = true;\n+    this.deletePendingPartitions = Collections.emptySet();\n+    this.version = JSON_VERSION_1;\n+  }\n+\n+  /**\n+   * Constructor for {@link ContainerDeletionEntry}.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   * @param isDeleted {@code true} if all container blobs are deleted in cloud. {@code false} otherwise.\n+   * @param partitionIds {@link Collection} of all the cloud partition ids from which container is yet to be deleted.\n+   */\n+  public ContainerDeletionEntry(short containerId, short accountId, long deleteTriggerTimestamp, boolean isDeleted,\n+      Collection<String> partitionIds) {\n+    this.version = JSON_VERSION_1;\n+    this.containerId = containerId;\n+    this.accountId = accountId;\n+    this.deleteTriggerTimestamp = deleteTriggerTimestamp;\n+    this.isDeleted = isDeleted;\n+    if (!isDeleted) {\n+      deletePendingPartitions = new HashSet<>();\n+      deletePendingPartitions.addAll(partitionIds);\n+    } else {\n+      deletePendingPartitions = Collections.emptySet();\n+    }\n+  }\n+\n+  /**\n+   * Private constructor for {@link ContainerDeletionEntry}. Used from deserialization.\n+   * @param version deserialized version.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   * @param isDeleted {@code true} if all container blobs are deleted in cloud. {@code false} otherwise.\n+   * @param deletePendingPartitions {@link Collection} of all the cloud partition ids from which container is yet to be deleted.\n+   */\n+  private ContainerDeletionEntry(short version, short containerId, short accountId, long deleteTriggerTimestamp,\n+      boolean isDeleted, Collection<Object> deletePendingPartitions) {\n+    this.version = version;\n+    this.containerId = containerId;\n+    this.accountId = accountId;\n+    this.deleteTriggerTimestamp = deleteTriggerTimestamp;\n+    this.isDeleted = isDeleted;\n+    this.deletePendingPartitions = new HashSet<>();\n+    deletePendingPartitions.forEach(partitionId -> this.deletePendingPartitions.add((String) partitionId));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIyNTQ2OA==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501225468", "createdAt": "2020-10-07T18:29:42Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/ContainerDeletionEntry.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.account.Container;\n+import com.github.ambry.clustermap.ClusterMap;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.json.JSONObject;\n+\n+\n+/**\n+ * Class representing container deletion status in cloud.\n+ */\n+public class ContainerDeletionEntry {\n+\n+  static final String VERSION_KEY = \"version\";\n+  static final String CONTAINER_ID_KEY = \"containerId\";\n+  static final String ACCOUNT_ID_KEY = \"accountId\";\n+  static final String CONTAINER_DELETE_TRIGGER_TIME_KEY = \"deleteTriggerTime\";\n+  static final String IS_DELETED_KEY = \"isDeleted\";\n+  static final String DELETE_PENDING_PARTITIONS_KEY = \"deletePendingPartitions\";\n+\n+  private static short JSON_VERSION_1 = 1;\n+\n+  private final short version;\n+  private final short containerId;\n+  private final short accountId;\n+  private final Set<String> deletePendingPartitions;\n+  private final long deleteTriggerTimestamp;\n+  private boolean isDeleted;\n+\n+  /**\n+   * Constructor for {@link ContainerDeletionEntry}.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   */\n+  public ContainerDeletionEntry(short containerId, short accountId, long deleteTriggerTimestamp) {\n+    this.containerId = containerId;\n+    this.accountId = accountId;\n+    this.deleteTriggerTimestamp = deleteTriggerTimestamp;\n+    this.isDeleted = true;\n+    this.deletePendingPartitions = Collections.emptySet();\n+    this.version = JSON_VERSION_1;\n+  }\n+\n+  /**\n+   * Constructor for {@link ContainerDeletionEntry}.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   * @param isDeleted {@code true} if all container blobs are deleted in cloud. {@code false} otherwise.\n+   * @param partitionIds {@link Collection} of all the cloud partition ids from which container is yet to be deleted.\n+   */\n+  public ContainerDeletionEntry(short containerId, short accountId, long deleteTriggerTimestamp, boolean isDeleted,\n+      Collection<String> partitionIds) {\n+    this.version = JSON_VERSION_1;\n+    this.containerId = containerId;\n+    this.accountId = accountId;\n+    this.deleteTriggerTimestamp = deleteTriggerTimestamp;\n+    this.isDeleted = isDeleted;\n+    if (!isDeleted) {\n+      deletePendingPartitions = new HashSet<>();\n+      deletePendingPartitions.addAll(partitionIds);\n+    } else {\n+      deletePendingPartitions = Collections.emptySet();\n+    }\n+  }\n+\n+  /**\n+   * Private constructor for {@link ContainerDeletionEntry}. Used from deserialization.\n+   * @param version deserialized version.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   * @param isDeleted {@code true} if all container blobs are deleted in cloud. {@code false} otherwise.\n+   * @param deletePendingPartitions {@link Collection} of all the cloud partition ids from which container is yet to be deleted.\n+   */\n+  private ContainerDeletionEntry(short version, short containerId, short accountId, long deleteTriggerTimestamp,\n+      boolean isDeleted, Collection<Object> deletePendingPartitions) {\n+    this.version = version;\n+    this.containerId = containerId;\n+    this.accountId = accountId;\n+    this.deleteTriggerTimestamp = deleteTriggerTimestamp;\n+    this.isDeleted = isDeleted;\n+    this.deletePendingPartitions = new HashSet<>();\n+    deletePendingPartitions.forEach(partitionId -> this.deletePendingPartitions.add((String) partitionId));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ5Mzk0OA=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMzc3OTcwOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/ContainerDeletionEntry.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxODowNTo0NFrOHdT0qQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxODozMDo1NFrOHeAa2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ5NTUyOQ==", "bodyText": "If this is Cosmos specific, please move it to the cloud/azure package.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r500495529", "createdAt": "2020-10-06T18:05:44Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/ContainerDeletionEntry.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.account.Container;\n+import com.github.ambry.clustermap.ClusterMap;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.json.JSONObject;\n+\n+\n+/**\n+ * Class representing container deletion status in cloud.\n+ */\n+public class ContainerDeletionEntry {\n+\n+  static final String VERSION_KEY = \"version\";\n+  static final String CONTAINER_ID_KEY = \"containerId\";\n+  static final String ACCOUNT_ID_KEY = \"accountId\";\n+  static final String CONTAINER_DELETE_TRIGGER_TIME_KEY = \"deleteTriggerTime\";\n+  static final String IS_DELETED_KEY = \"isDeleted\";\n+  static final String DELETE_PENDING_PARTITIONS_KEY = \"deletePendingPartitions\";\n+\n+  private static short JSON_VERSION_1 = 1;\n+\n+  private final short version;\n+  private final short containerId;\n+  private final short accountId;\n+  private final Set<String> deletePendingPartitions;\n+  private final long deleteTriggerTimestamp;\n+  private boolean isDeleted;\n+\n+  /**\n+   * Constructor for {@link ContainerDeletionEntry}.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   */\n+  public ContainerDeletionEntry(short containerId, short accountId, long deleteTriggerTimestamp) {\n+    this.containerId = containerId;\n+    this.accountId = accountId;\n+    this.deleteTriggerTimestamp = deleteTriggerTimestamp;\n+    this.isDeleted = true;\n+    this.deletePendingPartitions = Collections.emptySet();\n+    this.version = JSON_VERSION_1;\n+  }\n+\n+  /**\n+   * Constructor for {@link ContainerDeletionEntry}.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   * @param isDeleted {@code true} if all container blobs are deleted in cloud. {@code false} otherwise.\n+   * @param partitionIds {@link Collection} of all the cloud partition ids from which container is yet to be deleted.\n+   */\n+  public ContainerDeletionEntry(short containerId, short accountId, long deleteTriggerTimestamp, boolean isDeleted,\n+      Collection<String> partitionIds) {\n+    this.version = JSON_VERSION_1;\n+    this.containerId = containerId;\n+    this.accountId = accountId;\n+    this.deleteTriggerTimestamp = deleteTriggerTimestamp;\n+    this.isDeleted = isDeleted;\n+    if (!isDeleted) {\n+      deletePendingPartitions = new HashSet<>();\n+      deletePendingPartitions.addAll(partitionIds);\n+    } else {\n+      deletePendingPartitions = Collections.emptySet();\n+    }\n+  }\n+\n+  /**\n+   * Private constructor for {@link ContainerDeletionEntry}. Used from deserialization.\n+   * @param version deserialized version.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   * @param isDeleted {@code true} if all container blobs are deleted in cloud. {@code false} otherwise.\n+   * @param deletePendingPartitions {@link Collection} of all the cloud partition ids from which container is yet to be deleted.\n+   */\n+  private ContainerDeletionEntry(short version, short containerId, short accountId, long deleteTriggerTimestamp,\n+      boolean isDeleted, Collection<Object> deletePendingPartitions) {\n+    this.version = version;\n+    this.containerId = containerId;\n+    this.accountId = accountId;\n+    this.deleteTriggerTimestamp = deleteTriggerTimestamp;\n+    this.isDeleted = isDeleted;\n+    this.deletePendingPartitions = new HashSet<>();\n+    deletePendingPartitions.forEach(partitionId -> this.deletePendingPartitions.add((String) partitionId));\n+  }\n+\n+  /**\n+   * Create a {@link ContainerDeletionEntry} from specified {@link Container} in specified {@link ClusterMap}.\n+   * @param container {@link Container} from which to create deletion entry.\n+   * @param partitionIds {@link Collection} of partition ids.\n+   * @return {@link ContainerDeletionEntry} object.\n+   */\n+  public static ContainerDeletionEntry fromContainer(Container container, Collection<String> partitionIds) {\n+    return new ContainerDeletionEntry(container.getId(), container.getParentAccountId(),\n+        container.getDeleteTriggerTime(), false, partitionIds);\n+  }\n+\n+  /**\n+   * Create {@link ContainerDeletionEntry} from specified json.\n+   * @param jsonObject {@link JSONObject} representing the serialized {@link ContainerDeletionEntry}.\n+   * @return deserialized {@link ContainerDeletionEntry} object.\n+   */\n+  public static ContainerDeletionEntry fromJson(JSONObject jsonObject) {\n+    return new ContainerDeletionEntry((short) jsonObject.getInt(VERSION_KEY),\n+        (short) jsonObject.getInt(CONTAINER_ID_KEY), (short) jsonObject.getInt(ACCOUNT_ID_KEY),\n+        jsonObject.getLong(CONTAINER_DELETE_TRIGGER_TIME_KEY), jsonObject.getBoolean(IS_DELETED_KEY),\n+        jsonObject.getJSONArray(DELETE_PENDING_PARTITIONS_KEY).toList());\n+  }\n+\n+  /**\n+   * Mark the container as deleted in cloud.\n+   */\n+  public void markDeleted() {\n+    isDeleted = true;\n+  }\n+\n+  /**\n+   * Remove a delete pending partition.\n+   * @param partitionId partition to remove.\n+   */\n+  public void removePartition(String partitionId) {\n+    deletePendingPartitions.remove(partitionId);\n+  }\n+\n+  /**\n+   * @return deletion status of the container.\n+   */\n+  public boolean isDeleted() {\n+    return isDeleted;\n+  }\n+\n+  /**\n+   * @return delete trigger timestamp when the container deletion was triggered by customer.\n+   */\n+  public long getDeleteTriggerTimestamp() {\n+    return deleteTriggerTimestamp;\n+  }\n+\n+  /**\n+   * Serialize {@link Container} object to save to Cosmos.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIyNjIwMQ==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501226201", "createdAt": "2020-10-07T18:30:54Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/ContainerDeletionEntry.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.account.Container;\n+import com.github.ambry.clustermap.ClusterMap;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.json.JSONObject;\n+\n+\n+/**\n+ * Class representing container deletion status in cloud.\n+ */\n+public class ContainerDeletionEntry {\n+\n+  static final String VERSION_KEY = \"version\";\n+  static final String CONTAINER_ID_KEY = \"containerId\";\n+  static final String ACCOUNT_ID_KEY = \"accountId\";\n+  static final String CONTAINER_DELETE_TRIGGER_TIME_KEY = \"deleteTriggerTime\";\n+  static final String IS_DELETED_KEY = \"isDeleted\";\n+  static final String DELETE_PENDING_PARTITIONS_KEY = \"deletePendingPartitions\";\n+\n+  private static short JSON_VERSION_1 = 1;\n+\n+  private final short version;\n+  private final short containerId;\n+  private final short accountId;\n+  private final Set<String> deletePendingPartitions;\n+  private final long deleteTriggerTimestamp;\n+  private boolean isDeleted;\n+\n+  /**\n+   * Constructor for {@link ContainerDeletionEntry}.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   */\n+  public ContainerDeletionEntry(short containerId, short accountId, long deleteTriggerTimestamp) {\n+    this.containerId = containerId;\n+    this.accountId = accountId;\n+    this.deleteTriggerTimestamp = deleteTriggerTimestamp;\n+    this.isDeleted = true;\n+    this.deletePendingPartitions = Collections.emptySet();\n+    this.version = JSON_VERSION_1;\n+  }\n+\n+  /**\n+   * Constructor for {@link ContainerDeletionEntry}.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   * @param isDeleted {@code true} if all container blobs are deleted in cloud. {@code false} otherwise.\n+   * @param partitionIds {@link Collection} of all the cloud partition ids from which container is yet to be deleted.\n+   */\n+  public ContainerDeletionEntry(short containerId, short accountId, long deleteTriggerTimestamp, boolean isDeleted,\n+      Collection<String> partitionIds) {\n+    this.version = JSON_VERSION_1;\n+    this.containerId = containerId;\n+    this.accountId = accountId;\n+    this.deleteTriggerTimestamp = deleteTriggerTimestamp;\n+    this.isDeleted = isDeleted;\n+    if (!isDeleted) {\n+      deletePendingPartitions = new HashSet<>();\n+      deletePendingPartitions.addAll(partitionIds);\n+    } else {\n+      deletePendingPartitions = Collections.emptySet();\n+    }\n+  }\n+\n+  /**\n+   * Private constructor for {@link ContainerDeletionEntry}. Used from deserialization.\n+   * @param version deserialized version.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   * @param isDeleted {@code true} if all container blobs are deleted in cloud. {@code false} otherwise.\n+   * @param deletePendingPartitions {@link Collection} of all the cloud partition ids from which container is yet to be deleted.\n+   */\n+  private ContainerDeletionEntry(short version, short containerId, short accountId, long deleteTriggerTimestamp,\n+      boolean isDeleted, Collection<Object> deletePendingPartitions) {\n+    this.version = version;\n+    this.containerId = containerId;\n+    this.accountId = accountId;\n+    this.deleteTriggerTimestamp = deleteTriggerTimestamp;\n+    this.isDeleted = isDeleted;\n+    this.deletePendingPartitions = new HashSet<>();\n+    deletePendingPartitions.forEach(partitionId -> this.deletePendingPartitions.add((String) partitionId));\n+  }\n+\n+  /**\n+   * Create a {@link ContainerDeletionEntry} from specified {@link Container} in specified {@link ClusterMap}.\n+   * @param container {@link Container} from which to create deletion entry.\n+   * @param partitionIds {@link Collection} of partition ids.\n+   * @return {@link ContainerDeletionEntry} object.\n+   */\n+  public static ContainerDeletionEntry fromContainer(Container container, Collection<String> partitionIds) {\n+    return new ContainerDeletionEntry(container.getId(), container.getParentAccountId(),\n+        container.getDeleteTriggerTime(), false, partitionIds);\n+  }\n+\n+  /**\n+   * Create {@link ContainerDeletionEntry} from specified json.\n+   * @param jsonObject {@link JSONObject} representing the serialized {@link ContainerDeletionEntry}.\n+   * @return deserialized {@link ContainerDeletionEntry} object.\n+   */\n+  public static ContainerDeletionEntry fromJson(JSONObject jsonObject) {\n+    return new ContainerDeletionEntry((short) jsonObject.getInt(VERSION_KEY),\n+        (short) jsonObject.getInt(CONTAINER_ID_KEY), (short) jsonObject.getInt(ACCOUNT_ID_KEY),\n+        jsonObject.getLong(CONTAINER_DELETE_TRIGGER_TIME_KEY), jsonObject.getBoolean(IS_DELETED_KEY),\n+        jsonObject.getJSONArray(DELETE_PENDING_PARTITIONS_KEY).toList());\n+  }\n+\n+  /**\n+   * Mark the container as deleted in cloud.\n+   */\n+  public void markDeleted() {\n+    isDeleted = true;\n+  }\n+\n+  /**\n+   * Remove a delete pending partition.\n+   * @param partitionId partition to remove.\n+   */\n+  public void removePartition(String partitionId) {\n+    deletePendingPartitions.remove(partitionId);\n+  }\n+\n+  /**\n+   * @return deletion status of the container.\n+   */\n+  public boolean isDeleted() {\n+    return isDeleted;\n+  }\n+\n+  /**\n+   * @return delete trigger timestamp when the container deletion was triggered by customer.\n+   */\n+  public long getDeleteTriggerTimestamp() {\n+    return deleteTriggerTimestamp;\n+  }\n+\n+  /**\n+   * Serialize {@link Container} object to save to Cosmos.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ5NTUyOQ=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 157}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMzc5MDE1OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxODowODoyNVrOHdT63A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxOToyNjo1NVrOHeCTfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ5NzExNg==", "bodyText": "This change doesn't add much IMO.  I would move the retention part inside the original loop.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r500497116", "createdAt": "2020-10-06T18:08:25Z", "author": {"login": "lightningrob"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -204,16 +202,11 @@ void compact(CompactionDetails details, byte[] bundleReadBuffer) throws IOExcept\n   private void getDeprecatedContainers() {\n     deprecatedContainers.clear();\n     if (accountService != null) {\n-      accountService.getContainersByStatus(Container.ContainerStatus.DELETE_IN_PROGRESS).forEach((container) -> {\n-        if (container.getDeleteTriggerTime() + TimeUnit.DAYS.toMillis(config.storeContainerDeletionRetentionDays)\n-            <= System.currentTimeMillis()) {\n-          deprecatedContainers.add(new Pair<>(container.getParentAccountId(), container.getId()));\n-        }\n-      });\n+      Set<Container> containers = accountService.getDeprecatedContainers(config.storeContainerDeletionRetentionDays);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI1NzA4Ng==", "bodyText": "This change moves the logic to get deprecated containers from AccountService to a common place so. that it can be reused by multiple callers.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501257086", "createdAt": "2020-10-07T19:26:55Z", "author": {"login": "ankagrawal"}, "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -204,16 +202,11 @@ void compact(CompactionDetails details, byte[] bundleReadBuffer) throws IOExcept\n   private void getDeprecatedContainers() {\n     deprecatedContainers.clear();\n     if (accountService != null) {\n-      accountService.getContainersByStatus(Container.ContainerStatus.DELETE_IN_PROGRESS).forEach((container) -> {\n-        if (container.getDeleteTriggerTime() + TimeUnit.DAYS.toMillis(config.storeContainerDeletionRetentionDays)\n-            <= System.currentTimeMillis()) {\n-          deprecatedContainers.add(new Pair<>(container.getParentAccountId(), container.getId()));\n-        }\n-      });\n+      Set<Container> containers = accountService.getDeprecatedContainers(config.storeContainerDeletionRetentionDays);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQ5NzExNg=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMzk1ODEyOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/CosmosDataAccessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxODo0OTozNFrOHdVeqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxOTo0NDo0NVrOHeC3_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUyMjY2Nw==", "bodyText": "IMO it's too dangerous to do this as an upsert, since could potentially reset the set of partitions (to the full set) in a container that has already been deleted for some partitions.  If the entry already exist, is there any reason to update it?", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r500522667", "createdAt": "2020-10-06T18:49:34Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -519,6 +528,24 @@ public String queryChangeFeed(String requestContinuationToken, int maxFeedSize,\n     }\n   }\n \n+  /**\n+   * Add the {@link ContainerDeletionEntry} for newly deleted {@link Container}s to cosmos table.\n+   * @param deletedContainers {@link Set} of deleted {@link ContainerDeletionEntry}s.\n+   * @return the max deletion trigger time of all the added containers to serve as checkpoint for future update.\n+   */\n+  public long updateDeletedContainers(Set<ContainerDeletionEntry> deletedContainers) throws DocumentClientException {\n+    long latestContainerDeletionTimestamp = -1;\n+    for (ContainerDeletionEntry containerDeletionEntry : deletedContainers) {\n+      executeCosmosAction(() -> asyncDocumentClient.upsertDocument(cosmosDeletedContainerCollectionLink,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI2NjQyOQ==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501266429", "createdAt": "2020-10-07T19:44:45Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -519,6 +528,24 @@ public String queryChangeFeed(String requestContinuationToken, int maxFeedSize,\n     }\n   }\n \n+  /**\n+   * Add the {@link ContainerDeletionEntry} for newly deleted {@link Container}s to cosmos table.\n+   * @param deletedContainers {@link Set} of deleted {@link ContainerDeletionEntry}s.\n+   * @return the max deletion trigger time of all the added containers to serve as checkpoint for future update.\n+   */\n+  public long updateDeletedContainers(Set<ContainerDeletionEntry> deletedContainers) throws DocumentClientException {\n+    long latestContainerDeletionTimestamp = -1;\n+    for (ContainerDeletionEntry containerDeletionEntry : deletedContainers) {\n+      executeCosmosAction(() -> asyncDocumentClient.upsertDocument(cosmosDeletedContainerCollectionLink,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUyMjY2Nw=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMzk2NTI2OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/azure/AzureContainerCompactorIntegrationTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxODo1MToyOVrOHdVjEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxOTozMDozMlrOHeCamQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUyMzc5Mw==", "bodyText": "What does this do?", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r500523793", "createdAt": "2020-10-06T18:51:29Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/azure/AzureContainerCompactorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.codahale.metrics.MetricRegistry;\n+import com.codahale.metrics.Timer;\n+import com.github.ambry.account.Container;\n+import com.github.ambry.account.ContainerBuilder;\n+import com.github.ambry.cloud.CloudDestinationFactory;\n+import com.github.ambry.cloud.CloudStorageException;\n+import com.github.ambry.config.CloudConfig;\n+import com.github.ambry.config.VerifiableProperties;\n+import com.github.ambry.utils.TestUtils;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.FeedOptions;\n+import com.microsoft.azure.cosmosdb.SqlQuerySpec;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.UUID;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+\n+@Ignore\n+@RunWith(MockitoJUnitRunner.class)\n+public class AzureContainerCompactorIntegrationTest {\n+\n+  private final Random random = new Random();\n+  private final String PROPS_FILE_NAME = \"azure-test.properties\";\n+  private AzureCloudConfig azureConfig;\n+  private VerifiableProperties verifiableProperties;\n+  private AzureCloudDestination cloudDestination;\n+\n+  @Before\n+  public void setup() throws ReflectiveOperationException {\n+    // TODO Create the required cosmos table as well as the required azure blob.\n+    Properties testProperties = new Properties();\n+    try (InputStream input = this.getClass().getClassLoader().getResourceAsStream(PROPS_FILE_NAME)) {\n+      if (input == null) {\n+        throw new IllegalStateException(\"Could not find resource: \" + PROPS_FILE_NAME);\n+      }\n+      testProperties.load(input);\n+    } catch (IOException ex) {\n+      throw new IllegalStateException(\"Could not load properties from resource: \" + PROPS_FILE_NAME);\n+    }\n+    testProperties.setProperty(\"clustermap.cluster.name\", \"Integration-Test\");\n+    testProperties.setProperty(\"clustermap.datacenter.name\", \"uswest\");\n+    testProperties.setProperty(\"clustermap.host.name\", \"localhost\");\n+    testProperties.setProperty(\"kms.default.container.key\",\n+        \"B374A26A71490437AA024E4FADD5B497FDFF1A8EA6FF12F6FB65AF2720B59CCF\");\n+\n+    testProperties.setProperty(CloudConfig.CLOUD_DELETED_BLOB_RETENTION_DAYS, String.valueOf(1));\n+    testProperties.setProperty(AzureCloudConfig.AZURE_PURGE_BATCH_SIZE, \"10\");\n+    verifiableProperties = new VerifiableProperties(testProperties);\n+    CloudConfig cloudConfig = new CloudConfig(verifiableProperties);\n+    this.azureConfig = new AzureCloudConfig(verifiableProperties);\n+    MetricRegistry registry = new MetricRegistry();\n+    CloudDestinationFactory cloudDestinationFactory =\n+        Utils.getObj(cloudConfig.cloudDestinationFactoryClass, verifiableProperties, registry);\n+    cloudDestination = (AzureCloudDestination) cloudDestinationFactory.getCloudDestination();\n+  }\n+\n+  @After\n+  public void destroy() throws IOException {\n+    // TODO destroy the abs blob and cosmos db\n+    if (cloudDestination != null) {\n+      cloudDestination.close();\n+    }\n+  }\n+\n+  @Test\n+  public void testUpdateDeletedContainer() throws CloudStorageException {\n+    Set<Container> containers = generateContainers(5);\n+    cloudDestination.updateDeletedContainers(containers, null);\n+    verifyCosmosData(containers);\n+    verifyCheckpoint(containers);\n+  }\n+\n+  private void verifyCosmosData(Set<Container> containers) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI1ODkwNQ==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501258905", "createdAt": "2020-10-07T19:30:32Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/azure/AzureContainerCompactorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.codahale.metrics.MetricRegistry;\n+import com.codahale.metrics.Timer;\n+import com.github.ambry.account.Container;\n+import com.github.ambry.account.ContainerBuilder;\n+import com.github.ambry.cloud.CloudDestinationFactory;\n+import com.github.ambry.cloud.CloudStorageException;\n+import com.github.ambry.config.CloudConfig;\n+import com.github.ambry.config.VerifiableProperties;\n+import com.github.ambry.utils.TestUtils;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.FeedOptions;\n+import com.microsoft.azure.cosmosdb.SqlQuerySpec;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.UUID;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+\n+@Ignore\n+@RunWith(MockitoJUnitRunner.class)\n+public class AzureContainerCompactorIntegrationTest {\n+\n+  private final Random random = new Random();\n+  private final String PROPS_FILE_NAME = \"azure-test.properties\";\n+  private AzureCloudConfig azureConfig;\n+  private VerifiableProperties verifiableProperties;\n+  private AzureCloudDestination cloudDestination;\n+\n+  @Before\n+  public void setup() throws ReflectiveOperationException {\n+    // TODO Create the required cosmos table as well as the required azure blob.\n+    Properties testProperties = new Properties();\n+    try (InputStream input = this.getClass().getClassLoader().getResourceAsStream(PROPS_FILE_NAME)) {\n+      if (input == null) {\n+        throw new IllegalStateException(\"Could not find resource: \" + PROPS_FILE_NAME);\n+      }\n+      testProperties.load(input);\n+    } catch (IOException ex) {\n+      throw new IllegalStateException(\"Could not load properties from resource: \" + PROPS_FILE_NAME);\n+    }\n+    testProperties.setProperty(\"clustermap.cluster.name\", \"Integration-Test\");\n+    testProperties.setProperty(\"clustermap.datacenter.name\", \"uswest\");\n+    testProperties.setProperty(\"clustermap.host.name\", \"localhost\");\n+    testProperties.setProperty(\"kms.default.container.key\",\n+        \"B374A26A71490437AA024E4FADD5B497FDFF1A8EA6FF12F6FB65AF2720B59CCF\");\n+\n+    testProperties.setProperty(CloudConfig.CLOUD_DELETED_BLOB_RETENTION_DAYS, String.valueOf(1));\n+    testProperties.setProperty(AzureCloudConfig.AZURE_PURGE_BATCH_SIZE, \"10\");\n+    verifiableProperties = new VerifiableProperties(testProperties);\n+    CloudConfig cloudConfig = new CloudConfig(verifiableProperties);\n+    this.azureConfig = new AzureCloudConfig(verifiableProperties);\n+    MetricRegistry registry = new MetricRegistry();\n+    CloudDestinationFactory cloudDestinationFactory =\n+        Utils.getObj(cloudConfig.cloudDestinationFactoryClass, verifiableProperties, registry);\n+    cloudDestination = (AzureCloudDestination) cloudDestinationFactory.getCloudDestination();\n+  }\n+\n+  @After\n+  public void destroy() throws IOException {\n+    // TODO destroy the abs blob and cosmos db\n+    if (cloudDestination != null) {\n+      cloudDestination.close();\n+    }\n+  }\n+\n+  @Test\n+  public void testUpdateDeletedContainer() throws CloudStorageException {\n+    Set<Container> containers = generateContainers(5);\n+    cloudDestination.updateDeletedContainers(containers, null);\n+    verifyCosmosData(containers);\n+    verifyCheckpoint(containers);\n+  }\n+\n+  private void verifyCosmosData(Set<Container> containers) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUyMzc5Mw=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMzk2NzYxOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/azure/AzureContainerCompactorIntegrationTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxODo1MjoxMVrOHdVkmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxOTozMDoxOVrOHeCaJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUyNDE4NA==", "bodyText": "Please add TODO lines for incomplete methods.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r500524184", "createdAt": "2020-10-06T18:52:11Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/azure/AzureContainerCompactorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.codahale.metrics.MetricRegistry;\n+import com.codahale.metrics.Timer;\n+import com.github.ambry.account.Container;\n+import com.github.ambry.account.ContainerBuilder;\n+import com.github.ambry.cloud.CloudDestinationFactory;\n+import com.github.ambry.cloud.CloudStorageException;\n+import com.github.ambry.config.CloudConfig;\n+import com.github.ambry.config.VerifiableProperties;\n+import com.github.ambry.utils.TestUtils;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.FeedOptions;\n+import com.microsoft.azure.cosmosdb.SqlQuerySpec;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.UUID;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+\n+@Ignore\n+@RunWith(MockitoJUnitRunner.class)\n+public class AzureContainerCompactorIntegrationTest {\n+\n+  private final Random random = new Random();\n+  private final String PROPS_FILE_NAME = \"azure-test.properties\";\n+  private AzureCloudConfig azureConfig;\n+  private VerifiableProperties verifiableProperties;\n+  private AzureCloudDestination cloudDestination;\n+\n+  @Before\n+  public void setup() throws ReflectiveOperationException {\n+    // TODO Create the required cosmos table as well as the required azure blob.\n+    Properties testProperties = new Properties();\n+    try (InputStream input = this.getClass().getClassLoader().getResourceAsStream(PROPS_FILE_NAME)) {\n+      if (input == null) {\n+        throw new IllegalStateException(\"Could not find resource: \" + PROPS_FILE_NAME);\n+      }\n+      testProperties.load(input);\n+    } catch (IOException ex) {\n+      throw new IllegalStateException(\"Could not load properties from resource: \" + PROPS_FILE_NAME);\n+    }\n+    testProperties.setProperty(\"clustermap.cluster.name\", \"Integration-Test\");\n+    testProperties.setProperty(\"clustermap.datacenter.name\", \"uswest\");\n+    testProperties.setProperty(\"clustermap.host.name\", \"localhost\");\n+    testProperties.setProperty(\"kms.default.container.key\",\n+        \"B374A26A71490437AA024E4FADD5B497FDFF1A8EA6FF12F6FB65AF2720B59CCF\");\n+\n+    testProperties.setProperty(CloudConfig.CLOUD_DELETED_BLOB_RETENTION_DAYS, String.valueOf(1));\n+    testProperties.setProperty(AzureCloudConfig.AZURE_PURGE_BATCH_SIZE, \"10\");\n+    verifiableProperties = new VerifiableProperties(testProperties);\n+    CloudConfig cloudConfig = new CloudConfig(verifiableProperties);\n+    this.azureConfig = new AzureCloudConfig(verifiableProperties);\n+    MetricRegistry registry = new MetricRegistry();\n+    CloudDestinationFactory cloudDestinationFactory =\n+        Utils.getObj(cloudConfig.cloudDestinationFactoryClass, verifiableProperties, registry);\n+    cloudDestination = (AzureCloudDestination) cloudDestinationFactory.getCloudDestination();\n+  }\n+\n+  @After\n+  public void destroy() throws IOException {\n+    // TODO destroy the abs blob and cosmos db\n+    if (cloudDestination != null) {\n+      cloudDestination.close();\n+    }\n+  }\n+\n+  @Test\n+  public void testUpdateDeletedContainer() throws CloudStorageException {\n+    Set<Container> containers = generateContainers(5);\n+    cloudDestination.updateDeletedContainers(containers, null);\n+    verifyCosmosData(containers);\n+    verifyCheckpoint(containers);\n+  }\n+\n+  private void verifyCosmosData(Set<Container> containers) {\n+    String query = \"SELECT VALUE COUNT(1) FROM C\";\n+    SqlQuerySpec querySpec = new SqlQuerySpec(query);\n+    Timer timer = new Timer();\n+    cloudDestination.getCosmosDataAccessor()\n+        .executeCosmosQuery(azureConfig.cosmosDeletedContainerCollectionLink, null, querySpec, new FeedOptions(),\n+            timer);\n+  }\n+\n+  private void verifyCheckpoint(Set<Container> containers) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI1ODc5MA==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501258790", "createdAt": "2020-10-07T19:30:19Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/azure/AzureContainerCompactorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.codahale.metrics.MetricRegistry;\n+import com.codahale.metrics.Timer;\n+import com.github.ambry.account.Container;\n+import com.github.ambry.account.ContainerBuilder;\n+import com.github.ambry.cloud.CloudDestinationFactory;\n+import com.github.ambry.cloud.CloudStorageException;\n+import com.github.ambry.config.CloudConfig;\n+import com.github.ambry.config.VerifiableProperties;\n+import com.github.ambry.utils.TestUtils;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.FeedOptions;\n+import com.microsoft.azure.cosmosdb.SqlQuerySpec;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.UUID;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+\n+@Ignore\n+@RunWith(MockitoJUnitRunner.class)\n+public class AzureContainerCompactorIntegrationTest {\n+\n+  private final Random random = new Random();\n+  private final String PROPS_FILE_NAME = \"azure-test.properties\";\n+  private AzureCloudConfig azureConfig;\n+  private VerifiableProperties verifiableProperties;\n+  private AzureCloudDestination cloudDestination;\n+\n+  @Before\n+  public void setup() throws ReflectiveOperationException {\n+    // TODO Create the required cosmos table as well as the required azure blob.\n+    Properties testProperties = new Properties();\n+    try (InputStream input = this.getClass().getClassLoader().getResourceAsStream(PROPS_FILE_NAME)) {\n+      if (input == null) {\n+        throw new IllegalStateException(\"Could not find resource: \" + PROPS_FILE_NAME);\n+      }\n+      testProperties.load(input);\n+    } catch (IOException ex) {\n+      throw new IllegalStateException(\"Could not load properties from resource: \" + PROPS_FILE_NAME);\n+    }\n+    testProperties.setProperty(\"clustermap.cluster.name\", \"Integration-Test\");\n+    testProperties.setProperty(\"clustermap.datacenter.name\", \"uswest\");\n+    testProperties.setProperty(\"clustermap.host.name\", \"localhost\");\n+    testProperties.setProperty(\"kms.default.container.key\",\n+        \"B374A26A71490437AA024E4FADD5B497FDFF1A8EA6FF12F6FB65AF2720B59CCF\");\n+\n+    testProperties.setProperty(CloudConfig.CLOUD_DELETED_BLOB_RETENTION_DAYS, String.valueOf(1));\n+    testProperties.setProperty(AzureCloudConfig.AZURE_PURGE_BATCH_SIZE, \"10\");\n+    verifiableProperties = new VerifiableProperties(testProperties);\n+    CloudConfig cloudConfig = new CloudConfig(verifiableProperties);\n+    this.azureConfig = new AzureCloudConfig(verifiableProperties);\n+    MetricRegistry registry = new MetricRegistry();\n+    CloudDestinationFactory cloudDestinationFactory =\n+        Utils.getObj(cloudConfig.cloudDestinationFactoryClass, verifiableProperties, registry);\n+    cloudDestination = (AzureCloudDestination) cloudDestinationFactory.getCloudDestination();\n+  }\n+\n+  @After\n+  public void destroy() throws IOException {\n+    // TODO destroy the abs blob and cosmos db\n+    if (cloudDestination != null) {\n+      cloudDestination.close();\n+    }\n+  }\n+\n+  @Test\n+  public void testUpdateDeletedContainer() throws CloudStorageException {\n+    Set<Container> containers = generateContainers(5);\n+    cloudDestination.updateDeletedContainers(containers, null);\n+    verifyCosmosData(containers);\n+    verifyCheckpoint(containers);\n+  }\n+\n+  private void verifyCosmosData(Set<Container> containers) {\n+    String query = \"SELECT VALUE COUNT(1) FROM C\";\n+    SqlQuerySpec querySpec = new SqlQuerySpec(query);\n+    Timer timer = new Timer();\n+    cloudDestination.getCosmosDataAccessor()\n+        .executeCosmosQuery(azureConfig.cosmosDeletedContainerCollectionLink, null, querySpec, new FeedOptions(),\n+            timer);\n+  }\n+\n+  private void verifyCheckpoint(Set<Container> containers) {\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUyNDE4NA=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMzk3ODgwOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/azure/AzureContainerCompactorIntegrationTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxODo1NToxOFrOHdVreQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxOTozMDoyNlrOHeCacA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUyNTk0NQ==", "bodyText": "Please add TODO lines for incomplete methods.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r500525945", "createdAt": "2020-10-06T18:55:18Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/azure/AzureContainerCompactorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.codahale.metrics.MetricRegistry;\n+import com.codahale.metrics.Timer;\n+import com.github.ambry.account.Container;\n+import com.github.ambry.account.ContainerBuilder;\n+import com.github.ambry.cloud.CloudDestinationFactory;\n+import com.github.ambry.cloud.CloudStorageException;\n+import com.github.ambry.config.CloudConfig;\n+import com.github.ambry.config.VerifiableProperties;\n+import com.github.ambry.utils.TestUtils;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.FeedOptions;\n+import com.microsoft.azure.cosmosdb.SqlQuerySpec;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.UUID;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+\n+@Ignore\n+@RunWith(MockitoJUnitRunner.class)\n+public class AzureContainerCompactorIntegrationTest {\n+\n+  private final Random random = new Random();\n+  private final String PROPS_FILE_NAME = \"azure-test.properties\";\n+  private AzureCloudConfig azureConfig;\n+  private VerifiableProperties verifiableProperties;\n+  private AzureCloudDestination cloudDestination;\n+\n+  @Before\n+  public void setup() throws ReflectiveOperationException {\n+    // TODO Create the required cosmos table as well as the required azure blob.\n+    Properties testProperties = new Properties();\n+    try (InputStream input = this.getClass().getClassLoader().getResourceAsStream(PROPS_FILE_NAME)) {\n+      if (input == null) {\n+        throw new IllegalStateException(\"Could not find resource: \" + PROPS_FILE_NAME);\n+      }\n+      testProperties.load(input);\n+    } catch (IOException ex) {\n+      throw new IllegalStateException(\"Could not load properties from resource: \" + PROPS_FILE_NAME);\n+    }\n+    testProperties.setProperty(\"clustermap.cluster.name\", \"Integration-Test\");\n+    testProperties.setProperty(\"clustermap.datacenter.name\", \"uswest\");\n+    testProperties.setProperty(\"clustermap.host.name\", \"localhost\");\n+    testProperties.setProperty(\"kms.default.container.key\",\n+        \"B374A26A71490437AA024E4FADD5B497FDFF1A8EA6FF12F6FB65AF2720B59CCF\");\n+\n+    testProperties.setProperty(CloudConfig.CLOUD_DELETED_BLOB_RETENTION_DAYS, String.valueOf(1));\n+    testProperties.setProperty(AzureCloudConfig.AZURE_PURGE_BATCH_SIZE, \"10\");\n+    verifiableProperties = new VerifiableProperties(testProperties);\n+    CloudConfig cloudConfig = new CloudConfig(verifiableProperties);\n+    this.azureConfig = new AzureCloudConfig(verifiableProperties);\n+    MetricRegistry registry = new MetricRegistry();\n+    CloudDestinationFactory cloudDestinationFactory =\n+        Utils.getObj(cloudConfig.cloudDestinationFactoryClass, verifiableProperties, registry);\n+    cloudDestination = (AzureCloudDestination) cloudDestinationFactory.getCloudDestination();\n+  }\n+\n+  @After\n+  public void destroy() throws IOException {\n+    // TODO destroy the abs blob and cosmos db\n+    if (cloudDestination != null) {\n+      cloudDestination.close();\n+    }\n+  }\n+\n+  @Test\n+  public void testUpdateDeletedContainer() throws CloudStorageException {\n+    Set<Container> containers = generateContainers(5);\n+    cloudDestination.updateDeletedContainers(containers, null);\n+    verifyCosmosData(containers);\n+    verifyCheckpoint(containers);\n+  }\n+\n+  private void verifyCosmosData(Set<Container> containers) {\n+    String query = \"SELECT VALUE COUNT(1) FROM C\";\n+    SqlQuerySpec querySpec = new SqlQuerySpec(query);\n+    Timer timer = new Timer();\n+    cloudDestination.getCosmosDataAccessor()\n+        .executeCosmosQuery(azureConfig.cosmosDeletedContainerCollectionLink, null, querySpec, new FeedOptions(),\n+            timer);\n+  }\n+\n+  private void verifyCheckpoint(Set<Container> containers) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI1ODg2NA==", "bodyText": "fixed.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501258864", "createdAt": "2020-10-07T19:30:26Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/azure/AzureContainerCompactorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.codahale.metrics.MetricRegistry;\n+import com.codahale.metrics.Timer;\n+import com.github.ambry.account.Container;\n+import com.github.ambry.account.ContainerBuilder;\n+import com.github.ambry.cloud.CloudDestinationFactory;\n+import com.github.ambry.cloud.CloudStorageException;\n+import com.github.ambry.config.CloudConfig;\n+import com.github.ambry.config.VerifiableProperties;\n+import com.github.ambry.utils.TestUtils;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.FeedOptions;\n+import com.microsoft.azure.cosmosdb.SqlQuerySpec;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.UUID;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+\n+@Ignore\n+@RunWith(MockitoJUnitRunner.class)\n+public class AzureContainerCompactorIntegrationTest {\n+\n+  private final Random random = new Random();\n+  private final String PROPS_FILE_NAME = \"azure-test.properties\";\n+  private AzureCloudConfig azureConfig;\n+  private VerifiableProperties verifiableProperties;\n+  private AzureCloudDestination cloudDestination;\n+\n+  @Before\n+  public void setup() throws ReflectiveOperationException {\n+    // TODO Create the required cosmos table as well as the required azure blob.\n+    Properties testProperties = new Properties();\n+    try (InputStream input = this.getClass().getClassLoader().getResourceAsStream(PROPS_FILE_NAME)) {\n+      if (input == null) {\n+        throw new IllegalStateException(\"Could not find resource: \" + PROPS_FILE_NAME);\n+      }\n+      testProperties.load(input);\n+    } catch (IOException ex) {\n+      throw new IllegalStateException(\"Could not load properties from resource: \" + PROPS_FILE_NAME);\n+    }\n+    testProperties.setProperty(\"clustermap.cluster.name\", \"Integration-Test\");\n+    testProperties.setProperty(\"clustermap.datacenter.name\", \"uswest\");\n+    testProperties.setProperty(\"clustermap.host.name\", \"localhost\");\n+    testProperties.setProperty(\"kms.default.container.key\",\n+        \"B374A26A71490437AA024E4FADD5B497FDFF1A8EA6FF12F6FB65AF2720B59CCF\");\n+\n+    testProperties.setProperty(CloudConfig.CLOUD_DELETED_BLOB_RETENTION_DAYS, String.valueOf(1));\n+    testProperties.setProperty(AzureCloudConfig.AZURE_PURGE_BATCH_SIZE, \"10\");\n+    verifiableProperties = new VerifiableProperties(testProperties);\n+    CloudConfig cloudConfig = new CloudConfig(verifiableProperties);\n+    this.azureConfig = new AzureCloudConfig(verifiableProperties);\n+    MetricRegistry registry = new MetricRegistry();\n+    CloudDestinationFactory cloudDestinationFactory =\n+        Utils.getObj(cloudConfig.cloudDestinationFactoryClass, verifiableProperties, registry);\n+    cloudDestination = (AzureCloudDestination) cloudDestinationFactory.getCloudDestination();\n+  }\n+\n+  @After\n+  public void destroy() throws IOException {\n+    // TODO destroy the abs blob and cosmos db\n+    if (cloudDestination != null) {\n+      cloudDestination.close();\n+    }\n+  }\n+\n+  @Test\n+  public void testUpdateDeletedContainer() throws CloudStorageException {\n+    Set<Container> containers = generateContainers(5);\n+    cloudDestination.updateDeletedContainers(containers, null);\n+    verifyCosmosData(containers);\n+    verifyCheckpoint(containers);\n+  }\n+\n+  private void verifyCosmosData(Set<Container> containers) {\n+    String query = \"SELECT VALUE COUNT(1) FROM C\";\n+    SqlQuerySpec querySpec = new SqlQuerySpec(query);\n+    Timer timer = new Timer();\n+    cloudDestination.getCosmosDataAccessor()\n+        .executeCosmosQuery(azureConfig.cosmosDeletedContainerCollectionLink, null, querySpec, new FeedOptions(),\n+            timer);\n+  }\n+\n+  private void verifyCheckpoint(Set<Container> containers) {\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUyNTk0NQ=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzNDAxNzQ2OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/azure/AzureContainerCompactorIntegrationTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxOTowNjoyMlrOHdWDrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQyMDoxNDo0N1rOHfXY7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzMjE0MQ==", "bodyText": "Please move the logic to generate a random container to AccountTestUtils.\nThis code looks copied from AccountContainerTest.initializeRefContainers.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r500532141", "createdAt": "2020-10-06T19:06:22Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/azure/AzureContainerCompactorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.codahale.metrics.MetricRegistry;\n+import com.codahale.metrics.Timer;\n+import com.github.ambry.account.Container;\n+import com.github.ambry.account.ContainerBuilder;\n+import com.github.ambry.cloud.CloudDestinationFactory;\n+import com.github.ambry.cloud.CloudStorageException;\n+import com.github.ambry.config.CloudConfig;\n+import com.github.ambry.config.VerifiableProperties;\n+import com.github.ambry.utils.TestUtils;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.FeedOptions;\n+import com.microsoft.azure.cosmosdb.SqlQuerySpec;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.UUID;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+\n+@Ignore\n+@RunWith(MockitoJUnitRunner.class)\n+public class AzureContainerCompactorIntegrationTest {\n+\n+  private final Random random = new Random();\n+  private final String PROPS_FILE_NAME = \"azure-test.properties\";\n+  private AzureCloudConfig azureConfig;\n+  private VerifiableProperties verifiableProperties;\n+  private AzureCloudDestination cloudDestination;\n+\n+  @Before\n+  public void setup() throws ReflectiveOperationException {\n+    // TODO Create the required cosmos table as well as the required azure blob.\n+    Properties testProperties = new Properties();\n+    try (InputStream input = this.getClass().getClassLoader().getResourceAsStream(PROPS_FILE_NAME)) {\n+      if (input == null) {\n+        throw new IllegalStateException(\"Could not find resource: \" + PROPS_FILE_NAME);\n+      }\n+      testProperties.load(input);\n+    } catch (IOException ex) {\n+      throw new IllegalStateException(\"Could not load properties from resource: \" + PROPS_FILE_NAME);\n+    }\n+    testProperties.setProperty(\"clustermap.cluster.name\", \"Integration-Test\");\n+    testProperties.setProperty(\"clustermap.datacenter.name\", \"uswest\");\n+    testProperties.setProperty(\"clustermap.host.name\", \"localhost\");\n+    testProperties.setProperty(\"kms.default.container.key\",\n+        \"B374A26A71490437AA024E4FADD5B497FDFF1A8EA6FF12F6FB65AF2720B59CCF\");\n+\n+    testProperties.setProperty(CloudConfig.CLOUD_DELETED_BLOB_RETENTION_DAYS, String.valueOf(1));\n+    testProperties.setProperty(AzureCloudConfig.AZURE_PURGE_BATCH_SIZE, \"10\");\n+    verifiableProperties = new VerifiableProperties(testProperties);\n+    CloudConfig cloudConfig = new CloudConfig(verifiableProperties);\n+    this.azureConfig = new AzureCloudConfig(verifiableProperties);\n+    MetricRegistry registry = new MetricRegistry();\n+    CloudDestinationFactory cloudDestinationFactory =\n+        Utils.getObj(cloudConfig.cloudDestinationFactoryClass, verifiableProperties, registry);\n+    cloudDestination = (AzureCloudDestination) cloudDestinationFactory.getCloudDestination();\n+  }\n+\n+  @After\n+  public void destroy() throws IOException {\n+    // TODO destroy the abs blob and cosmos db\n+    if (cloudDestination != null) {\n+      cloudDestination.close();\n+    }\n+  }\n+\n+  @Test\n+  public void testUpdateDeletedContainer() throws CloudStorageException {\n+    Set<Container> containers = generateContainers(5);\n+    cloudDestination.updateDeletedContainers(containers, null);\n+    verifyCosmosData(containers);\n+    verifyCheckpoint(containers);\n+  }\n+\n+  private void verifyCosmosData(Set<Container> containers) {\n+    String query = \"SELECT VALUE COUNT(1) FROM C\";\n+    SqlQuerySpec querySpec = new SqlQuerySpec(query);\n+    Timer timer = new Timer();\n+    cloudDestination.getCosmosDataAccessor()\n+        .executeCosmosQuery(azureConfig.cosmosDeletedContainerCollectionLink, null, querySpec, new FeedOptions(),\n+            timer);\n+  }\n+\n+  private void verifyCheckpoint(Set<Container> containers) {\n+\n+  }\n+\n+  /**\n+   * Generate specified number of {@link Container}s.\n+   * @param numContainers number of {@link Container}s to generate.\n+   * @return {@link Set} of {@link Container}s.\n+   */\n+  private Set<Container> generateContainers(int numContainers) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI1OTEwMw==", "bodyText": "will do.. this class is work in progress right now.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501259103", "createdAt": "2020-10-07T19:30:52Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/azure/AzureContainerCompactorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.codahale.metrics.MetricRegistry;\n+import com.codahale.metrics.Timer;\n+import com.github.ambry.account.Container;\n+import com.github.ambry.account.ContainerBuilder;\n+import com.github.ambry.cloud.CloudDestinationFactory;\n+import com.github.ambry.cloud.CloudStorageException;\n+import com.github.ambry.config.CloudConfig;\n+import com.github.ambry.config.VerifiableProperties;\n+import com.github.ambry.utils.TestUtils;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.FeedOptions;\n+import com.microsoft.azure.cosmosdb.SqlQuerySpec;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.UUID;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+\n+@Ignore\n+@RunWith(MockitoJUnitRunner.class)\n+public class AzureContainerCompactorIntegrationTest {\n+\n+  private final Random random = new Random();\n+  private final String PROPS_FILE_NAME = \"azure-test.properties\";\n+  private AzureCloudConfig azureConfig;\n+  private VerifiableProperties verifiableProperties;\n+  private AzureCloudDestination cloudDestination;\n+\n+  @Before\n+  public void setup() throws ReflectiveOperationException {\n+    // TODO Create the required cosmos table as well as the required azure blob.\n+    Properties testProperties = new Properties();\n+    try (InputStream input = this.getClass().getClassLoader().getResourceAsStream(PROPS_FILE_NAME)) {\n+      if (input == null) {\n+        throw new IllegalStateException(\"Could not find resource: \" + PROPS_FILE_NAME);\n+      }\n+      testProperties.load(input);\n+    } catch (IOException ex) {\n+      throw new IllegalStateException(\"Could not load properties from resource: \" + PROPS_FILE_NAME);\n+    }\n+    testProperties.setProperty(\"clustermap.cluster.name\", \"Integration-Test\");\n+    testProperties.setProperty(\"clustermap.datacenter.name\", \"uswest\");\n+    testProperties.setProperty(\"clustermap.host.name\", \"localhost\");\n+    testProperties.setProperty(\"kms.default.container.key\",\n+        \"B374A26A71490437AA024E4FADD5B497FDFF1A8EA6FF12F6FB65AF2720B59CCF\");\n+\n+    testProperties.setProperty(CloudConfig.CLOUD_DELETED_BLOB_RETENTION_DAYS, String.valueOf(1));\n+    testProperties.setProperty(AzureCloudConfig.AZURE_PURGE_BATCH_SIZE, \"10\");\n+    verifiableProperties = new VerifiableProperties(testProperties);\n+    CloudConfig cloudConfig = new CloudConfig(verifiableProperties);\n+    this.azureConfig = new AzureCloudConfig(verifiableProperties);\n+    MetricRegistry registry = new MetricRegistry();\n+    CloudDestinationFactory cloudDestinationFactory =\n+        Utils.getObj(cloudConfig.cloudDestinationFactoryClass, verifiableProperties, registry);\n+    cloudDestination = (AzureCloudDestination) cloudDestinationFactory.getCloudDestination();\n+  }\n+\n+  @After\n+  public void destroy() throws IOException {\n+    // TODO destroy the abs blob and cosmos db\n+    if (cloudDestination != null) {\n+      cloudDestination.close();\n+    }\n+  }\n+\n+  @Test\n+  public void testUpdateDeletedContainer() throws CloudStorageException {\n+    Set<Container> containers = generateContainers(5);\n+    cloudDestination.updateDeletedContainers(containers, null);\n+    verifyCosmosData(containers);\n+    verifyCheckpoint(containers);\n+  }\n+\n+  private void verifyCosmosData(Set<Container> containers) {\n+    String query = \"SELECT VALUE COUNT(1) FROM C\";\n+    SqlQuerySpec querySpec = new SqlQuerySpec(query);\n+    Timer timer = new Timer();\n+    cloudDestination.getCosmosDataAccessor()\n+        .executeCosmosQuery(azureConfig.cosmosDeletedContainerCollectionLink, null, querySpec, new FeedOptions(),\n+            timer);\n+  }\n+\n+  private void verifyCheckpoint(Set<Container> containers) {\n+\n+  }\n+\n+  /**\n+   * Generate specified number of {@link Container}s.\n+   * @param numContainers number of {@link Container}s to generate.\n+   * @return {@link Set} of {@link Container}s.\n+   */\n+  private Set<Container> generateContainers(int numContainers) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzMjE0MQ=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY1MTExNg==", "bodyText": "this is fixed now.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r502651116", "createdAt": "2020-10-09T20:14:47Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/test/java/com/github/ambry/cloud/azure/AzureContainerCompactorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.codahale.metrics.MetricRegistry;\n+import com.codahale.metrics.Timer;\n+import com.github.ambry.account.Container;\n+import com.github.ambry.account.ContainerBuilder;\n+import com.github.ambry.cloud.CloudDestinationFactory;\n+import com.github.ambry.cloud.CloudStorageException;\n+import com.github.ambry.config.CloudConfig;\n+import com.github.ambry.config.VerifiableProperties;\n+import com.github.ambry.utils.TestUtils;\n+import com.github.ambry.utils.Utils;\n+import com.microsoft.azure.cosmosdb.FeedOptions;\n+import com.microsoft.azure.cosmosdb.SqlQuerySpec;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.UUID;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+\n+@Ignore\n+@RunWith(MockitoJUnitRunner.class)\n+public class AzureContainerCompactorIntegrationTest {\n+\n+  private final Random random = new Random();\n+  private final String PROPS_FILE_NAME = \"azure-test.properties\";\n+  private AzureCloudConfig azureConfig;\n+  private VerifiableProperties verifiableProperties;\n+  private AzureCloudDestination cloudDestination;\n+\n+  @Before\n+  public void setup() throws ReflectiveOperationException {\n+    // TODO Create the required cosmos table as well as the required azure blob.\n+    Properties testProperties = new Properties();\n+    try (InputStream input = this.getClass().getClassLoader().getResourceAsStream(PROPS_FILE_NAME)) {\n+      if (input == null) {\n+        throw new IllegalStateException(\"Could not find resource: \" + PROPS_FILE_NAME);\n+      }\n+      testProperties.load(input);\n+    } catch (IOException ex) {\n+      throw new IllegalStateException(\"Could not load properties from resource: \" + PROPS_FILE_NAME);\n+    }\n+    testProperties.setProperty(\"clustermap.cluster.name\", \"Integration-Test\");\n+    testProperties.setProperty(\"clustermap.datacenter.name\", \"uswest\");\n+    testProperties.setProperty(\"clustermap.host.name\", \"localhost\");\n+    testProperties.setProperty(\"kms.default.container.key\",\n+        \"B374A26A71490437AA024E4FADD5B497FDFF1A8EA6FF12F6FB65AF2720B59CCF\");\n+\n+    testProperties.setProperty(CloudConfig.CLOUD_DELETED_BLOB_RETENTION_DAYS, String.valueOf(1));\n+    testProperties.setProperty(AzureCloudConfig.AZURE_PURGE_BATCH_SIZE, \"10\");\n+    verifiableProperties = new VerifiableProperties(testProperties);\n+    CloudConfig cloudConfig = new CloudConfig(verifiableProperties);\n+    this.azureConfig = new AzureCloudConfig(verifiableProperties);\n+    MetricRegistry registry = new MetricRegistry();\n+    CloudDestinationFactory cloudDestinationFactory =\n+        Utils.getObj(cloudConfig.cloudDestinationFactoryClass, verifiableProperties, registry);\n+    cloudDestination = (AzureCloudDestination) cloudDestinationFactory.getCloudDestination();\n+  }\n+\n+  @After\n+  public void destroy() throws IOException {\n+    // TODO destroy the abs blob and cosmos db\n+    if (cloudDestination != null) {\n+      cloudDestination.close();\n+    }\n+  }\n+\n+  @Test\n+  public void testUpdateDeletedContainer() throws CloudStorageException {\n+    Set<Container> containers = generateContainers(5);\n+    cloudDestination.updateDeletedContainers(containers, null);\n+    verifyCosmosData(containers);\n+    verifyCheckpoint(containers);\n+  }\n+\n+  private void verifyCosmosData(Set<Container> containers) {\n+    String query = \"SELECT VALUE COUNT(1) FROM C\";\n+    SqlQuerySpec querySpec = new SqlQuerySpec(query);\n+    Timer timer = new Timer();\n+    cloudDestination.getCosmosDataAccessor()\n+        .executeCosmosQuery(azureConfig.cosmosDeletedContainerCollectionLink, null, querySpec, new FeedOptions(),\n+            timer);\n+  }\n+\n+  private void verifyCheckpoint(Set<Container> containers) {\n+\n+  }\n+\n+  /**\n+   * Generate specified number of {@link Container}s.\n+   * @param numContainers number of {@link Container}s to generate.\n+   * @return {@link Set} of {@link Container}s.\n+   */\n+  private Set<Container> generateContainers(int numContainers) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzMjE0MQ=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzNDAzMTE5OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudContainerDeletionSyncTask.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxOToxMDoxN1rOHdWL8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxODoxMjozMVrOHd_pRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzNDI1OA==", "bodyText": "This task, like the updateDeletedContainers() methods, seems fairly specific to the Azure/Cosmos impl.  Can we move this to the cloud/azure package (make it Azure only), remove the method from CloudDestination interface, and call the method directly on the AzureContainerCompactor?", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r500534258", "createdAt": "2020-10-06T19:10:17Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudContainerDeletionSyncTask.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.account.AccountService;\n+import com.github.ambry.account.Container;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Set;\n+import org.apache.helix.task.Task;\n+import org.apache.helix.task.TaskResult;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Helix task to get the deleted containers information from {@link com.github.ambry.account.AccountService} and update\n+ * it in CosmosDb DeleteContainers table.\n+ */\n+public class CloudContainerDeletionSyncTask implements Task {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIxMjgzMw==", "bodyText": "The intention of this class is not cloud implementation specific. The intention is to keep the cloud and account service in sync with regards to deprecated container information. As such this class just periodically takes the deprecated containers from account service and updates it to cloud, without any consideration about what cloud implementation it is.\nEach cloud implementation can then implement CloudDestination.updateDeletedContainers()  to decide what to do with the newly deprecated containers and how and where to store them.\nLet me know what do you think about this.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501212833", "createdAt": "2020-10-07T18:11:21Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudContainerDeletionSyncTask.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.account.AccountService;\n+import com.github.ambry.account.Container;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Set;\n+import org.apache.helix.task.Task;\n+import org.apache.helix.task.TaskResult;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Helix task to get the deleted containers information from {@link com.github.ambry.account.AccountService} and update\n+ * it in CosmosDb DeleteContainers table.\n+ */\n+public class CloudContainerDeletionSyncTask implements Task {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzNDI1OA=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTIxMzUxMA==", "bodyText": "I see that my comments for the class were misleading. I have fixed that. Have also renamed the class as an attempt to better clarify the purpose.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r501213510", "createdAt": "2020-10-07T18:12:31Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudContainerDeletionSyncTask.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+import com.github.ambry.account.AccountService;\n+import com.github.ambry.account.Container;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Set;\n+import org.apache.helix.task.Task;\n+import org.apache.helix.task.TaskResult;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Helix task to get the deleted containers information from {@link com.github.ambry.account.AccountService} and update\n+ * it in CosmosDb DeleteContainers table.\n+ */\n+public class CloudContainerDeletionSyncTask implements Task {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzNDI1OA=="}, "originalCommit": {"oid": "572c1e21f5d50d9cb98d4f9c2cbf0570e55d435d"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MzU3MDk3OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudDestination.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMToxOToxMlrOHewydQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQxNjozMDo0MVrOHfQ6Aw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAxODY3Nw==", "bodyText": "Minor: Let's use Collection instead of Set for accounts/updateContainers to make it more general.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r502018677", "createdAt": "2020-10-08T21:19:12Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudDestination.java", "diffHunk": "@@ -137,4 +139,13 @@ boolean retrieveTokens(String partitionPath, String tokenFileName, OutputStream\n    * Halt any compactions in progress.\n    */\n   void stopCompaction();\n+\n+  /**\n+   * Deprecate the specified {@link Container}s for specified partitions in cloud.\n+   * @param deprecatedContainers {@link Set} of deprecated {@link Container}s.\n+   * @param partitionIds {@link Collection} of cloud partition ids from where the container needs to be deprecated.\n+   * @throws {@link CloudStorageException} if updating fails.\n+   */\n+  void deprecateContainers(Set<Container> deprecatedContainers, Collection<String> partitionIds)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b1a37ad3ef62e03c512d9ce8def77b56d12b8b"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAyMTM3Mw==", "bodyText": "Also, I wonder in which case the partitionIds don't equal to allPartitionIds? I am thinking if we can remove the partitionIds because container-to-partitions mapping is not clear to me.\n(Maybe it's not easy but I am trying to say the partitions might be passed into cloudDestination class, not in this method)", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r502021373", "createdAt": "2020-10-08T21:25:05Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudDestination.java", "diffHunk": "@@ -137,4 +139,13 @@ boolean retrieveTokens(String partitionPath, String tokenFileName, OutputStream\n    * Halt any compactions in progress.\n    */\n   void stopCompaction();\n+\n+  /**\n+   * Deprecate the specified {@link Container}s for specified partitions in cloud.\n+   * @param deprecatedContainers {@link Set} of deprecated {@link Container}s.\n+   * @param partitionIds {@link Collection} of cloud partition ids from where the container needs to be deprecated.\n+   * @throws {@link CloudStorageException} if updating fails.\n+   */\n+  void deprecateContainers(Set<Container> deprecatedContainers, Collection<String> partitionIds)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAxODY3Nw=="}, "originalCommit": {"oid": "b1b1a37ad3ef62e03c512d9ce8def77b56d12b8b"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU0NDg5OQ==", "bodyText": "I had actually explored both the things that you have mentioned.\nThe reason I didn't go with Collections is that we would want a Set because it would be good to not have duplicates in the list of deprecatedContainers. Getting a non-duplicate list would mean that CloudDestination doesn't have to do any work to remove any duplicates.\nAbout the second comment, you are right that in production there is no scenario where partitionIds is not equal to allPartitionIds. Sure I can move the partitionIds to CloudDestination, the reason I didn't was because I didn't want to bring clustermap related logic to CloudDestination.\nI do not have any particular reservations about both the changes so I will go ahead and make the changes if you want.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r502544899", "createdAt": "2020-10-09T16:30:41Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudDestination.java", "diffHunk": "@@ -137,4 +139,13 @@ boolean retrieveTokens(String partitionPath, String tokenFileName, OutputStream\n    * Halt any compactions in progress.\n    */\n   void stopCompaction();\n+\n+  /**\n+   * Deprecate the specified {@link Container}s for specified partitions in cloud.\n+   * @param deprecatedContainers {@link Set} of deprecated {@link Container}s.\n+   * @param partitionIds {@link Collection} of cloud partition ids from where the container needs to be deprecated.\n+   * @throws {@link CloudStorageException} if updating fails.\n+   */\n+  void deprecateContainers(Set<Container> deprecatedContainers, Collection<String> partitionIds)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAxODY3Nw=="}, "originalCommit": {"oid": "b1b1a37ad3ef62e03c512d9ce8def77b56d12b8b"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MzYxNzMxOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/VcrServer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMTozNDozOFrOHexOXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMTozNDozOFrOHexOXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAyNTgyMQ==", "bodyText": "Can you point me to the correct CloudDestinationFactory class whose constructor takes in clusterMap?  I didn't find one. Is it AzureCloudDestinationFactory?", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r502025821", "createdAt": "2020-10-08T21:34:38Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/VcrServer.java", "diffHunk": "@@ -131,14 +133,21 @@ public void startup() throws InstantiationException {\n       // verify the configs\n       properties.verify();\n \n-      virtualReplicatorCluster =\n-          ((VirtualReplicatorClusterFactory) Utils.getObj(cloudConfig.virtualReplicatorClusterFactoryClass, cloudConfig,\n-              clusterMapConfig, clusterMap)).getVirtualReplicatorCluster();\n-\n       // initialize cloud destination\n       if (cloudDestinationFactory == null) {\n-        cloudDestinationFactory = Utils.getObj(cloudConfig.cloudDestinationFactoryClass, properties, registry);\n+        cloudDestinationFactory = Utils.getObj(cloudConfig.cloudDestinationFactoryClass, properties, registry, clusterMap);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1b1a37ad3ef62e03c512d9ce8def77b56d12b8b"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0NzE5MzYzOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudContainerCompactor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQxNzozMzozNFrOHfS5Hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQyMDoxMDo0NlrOHfXSdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU3NzQzOA==", "bodyText": "Class isn't referenced anywhere.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r502577438", "createdAt": "2020-10-09T17:33:34Z", "author": {"login": "lightningrob"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudContainerCompactor.java", "diffHunk": "@@ -0,0 +1,22 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+\n+/**\n+ * Class that runs scheduled or on-demand compaction of blobs of deprecated containers.\n+ */\n+public class CloudContainerCompactor {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31442edb6e5c26bf3688eb449d9ba3a30bf864db"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjY0OTQ2Mw==", "bodyText": "It was going to be reference in the next PR, but have removed it for now.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r502649463", "createdAt": "2020-10-09T20:10:46Z", "author": {"login": "ankagrawal"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudContainerCompactor.java", "diffHunk": "@@ -0,0 +1,22 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud;\n+\n+\n+/**\n+ * Class that runs scheduled or on-demand compaction of blobs of deprecated containers.\n+ */\n+public class CloudContainerCompactor {\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU3NzQzOA=="}, "originalCommit": {"oid": "31442edb6e5c26bf3688eb449d9ba3a30bf864db"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MzgxMDkwOnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/ContainerDeletionEntry.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMDozNjowMlrOHgMaMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMDozNjowMlrOHgMaMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzUxOTc5NA==", "bodyText": "Where is this method used?  If this will be used in future PR, try to re-use the other ctor to remove duplicate code:\n  public ContainerDeletionEntry(short containerId, short accountId, long deleteTriggerTimestamp) {\n    this(containerId, accountId, deleteTriggerTimestamp, true, Collections.emptyList());\n  }", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r503519794", "createdAt": "2020-10-12T20:36:02Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/ContainerDeletionEntry.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.account.Container;\n+import com.github.ambry.clustermap.ClusterMap;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.json.JSONObject;\n+\n+\n+/**\n+ * Class representing container deletion status in cloud.\n+ */\n+public class ContainerDeletionEntry {\n+\n+  static final String VERSION_KEY = \"version\";\n+  static final String CONTAINER_ID_KEY = \"containerId\";\n+  static final String ACCOUNT_ID_KEY = \"accountId\";\n+  static final String CONTAINER_DELETE_TRIGGER_TIME_KEY = \"deleteTriggerTime\";\n+  static final String IS_DELETED_KEY = \"isDeleted\";\n+  static final String DELETE_PENDING_PARTITIONS_KEY = \"deletePendingPartitions\";\n+\n+  private static short JSON_VERSION_1 = 1;\n+\n+  private final short version;\n+  private final short containerId;\n+  private final short accountId;\n+  private final Set<String> deletePendingPartitions;\n+  private final long deleteTriggerTimestamp;\n+  private boolean isDeleted;\n+\n+  /**\n+   * Constructor for {@link ContainerDeletionEntry}.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   */\n+  public ContainerDeletionEntry(short containerId, short accountId, long deleteTriggerTimestamp) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "556ac6cd5593d8453c73dcf1d430a93b86b74ccd"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MzgxNzc4OnYy", "diffSide": "RIGHT", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/ContainerDeletionEntry.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMDozOTowNVrOHgMePQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMDozOTowNVrOHgMePQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzUyMDgyOQ==", "bodyText": "nit: according to this java doc, I think we can rename isDeleted to allBlobsDeleted to make it more accurate and descriptive.", "url": "https://github.com/linkedin/ambry/pull/1632#discussion_r503520829", "createdAt": "2020-10-12T20:39:05Z", "author": {"login": "jsjtzyy"}, "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/ContainerDeletionEntry.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/**\n+ * Copyright 2020 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.cloud.azure;\n+\n+import com.github.ambry.account.Container;\n+import com.github.ambry.clustermap.ClusterMap;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.json.JSONObject;\n+\n+\n+/**\n+ * Class representing container deletion status in cloud.\n+ */\n+public class ContainerDeletionEntry {\n+\n+  static final String VERSION_KEY = \"version\";\n+  static final String CONTAINER_ID_KEY = \"containerId\";\n+  static final String ACCOUNT_ID_KEY = \"accountId\";\n+  static final String CONTAINER_DELETE_TRIGGER_TIME_KEY = \"deleteTriggerTime\";\n+  static final String IS_DELETED_KEY = \"isDeleted\";\n+  static final String DELETE_PENDING_PARTITIONS_KEY = \"deletePendingPartitions\";\n+\n+  private static short JSON_VERSION_1 = 1;\n+\n+  private final short version;\n+  private final short containerId;\n+  private final short accountId;\n+  private final Set<String> deletePendingPartitions;\n+  private final long deleteTriggerTimestamp;\n+  private boolean isDeleted;\n+\n+  /**\n+   * Constructor for {@link ContainerDeletionEntry}.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   */\n+  public ContainerDeletionEntry(short containerId, short accountId, long deleteTriggerTimestamp) {\n+    this.containerId = containerId;\n+    this.accountId = accountId;\n+    this.deleteTriggerTimestamp = deleteTriggerTimestamp;\n+    this.isDeleted = true;\n+    this.deletePendingPartitions = Collections.emptySet();\n+    this.version = JSON_VERSION_1;\n+  }\n+\n+  /**\n+   * Constructor for {@link ContainerDeletionEntry}.\n+   * @param containerId container id.\n+   * @param accountId account id of the container.\n+   * @param deleteTriggerTimestamp timestamp at which delete was triggered.\n+   * @param isDeleted {@code true} if all container blobs are deleted in cloud. {@code false} otherwise.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "556ac6cd5593d8453c73dcf1d430a93b86b74ccd"}, "originalPosition": 66}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1394, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}