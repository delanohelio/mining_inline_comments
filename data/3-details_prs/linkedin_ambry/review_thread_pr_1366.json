{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY2OTYyMDAz", "number": 1366, "reviewThreads": {"totalCount": 39, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNjoyNjo0NVrODbdtsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxODo0NjoxMVrODcvFgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMTI0OTc5OnYy", "diffSide": "RIGHT", "path": "ambry-api/src/main/java/com.github.ambry/store/StoreErrorCodes.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNjoyNjo0NVrOFi9w2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQwMDoxNjoyMVrOFjb4xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjIwNzgzMw==", "bodyText": "Do we still need ID_Deleted?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372207833", "createdAt": "2020-01-29T06:26:45Z", "author": {"login": "jsjtzyy"}, "path": "ambry-api/src/main/java/com.github.ambry/store/StoreErrorCodes.java", "diffHunk": "@@ -35,5 +35,9 @@\n   Already_Updated,\n   Update_Not_Allowed,\n   File_Not_Found,\n-  Channel_Closed\n+  Channel_Closed,\n+  Life_Version_Conflict,\n+  ID_Not_Deleted,\n+  ID_Undeleted,\n+  ID_Deleted_Permanently", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjcwMTM4Mw==", "bodyText": "we still need it, it will be returned when ambry-frontend requests to delete a already deleted blob.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372701383", "createdAt": "2020-01-30T00:16:21Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-api/src/main/java/com.github.ambry/store/StoreErrorCodes.java", "diffHunk": "@@ -35,5 +35,9 @@\n   Already_Updated,\n   Update_Not_Allowed,\n   File_Not_Found,\n-  Channel_Closed\n+  Channel_Closed,\n+  Life_Version_Conflict,\n+  ID_Not_Deleted,\n+  ID_Undeleted,\n+  ID_Deleted_Permanently", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjIwNzgzMw=="}, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMzk5MDQ0OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMTozMDozMVrOFjYK4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQwMDoyMDowNFrOFjb89g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY0MDQ4Mw==", "bodyText": "Can you explain a little bit more about difference between life version from frontend and that is not?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372640483", "createdAt": "2020-01-29T21:30:31Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "diffHunk": "@@ -55,6 +55,8 @@\n \n   final static byte FLAGS_DEFAULT_VALUE = (byte) 0;\n   final static long UNKNOWN_ORIGINAL_MESSAGE_OFFSET = -1;\n+  // The life version when the operation is trigger by the requests from frontend.\n+  final static short LIFE_VERSION_FROM_FRONTEND = -1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjcwMjQ1NA==", "bodyText": "frontend requests don't carry any lifeVersion information. This just indicates that markAsPermanent, markAsDeleted and markAsUndeleted is called by the requests from ambry frontend. When it's not from frontend, it should be from recovery and replication.\nIn the end, we will remove this variable. In the end, after we start to support lifeVersion, we will have to know lifeVersion for every frontend and replication requests.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372702454", "createdAt": "2020-01-30T00:20:04Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "diffHunk": "@@ -55,6 +55,8 @@\n \n   final static byte FLAGS_DEFAULT_VALUE = (byte) 0;\n   final static long UNKNOWN_ORIGINAL_MESSAGE_OFFSET = -1;\n+  // The life version when the operation is trigger by the requests from frontend.\n+  final static short LIFE_VERSION_FROM_FRONTEND = -1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY0MDQ4Mw=="}, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMzk5NzExOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMTozMjo1N1rOFjYPFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMTozMjo1N1rOFjYPFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY0MTU1Nw==", "bodyText": "Replace isFlagSet(Flags.Undelete_Index) with isUndelete(). Same for DELETE and TTLUpdate.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372641557", "createdAt": "2020-01-29T21:32:57Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "diffHunk": "@@ -386,11 +429,11 @@ ByteBuffer getBytes() {\n   @Override\n   public String toString() {\n     return \"Offset: \" + offset + \", Size: \" + getSize() + \", Deleted: \" + isFlagSet(Flags.Delete_Index)\n-        + \", TTL Updated: \" + isFlagSet(Flags.Ttl_Update_Index) + \", Undelete: \" + isFlagSet(\n-        Flags.Undelete_Index) + \", ExpiresAtMs: \" + getExpiresAtMs() + \", Original Message Offset: \"\n-        + getOriginalMessageOffset() + (formatVersion != PersistentIndex.VERSION_0 ? (\", OperationTimeAtSecs \"\n-        + getOperationTimeInMs() + \", AccountId \" + getAccountId() + \", ContainerId \" + getContainerId())\n-        : \"\") + (formatVersion > PersistentIndex.VERSION_2 ? \", Life Version:\" + lifeVersion : \"\");\n+        + \", TTL Updated: \" + isFlagSet(Flags.Ttl_Update_Index) + \", Undelete: \" + isFlagSet(Flags.Undelete_Index)\n+        + \", ExpiresAtMs: \" + getExpiresAtMs() + \", Original Message Offset: \" + getOriginalMessageOffset() + (", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNDAyOTc1OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMTo0Mjo1NVrOFjYifQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQwMDoyMToyMVrOFjb-aA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY0NjUyNQ==", "bodyText": "If life version is not from frontend, dose it mean that the version comes from replication or compaction?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372646525", "createdAt": "2020-01-29T21:42:55Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "diffHunk": "@@ -279,6 +313,15 @@ short getContainerId() {\n     return containerId;\n   }\n \n+  /**\n+   * True when the life version is not from frontend requests.\n+   * @param lifeVersion the given life version.\n+   * @return true when it's not from frontend requests.\n+   */\n+  static boolean hasLifeVersion(short lifeVersion) {\n+    return lifeVersion > LIFE_VERSION_FROM_FRONTEND;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjcwMjgyNA==", "bodyText": "and recovery also", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372702824", "createdAt": "2020-01-30T00:21:21Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "diffHunk": "@@ -279,6 +313,15 @@ short getContainerId() {\n     return containerId;\n   }\n \n+  /**\n+   * True when the life version is not from frontend requests.\n+   * @param lifeVersion the given life version.\n+   * @return true when it's not from frontend requests.\n+   */\n+  static boolean hasLifeVersion(short lifeVersion) {\n+    return lifeVersion > LIFE_VERSION_FROM_FRONTEND;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY0NjUyNQ=="}, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNDEyNDQxOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMjoxNjozMVrOFjZc2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQwMDo1MTo0OVrOFjcdAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY2MTQ2Ng==", "bodyText": "Can this be simplified as flags == FLAGS_DEFAULT_VALUE ?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372661466", "createdAt": "2020-01-29T22:16:31Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "diffHunk": "@@ -243,6 +245,38 @@ boolean isFlagSet(Flags flag) {\n     return ((getFlags() & (1 << flag.ordinal())) != 0);\n   }\n \n+  /**\n+   * Helper function for isFlagSet(Flags.Ttl_Update_Index).\n+   * @return true when the Ttl_Update_Index is set.\n+   */\n+  boolean isTTLUpdate() {\n+    return isFlagSet(Flags.Ttl_Update_Index);\n+  }\n+\n+  /**\n+   * Helper function for isFlagSet(Flags.Delete_Index).\n+   * @return true when the Delete_Index is set.\n+   */\n+  boolean isDelete() {\n+    return isFlagSet(Flags.Delete_Index);\n+  }\n+\n+  /**\n+   * Helper function for isFlagSet(Flags.Undelete_Index).\n+   * @return true when the Undelete_Index is set.\n+   */\n+  boolean isUndelete() {\n+    return isFlagSet(Flags.Undelete_Index);\n+  }\n+\n+  /**\n+   * Helper function to decide if this value is a put value or not.\n+   * @return true when it's not a put record.\n+   */\n+  boolean isPut() {\n+    return !(isTTLUpdate() || isDelete() || isUndelete());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjcwMjg3Ng==", "bodyText": "make sense.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372702876", "createdAt": "2020-01-30T00:21:32Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "diffHunk": "@@ -243,6 +245,38 @@ boolean isFlagSet(Flags flag) {\n     return ((getFlags() & (1 << flag.ordinal())) != 0);\n   }\n \n+  /**\n+   * Helper function for isFlagSet(Flags.Ttl_Update_Index).\n+   * @return true when the Ttl_Update_Index is set.\n+   */\n+  boolean isTTLUpdate() {\n+    return isFlagSet(Flags.Ttl_Update_Index);\n+  }\n+\n+  /**\n+   * Helper function for isFlagSet(Flags.Delete_Index).\n+   * @return true when the Delete_Index is set.\n+   */\n+  boolean isDelete() {\n+    return isFlagSet(Flags.Delete_Index);\n+  }\n+\n+  /**\n+   * Helper function for isFlagSet(Flags.Undelete_Index).\n+   * @return true when the Undelete_Index is set.\n+   */\n+  boolean isUndelete() {\n+    return isFlagSet(Flags.Undelete_Index);\n+  }\n+\n+  /**\n+   * Helper function to decide if this value is a put value or not.\n+   * @return true when it's not a put record.\n+   */\n+  boolean isPut() {\n+    return !(isTTLUpdate() || isDelete() || isUndelete());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY2MTQ2Ng=="}, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjcxMDY1Nw==", "bodyText": "+1", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372710657", "createdAt": "2020-01-30T00:51:49Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/IndexValue.java", "diffHunk": "@@ -243,6 +245,38 @@ boolean isFlagSet(Flags flag) {\n     return ((getFlags() & (1 << flag.ordinal())) != 0);\n   }\n \n+  /**\n+   * Helper function for isFlagSet(Flags.Ttl_Update_Index).\n+   * @return true when the Ttl_Update_Index is set.\n+   */\n+  boolean isTTLUpdate() {\n+    return isFlagSet(Flags.Ttl_Update_Index);\n+  }\n+\n+  /**\n+   * Helper function for isFlagSet(Flags.Delete_Index).\n+   * @return true when the Delete_Index is set.\n+   */\n+  boolean isDelete() {\n+    return isFlagSet(Flags.Delete_Index);\n+  }\n+\n+  /**\n+   * Helper function for isFlagSet(Flags.Undelete_Index).\n+   * @return true when the Undelete_Index is set.\n+   */\n+  boolean isUndelete() {\n+    return isFlagSet(Flags.Undelete_Index);\n+  }\n+\n+  /**\n+   * Helper function to decide if this value is a put value or not.\n+   * @return true when it's not a put record.\n+   */\n+  boolean isPut() {\n+    return !(isTTLUpdate() || isDelete() || isUndelete());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY2MTQ2Ng=="}, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNDE5NjU1OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMjo0NDoyNFrOFjaI8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMjo0NDoyNFrOFjaI8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY3Mjc1Mg==", "bodyText": "If this is for testing purpose, I suggest removing this and either use mockito or extends PersisentIndex to override getVersion() method.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372672752", "createdAt": "2020-01-29T22:44:24Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -69,6 +70,30 @@\n   static final short CURRENT_VERSION = VERSION_2;\n   static final String CLEAN_SHUTDOWN_FILENAME = \"cleanshutdown\";\n \n+  // set by the setVersion method from test cases to test IndexValue at next version.\n+  private static Short externalSetVersion = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNDIwMjUxOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMjo0Njo0OVrOFjaMqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMjo0Njo0OVrOFjaMqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY3MzcwNg==", "bodyText": "same here, we probably could remove this method", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372673706", "createdAt": "2020-01-29T22:46:49Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -69,6 +70,30 @@\n   static final short CURRENT_VERSION = VERSION_2;\n   static final String CLEAN_SHUTDOWN_FILENAME = \"cleanshutdown\";\n \n+  // set by the setVersion method from test cases to test IndexValue at next version.\n+  private static Short externalSetVersion = null;\n+\n+  /**\n+   * Return the version. If {@link #setVersion(Short)} was invoked before, the version provided in the {@link #setVersion(Short)}\n+   * would be returned here.\n+   * @return The version of {@link PersistentIndex}.\n+   */\n+  static short getVersion() {\n+    if (externalSetVersion != null) {\n+      return externalSetVersion;\n+    }\n+    return CURRENT_VERSION;\n+  }\n+\n+  /**\n+   * Set the version for {@link PersistentIndex}. To resume the version to default one, pass a null to this method.\n+   * This method should only be used in the test cases.\n+   * @param external the version to set for {@link PersistentIndex}.\n+   */\n+  static void setVersion(Short external) {\n+    externalSetVersion = external;\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNDI1NzUzOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMzoxMjoyM1rOFjavLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQwMDo0MDozNVrOFjcR6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY4MjU0MQ==", "bodyText": "The info here comes from recovery.recover() but I don't see BlobStoreRecovery handles UNDELETE case. Are you planning to add that in future PR?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372682541", "createdAt": "2020-01-29T23:12:23Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -332,14 +357,24 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n             // DELETE must have been present)\n             deleteExpectedKeys.add(info.getStoreKey());\n           }\n+        } else if (info.isUndeleted()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjcwNzgxOQ==", "bodyText": "It does, I have to rebase.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372707819", "createdAt": "2020-01-30T00:40:35Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -332,14 +357,24 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n             // DELETE must have been present)\n             deleteExpectedKeys.add(info.getStoreKey());\n           }\n+        } else if (info.isUndeleted()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY4MjU0MQ=="}, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNDM0NTAwOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQyMzo1Nzo0N1rOFjbkEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQwMDozOTozNFrOFjcQ_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY5NjA4Mg==", "bodyText": "why we throw exception here?  If ttlUpdate doesn't have version, it is invalid?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372696082", "createdAt": "2020-01-29T23:57:47Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -712,37 +897,106 @@ IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, long operationTimeMs)\n    * @return the {@link IndexValue} of the ttl update record\n    * @throws StoreException if there is any problem writing the index record\n    */\n-  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs)\n-      throws StoreException {\n+  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs,\n+      short lifeVersion) throws StoreException {\n     validateFileSpan(fileSpan, true);\n+    boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     IndexValue value = findKey(id);\n     if (value == null && info == null) {\n       throw new StoreException(\"Id \" + id + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n-    } else if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+    }\n+    short retrievedLifeVersion = value == null ? info.getLifeVersion() : value.getLifeVersion();\n+    if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n       throw new StoreException(\"Id \" + id + \" deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n     } else if (value != null && value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n       throw new StoreException(\"TTL of \" + id + \" already updated in index\" + dataDir, StoreErrorCodes.Already_Updated);\n+    } else if (hasLifeVersion && retrievedLifeVersion > lifeVersion) {\n+      throw new StoreException(\"LifeVersion conflict in index. Id \" + id + \" LifeVersion: \" + retrievedLifeVersion\n+          + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n     }\n     long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     IndexValue newValue;\n+\n     if (value == null) {\n       // It is possible that the PUT has been cleaned by compaction\n+      // but the TTL update is going to still be placed?\n+      if (!hasLifeVersion) {\n+        throw new StoreException(\"MessageInfo of ttlUpdate carries invalid lifeVersion\",\n+            StoreErrorCodes.Initialization_Error);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 360}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjcwNzU4MA==", "bodyText": "If it doesn't have lifeVersion, it mean lifeVersion is -1. This will never happen, for 1 when the method is called in recovery or replication, we already the lifeVersion from MessageInfo 2 when the method is called in handling frontend requests, we will have a put record in the persistent index so the value == null will be false.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372707580", "createdAt": "2020-01-30T00:39:34Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -712,37 +897,106 @@ IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, long operationTimeMs)\n    * @return the {@link IndexValue} of the ttl update record\n    * @throws StoreException if there is any problem writing the index record\n    */\n-  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs)\n-      throws StoreException {\n+  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs,\n+      short lifeVersion) throws StoreException {\n     validateFileSpan(fileSpan, true);\n+    boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     IndexValue value = findKey(id);\n     if (value == null && info == null) {\n       throw new StoreException(\"Id \" + id + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n-    } else if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+    }\n+    short retrievedLifeVersion = value == null ? info.getLifeVersion() : value.getLifeVersion();\n+    if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n       throw new StoreException(\"Id \" + id + \" deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n     } else if (value != null && value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n       throw new StoreException(\"TTL of \" + id + \" already updated in index\" + dataDir, StoreErrorCodes.Already_Updated);\n+    } else if (hasLifeVersion && retrievedLifeVersion > lifeVersion) {\n+      throw new StoreException(\"LifeVersion conflict in index. Id \" + id + \" LifeVersion: \" + retrievedLifeVersion\n+          + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n     }\n     long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     IndexValue newValue;\n+\n     if (value == null) {\n       // It is possible that the PUT has been cleaned by compaction\n+      // but the TTL update is going to still be placed?\n+      if (!hasLifeVersion) {\n+        throw new StoreException(\"MessageInfo of ttlUpdate carries invalid lifeVersion\",\n+            StoreErrorCodes.Initialization_Error);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY5NjA4Mg=="}, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 360}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNDM0OTQ1OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQwMDowMDowOFrOFjbmzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQwMDowMDowOFrOFjbmzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjY5Njc4MA==", "bodyText": "update the java doc of this method by adding lifeVersion", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372696780", "createdAt": "2020-01-30T00:00:08Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -712,37 +897,106 @@ IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, long operationTimeMs)\n    * @return the {@link IndexValue} of the ttl update record\n    * @throws StoreException if there is any problem writing the index record\n    */\n-  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs)\n-      throws StoreException {\n+  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs,\n+      short lifeVersion) throws StoreException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 335}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNDQ0MzgxOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQwMDo1NTowMlrOFjcgIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQwMDo1NTo1N1rOFjcg_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjcxMTQ1Ng==", "bodyText": "logger.info add info.getLifeVersion()?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372711456", "createdAt": "2020-01-30T00:55:02Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -315,15 +340,15 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n         Offset infoEndOffset = new Offset(runningOffset.getName(), runningOffset.getOffset() + info.getSize());\n         IndexValue value = findKey(info.getStoreKey());\n         if (info.isDeleted()) {\n-          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n+          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info, info.getOperationTimeMs(),\n+              info.getLifeVersion());\n           logger.info(\"Index : {} updated message with key {} by inserting delete entry of size {} ttl {}\", dataDir,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjcxMTY3OA==", "bodyText": "same for logs below.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r372711678", "createdAt": "2020-01-30T00:55:57Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -315,15 +340,15 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n         Offset infoEndOffset = new Offset(runningOffset.getName(), runningOffset.getOffset() + info.getSize());\n         IndexValue value = findKey(info.getStoreKey());\n         if (info.isDeleted()) {\n-          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n+          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info, info.getOperationTimeMs(),\n+              info.getLifeVersion());\n           logger.info(\"Index : {} updated message with key {} by inserting delete entry of size {} ttl {}\", dataDir,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjcxMTQ1Ng=="}, "originalCommit": {"oid": "09d5ddedaea1c3a9a8c465187e556da1c082f5ac"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDI1OTU0OnYy", "diffSide": "RIGHT", "path": "ambry-api/src/main/java/com.github.ambry/store/StoreErrorCodes.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0MTo1M1rOFkUW7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxOTo1MzozN1rOFkWQUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyNjYwNg==", "bodyText": "Minor: Life_Version_Invalid", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373626606", "createdAt": "2020-01-31T18:41:53Z", "author": {"login": "zzmao"}, "path": "ambry-api/src/main/java/com.github.ambry/store/StoreErrorCodes.java", "diffHunk": "@@ -35,5 +35,9 @@\n   Already_Updated,\n   Update_Not_Allowed,\n   File_Not_Found,\n-  Channel_Closed\n+  Channel_Closed,\n+  Life_Version_Conflict,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzY1NzY4Mg==", "bodyText": "Feel like conflict is better here, invalid seems to indicate the lifeVersion is not a valid number.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373657682", "createdAt": "2020-01-31T19:53:37Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-api/src/main/java/com.github.ambry/store/StoreErrorCodes.java", "diffHunk": "@@ -35,5 +35,9 @@\n   Already_Updated,\n   Update_Not_Allowed,\n   File_Not_Found,\n-  Channel_Closed\n+  Channel_Closed,\n+  Life_Version_Conflict,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyNjYwNg=="}, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDI3MTA1OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0NTo1N1rOFkUeOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxOTo1NDowMVrOFkWRAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyODQ3Mg==", "bodyText": "Does U/D carry expiration time?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373628472", "createdAt": "2020-01-31T18:45:57Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -518,17 +532,18 @@ private boolean needToRollOverIndex(IndexEntry entry) {\n   }\n \n   /**\n-   * Finds a key in the index and returns the blob index value associated with it. If not found,\n-   * returns null\n+   * Finds {@link IndexValue} that represents the latest state of the given {@code key} in the index and return it. If\n+   * not found, returns null.\n    * <br>\n-   * This method only returns PUT or DELETE index entries. It does not return TTL_UPDATE entries but accounts for\n-   * TTL updates by updating the flag and expiry time (if applicable).\n+   * This method returns the final state of the given {@code key}. The final state of a key can be a Put value, a Delete\n+   * value and a Undelete value. Ttl_update isn't considered as final state as it just update the expiration date.\n+   * {@link IndexValue} returned by this method would carry expiration date from Ttl_update if there is one.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYzMjY1NA==", "bodyText": "Is it possible to embed TTL_UPDATE FLAG in D/U all the time?\nIf we support UNDELETE to DELETE after we release this feature, there won't be backward compatible  issue.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373632654", "createdAt": "2020-01-31T18:55:33Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -518,17 +532,18 @@ private boolean needToRollOverIndex(IndexEntry entry) {\n   }\n \n   /**\n-   * Finds a key in the index and returns the blob index value associated with it. If not found,\n-   * returns null\n+   * Finds {@link IndexValue} that represents the latest state of the given {@code key} in the index and return it. If\n+   * not found, returns null.\n    * <br>\n-   * This method only returns PUT or DELETE index entries. It does not return TTL_UPDATE entries but accounts for\n-   * TTL updates by updating the flag and expiry time (if applicable).\n+   * This method returns the final state of the given {@code key}. The final state of a key can be a Put value, a Delete\n+   * value and a Undelete value. Ttl_update isn't considered as final state as it just update the expiration date.\n+   * {@link IndexValue} returned by this method would carry expiration date from Ttl_update if there is one.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyODQ3Mg=="}, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYzNDg0NA==", "bodyText": "Or, can we set it permanent if a blob with TTL is undeleted?  To make TTL_UPDATE easier.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373634844", "createdAt": "2020-01-31T19:00:32Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -518,17 +532,18 @@ private boolean needToRollOverIndex(IndexEntry entry) {\n   }\n \n   /**\n-   * Finds a key in the index and returns the blob index value associated with it. If not found,\n-   * returns null\n+   * Finds {@link IndexValue} that represents the latest state of the given {@code key} in the index and return it. If\n+   * not found, returns null.\n    * <br>\n-   * This method only returns PUT or DELETE index entries. It does not return TTL_UPDATE entries but accounts for\n-   * TTL updates by updating the flag and expiry time (if applicable).\n+   * This method returns the final state of the given {@code key}. The final state of a key can be a Put value, a Delete\n+   * value and a Undelete value. Ttl_update isn't considered as final state as it just update the expiration date.\n+   * {@link IndexValue} returned by this method would carry expiration date from Ttl_update if there is one.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyODQ3Mg=="}, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzY1Nzg1OA==", "bodyText": "talked offline.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373657858", "createdAt": "2020-01-31T19:54:01Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -518,17 +532,18 @@ private boolean needToRollOverIndex(IndexEntry entry) {\n   }\n \n   /**\n-   * Finds a key in the index and returns the blob index value associated with it. If not found,\n-   * returns null\n+   * Finds {@link IndexValue} that represents the latest state of the given {@code key} in the index and return it. If\n+   * not found, returns null.\n    * <br>\n-   * This method only returns PUT or DELETE index entries. It does not return TTL_UPDATE entries but accounts for\n-   * TTL updates by updating the flag and expiry time (if applicable).\n+   * This method returns the final state of the given {@code key}. The final state of a key can be a Put value, a Delete\n+   * value and a Undelete value. Ttl_update isn't considered as final state as it just update the expiration date.\n+   * {@link IndexValue} returned by this method would carry expiration date from Ttl_update if there is one.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyODQ3Mg=="}, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDI3OTQ0OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0ODo1NlrOFkUjZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxOTo1NzowN1rOFkWWcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyOTc5OA==", "bodyText": "The word reversed is confused.\nBecause the default order in our mind is latest to oldest. How about not use reverse in function name, just add some comments about what the order is.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373629798", "createdAt": "2020-01-31T18:48:56Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKeyInReverseOrder(StoreKey key, FileSpan fileSpan,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzY1ODE0Ng==", "bodyText": "I can do that. Given that this is the only order we present the index values.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373658146", "createdAt": "2020-01-31T19:54:38Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKeyInReverseOrder(StoreKey key, FileSpan fileSpan,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyOTc5OA=="}, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzY1OTI0OA==", "bodyText": "yes.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373659248", "createdAt": "2020-01-31T19:57:07Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKeyInReverseOrder(StoreKey key, FileSpan fileSpan,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyOTc5OA=="}, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDMwMDQ4OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo1Njo0NVrOFkUw2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxOTo1NDo0OVrOFkWSZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYzMzI0MA==", "bodyText": "Why getDeletedBlobReadOptions is so complicated?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373633240", "createdAt": "2020-01-31T18:56:45Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -839,6 +1099,33 @@ private BlobReadOptions getDeletedBlobReadOptions(IndexValue value, StoreKey key\n     return readOptions;\n   }\n \n+  /**\n+   * Gets {@link BlobReadOptions} for a undeleted blob.\n+   * @param value the {@link IndexValue} of the delete index entry for the blob.\n+   * @param key the {@link StoreKey} for which {@code value} is the delete {@link IndexValue}\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return the {@link BlobReadOptions} that contains the information for the given {@code id}\n+   * @throws StoreException\n+   */\n+  private BlobReadOptions getUndeletedBlobReadOptions(IndexValue value, StoreKey key,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 502}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzY1ODIxNA==", "bodyText": "talked offline.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373658214", "createdAt": "2020-01-31T19:54:49Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -839,6 +1099,33 @@ private BlobReadOptions getDeletedBlobReadOptions(IndexValue value, StoreKey key\n     return readOptions;\n   }\n \n+  /**\n+   * Gets {@link BlobReadOptions} for a undeleted blob.\n+   * @param value the {@link IndexValue} of the delete index entry for the blob.\n+   * @param key the {@link StoreKey} for which {@code value} is the delete {@link IndexValue}\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return the {@link BlobReadOptions} that contains the information for the given {@code id}\n+   * @throws StoreException\n+   */\n+  private BlobReadOptions getUndeletedBlobReadOptions(IndexValue value, StoreKey key,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYzMzI0MA=="}, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 502}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDMwMjkyOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/test/java/com.github.ambry.store/CuratedLogIndexState.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo1NzozOFrOFkUyYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxOTo1Njo0MFrOFkWVkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYzMzYzNQ==", "bodyText": "Why is size changed?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373633635", "createdAt": "2020-01-31T18:57:38Z", "author": {"login": "zzmao"}, "path": "ambry-store/src/test/java/com.github.ambry.store/CuratedLogIndexState.java", "diffHunk": "@@ -77,7 +77,10 @@\n   // deliberately do not divide the capacities perfectly.\n   static final long PUT_RECORD_SIZE = 53;\n   static final long DELETE_RECORD_SIZE = 29;\n-  static final long TTL_UPDATE_RECORD_SIZE = 37;\n+  static final long TTL_UPDATE_RECORD_SIZE = 29;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzY1OTAyNQ==", "bodyText": "That's minor change, size doesn't really matter, it seems like those are just random prime number greater than 24. I added some new records for undelete and the log segment is over flowed. I will change this back and change the log segment size instead.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373659025", "createdAt": "2020-01-31T19:56:40Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/test/java/com.github.ambry.store/CuratedLogIndexState.java", "diffHunk": "@@ -77,7 +77,10 @@\n   // deliberately do not divide the capacities perfectly.\n   static final long PUT_RECORD_SIZE = 53;\n   static final long DELETE_RECORD_SIZE = 29;\n-  static final long TTL_UPDATE_RECORD_SIZE = 37;\n+  static final long TTL_UPDATE_RECORD_SIZE = 29;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYzMzYzNQ=="}, "originalCommit": {"oid": "d52a85b804a74458ad517fcd7c373895a3ce2b1f"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDc2ODc2OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMjoxNDo1NFrOFkZaFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMjoxNDo1NFrOFkZaFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcwOTMzNQ==", "bodyText": "nit: inserting undelete entry of size ...", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373709335", "createdAt": "2020-01-31T22:14:54Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -315,31 +316,44 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n         Offset infoEndOffset = new Offset(runningOffset.getName(), runningOffset.getOffset() + info.getSize());\n         IndexValue value = findKey(info.getStoreKey());\n         if (info.isDeleted()) {\n-          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n-          logger.info(\"Index : {} updated message with key {} by inserting delete entry of size {} ttl {}\", dataDir,\n-              info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs());\n+          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info, info.getOperationTimeMs(),\n+              info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting delete entry of size {} ttl {} lifeVersion{}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           // removes from the tracking structure if a delete was being expected for the key\n           deleteExpectedKeys.remove(info.getStoreKey());\n         } else if (info.isTtlUpdated()) {\n           markAsPermanent(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n-          logger.info(\"Index : {} updated message with key {} by inserting TTL update entry of size {} ttl {}\", dataDir,\n-              info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs());\n+              info.getOperationTimeMs(), info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting TTL update entry of size {} ttl {} lifeVersion {}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           if (value == null) {\n             // this TTL update was forced even though there was no equivalent PUT record - this means that we MUST see\n             // a DELETE for this key (because the PUT record is gone, compaction must have cleaned it up because a\n             // DELETE must have been present)\n             deleteExpectedKeys.add(info.getStoreKey());\n           }\n+        } else if (info.isUndeleted()) {\n+          markAsUndeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info.getOperationTimeMs(),\n+              info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting undelete update entry of size {} ttl {} lifeVersion {}\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDc5NTM2OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMjoyODoyNVrOFkZqew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQyMzowODo0OFrOFkfZzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcxMzUzMQ==", "bodyText": "minor: PUT or DELETE were expected, right?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373713531", "createdAt": "2020-01-31T22:28:25Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -315,31 +316,44 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n         Offset infoEndOffset = new Offset(runningOffset.getName(), runningOffset.getOffset() + info.getSize());\n         IndexValue value = findKey(info.getStoreKey());\n         if (info.isDeleted()) {\n-          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n-          logger.info(\"Index : {} updated message with key {} by inserting delete entry of size {} ttl {}\", dataDir,\n-              info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs());\n+          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info, info.getOperationTimeMs(),\n+              info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting delete entry of size {} ttl {} lifeVersion{}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           // removes from the tracking structure if a delete was being expected for the key\n           deleteExpectedKeys.remove(info.getStoreKey());\n         } else if (info.isTtlUpdated()) {\n           markAsPermanent(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n-          logger.info(\"Index : {} updated message with key {} by inserting TTL update entry of size {} ttl {}\", dataDir,\n-              info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs());\n+              info.getOperationTimeMs(), info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting TTL update entry of size {} ttl {} lifeVersion {}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           if (value == null) {\n             // this TTL update was forced even though there was no equivalent PUT record - this means that we MUST see\n             // a DELETE for this key (because the PUT record is gone, compaction must have cleaned it up because a\n             // DELETE must have been present)\n             deleteExpectedKeys.add(info.getStoreKey());\n           }\n+        } else if (info.isUndeleted()) {\n+          markAsUndeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info.getOperationTimeMs(),\n+              info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting undelete update entry of size {} ttl {} lifeVersion {}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n+          if (value == null) {\n+            // Undelete record indicates that there might be a put and delete record before it.\n+            throw new StoreException(\"Put record were expected but were not encountered for key: \" + info.getStoreKey(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcxNDc4Ng==", "bodyText": "After a second thought, is it possible that Put record has been compacted. Just like value == null for TTL update? My point is, it may not need an exception here.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373714786", "createdAt": "2020-01-31T22:32:49Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -315,31 +316,44 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n         Offset infoEndOffset = new Offset(runningOffset.getName(), runningOffset.getOffset() + info.getSize());\n         IndexValue value = findKey(info.getStoreKey());\n         if (info.isDeleted()) {\n-          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n-          logger.info(\"Index : {} updated message with key {} by inserting delete entry of size {} ttl {}\", dataDir,\n-              info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs());\n+          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info, info.getOperationTimeMs(),\n+              info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting delete entry of size {} ttl {} lifeVersion{}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           // removes from the tracking structure if a delete was being expected for the key\n           deleteExpectedKeys.remove(info.getStoreKey());\n         } else if (info.isTtlUpdated()) {\n           markAsPermanent(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n-          logger.info(\"Index : {} updated message with key {} by inserting TTL update entry of size {} ttl {}\", dataDir,\n-              info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs());\n+              info.getOperationTimeMs(), info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting TTL update entry of size {} ttl {} lifeVersion {}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           if (value == null) {\n             // this TTL update was forced even though there was no equivalent PUT record - this means that we MUST see\n             // a DELETE for this key (because the PUT record is gone, compaction must have cleaned it up because a\n             // DELETE must have been present)\n             deleteExpectedKeys.add(info.getStoreKey());\n           }\n+        } else if (info.isUndeleted()) {\n+          markAsUndeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info.getOperationTimeMs(),\n+              info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting undelete update entry of size {} ttl {} lifeVersion {}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n+          if (value == null) {\n+            // Undelete record indicates that there might be a put and delete record before it.\n+            throw new StoreException(\"Put record were expected but were not encountered for key: \" + info.getStoreKey(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcxMzUzMQ=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzgwNzU2NA==", "bodyText": "it's not possible, if a put is already compacted, then an undelete operation would be rejected since undelete doesn't do anything.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373807564", "createdAt": "2020-02-01T23:08:48Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -315,31 +316,44 @@ private void recover(MessageStoreRecovery recovery) throws StoreException, IOExc\n         Offset infoEndOffset = new Offset(runningOffset.getName(), runningOffset.getOffset() + info.getSize());\n         IndexValue value = findKey(info.getStoreKey());\n         if (info.isDeleted()) {\n-          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n-          logger.info(\"Index : {} updated message with key {} by inserting delete entry of size {} ttl {}\", dataDir,\n-              info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs());\n+          markAsDeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info, info.getOperationTimeMs(),\n+              info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting delete entry of size {} ttl {} lifeVersion{}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           // removes from the tracking structure if a delete was being expected for the key\n           deleteExpectedKeys.remove(info.getStoreKey());\n         } else if (info.isTtlUpdated()) {\n           markAsPermanent(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info,\n-              info.getOperationTimeMs());\n-          logger.info(\"Index : {} updated message with key {} by inserting TTL update entry of size {} ttl {}\", dataDir,\n-              info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs());\n+              info.getOperationTimeMs(), info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting TTL update entry of size {} ttl {} lifeVersion {}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n           if (value == null) {\n             // this TTL update was forced even though there was no equivalent PUT record - this means that we MUST see\n             // a DELETE for this key (because the PUT record is gone, compaction must have cleaned it up because a\n             // DELETE must have been present)\n             deleteExpectedKeys.add(info.getStoreKey());\n           }\n+        } else if (info.isUndeleted()) {\n+          markAsUndeleted(info.getStoreKey(), new FileSpan(runningOffset, infoEndOffset), info.getOperationTimeMs(),\n+              info.getLifeVersion());\n+          logger.info(\n+              \"Index : {} updated message with key {} by inserting undelete update entry of size {} ttl {} lifeVersion {}\",\n+              dataDir, info.getStoreKey(), info.getSize(), info.getExpirationTimeInMs(), info.getLifeVersion());\n+          if (value == null) {\n+            // Undelete record indicates that there might be a put and delete record before it.\n+            throw new StoreException(\"Put record were expected but were not encountered for key: \" + info.getStoreKey(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcxMzUzMQ=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDg1NDkzOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzowMTozNlrOFkaO3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQyMzoxNTowOVrOFkfavA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyMjg0NQ==", "bodyText": "Can you present a case where we do need to check !value.isDelete() && !value.isUndelete()? I know the piece of code comes from previous version, I just don't quite understand why we need such additional check.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373722845", "createdAt": "2020-01-31T23:01:36Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -590,15 +605,17 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n             }\n             logger.trace(\"Index : {} found value offset {} size {} ttl {}\", dataDir, value.getOffset(), value.getSize(),\n                 value.getExpiresAtMs());\n-            if (types.contains(IndexEntryType.DELETE) && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+            if (types.contains(IndexEntryType.DELETE) && value.isDelete()) {\n+              retCandidate = value;\n+              break;\n+            } else if (types.contains(IndexEntryType.UNDELETE) && value.isUndelete()) {\n               retCandidate = value;\n               break;\n-            } else if (types.contains(IndexEntryType.TTL_UPDATE) && !value.isFlagSet(IndexValue.Flags.Delete_Index)\n-                && value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+            } else if (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete() && !value.isUndelete()\n+                && value.isTTLUpdate()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzgwNzgwNA==", "bodyText": "A delete or undelete entry will carry TTL_UPDATE_INDEX flag if there is a ttl_update record comes before it. So here if the search types only contain TTL_UPDATE_INDEX and we are seeing a delete record, then we have to make sure it doesn't get returned, therefore, we have to check !value.isDelete() && !value.isUndelete().", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373807804", "createdAt": "2020-02-01T23:15:09Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -590,15 +605,17 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n             }\n             logger.trace(\"Index : {} found value offset {} size {} ttl {}\", dataDir, value.getOffset(), value.getSize(),\n                 value.getExpiresAtMs());\n-            if (types.contains(IndexEntryType.DELETE) && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+            if (types.contains(IndexEntryType.DELETE) && value.isDelete()) {\n+              retCandidate = value;\n+              break;\n+            } else if (types.contains(IndexEntryType.UNDELETE) && value.isUndelete()) {\n               retCandidate = value;\n               break;\n-            } else if (types.contains(IndexEntryType.TTL_UPDATE) && !value.isFlagSet(IndexValue.Flags.Delete_Index)\n-                && value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n+            } else if (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete() && !value.isUndelete()\n+                && value.isTTLUpdate()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyMjg0NQ=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDg3MTE2OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzoxMjoyNlrOFkaY-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQyMzoxNjoxNFrOFkfa8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyNTQzNA==", "bodyText": "can be private", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373725434", "createdAt": "2020-01-31T23:12:26Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzgwNzg1Ng==", "bodyText": "I have to expose this method in package scope, since in later PR, blobstore will call this method to do a precheck before adding undelete record to persistent index.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373807856", "createdAt": "2020-02-01T23:16:14Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyNTQzNA=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDg3NTQxOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzoxNTozOFrOFkabtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzoxNTozOFrOFkabtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyNjEzMw==", "bodyText": "minor: change to Searching all index values for to distinguish from findKey method", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373726133", "createdAt": "2020-01-31T23:15:38Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 140}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDg3NzI1OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzoxNjo1M1rOFkaczA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzoxNjo1M1rOFkaczA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyNjQxMg==", "bodyText": "same here", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373726412", "createdAt": "2020-01-31T23:16:53Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 144}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDg3ODc3OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzoxNzo0NVrOFkadng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzoxNzo0NVrOFkadng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyNjYyMg==", "bodyText": "same here", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373726622", "createdAt": "2020-01-31T23:17:45Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDg4NDIzOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzoyMTozM1rOFkag0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQyMzoxODo0MlrOFkfbUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyNzQ0Mw==", "bodyText": "types.contains(IndexEntryType.TTL_UPDATE logic doesn't consider !value.isUndelete() like that in findKey method. Any specific reason?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373727443", "createdAt": "2020-01-31T23:21:33Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzgwNzk1Mg==", "bodyText": "good catch, it's my bad, will update.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373807952", "createdAt": "2020-02-01T23:18:42Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyNzQ0Mw=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 164}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDg4Nzk4OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzoyNDoxOVrOFkajEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQyMzoxOTozNFrOFkfbeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyODAxNw==", "bodyText": "Optional: probably worth adding a constructor for IndexValue that takes in another index value for deep copy purpose. (Pass in many parameters is error-prone)", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373728017", "createdAt": "2020-01-31T23:24:19Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzgwNzk5Mg==", "bodyText": "make sense, will add a new constructor.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373807992", "createdAt": "2020-02-01T23:19:34Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyODAxNw=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 167}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDg4OTQ2OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzoyNTozNFrOFkakAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzoyNTozNFrOFkakAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyODI1Nw==", "bodyText": "typo Returning", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373728257", "createdAt": "2020-01-31T23:25:34Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 180}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDg5NDc0OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzoyODowNFrOFkam5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QwNDo1NTowN1rOFkmYxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyODk5Ng==", "bodyText": "Looks like result may be null. Can you explicitly mention this in java doc?\nAlso, if null means not find index value related to given key. We can make result = new ArrayList<>() at the very beginning and in the end, return result.isEmpty() ? null : result;\nThen we can remove\n if (result == null) {\n            result = new LinkedList<>();\n  }", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373728996", "createdAt": "2020-01-31T23:28:04Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyOTExMA==", "bodyText": "Or probably we can accept an empty result as a return value.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373729110", "createdAt": "2020-01-31T23:28:30Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyODk5Ng=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzgwODM4Ng==", "bodyText": "I think returning a null is find here. I will comment it.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373808386", "createdAt": "2020-02-01T23:29:23Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyODk5Ng=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkyMTk5MQ==", "bodyText": "I see your comment. Also, like I said, we are able to remove\n if (result == null) {\n            result = new LinkedList<>();\n  }\n\nin the for loop.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373921991", "createdAt": "2020-02-03T04:55:07Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzcyODk5Ng=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 182}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDkxNzExOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzo0NjowNFrOFka0iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQyMzozMToxMFrOFkfdTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzczMjQ4OA==", "bodyText": "I get you point here but I would suggest reverse values before hand which makes logic easier to understand and more consistent with your java docs of this method.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373732488", "createdAt": "2020-01-31T23:46:04Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key}.\n+   * <p/>\n+   * Undelete should be permitted only when the first record is a Put and last record is a Delete, and the Put record\n+   * hasn't expired yet.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   * @param lifeVersion lifeVersion for the undelete record, it's only valid when in recovery or replication.\n+   */\n+  void validateSanityForUndelete(StoreKey key, List<IndexValue> values, short lifeVersion) throws StoreException {\n+    if (values == null || values.isEmpty()) {\n+      throw new StoreException(\"Id \" + key + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n+    }\n+    if (!IndexValue.hasLifeVersion(lifeVersion)) {\n+      validateSanityForUndeleteWithoutLifeVersion(key, values);\n+      return;\n+    }\n+    IndexValue firstValue = values.get(values.size() - 1);\n+    IndexValue lastValue = values.get(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzgwODQ2MA==", "bodyText": "I think the order in list is correct, i will change the variable name.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373808460", "createdAt": "2020-02-01T23:31:10Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key}.\n+   * <p/>\n+   * Undelete should be permitted only when the first record is a Put and last record is a Delete, and the Put record\n+   * hasn't expired yet.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   * @param lifeVersion lifeVersion for the undelete record, it's only valid when in recovery or replication.\n+   */\n+  void validateSanityForUndelete(StoreKey key, List<IndexValue> values, short lifeVersion) throws StoreException {\n+    if (values == null || values.isEmpty()) {\n+      throw new StoreException(\"Id \" + key + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n+    }\n+    if (!IndexValue.hasLifeVersion(lifeVersion)) {\n+      validateSanityForUndeleteWithoutLifeVersion(key, values);\n+      return;\n+    }\n+    IndexValue firstValue = values.get(values.size() - 1);\n+    IndexValue lastValue = values.get(0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzczMjQ4OA=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 203}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDkxOTk4OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzo0ODozMVrOFka2WA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQyMzozNTo1MVrOFkfd-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzczMjk1Mg==", "bodyText": "Why we didn't check last value type and expiration time of PUT in this method?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373732952", "createdAt": "2020-01-31T23:48:31Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key}.\n+   * <p/>\n+   * Undelete should be permitted only when the first record is a Put and last record is a Delete, and the Put record\n+   * hasn't expired yet.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   * @param lifeVersion lifeVersion for the undelete record, it's only valid when in recovery or replication.\n+   */\n+  void validateSanityForUndelete(StoreKey key, List<IndexValue> values, short lifeVersion) throws StoreException {\n+    if (values == null || values.isEmpty()) {\n+      throw new StoreException(\"Id \" + key + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n+    }\n+    if (!IndexValue.hasLifeVersion(lifeVersion)) {\n+      validateSanityForUndeleteWithoutLifeVersion(key, values);\n+      return;\n+    }\n+    IndexValue firstValue = values.get(values.size() - 1);\n+    IndexValue lastValue = values.get(0);\n+    if (!firstValue.isPut()) {\n+      throw new StoreException(\"Id \" + key + \" requires first value to be a put in index \" + dataDir,\n+          StoreErrorCodes.ID_Deleted_Permanently);\n+    }\n+    if (lastValue.getLifeVersion() >= lifeVersion) {\n+      throw new StoreException(\n+          \"LifeVersion conflict in index. Id \" + key + \" LifeVersion: \" + lastValue.getLifeVersion()\n+              + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzgwODYzNA==", "bodyText": "you are right, we should check the the expiration date.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373808634", "createdAt": "2020-02-01T23:35:51Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key}.\n+   * <p/>\n+   * Undelete should be permitted only when the first record is a Put and last record is a Delete, and the Put record\n+   * hasn't expired yet.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   * @param lifeVersion lifeVersion for the undelete record, it's only valid when in recovery or replication.\n+   */\n+  void validateSanityForUndelete(StoreKey key, List<IndexValue> values, short lifeVersion) throws StoreException {\n+    if (values == null || values.isEmpty()) {\n+      throw new StoreException(\"Id \" + key + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n+    }\n+    if (!IndexValue.hasLifeVersion(lifeVersion)) {\n+      validateSanityForUndeleteWithoutLifeVersion(key, values);\n+      return;\n+    }\n+    IndexValue firstValue = values.get(values.size() - 1);\n+    IndexValue lastValue = values.get(0);\n+    if (!firstValue.isPut()) {\n+      throw new StoreException(\"Id \" + key + \" requires first value to be a put in index \" + dataDir,\n+          StoreErrorCodes.ID_Deleted_Permanently);\n+    }\n+    if (lastValue.getLifeVersion() >= lifeVersion) {\n+      throw new StoreException(\n+          \"LifeVersion conflict in index. Id \" + key + \" LifeVersion: \" + lastValue.getLifeVersion()\n+              + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzczMjk1Mg=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 212}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDkyODYyOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzo1NTo1MFrOFka7rQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMzo1NTo1MFrOFka7rQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzczNDMxNw==", "bodyText": "if target is already updated or expiration time = -1, in either case we can skip update", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373734317", "createdAt": "2020-01-31T23:55:50Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,165 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order.\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset() + \" to \"\n+                + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching index with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT) && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              IndexValue newValue =\n+                  new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(),\n+                      value.getOperationTimeInMs(), value.getAccountId(), value.getContainerId(),\n+                      value.getLifeVersion());\n+              result.add(newValue);\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returninng values {}\", dataDir, result);\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key}.\n+   * <p/>\n+   * Undelete should be permitted only when the first record is a Put and last record is a Delete, and the Put record\n+   * hasn't expired yet.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   * @param lifeVersion lifeVersion for the undelete record, it's only valid when in recovery or replication.\n+   */\n+  void validateSanityForUndelete(StoreKey key, List<IndexValue> values, short lifeVersion) throws StoreException {\n+    if (values == null || values.isEmpty()) {\n+      throw new StoreException(\"Id \" + key + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n+    }\n+    if (!IndexValue.hasLifeVersion(lifeVersion)) {\n+      validateSanityForUndeleteWithoutLifeVersion(key, values);\n+      return;\n+    }\n+    IndexValue firstValue = values.get(values.size() - 1);\n+    IndexValue lastValue = values.get(0);\n+    if (!firstValue.isPut()) {\n+      throw new StoreException(\"Id \" + key + \" requires first value to be a put in index \" + dataDir,\n+          StoreErrorCodes.ID_Deleted_Permanently);\n+    }\n+    if (lastValue.getLifeVersion() >= lifeVersion) {\n+      throw new StoreException(\n+          \"LifeVersion conflict in index. Id \" + key + \" LifeVersion: \" + lastValue.getLifeVersion()\n+              + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n+    }\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key} when there is\n+   * no lifeVersion provided.\n+   * <p/>\n+   * Undelete should be permitted only when the first record is a Put and last record is a Delete, and the Put record\n+   * hasn't expired yet.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   */\n+  void validateSanityForUndeleteWithoutLifeVersion(StoreKey key, List<IndexValue> values) throws StoreException {\n+    // When it's valid to undelete this key\n+    // P/T + D\n+    // P/T + D + U + D\n+    if (values.size() == 1) {\n+      IndexValue value = values.get(0);\n+      if (value.isDelete() || value.isTTLUpdate()) {\n+        throw new StoreException(\"Id \" + key + \" is compacted in index\" + dataDir,\n+            StoreErrorCodes.ID_Deleted_Permanently);\n+      } else if (value.isPut()) {\n+        throw new StoreException(\"Id \" + key + \" is not deleted yet in index \" + dataDir,\n+            StoreErrorCodes.ID_Not_Deleted);\n+      } else {\n+        throw new StoreException(\"Id \" + key + \" is already undeleted in index\" + dataDir,\n+            StoreErrorCodes.ID_Undeleted);\n+      }\n+    }\n+    // First item has to be put and last item has to be a delete.\n+    // PutRecord can't expire and delete record can't be older than the delete retention time.\n+    IndexValue firstValue = values.get(values.size() - 1);\n+    IndexValue lastValue = values.get(0);\n+    if (lastValue.isUndelete()) {\n+      throw new StoreException(\"Id \" + key + \" is already undeleted in index\" + dataDir, StoreErrorCodes.ID_Undeleted);\n+    }\n+    if (!firstValue.isPut() || !lastValue.isDelete()) {\n+      throw new StoreException(\n+          \"Id \" + key + \" requires first value to be a put and last value to be a delete in index \" + dataDir,\n+          StoreErrorCodes.ID_Not_Deleted);\n+    }\n+    if (lastValue.getOperationTimeInMs() + TimeUnit.DAYS.toMillis(config.storeDeletedMessageRetentionDays)\n+        < time.milliseconds()) {\n+      throw new StoreException(\"Id \" + key + \" already permanently deleted in index \" + dataDir,\n+          StoreErrorCodes.ID_Deleted_Permanently);\n+    }\n+    maybeChangeExpirationDate(firstValue, values);\n+    if (isExpired(firstValue)) {\n+      throw new StoreException(\"Id \" + key + \" already expired in index \" + dataDir, StoreErrorCodes.TTL_Expired);\n+    }\n+  }\n+\n+  /**\n+   * Change the target's expiration date and set the ttl_update_index to be true if there is a ttl index in given list.\n+   * @param target the {@link IndexValue} to change expiration date.\n+   * @param allValues the given list of {@link IndexValue}s.\n+   */\n+  void maybeChangeExpirationDate(IndexValue target, List<IndexValue> allValues) {\n+    if (target.isTTLUpdate()) {\n+      return;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 271}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMTAyMDY1OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQwMTo0MDowNVrOFkbxlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQwMTo0MDowNVrOFkbxlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc0ODExOA==", "bodyText": "nit: carries", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373748118", "createdAt": "2020-02-01T01:40:05Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -658,33 +834,55 @@ IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, long deletionTimeMs) th\n    * @param info this needs to be non-null in the case of recovery. Can be {@code null} otherwise. Used if the PUT\n    *             record could not be found\n    * @param deletionTimeMs deletion time of the blob. In-case of recovery, deletion time is obtained from {@code info}.\n+   * @param lifeVersion lifeVersion of this undelete record.\n    * @return the {@link IndexValue} of the delete record\n    * @throws StoreException\n    */\n-  private IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, MessageInfo info, long deletionTimeMs)\n+  IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, MessageInfo info, long deletionTimeMs, short lifeVersion)\n       throws StoreException {\n+    boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     validateFileSpan(fileSpan, true);\n     IndexValue value = findKey(id);\n     if (value == null && info == null) {\n       throw new StoreException(\"Id \" + id + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n-    } else if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n-      throw new StoreException(\"Id \" + id + \" already deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n+    } else if (value != null) {\n+      if (hasLifeVersion) {\n+        // When this method is invoked in either recovery or replication, delete can follow any index value.\n+        if ((value.isDelete() && value.getLifeVersion() >= lifeVersion) || (value.getLifeVersion() > lifeVersion)) {\n+          throw new StoreException(\"LifeVersion conflict in index. Id \" + id + \" LifeVersion: \" + value.getLifeVersion()\n+              + \" Delete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n+        }\n+      } else {\n+        if (value.isDelete()) {\n+          throw new StoreException(\"Id \" + id + \" already deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n+        }\n+      }\n     }\n     long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     IndexValue newValue;\n     if (value == null) {\n       // It is possible that the PUT has been cleaned by compaction\n+      if (!hasLifeVersion) {\n+        throw new StoreException(\"MessageInfo of delete carry invalid lifeVersion\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 330}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMTAyNjEyOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQwMTo0OTo0OFrOFkb0rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQyMzo0NDo0NVrOFkffHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc0ODkxMA==", "bodyText": "should we clear TTL Update flag as well?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373748910", "createdAt": "2020-02-01T01:49:48Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -658,33 +834,55 @@ IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, long deletionTimeMs) th\n    * @param info this needs to be non-null in the case of recovery. Can be {@code null} otherwise. Used if the PUT\n    *             record could not be found\n    * @param deletionTimeMs deletion time of the blob. In-case of recovery, deletion time is obtained from {@code info}.\n+   * @param lifeVersion lifeVersion of this undelete record.\n    * @return the {@link IndexValue} of the delete record\n    * @throws StoreException\n    */\n-  private IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, MessageInfo info, long deletionTimeMs)\n+  IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, MessageInfo info, long deletionTimeMs, short lifeVersion)\n       throws StoreException {\n+    boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     validateFileSpan(fileSpan, true);\n     IndexValue value = findKey(id);\n     if (value == null && info == null) {\n       throw new StoreException(\"Id \" + id + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n-    } else if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n-      throw new StoreException(\"Id \" + id + \" already deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n+    } else if (value != null) {\n+      if (hasLifeVersion) {\n+        // When this method is invoked in either recovery or replication, delete can follow any index value.\n+        if ((value.isDelete() && value.getLifeVersion() >= lifeVersion) || (value.getLifeVersion() > lifeVersion)) {\n+          throw new StoreException(\"LifeVersion conflict in index. Id \" + id + \" LifeVersion: \" + value.getLifeVersion()\n+              + \" Delete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n+        }\n+      } else {\n+        if (value.isDelete()) {\n+          throw new StoreException(\"Id \" + id + \" already deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n+        }\n+      }\n     }\n     long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     IndexValue newValue;\n     if (value == null) {\n       // It is possible that the PUT has been cleaned by compaction\n+      if (!hasLifeVersion) {\n+        throw new StoreException(\"MessageInfo of delete carry invalid lifeVersion\",\n+            StoreErrorCodes.Initialization_Error);\n+      }\n       newValue =\n-          new IndexValue(size, fileSpan.getStartOffset(), info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n-              info.getAccountId(), info.getContainerId());\n+          new IndexValue(size, fileSpan.getStartOffset(), IndexValue.FLAGS_DEFAULT_VALUE, info.getExpirationTimeInMs(),\n+              info.getOperationTimeMs(), info.getAccountId(), info.getContainerId(), lifeVersion);\n       newValue.clearOriginalMessageOffset();\n     } else {\n+      lifeVersion = hasLifeVersion ? lifeVersion : value.getLifeVersion();\n       newValue =\n           new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(), deletionTimeMs,\n-              value.getAccountId(), value.getContainerId(), (short) 0);\n+              value.getAccountId(), value.getContainerId(), lifeVersion);\n       newValue.setNewOffset(fileSpan.getStartOffset());\n+      // Only set the original message offset when the value is put\n+      if (!value.isPut()) {\n+        newValue.clearOriginalMessageOffset();\n+      }\n       newValue.setNewSize(size);\n     }\n+    newValue.clearFlag(IndexValue.Flags.Undelete_Index);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 352}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzgwODkyNQ==", "bodyText": "we don't, a delete or undelete can carry ttl_update_index flag and it's expiration date, since a delete and a undelete should represent the final state of a key. And expiration date is the final state of this key.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373808925", "createdAt": "2020-02-01T23:44:45Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -658,33 +834,55 @@ IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, long deletionTimeMs) th\n    * @param info this needs to be non-null in the case of recovery. Can be {@code null} otherwise. Used if the PUT\n    *             record could not be found\n    * @param deletionTimeMs deletion time of the blob. In-case of recovery, deletion time is obtained from {@code info}.\n+   * @param lifeVersion lifeVersion of this undelete record.\n    * @return the {@link IndexValue} of the delete record\n    * @throws StoreException\n    */\n-  private IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, MessageInfo info, long deletionTimeMs)\n+  IndexValue markAsDeleted(StoreKey id, FileSpan fileSpan, MessageInfo info, long deletionTimeMs, short lifeVersion)\n       throws StoreException {\n+    boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     validateFileSpan(fileSpan, true);\n     IndexValue value = findKey(id);\n     if (value == null && info == null) {\n       throw new StoreException(\"Id \" + id + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n-    } else if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n-      throw new StoreException(\"Id \" + id + \" already deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n+    } else if (value != null) {\n+      if (hasLifeVersion) {\n+        // When this method is invoked in either recovery or replication, delete can follow any index value.\n+        if ((value.isDelete() && value.getLifeVersion() >= lifeVersion) || (value.getLifeVersion() > lifeVersion)) {\n+          throw new StoreException(\"LifeVersion conflict in index. Id \" + id + \" LifeVersion: \" + value.getLifeVersion()\n+              + \" Delete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n+        }\n+      } else {\n+        if (value.isDelete()) {\n+          throw new StoreException(\"Id \" + id + \" already deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n+        }\n+      }\n     }\n     long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     IndexValue newValue;\n     if (value == null) {\n       // It is possible that the PUT has been cleaned by compaction\n+      if (!hasLifeVersion) {\n+        throw new StoreException(\"MessageInfo of delete carry invalid lifeVersion\",\n+            StoreErrorCodes.Initialization_Error);\n+      }\n       newValue =\n-          new IndexValue(size, fileSpan.getStartOffset(), info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n-              info.getAccountId(), info.getContainerId());\n+          new IndexValue(size, fileSpan.getStartOffset(), IndexValue.FLAGS_DEFAULT_VALUE, info.getExpirationTimeInMs(),\n+              info.getOperationTimeMs(), info.getAccountId(), info.getContainerId(), lifeVersion);\n       newValue.clearOriginalMessageOffset();\n     } else {\n+      lifeVersion = hasLifeVersion ? lifeVersion : value.getLifeVersion();\n       newValue =\n           new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), value.getExpiresAtMs(), deletionTimeMs,\n-              value.getAccountId(), value.getContainerId(), (short) 0);\n+              value.getAccountId(), value.getContainerId(), lifeVersion);\n       newValue.setNewOffset(fileSpan.getStartOffset());\n+      // Only set the original message offset when the value is put\n+      if (!value.isPut()) {\n+        newValue.clearOriginalMessageOffset();\n+      }\n       newValue.setNewSize(size);\n     }\n+    newValue.clearFlag(IndexValue.Flags.Undelete_Index);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc0ODkxMA=="}, "originalCommit": {"oid": "cc0dd0855f4405b60609475b40e1f25f66c3e336"}, "originalPosition": 352}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMjMzOTM3OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QwNToyMjozOFrOFkmorA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxNzo1MToxM1rOFk6QSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkyNjA2MA==", "bodyText": "Looks like firstValue/lastValue ordering is different from that in method validateSanityForUndelete().  I don't see any issue here but could we make them consistent?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373926060", "createdAt": "2020-02-03T05:22:38Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,167 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order. If there is\n+   * no matched {@link IndexValue}, this method would return null;\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching all indexes for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching all indexes for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset()\n+                + \" to \" + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching all indexes with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && !value.isUndelete() && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT)\n+                && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              result.add(new IndexValue(value));\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returning values {}\", dataDir, result);\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key}.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   * @param lifeVersion lifeVersion for the undelete record, it's only valid when in recovery or replication.\n+   */\n+  void validateSanityForUndelete(StoreKey key, List<IndexValue> values, short lifeVersion) throws StoreException {\n+    if (values == null || values.isEmpty()) {\n+      throw new StoreException(\"Id \" + key + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n+    }\n+    if (!IndexValue.hasLifeVersion(lifeVersion)) {\n+      validateSanityForUndeleteWithoutLifeVersion(key, values);\n+      return;\n+    }\n+    // This is from recovery or replication, make sure the last value is a put and the first value's lifeVersion is strictly\n+    // less than the given lifeVersion. We don't care about the first value's type, it can be a put, ttl_update or delete, it\n+    // can even be an undelete.\n+    IndexValue lastValue = values.get(values.size() - 1);\n+    IndexValue firstValue = values.get(0);\n+    if (!lastValue.isPut()) {\n+      throw new StoreException(\"Id \" + key + \" requires first value to be a put in index \" + dataDir,\n+          StoreErrorCodes.ID_Deleted_Permanently);\n+    }\n+    if (firstValue.getLifeVersion() >= lifeVersion) {\n+      throw new StoreException(\n+          \"LifeVersion conflict in index. Id \" + key + \" LifeVersion: \" + firstValue.getLifeVersion()\n+              + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n+    }\n+    maybeChangeExpirationDate(lastValue, values);\n+    if (isExpired(lastValue)) {\n+      throw new StoreException(\"Id \" + key + \" already expired in index \" + dataDir, StoreErrorCodes.TTL_Expired);\n+    }\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key} when there is\n+   * no lifeVersion provided.\n+   * <p/>\n+   * Undelete should be permitted only when the last value is a Put and first record is a Delete, and the Put record\n+   * hasn't expired yet.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   */\n+  void validateSanityForUndeleteWithoutLifeVersion(StoreKey key, List<IndexValue> values) throws StoreException {\n+    // When it's valid to undelete this key\n+    // P/T + D\n+    // P/T + D + U + D\n+    if (values.size() == 1) {\n+      IndexValue value = values.get(0);\n+      if (value.isDelete() || value.isTTLUpdate()) {\n+        throw new StoreException(\"Id \" + key + \" is compacted in index\" + dataDir,\n+            StoreErrorCodes.ID_Deleted_Permanently);\n+      } else if (value.isPut()) {\n+        throw new StoreException(\"Id \" + key + \" is not deleted yet in index \" + dataDir,\n+            StoreErrorCodes.ID_Not_Deleted);\n+      } else {\n+        throw new StoreException(\"Id \" + key + \" is already undeleted in index\" + dataDir,\n+            StoreErrorCodes.ID_Undeleted);\n+      }\n+    }\n+    // First item has to be put and last item has to be a delete.\n+    // PutRecord can't expire and delete record can't be older than the delete retention time.\n+    IndexValue firstValue = values.get(0);\n+    IndexValue lastValue = values.get(values.size() - 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ea2ba25eebd566b2d94119211b5f549a59fc2da3"}, "originalPosition": 242}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI0NzQ5Nw==", "bodyText": "updated.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r374247497", "createdAt": "2020-02-03T17:51:13Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -629,6 +646,167 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n     return retCandidate;\n   }\n \n+  /**\n+   * Finds all the {@link IndexValue}s associated with the given {@code key} that matches any of the provided {@code types}\n+   * if present in the index with the given {@code fileSpan} and return them in reversed chronological order. If there is\n+   * no matched {@link IndexValue}, this method would return null;\n+   * @param key the {@link StoreKey} whose {@link IndexValue} is required.\n+   * @param fileSpan {@link FileSpan} which specifies the range within which search should be made.\n+   * @param types the types of {@link IndexEntryType} to look for.\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return The list of the {@link IndexValue}s for {@code key} conforming to one of the types {@code types}.\n+   * @throws StoreException any error.\n+   */\n+  List<IndexValue> findAllIndexValuesForKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    List<IndexValue> result = null;\n+    final Timer.Context context = metrics.findTime.time();\n+    try {\n+      ConcurrentNavigableMap<Offset, IndexSegment> segmentsMapToSearch;\n+      if (fileSpan == null) {\n+        logger.trace(\"Searching all indexes for \" + key + \" in the entire index\");\n+        segmentsMapToSearch = indexSegments.descendingMap();\n+      } else {\n+        logger.trace(\n+            \"Searching all indexes for \" + key + \" in index with filespan ranging from \" + fileSpan.getStartOffset()\n+                + \" to \" + fileSpan.getEndOffset());\n+        segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n+            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+        metrics.segmentSizeForExists.update(segmentsMapToSearch.size());\n+      }\n+      int segmentsSearched = 0;\n+      for (Map.Entry<Offset, IndexSegment> entry : segmentsMapToSearch.entrySet()) {\n+        segmentsSearched++;\n+        logger.trace(\"Index : {} searching all indexes with start offset {}\", dataDir, entry.getKey());\n+        NavigableSet<IndexValue> values = entry.getValue().find(key);\n+        if (values != null) {\n+          if (result == null) {\n+            result = new LinkedList<>();\n+          }\n+          Iterator<IndexValue> it = values.descendingIterator();\n+          while (it.hasNext()) {\n+            IndexValue value = it.next();\n+            if ((types.contains(IndexEntryType.DELETE) && value.isDelete()) || (types.contains(IndexEntryType.UNDELETE)\n+                && value.isUndelete()) || (types.contains(IndexEntryType.TTL_UPDATE) && !value.isDelete()\n+                && !value.isUndelete() && value.isTTLUpdate()) || (types.contains(IndexEntryType.PUT)\n+                && value.isPut())) {\n+              // Add a copy of the value to the result since we return a modifiable list to the caller.\n+              result.add(new IndexValue(value));\n+            }\n+          }\n+        }\n+      }\n+      metrics.segmentsAccessedPerBlobCount.update(segmentsSearched);\n+    } finally {\n+      context.stop();\n+    }\n+    if (result != null) {\n+      logger.trace(\"Index: {} Returning values {}\", dataDir, result);\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key}.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   * @param lifeVersion lifeVersion for the undelete record, it's only valid when in recovery or replication.\n+   */\n+  void validateSanityForUndelete(StoreKey key, List<IndexValue> values, short lifeVersion) throws StoreException {\n+    if (values == null || values.isEmpty()) {\n+      throw new StoreException(\"Id \" + key + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n+    }\n+    if (!IndexValue.hasLifeVersion(lifeVersion)) {\n+      validateSanityForUndeleteWithoutLifeVersion(key, values);\n+      return;\n+    }\n+    // This is from recovery or replication, make sure the last value is a put and the first value's lifeVersion is strictly\n+    // less than the given lifeVersion. We don't care about the first value's type, it can be a put, ttl_update or delete, it\n+    // can even be an undelete.\n+    IndexValue lastValue = values.get(values.size() - 1);\n+    IndexValue firstValue = values.get(0);\n+    if (!lastValue.isPut()) {\n+      throw new StoreException(\"Id \" + key + \" requires first value to be a put in index \" + dataDir,\n+          StoreErrorCodes.ID_Deleted_Permanently);\n+    }\n+    if (firstValue.getLifeVersion() >= lifeVersion) {\n+      throw new StoreException(\n+          \"LifeVersion conflict in index. Id \" + key + \" LifeVersion: \" + firstValue.getLifeVersion()\n+              + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n+    }\n+    maybeChangeExpirationDate(lastValue, values);\n+    if (isExpired(lastValue)) {\n+      throw new StoreException(\"Id \" + key + \" already expired in index \" + dataDir, StoreErrorCodes.TTL_Expired);\n+    }\n+  }\n+\n+  /**\n+   * Ensure that the previous {@link IndexValue}s is structured correctly for undeleting the {@code key} when there is\n+   * no lifeVersion provided.\n+   * <p/>\n+   * Undelete should be permitted only when the last value is a Put and first record is a Delete, and the Put record\n+   * hasn't expired yet.\n+   * @param key the key to be undeleted.\n+   * @param values the previous {@link IndexValue}s in reversed order.\n+   */\n+  void validateSanityForUndeleteWithoutLifeVersion(StoreKey key, List<IndexValue> values) throws StoreException {\n+    // When it's valid to undelete this key\n+    // P/T + D\n+    // P/T + D + U + D\n+    if (values.size() == 1) {\n+      IndexValue value = values.get(0);\n+      if (value.isDelete() || value.isTTLUpdate()) {\n+        throw new StoreException(\"Id \" + key + \" is compacted in index\" + dataDir,\n+            StoreErrorCodes.ID_Deleted_Permanently);\n+      } else if (value.isPut()) {\n+        throw new StoreException(\"Id \" + key + \" is not deleted yet in index \" + dataDir,\n+            StoreErrorCodes.ID_Not_Deleted);\n+      } else {\n+        throw new StoreException(\"Id \" + key + \" is already undeleted in index\" + dataDir,\n+            StoreErrorCodes.ID_Undeleted);\n+      }\n+    }\n+    // First item has to be put and last item has to be a delete.\n+    // PutRecord can't expire and delete record can't be older than the delete retention time.\n+    IndexValue firstValue = values.get(0);\n+    IndexValue lastValue = values.get(values.size() - 1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkyNjA2MA=="}, "originalCommit": {"oid": "ea2ba25eebd566b2d94119211b5f549a59fc2da3"}, "originalPosition": 242}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMjM5Njk4OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QwNjoxMTowNlrOFknKnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QwNjoxMTowNlrOFknKnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkzNDc1MQ==", "bodyText": "typo: as undelete", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373934751", "createdAt": "2020-02-03T06:11:06Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -709,40 +909,100 @@ IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, long operationTimeMs)\n    * @param operationTimeMs the time of the update operation\n    * @param info this needs to be non-null in the case of recovery. Can be {@code null} otherwise. Used if the PUT\n    *             record could not be found\n+   * @param lifeVersion lifeVersion of this ttlUpdate record.\n    * @return the {@link IndexValue} of the ttl update record\n    * @throws StoreException if there is any problem writing the index record\n    */\n-  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs)\n-      throws StoreException {\n+  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs,\n+      short lifeVersion) throws StoreException {\n     validateFileSpan(fileSpan, true);\n+    boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     IndexValue value = findKey(id);\n     if (value == null && info == null) {\n       throw new StoreException(\"Id \" + id + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n-    } else if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+    }\n+    short retrievedLifeVersion = value == null ? info.getLifeVersion() : value.getLifeVersion();\n+    if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n       throw new StoreException(\"Id \" + id + \" deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n     } else if (value != null && value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n       throw new StoreException(\"TTL of \" + id + \" already updated in index\" + dataDir, StoreErrorCodes.Already_Updated);\n+    } else if (hasLifeVersion && retrievedLifeVersion > lifeVersion) {\n+      throw new StoreException(\"LifeVersion conflict in index. Id \" + id + \" LifeVersion: \" + retrievedLifeVersion\n+          + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n     }\n     long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     IndexValue newValue;\n+\n     if (value == null) {\n       // It is possible that the PUT has been cleaned by compaction\n+      // but the TTL update is going to still be placed?\n+      if (!hasLifeVersion) {\n+        throw new StoreException(\"MessageInfo of ttlUpdate carries invalid lifeVersion\",\n+            StoreErrorCodes.Initialization_Error);\n+      }\n       newValue =\n-          new IndexValue(size, fileSpan.getStartOffset(), info.getExpirationTimeInMs(), info.getOperationTimeMs(),\n-              info.getAccountId(), info.getContainerId());\n+          new IndexValue(size, fileSpan.getStartOffset(), IndexValue.FLAGS_DEFAULT_VALUE, info.getExpirationTimeInMs(),\n+              info.getOperationTimeMs(), info.getAccountId(), info.getContainerId(), lifeVersion);\n       newValue.clearOriginalMessageOffset();\n     } else {\n+      lifeVersion = hasLifeVersion ? lifeVersion : value.getLifeVersion();\n       newValue =\n           new IndexValue(value.getSize(), value.getOffset(), value.getFlags(), Utils.Infinite_Time, operationTimeMs,\n-              value.getAccountId(), value.getContainerId(), (short) 0);\n+              value.getAccountId(), value.getContainerId(), lifeVersion);\n       newValue.setNewOffset(fileSpan.getStartOffset());\n       newValue.setNewSize(size);\n     }\n+    newValue.clearFlag(IndexValue.Flags.Undelete_Index);\n     newValue.setFlag(IndexValue.Flags.Ttl_Update_Index);\n     addToIndex(new IndexEntry(id, newValue, null), fileSpan);\n     return newValue;\n   }\n \n+  /**\n+   * Marks a blob as permanent", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ea2ba25eebd566b2d94119211b5f549a59fc2da3"}, "originalPosition": 423}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMjQwNTEzOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QwNjoxNjozOFrOFknPag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxNzo1NzoyNFrOFk6b7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkzNTk3OA==", "bodyText": "I have same question here, do you have any idea why TTL update is placed even when PUT has been cleaned up?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r373935978", "createdAt": "2020-02-03T06:16:38Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -709,40 +909,100 @@ IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, long operationTimeMs)\n    * @param operationTimeMs the time of the update operation\n    * @param info this needs to be non-null in the case of recovery. Can be {@code null} otherwise. Used if the PUT\n    *             record could not be found\n+   * @param lifeVersion lifeVersion of this ttlUpdate record.\n    * @return the {@link IndexValue} of the ttl update record\n    * @throws StoreException if there is any problem writing the index record\n    */\n-  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs)\n-      throws StoreException {\n+  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs,\n+      short lifeVersion) throws StoreException {\n     validateFileSpan(fileSpan, true);\n+    boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     IndexValue value = findKey(id);\n     if (value == null && info == null) {\n       throw new StoreException(\"Id \" + id + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n-    } else if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+    }\n+    short retrievedLifeVersion = value == null ? info.getLifeVersion() : value.getLifeVersion();\n+    if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n       throw new StoreException(\"Id \" + id + \" deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n     } else if (value != null && value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n       throw new StoreException(\"TTL of \" + id + \" already updated in index\" + dataDir, StoreErrorCodes.Already_Updated);\n+    } else if (hasLifeVersion && retrievedLifeVersion > lifeVersion) {\n+      throw new StoreException(\"LifeVersion conflict in index. Id \" + id + \" LifeVersion: \" + retrievedLifeVersion\n+          + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n     }\n     long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     IndexValue newValue;\n+\n     if (value == null) {\n       // It is possible that the PUT has been cleaned by compaction\n+      // but the TTL update is going to still be placed?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ea2ba25eebd566b2d94119211b5f549a59fc2da3"}, "originalPosition": 396}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI1MDQ3Ng==", "bodyText": "i don't think it's possible. But I will take another look at later PR", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r374250476", "createdAt": "2020-02-03T17:57:24Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -709,40 +909,100 @@ IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, long operationTimeMs)\n    * @param operationTimeMs the time of the update operation\n    * @param info this needs to be non-null in the case of recovery. Can be {@code null} otherwise. Used if the PUT\n    *             record could not be found\n+   * @param lifeVersion lifeVersion of this ttlUpdate record.\n    * @return the {@link IndexValue} of the ttl update record\n    * @throws StoreException if there is any problem writing the index record\n    */\n-  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs)\n-      throws StoreException {\n+  private IndexValue markAsPermanent(StoreKey id, FileSpan fileSpan, MessageInfo info, long operationTimeMs,\n+      short lifeVersion) throws StoreException {\n     validateFileSpan(fileSpan, true);\n+    boolean hasLifeVersion = IndexValue.hasLifeVersion(lifeVersion);\n     IndexValue value = findKey(id);\n     if (value == null && info == null) {\n       throw new StoreException(\"Id \" + id + \" not present in index \" + dataDir, StoreErrorCodes.ID_Not_Found);\n-    } else if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n+    }\n+    short retrievedLifeVersion = value == null ? info.getLifeVersion() : value.getLifeVersion();\n+    if (value != null && value.isFlagSet(IndexValue.Flags.Delete_Index)) {\n       throw new StoreException(\"Id \" + id + \" deleted in index \" + dataDir, StoreErrorCodes.ID_Deleted);\n     } else if (value != null && value.isFlagSet(IndexValue.Flags.Ttl_Update_Index)) {\n       throw new StoreException(\"TTL of \" + id + \" already updated in index\" + dataDir, StoreErrorCodes.Already_Updated);\n+    } else if (hasLifeVersion && retrievedLifeVersion > lifeVersion) {\n+      throw new StoreException(\"LifeVersion conflict in index. Id \" + id + \" LifeVersion: \" + retrievedLifeVersion\n+          + \" Undelete LifeVersion: \" + lifeVersion, StoreErrorCodes.Life_Version_Conflict);\n     }\n     long size = fileSpan.getEndOffset().getOffset() - fileSpan.getStartOffset().getOffset();\n     IndexValue newValue;\n+\n     if (value == null) {\n       // It is possible that the PUT has been cleaned by compaction\n+      // but the TTL update is going to still be placed?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkzNTk3OA=="}, "originalCommit": {"oid": "ea2ba25eebd566b2d94119211b5f549a59fc2da3"}, "originalPosition": 396}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNDUzODQxOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxODozMjo0NFrOFk7d9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxODozMjo0NFrOFk7d9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI2NzM4Mw==", "bodyText": "please update comment for value and key parameters (change delete to undelete)", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r374267383", "createdAt": "2020-02-03T18:32:44Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -839,6 +1098,33 @@ private BlobReadOptions getDeletedBlobReadOptions(IndexValue value, StoreKey key\n     return readOptions;\n   }\n \n+  /**\n+   * Gets {@link BlobReadOptions} for a undeleted blob.\n+   * @param value the {@link IndexValue} of the delete index entry for the blob.\n+   * @param key the {@link StoreKey} for which {@code value} is the delete {@link IndexValue}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f65940cac1fe13b1d839c94d2ff81df94299c6fa"}, "originalPosition": 492}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNDU2MDU1OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxODozOToyOFrOFk7roQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxODozOToyOFrOFk7roQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI3MDg4MQ==", "bodyText": "nit: value.isTTLUpdate()", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r374270881", "createdAt": "2020-02-03T18:39:28Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -839,6 +1098,33 @@ private BlobReadOptions getDeletedBlobReadOptions(IndexValue value, StoreKey key\n     return readOptions;\n   }\n \n+  /**\n+   * Gets {@link BlobReadOptions} for a undeleted blob.\n+   * @param value the {@link IndexValue} of the delete index entry for the blob.\n+   * @param key the {@link StoreKey} for which {@code value} is the delete {@link IndexValue}\n+   * @param indexSegments the map of index segment start {@link Offset} to {@link IndexSegment} instances\n+   * @return the {@link BlobReadOptions} that contains the information for the given {@code id}\n+   * @throws StoreException\n+   */\n+  private BlobReadOptions getUndeletedBlobReadOptions(IndexValue value, StoreKey key,\n+      ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    IndexValue putValue =\n+        findKey(key, new FileSpan(getStartOffset(indexSegments), value.getOffset()), EnumSet.of(IndexEntryType.PUT),\n+            indexSegments);\n+    if (putValue != null) {\n+      // use the expiration time from the original value because it may have been updated\n+      // since we are here dealing with undelete blob, we have to return the right life version\n+      return new BlobReadOptions(log, putValue.getOffset(),\n+          new MessageInfo(key, putValue.getSize(), false, value.isFlagSet(IndexValue.Flags.Ttl_Update_Index), true,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f65940cac1fe13b1d839c94d2ff81df94299c6fa"}, "originalPosition": 506}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNDU3OTcwOnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxODo0NTozMVrOFk73Fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxOTozOTo1OFrOFk9fAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI3MzgxNA==", "bodyText": "why here we filter delete entries again? Is this because messageInfo is updated in updateStateForMessages?", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r374273814", "createdAt": "2020-02-03T18:45:31Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -1555,6 +1839,10 @@ FindInfo findDeletedEntriesSince(FindToken token, long maxTotalSizeOfEntries, lo\n         }\n       }\n     }\n+    // Filter out all the messages that are not \"deleted\", then update state for remaining deleted message.\n+    filterDeleteEntries(messageEntries);\n+    updateStateForMessages(messageEntries);\n+\n     filterDeleteEntries(messageEntries);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f65940cac1fe13b1d839c94d2ff81df94299c6fa"}, "originalPosition": 593}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDMwMDQxOA==", "bodyText": "talked offline", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r374300418", "createdAt": "2020-02-03T19:39:58Z", "author": {"login": "justinlin-linkedin"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -1555,6 +1839,10 @@ FindInfo findDeletedEntriesSince(FindToken token, long maxTotalSizeOfEntries, lo\n         }\n       }\n     }\n+    // Filter out all the messages that are not \"deleted\", then update state for remaining deleted message.\n+    filterDeleteEntries(messageEntries);\n+    updateStateForMessages(messageEntries);\n+\n     filterDeleteEntries(messageEntries);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI3MzgxNA=="}, "originalCommit": {"oid": "f65940cac1fe13b1d839c94d2ff81df94299c6fa"}, "originalPosition": 593}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNDU4MTc4OnYy", "diffSide": "RIGHT", "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxODo0NjoxMVrOFk74Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxODo0NjoxMVrOFk74Wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI3NDEzOA==", "bodyText": "minor: please update java doc for this method.", "url": "https://github.com/linkedin/ambry/pull/1366#discussion_r374274138", "createdAt": "2020-02-03T18:46:11Z", "author": {"login": "jsjtzyy"}, "path": "ambry-store/src/main/java/com.github.ambry.store/PersistentIndex.java", "diffHunk": "@@ -1342,17 +1627,16 @@ private void updateStateForMessages(List<MessageInfo> messageEntries) throws Sto\n     ListIterator<MessageInfo> messageEntriesIterator = messageEntries.listIterator();\n     while (messageEntriesIterator.hasNext()) {\n       MessageInfo messageInfo = messageEntriesIterator.next();\n-      if (!messageInfo.isDeleted()) {\n-        // ok to use most recent ref\n-        IndexValue indexValue =\n-            findKey(messageInfo.getStoreKey(), null, EnumSet.of(IndexEntryType.TTL_UPDATE, IndexEntryType.DELETE));\n-        if (indexValue != null) {\n-          messageInfo = new MessageInfo(messageInfo.getStoreKey(), messageInfo.getSize(),\n-              indexValue.isFlagSet(IndexValue.Flags.Delete_Index),\n-              indexValue.isFlagSet(IndexValue.Flags.Ttl_Update_Index), indexValue.getExpiresAtMs(),\n-              indexValue.getAccountId(), indexValue.getContainerId(), indexValue.getOperationTimeInMs());\n-          messageEntriesIterator.set(messageInfo);\n-        }\n+      // for all the message info, we use most recent ref even for delete. since a deleted entry can be undeleted.\n+      // ok to use most recent ref\n+      IndexValue indexValue = findKey(messageInfo.getStoreKey(), null,\n+          EnumSet.of(IndexEntryType.TTL_UPDATE, IndexEntryType.DELETE, IndexEntryType.UNDELETE));\n+      if (indexValue != null) {\n+        messageInfo = new MessageInfo(messageInfo.getStoreKey(), indexValue.getSize(), indexValue.isDelete(),\n+            indexValue.isTTLUpdate(), indexValue.isUndelete(), indexValue.getExpiresAtMs(), null,\n+            indexValue.getAccountId(), indexValue.getContainerId(), indexValue.getOperationTimeInMs(),\n+            indexValue.getLifeVersion());\n+        messageEntriesIterator.set(messageInfo);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f65940cac1fe13b1d839c94d2ff81df94299c6fa"}, "originalPosition": 581}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1669, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}