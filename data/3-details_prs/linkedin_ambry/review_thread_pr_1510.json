{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE0NDYyNDU3", "number": 1510, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwMjoyMDoxNVrOD9nP1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwMjoyMToyNFrOD9nQlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1OTMyNzU3OnYy", "diffSide": "RIGHT", "path": "ambry-router/src/main/java/com/github/ambry/router/SimpleOperationTracker.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwMjoyMDoxNVrOGXNg7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwMjoyMDoxNVrOGXNg7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk5MTg1NQ==", "bodyText": "Minor: please add blank lines before and after this new section for readability.", "url": "https://github.com/linkedin/ambry/pull/1510#discussion_r426991855", "createdAt": "2020-05-19T02:20:15Z", "author": {"login": "lightningrob"}, "path": "ambry-router/src/main/java/com/github/ambry/router/SimpleOperationTracker.java", "diffHunk": "@@ -159,48 +161,66 @@\n       case PutOperation:\n         eligibleReplicas =\n             getEligibleReplicas(partitionId, datacenterName, EnumSet.of(ReplicaState.STANDBY, ReplicaState.LEADER));\n-        diskSuccessTarget = routerConfig.routerGetEligibleReplicasByStateEnabled ? Math.max(eligibleReplicas.size() - 1,\n-            routerConfig.routerPutSuccessTarget) : routerConfig.routerPutSuccessTarget;\n-        diskParallelism = routerConfig.routerGetEligibleReplicasByStateEnabled ? eligibleReplicas.size()\n+        diskReplicaSuccessTarget =\n+            routerConfig.routerGetEligibleReplicasByStateEnabled ? Math.max(eligibleReplicas.size() - 1,\n+                routerConfig.routerPutSuccessTarget) : routerConfig.routerPutSuccessTarget;\n+        diskReplicaParallelism = routerConfig.routerGetEligibleReplicasByStateEnabled ? eligibleReplicas.size()\n             : routerConfig.routerPutRequestParallelism;\n         crossColoEnabled = false;\n         break;\n       case DeleteOperation:\n-        diskSuccessTarget = routerConfig.routerDeleteSuccessTarget;\n-        diskParallelism = routerConfig.routerDeleteRequestParallelism;\n+        diskReplicaSuccessTarget = routerConfig.routerDeleteSuccessTarget;\n+        diskReplicaParallelism = routerConfig.routerDeleteRequestParallelism;\n         crossColoEnabled = true;\n         eligibleReplicas = getEligibleReplicas(partitionId, null,\n             EnumSet.of(ReplicaState.BOOTSTRAP, ReplicaState.STANDBY, ReplicaState.LEADER));\n         break;\n       case TtlUpdateOperation:\n-        diskSuccessTarget = routerConfig.routerTtlUpdateSuccessTarget;\n-        diskParallelism = routerConfig.routerTtlUpdateRequestParallelism;\n+        diskReplicaSuccessTarget = routerConfig.routerTtlUpdateSuccessTarget;\n+        diskReplicaParallelism = routerConfig.routerTtlUpdateRequestParallelism;\n         crossColoEnabled = true;\n         eligibleReplicas = getEligibleReplicas(partitionId, null,\n             EnumSet.of(ReplicaState.BOOTSTRAP, ReplicaState.STANDBY, ReplicaState.LEADER));\n         break;\n       case UndeleteOperation:\n-        diskParallelism = routerConfig.routerUndeleteRequestParallelism;\n+        diskReplicaParallelism = routerConfig.routerUndeleteRequestParallelism;\n         crossColoEnabled = true;\n         eligibleReplicas = getEligibleReplicas(partitionId, null,\n             EnumSet.of(ReplicaState.BOOTSTRAP, ReplicaState.STANDBY, ReplicaState.LEADER));\n         // Undelete operation need to get global quorum. It will require a different criteria for success.\n         // Here set the success target to the number of eligible replicas.\n-        diskSuccessTarget = eligibleReplicas.size();\n+        diskReplicaSuccessTarget = eligibleReplicas.size();\n         break;\n       default:\n         throw new IllegalArgumentException(\"Unsupported operation: \" + routerOperation);\n     }\n-    if (diskParallelism < 1 || cloudParallelism < 1) {\n+    if (diskReplicaParallelism < 1 || cloudReplicaParallelism < 1) {\n       throw new IllegalArgumentException(\n-          \"Parallelism has to be > 0. diskParallelism=\" + diskParallelism + \", cloudParallelism=\" + cloudParallelism\n-              + \", routerOperation=\" + routerOperation);\n+          \"Parallelism has to be > 0. diskParallelism=\" + diskReplicaParallelism + \", cloudParallelism=\"\n+              + cloudReplicaParallelism + \", routerOperation=\" + routerOperation);\n     }\n \n     // Order the replicas so that local healthy replicas are ordered and returned first,\n     // then the remote healthy ones, and finally the possibly down ones.\n     List<? extends ReplicaId> replicas =\n         routerConfig.routerGetEligibleReplicasByStateEnabled ? eligibleReplicas : partitionId.getReplicaIds();\n+    // In a case where a certain dc is decommissioned and blobs previously uploaded to this dc now have a unrecognizable", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56c2e957a6bb0a64c67d5c6d24a19405899e2cea"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1OTMyOTUwOnYy", "diffSide": "RIGHT", "path": "ambry-router/src/main/java/com/github/ambry/router/SimpleOperationTracker.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwMjoyMToyNFrOGXNiMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwMjoyNzo0MlrOGXNong==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk5MjE3OQ==", "bodyText": "Minor: operation tracker", "url": "https://github.com/linkedin/ambry/pull/1510#discussion_r426992179", "createdAt": "2020-05-19T02:21:24Z", "author": {"login": "lightningrob"}, "path": "ambry-router/src/main/java/com/github/ambry/router/SimpleOperationTracker.java", "diffHunk": "@@ -159,48 +161,66 @@\n       case PutOperation:\n         eligibleReplicas =\n             getEligibleReplicas(partitionId, datacenterName, EnumSet.of(ReplicaState.STANDBY, ReplicaState.LEADER));\n-        diskSuccessTarget = routerConfig.routerGetEligibleReplicasByStateEnabled ? Math.max(eligibleReplicas.size() - 1,\n-            routerConfig.routerPutSuccessTarget) : routerConfig.routerPutSuccessTarget;\n-        diskParallelism = routerConfig.routerGetEligibleReplicasByStateEnabled ? eligibleReplicas.size()\n+        diskReplicaSuccessTarget =\n+            routerConfig.routerGetEligibleReplicasByStateEnabled ? Math.max(eligibleReplicas.size() - 1,\n+                routerConfig.routerPutSuccessTarget) : routerConfig.routerPutSuccessTarget;\n+        diskReplicaParallelism = routerConfig.routerGetEligibleReplicasByStateEnabled ? eligibleReplicas.size()\n             : routerConfig.routerPutRequestParallelism;\n         crossColoEnabled = false;\n         break;\n       case DeleteOperation:\n-        diskSuccessTarget = routerConfig.routerDeleteSuccessTarget;\n-        diskParallelism = routerConfig.routerDeleteRequestParallelism;\n+        diskReplicaSuccessTarget = routerConfig.routerDeleteSuccessTarget;\n+        diskReplicaParallelism = routerConfig.routerDeleteRequestParallelism;\n         crossColoEnabled = true;\n         eligibleReplicas = getEligibleReplicas(partitionId, null,\n             EnumSet.of(ReplicaState.BOOTSTRAP, ReplicaState.STANDBY, ReplicaState.LEADER));\n         break;\n       case TtlUpdateOperation:\n-        diskSuccessTarget = routerConfig.routerTtlUpdateSuccessTarget;\n-        diskParallelism = routerConfig.routerTtlUpdateRequestParallelism;\n+        diskReplicaSuccessTarget = routerConfig.routerTtlUpdateSuccessTarget;\n+        diskReplicaParallelism = routerConfig.routerTtlUpdateRequestParallelism;\n         crossColoEnabled = true;\n         eligibleReplicas = getEligibleReplicas(partitionId, null,\n             EnumSet.of(ReplicaState.BOOTSTRAP, ReplicaState.STANDBY, ReplicaState.LEADER));\n         break;\n       case UndeleteOperation:\n-        diskParallelism = routerConfig.routerUndeleteRequestParallelism;\n+        diskReplicaParallelism = routerConfig.routerUndeleteRequestParallelism;\n         crossColoEnabled = true;\n         eligibleReplicas = getEligibleReplicas(partitionId, null,\n             EnumSet.of(ReplicaState.BOOTSTRAP, ReplicaState.STANDBY, ReplicaState.LEADER));\n         // Undelete operation need to get global quorum. It will require a different criteria for success.\n         // Here set the success target to the number of eligible replicas.\n-        diskSuccessTarget = eligibleReplicas.size();\n+        diskReplicaSuccessTarget = eligibleReplicas.size();\n         break;\n       default:\n         throw new IllegalArgumentException(\"Unsupported operation: \" + routerOperation);\n     }\n-    if (diskParallelism < 1 || cloudParallelism < 1) {\n+    if (diskReplicaParallelism < 1 || cloudReplicaParallelism < 1) {\n       throw new IllegalArgumentException(\n-          \"Parallelism has to be > 0. diskParallelism=\" + diskParallelism + \", cloudParallelism=\" + cloudParallelism\n-              + \", routerOperation=\" + routerOperation);\n+          \"Parallelism has to be > 0. diskParallelism=\" + diskReplicaParallelism + \", cloudParallelism=\"\n+              + cloudReplicaParallelism + \", routerOperation=\" + routerOperation);\n     }\n \n     // Order the replicas so that local healthy replicas are ordered and returned first,\n     // then the remote healthy ones, and finally the possibly down ones.\n     List<? extends ReplicaId> replicas =\n         routerConfig.routerGetEligibleReplicasByStateEnabled ? eligibleReplicas : partitionId.getReplicaIds();\n+    // In a case where a certain dc is decommissioned and blobs previously uploaded to this dc now have a unrecognizable\n+    // dc id. Current clustermap code will treat originating dc as null if dc id is not identifiable. To improve success\n+    // rate of cross-colo requests(GET/DELETE/TTLUpdate), operation tracker should be allowed to try remote dc with most\n+    // replicas first. This is useful in cluster with \"unbalanced\" replica distribution (i.e. 3 replicas in local dc and\n+    // 1 replica per remote dc)\n+    if (originatingDcName == null && routerConfig.routerCrossColoRequestToDcWithMostReplicas) {\n+      Map<String, Long> dcToReplicaCnt = replicas.stream()\n+          .collect(Collectors.groupingBy(e -> e.getDataNodeId().getDatacenterName(), Collectors.counting()));\n+      List<Map.Entry<String, Long>> entryList = new ArrayList<>(dcToReplicaCnt.entrySet());\n+      entryList.sort(Map.Entry.comparingByValue());\n+      // we assign a dc with most replicas to \"originatingDcName\", which only takes effect when populating replica pool\n+      // (replicas in that colo have higher priority than other remote colos). Note that, \"this.originatingDcName\" still\n+      // keeps the actual originating dc name (which is null). This value forces operation track to go through replicas", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56c2e957a6bb0a64c67d5c6d24a19405899e2cea"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk5MzgyMg==", "bodyText": "I recommend using a different variable name than originatingDcName.  Making it different from the class variable is too confusing.", "url": "https://github.com/linkedin/ambry/pull/1510#discussion_r426993822", "createdAt": "2020-05-19T02:27:42Z", "author": {"login": "lightningrob"}, "path": "ambry-router/src/main/java/com/github/ambry/router/SimpleOperationTracker.java", "diffHunk": "@@ -159,48 +161,66 @@\n       case PutOperation:\n         eligibleReplicas =\n             getEligibleReplicas(partitionId, datacenterName, EnumSet.of(ReplicaState.STANDBY, ReplicaState.LEADER));\n-        diskSuccessTarget = routerConfig.routerGetEligibleReplicasByStateEnabled ? Math.max(eligibleReplicas.size() - 1,\n-            routerConfig.routerPutSuccessTarget) : routerConfig.routerPutSuccessTarget;\n-        diskParallelism = routerConfig.routerGetEligibleReplicasByStateEnabled ? eligibleReplicas.size()\n+        diskReplicaSuccessTarget =\n+            routerConfig.routerGetEligibleReplicasByStateEnabled ? Math.max(eligibleReplicas.size() - 1,\n+                routerConfig.routerPutSuccessTarget) : routerConfig.routerPutSuccessTarget;\n+        diskReplicaParallelism = routerConfig.routerGetEligibleReplicasByStateEnabled ? eligibleReplicas.size()\n             : routerConfig.routerPutRequestParallelism;\n         crossColoEnabled = false;\n         break;\n       case DeleteOperation:\n-        diskSuccessTarget = routerConfig.routerDeleteSuccessTarget;\n-        diskParallelism = routerConfig.routerDeleteRequestParallelism;\n+        diskReplicaSuccessTarget = routerConfig.routerDeleteSuccessTarget;\n+        diskReplicaParallelism = routerConfig.routerDeleteRequestParallelism;\n         crossColoEnabled = true;\n         eligibleReplicas = getEligibleReplicas(partitionId, null,\n             EnumSet.of(ReplicaState.BOOTSTRAP, ReplicaState.STANDBY, ReplicaState.LEADER));\n         break;\n       case TtlUpdateOperation:\n-        diskSuccessTarget = routerConfig.routerTtlUpdateSuccessTarget;\n-        diskParallelism = routerConfig.routerTtlUpdateRequestParallelism;\n+        diskReplicaSuccessTarget = routerConfig.routerTtlUpdateSuccessTarget;\n+        diskReplicaParallelism = routerConfig.routerTtlUpdateRequestParallelism;\n         crossColoEnabled = true;\n         eligibleReplicas = getEligibleReplicas(partitionId, null,\n             EnumSet.of(ReplicaState.BOOTSTRAP, ReplicaState.STANDBY, ReplicaState.LEADER));\n         break;\n       case UndeleteOperation:\n-        diskParallelism = routerConfig.routerUndeleteRequestParallelism;\n+        diskReplicaParallelism = routerConfig.routerUndeleteRequestParallelism;\n         crossColoEnabled = true;\n         eligibleReplicas = getEligibleReplicas(partitionId, null,\n             EnumSet.of(ReplicaState.BOOTSTRAP, ReplicaState.STANDBY, ReplicaState.LEADER));\n         // Undelete operation need to get global quorum. It will require a different criteria for success.\n         // Here set the success target to the number of eligible replicas.\n-        diskSuccessTarget = eligibleReplicas.size();\n+        diskReplicaSuccessTarget = eligibleReplicas.size();\n         break;\n       default:\n         throw new IllegalArgumentException(\"Unsupported operation: \" + routerOperation);\n     }\n-    if (diskParallelism < 1 || cloudParallelism < 1) {\n+    if (diskReplicaParallelism < 1 || cloudReplicaParallelism < 1) {\n       throw new IllegalArgumentException(\n-          \"Parallelism has to be > 0. diskParallelism=\" + diskParallelism + \", cloudParallelism=\" + cloudParallelism\n-              + \", routerOperation=\" + routerOperation);\n+          \"Parallelism has to be > 0. diskParallelism=\" + diskReplicaParallelism + \", cloudParallelism=\"\n+              + cloudReplicaParallelism + \", routerOperation=\" + routerOperation);\n     }\n \n     // Order the replicas so that local healthy replicas are ordered and returned first,\n     // then the remote healthy ones, and finally the possibly down ones.\n     List<? extends ReplicaId> replicas =\n         routerConfig.routerGetEligibleReplicasByStateEnabled ? eligibleReplicas : partitionId.getReplicaIds();\n+    // In a case where a certain dc is decommissioned and blobs previously uploaded to this dc now have a unrecognizable\n+    // dc id. Current clustermap code will treat originating dc as null if dc id is not identifiable. To improve success\n+    // rate of cross-colo requests(GET/DELETE/TTLUpdate), operation tracker should be allowed to try remote dc with most\n+    // replicas first. This is useful in cluster with \"unbalanced\" replica distribution (i.e. 3 replicas in local dc and\n+    // 1 replica per remote dc)\n+    if (originatingDcName == null && routerConfig.routerCrossColoRequestToDcWithMostReplicas) {\n+      Map<String, Long> dcToReplicaCnt = replicas.stream()\n+          .collect(Collectors.groupingBy(e -> e.getDataNodeId().getDatacenterName(), Collectors.counting()));\n+      List<Map.Entry<String, Long>> entryList = new ArrayList<>(dcToReplicaCnt.entrySet());\n+      entryList.sort(Map.Entry.comparingByValue());\n+      // we assign a dc with most replicas to \"originatingDcName\", which only takes effect when populating replica pool\n+      // (replicas in that colo have higher priority than other remote colos). Note that, \"this.originatingDcName\" still\n+      // keeps the actual originating dc name (which is null). This value forces operation track to go through replicas", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk5MjE3OQ=="}, "originalCommit": {"oid": "56c2e957a6bb0a64c67d5c6d24a19405899e2cea"}, "originalPosition": 117}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1568, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}