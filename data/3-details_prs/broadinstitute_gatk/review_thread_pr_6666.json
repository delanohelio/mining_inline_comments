{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM2MDQyMjk3", "number": 6666, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxMzo0OTozM1rOEGwk9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNTo1MTo1OFrOEGz99w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NTIyODA2OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxMzo0OTozM1rOGlwM3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxODoyOToxN1rOGmd3sw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI0MDIyMQ==", "bodyText": "is this static block a place holder for something?", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442240221", "createdAt": "2020-06-18T13:49:33Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package org.broadinstitute.hellbender.tools.variantdb;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import com.google.cloud.bigquery.TableResult;\n+import com.google.common.collect.Sets;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.variantcontext.VariantContextBuilder;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.variantdb.RawArrayData.ArrayGenotype;\n+import org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.bigquery.*;\n+import org.broadinstitute.hellbender.utils.localsort.AvroSortingCollectionCodec;\n+import org.broadinstitute.hellbender.utils.localsort.SortingCollection;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.text.DecimalFormat;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import static org.broadinstitute.hellbender.tools.variantdb.ExtractCohortBQ.*;\n+\n+\n+public class ArrayExtractCohortEngine {\n+    private final DecimalFormat df = new DecimalFormat();\n+    private final String DOT = \".\";\n+    \n+    static {\n+    }\n+    ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk4ODQ2Nw==", "bodyText": "nope just junk", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442988467", "createdAt": "2020-06-19T18:29:17Z", "author": {"login": "kcibul"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package org.broadinstitute.hellbender.tools.variantdb;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import com.google.cloud.bigquery.TableResult;\n+import com.google.common.collect.Sets;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.variantcontext.VariantContextBuilder;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.variantdb.RawArrayData.ArrayGenotype;\n+import org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.bigquery.*;\n+import org.broadinstitute.hellbender.utils.localsort.AvroSortingCollectionCodec;\n+import org.broadinstitute.hellbender.utils.localsort.SortingCollection;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.text.DecimalFormat;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import static org.broadinstitute.hellbender.tools.variantdb.ExtractCohortBQ.*;\n+\n+\n+public class ArrayExtractCohortEngine {\n+    private final DecimalFormat df = new DecimalFormat();\n+    private final String DOT = \".\";\n+    \n+    static {\n+    }\n+    ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI0MDIyMQ=="}, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NTY0MTg5OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNToxODo1MFrOGl0P3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxODoyOTozMlrOGmd4Dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMwNjUyNw==", "bodyText": "change to to match class name", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442306527", "createdAt": "2020-06-18T15:18:50Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package org.broadinstitute.hellbender.tools.variantdb;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import com.google.cloud.bigquery.TableResult;\n+import com.google.common.collect.Sets;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.variantcontext.VariantContextBuilder;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.variantdb.RawArrayData.ArrayGenotype;\n+import org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.bigquery.*;\n+import org.broadinstitute.hellbender.utils.localsort.AvroSortingCollectionCodec;\n+import org.broadinstitute.hellbender.utils.localsort.SortingCollection;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.text.DecimalFormat;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import static org.broadinstitute.hellbender.tools.variantdb.ExtractCohortBQ.*;\n+\n+\n+public class ArrayExtractCohortEngine {\n+    private final DecimalFormat df = new DecimalFormat();\n+    private final String DOT = \".\";\n+    \n+    static {\n+    }\n+    \n+    private static final Logger logger = LogManager.getLogger(ExtractCohortEngine.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk4ODU1OA==", "bodyText": "good catch!", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442988558", "createdAt": "2020-06-19T18:29:32Z", "author": {"login": "kcibul"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package org.broadinstitute.hellbender.tools.variantdb;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import com.google.cloud.bigquery.TableResult;\n+import com.google.common.collect.Sets;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.variantcontext.VariantContextBuilder;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.variantdb.RawArrayData.ArrayGenotype;\n+import org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.bigquery.*;\n+import org.broadinstitute.hellbender.utils.localsort.AvroSortingCollectionCodec;\n+import org.broadinstitute.hellbender.utils.localsort.SortingCollection;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.text.DecimalFormat;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import static org.broadinstitute.hellbender.tools.variantdb.ExtractCohortBQ.*;\n+\n+\n+public class ArrayExtractCohortEngine {\n+    private final DecimalFormat df = new DecimalFormat();\n+    private final String DOT = \".\";\n+    \n+    static {\n+    }\n+    \n+    private static final Logger logger = LogManager.getLogger(ExtractCohortEngine.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMwNjUyNw=="}, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NTY0NDcyOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNToxOToyOVrOGl0RqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNToxOToyOVrOGl0RqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMwNjk4NQ==", "bodyText": "i think we can remove this", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442306985", "createdAt": "2020-06-18T15:19:29Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package org.broadinstitute.hellbender.tools.variantdb;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import com.google.cloud.bigquery.TableResult;\n+import com.google.common.collect.Sets;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.variantcontext.VariantContextBuilder;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.variantdb.RawArrayData.ArrayGenotype;\n+import org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.bigquery.*;\n+import org.broadinstitute.hellbender.utils.localsort.AvroSortingCollectionCodec;\n+import org.broadinstitute.hellbender.utils.localsort.SortingCollection;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.text.DecimalFormat;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import static org.broadinstitute.hellbender.tools.variantdb.ExtractCohortBQ.*;\n+\n+\n+public class ArrayExtractCohortEngine {\n+    private final DecimalFormat df = new DecimalFormat();\n+    private final String DOT = \".\";\n+    \n+    static {\n+    }\n+    \n+    private static final Logger logger = LogManager.getLogger(ExtractCohortEngine.class);\n+\n+    private final VariantContextWriter vcfWriter;\n+\n+    private final boolean useCompressedData;\n+    private final boolean printDebugInformation;\n+    private final int localSortMaxRecordsInRam;\n+    private final TableReference cohortTableRef;\n+    private final ReferenceDataSource refSource;\n+\n+    private final ProgressMeter progressMeter;\n+    private final String projectID;\n+\n+    /** List of sample names seen in the variant data from BigQuery. */\n+    private final Map<Integer, String> sampleIdMap;\n+    private final Set<String> sampleNames;\n+\n+    private final Map<Long, ProbeInfo> probeIdMap;\n+    private final ReferenceConfidenceVariantContextMerger variantContextMerger;\n+\n+    private int totalNumberOfVariants = 0;\n+    private int totalNumberOfSites = 0;\n+\n+    /**\n+     * The conf threshold above which variants are not included in the position tables.\n+     * This value is used to construct the genotype information of those missing samples\n+     * when they are merged together into a {@link VariantContext} object\n+     */\n+    public static int MISSING_CONF_THRESHOLD = 60;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NTY2NjEyOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNToyNDoyOFrOGl0fWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxODozMjowNlrOGmd8CA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxMDQ5MQ==", "bodyText": "i wonder if it would be easier to get the ref allele from the probe_info table - since we already use it. and then we might not need the reference as an input?", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442310491", "createdAt": "2020-06-18T15:24:28Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package org.broadinstitute.hellbender.tools.variantdb;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import com.google.cloud.bigquery.TableResult;\n+import com.google.common.collect.Sets;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.variantcontext.VariantContextBuilder;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.variantdb.RawArrayData.ArrayGenotype;\n+import org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.bigquery.*;\n+import org.broadinstitute.hellbender.utils.localsort.AvroSortingCollectionCodec;\n+import org.broadinstitute.hellbender.utils.localsort.SortingCollection;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.text.DecimalFormat;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import static org.broadinstitute.hellbender.tools.variantdb.ExtractCohortBQ.*;\n+\n+\n+public class ArrayExtractCohortEngine {\n+    private final DecimalFormat df = new DecimalFormat();\n+    private final String DOT = \".\";\n+    \n+    static {\n+    }\n+    \n+    private static final Logger logger = LogManager.getLogger(ExtractCohortEngine.class);\n+\n+    private final VariantContextWriter vcfWriter;\n+\n+    private final boolean useCompressedData;\n+    private final boolean printDebugInformation;\n+    private final int localSortMaxRecordsInRam;\n+    private final TableReference cohortTableRef;\n+    private final ReferenceDataSource refSource;\n+\n+    private final ProgressMeter progressMeter;\n+    private final String projectID;\n+\n+    /** List of sample names seen in the variant data from BigQuery. */\n+    private final Map<Integer, String> sampleIdMap;\n+    private final Set<String> sampleNames;\n+\n+    private final Map<Long, ProbeInfo> probeIdMap;\n+    private final ReferenceConfidenceVariantContextMerger variantContextMerger;\n+\n+    private int totalNumberOfVariants = 0;\n+    private int totalNumberOfSites = 0;\n+\n+    /**\n+     * The conf threshold above which variants are not included in the position tables.\n+     * This value is used to construct the genotype information of those missing samples\n+     * when they are merged together into a {@link VariantContext} object\n+     */\n+    public static int MISSING_CONF_THRESHOLD = 60;\n+\n+\n+    public ArrayExtractCohortEngine(final String projectID,\n+                                    final VariantContextWriter vcfWriter,\n+                                    final VCFHeader vcfHeader,\n+                                    final VariantAnnotatorEngine annotationEngine,\n+                                    final ReferenceDataSource refSource,\n+                                    final Map<Integer, String> sampleIdMap,\n+                                    final Map<Long, ProbeInfo> probeIdMap,\n+                                    final String cohortTableName,\n+                                    final int localSortMaxRecordsInRam,\n+                                    final boolean useCompressedData,\n+                                    final boolean printDebugInformation,\n+                                    final ProgressMeter progressMeter) {\n+\n+        this.df.setMaximumFractionDigits(3);\n+        this.df.setGroupingSize(0);\n+                                \n+        this.localSortMaxRecordsInRam = localSortMaxRecordsInRam;\n+\n+        this.projectID = projectID;\n+        this.vcfWriter = vcfWriter;\n+        this.refSource = refSource;\n+        this.sampleIdMap = sampleIdMap;\n+        this.sampleNames = new HashSet<>(sampleIdMap.values());\n+\n+        this.probeIdMap = probeIdMap;\n+\n+        this.cohortTableRef = new TableReference(cohortTableName, useCompressedData?SchemaUtils.RAW_ARRAY_COHORT_FIELDS_COMPRESSED:SchemaUtils.RAW_ARRAY_COHORT_FIELDS_UNCOMPRESSED);\n+\n+        this.useCompressedData = useCompressedData;\n+        this.printDebugInformation = printDebugInformation;\n+        this.progressMeter = progressMeter;\n+\n+        // KCIBUL: what is the right variant context merger for arrays?\n+        this.variantContextMerger = new ReferenceConfidenceVariantContextMerger(annotationEngine, vcfHeader);\n+\n+    }\n+\n+    int getTotalNumberOfVariants() { return totalNumberOfVariants; }\n+    int getTotalNumberOfSites() { return totalNumberOfSites; }\n+\n+    public void traverse() {\n+        if (printDebugInformation) {\n+            logger.debug(\"using storage api with local sort\");\n+        }\n+        final StorageAPIAvroReader storageAPIAvroReader = new StorageAPIAvroReader(cohortTableRef);\n+        createVariantsFromUngroupedTableResult(storageAPIAvroReader);\n+    }\n+\n+\n+    private void createVariantsFromUngroupedTableResult(final GATKAvroReader avroReader) {\n+\n+        // stream out the data and sort locally\n+        final org.apache.avro.Schema schema = avroReader.getSchema();\n+        final Set<String> columnNames = new HashSet<>();\n+        schema.getFields().forEach(field -> columnNames.add(field.name()));\n+\n+        Comparator<GenericRecord> comparator = this.useCompressedData ? COMPRESSED_PROBE_ID_COMPARATOR : UNCOMPRESSED_PROBE_ID_COMPARATOR;\n+        SortingCollection<GenericRecord> sortingCollection =  getAvroProbeIdSortingCollection(schema, localSortMaxRecordsInRam, comparator);\n+        for ( final GenericRecord queryRow : avroReader ) {\n+            sortingCollection.add(queryRow);\n+        }\n+\n+        sortingCollection.printTempFileStats();\n+\n+        // iterate through records and process them\n+        final List<GenericRecord> currentPositionRecords = new ArrayList<>(sampleIdMap.size() * 2);\n+        long currentProbeId = -1;\n+\n+        for ( final GenericRecord sortedRow : sortingCollection ) {\n+            long probeId;\n+            if (useCompressedData) {\n+                final long rawData = (Long) sortedRow.get(SchemaUtils.RAW_ARRAY_DATA_FIELD_NAME);\n+                RawArrayData data = RawArrayData.decode(rawData);\n+                probeId = data.probeId;\n+            } else {\n+                probeId = (Long) sortedRow.get(\"probe_id\");\n+            }\n+\n+            if ( probeId != currentProbeId && currentProbeId != -1 ) {\n+                ++totalNumberOfSites;\n+                processSampleRecordsForLocation(currentProbeId, currentPositionRecords, columnNames);\n+                currentPositionRecords.clear();\n+            }\n+\n+            currentPositionRecords.add(sortedRow);\n+            currentProbeId = probeId;\n+        }\n+\n+        if ( ! currentPositionRecords.isEmpty() ) {\n+            ++totalNumberOfSites;\n+            processSampleRecordsForLocation(currentProbeId, currentPositionRecords, columnNames);\n+        }\n+    }\n+\n+    private void processSampleRecordsForLocation(final long probeId, final Iterable<GenericRecord> sampleRecordsAtPosition, final Set<String> columnNames) {\n+        final List<VariantContext> unmergedCalls = new ArrayList<>();\n+        final Set<String> currentPositionSamplesSeen = new HashSet<>();\n+        boolean currentPositionHasVariant = false;\n+\n+        final ProbeInfo probeInfo = probeIdMap.get(probeId);\n+        if (probeInfo == null) {\n+            throw new RuntimeException(\"Unable to find probeInfo for \" + probeId);\n+        }\n+\n+        final String contig = probeInfo.contig;\n+        final long position = probeInfo.position;\n+        final Allele refAllele = Allele.create(refSource.queryAndPrefetch(contig, position, position).getBaseString(), true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk4OTU3Ng==", "bodyText": "Something to think about, although we still need the reference when we initialize the vcf writer to get the sequence dictionary for the header", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442989576", "createdAt": "2020-06-19T18:32:06Z", "author": {"login": "kcibul"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package org.broadinstitute.hellbender.tools.variantdb;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import com.google.cloud.bigquery.TableResult;\n+import com.google.common.collect.Sets;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.variantcontext.VariantContextBuilder;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.variantdb.RawArrayData.ArrayGenotype;\n+import org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.bigquery.*;\n+import org.broadinstitute.hellbender.utils.localsort.AvroSortingCollectionCodec;\n+import org.broadinstitute.hellbender.utils.localsort.SortingCollection;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.text.DecimalFormat;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import static org.broadinstitute.hellbender.tools.variantdb.ExtractCohortBQ.*;\n+\n+\n+public class ArrayExtractCohortEngine {\n+    private final DecimalFormat df = new DecimalFormat();\n+    private final String DOT = \".\";\n+    \n+    static {\n+    }\n+    \n+    private static final Logger logger = LogManager.getLogger(ExtractCohortEngine.class);\n+\n+    private final VariantContextWriter vcfWriter;\n+\n+    private final boolean useCompressedData;\n+    private final boolean printDebugInformation;\n+    private final int localSortMaxRecordsInRam;\n+    private final TableReference cohortTableRef;\n+    private final ReferenceDataSource refSource;\n+\n+    private final ProgressMeter progressMeter;\n+    private final String projectID;\n+\n+    /** List of sample names seen in the variant data from BigQuery. */\n+    private final Map<Integer, String> sampleIdMap;\n+    private final Set<String> sampleNames;\n+\n+    private final Map<Long, ProbeInfo> probeIdMap;\n+    private final ReferenceConfidenceVariantContextMerger variantContextMerger;\n+\n+    private int totalNumberOfVariants = 0;\n+    private int totalNumberOfSites = 0;\n+\n+    /**\n+     * The conf threshold above which variants are not included in the position tables.\n+     * This value is used to construct the genotype information of those missing samples\n+     * when they are merged together into a {@link VariantContext} object\n+     */\n+    public static int MISSING_CONF_THRESHOLD = 60;\n+\n+\n+    public ArrayExtractCohortEngine(final String projectID,\n+                                    final VariantContextWriter vcfWriter,\n+                                    final VCFHeader vcfHeader,\n+                                    final VariantAnnotatorEngine annotationEngine,\n+                                    final ReferenceDataSource refSource,\n+                                    final Map<Integer, String> sampleIdMap,\n+                                    final Map<Long, ProbeInfo> probeIdMap,\n+                                    final String cohortTableName,\n+                                    final int localSortMaxRecordsInRam,\n+                                    final boolean useCompressedData,\n+                                    final boolean printDebugInformation,\n+                                    final ProgressMeter progressMeter) {\n+\n+        this.df.setMaximumFractionDigits(3);\n+        this.df.setGroupingSize(0);\n+                                \n+        this.localSortMaxRecordsInRam = localSortMaxRecordsInRam;\n+\n+        this.projectID = projectID;\n+        this.vcfWriter = vcfWriter;\n+        this.refSource = refSource;\n+        this.sampleIdMap = sampleIdMap;\n+        this.sampleNames = new HashSet<>(sampleIdMap.values());\n+\n+        this.probeIdMap = probeIdMap;\n+\n+        this.cohortTableRef = new TableReference(cohortTableName, useCompressedData?SchemaUtils.RAW_ARRAY_COHORT_FIELDS_COMPRESSED:SchemaUtils.RAW_ARRAY_COHORT_FIELDS_UNCOMPRESSED);\n+\n+        this.useCompressedData = useCompressedData;\n+        this.printDebugInformation = printDebugInformation;\n+        this.progressMeter = progressMeter;\n+\n+        // KCIBUL: what is the right variant context merger for arrays?\n+        this.variantContextMerger = new ReferenceConfidenceVariantContextMerger(annotationEngine, vcfHeader);\n+\n+    }\n+\n+    int getTotalNumberOfVariants() { return totalNumberOfVariants; }\n+    int getTotalNumberOfSites() { return totalNumberOfSites; }\n+\n+    public void traverse() {\n+        if (printDebugInformation) {\n+            logger.debug(\"using storage api with local sort\");\n+        }\n+        final StorageAPIAvroReader storageAPIAvroReader = new StorageAPIAvroReader(cohortTableRef);\n+        createVariantsFromUngroupedTableResult(storageAPIAvroReader);\n+    }\n+\n+\n+    private void createVariantsFromUngroupedTableResult(final GATKAvroReader avroReader) {\n+\n+        // stream out the data and sort locally\n+        final org.apache.avro.Schema schema = avroReader.getSchema();\n+        final Set<String> columnNames = new HashSet<>();\n+        schema.getFields().forEach(field -> columnNames.add(field.name()));\n+\n+        Comparator<GenericRecord> comparator = this.useCompressedData ? COMPRESSED_PROBE_ID_COMPARATOR : UNCOMPRESSED_PROBE_ID_COMPARATOR;\n+        SortingCollection<GenericRecord> sortingCollection =  getAvroProbeIdSortingCollection(schema, localSortMaxRecordsInRam, comparator);\n+        for ( final GenericRecord queryRow : avroReader ) {\n+            sortingCollection.add(queryRow);\n+        }\n+\n+        sortingCollection.printTempFileStats();\n+\n+        // iterate through records and process them\n+        final List<GenericRecord> currentPositionRecords = new ArrayList<>(sampleIdMap.size() * 2);\n+        long currentProbeId = -1;\n+\n+        for ( final GenericRecord sortedRow : sortingCollection ) {\n+            long probeId;\n+            if (useCompressedData) {\n+                final long rawData = (Long) sortedRow.get(SchemaUtils.RAW_ARRAY_DATA_FIELD_NAME);\n+                RawArrayData data = RawArrayData.decode(rawData);\n+                probeId = data.probeId;\n+            } else {\n+                probeId = (Long) sortedRow.get(\"probe_id\");\n+            }\n+\n+            if ( probeId != currentProbeId && currentProbeId != -1 ) {\n+                ++totalNumberOfSites;\n+                processSampleRecordsForLocation(currentProbeId, currentPositionRecords, columnNames);\n+                currentPositionRecords.clear();\n+            }\n+\n+            currentPositionRecords.add(sortedRow);\n+            currentProbeId = probeId;\n+        }\n+\n+        if ( ! currentPositionRecords.isEmpty() ) {\n+            ++totalNumberOfSites;\n+            processSampleRecordsForLocation(currentProbeId, currentPositionRecords, columnNames);\n+        }\n+    }\n+\n+    private void processSampleRecordsForLocation(final long probeId, final Iterable<GenericRecord> sampleRecordsAtPosition, final Set<String> columnNames) {\n+        final List<VariantContext> unmergedCalls = new ArrayList<>();\n+        final Set<String> currentPositionSamplesSeen = new HashSet<>();\n+        boolean currentPositionHasVariant = false;\n+\n+        final ProbeInfo probeInfo = probeIdMap.get(probeId);\n+        if (probeInfo == null) {\n+            throw new RuntimeException(\"Unable to find probeInfo for \" + probeId);\n+        }\n+\n+        final String contig = probeInfo.contig;\n+        final long position = probeInfo.position;\n+        final Allele refAllele = Allele.create(refSource.queryAndPrefetch(contig, position, position).getBaseString(), true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxMDQ5MQ=="}, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 180}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NTY3ODYxOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNToyNzoyMFrOGl0nbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxODozMjo1MFrOGmd9Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxMjU1Ng==", "bodyText": "do you think we are going to be dropping something we aren't already?", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442312556", "createdAt": "2020-06-18T15:27:20Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package org.broadinstitute.hellbender.tools.variantdb;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import com.google.cloud.bigquery.TableResult;\n+import com.google.common.collect.Sets;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.variantcontext.VariantContextBuilder;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.variantdb.RawArrayData.ArrayGenotype;\n+import org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.bigquery.*;\n+import org.broadinstitute.hellbender.utils.localsort.AvroSortingCollectionCodec;\n+import org.broadinstitute.hellbender.utils.localsort.SortingCollection;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.text.DecimalFormat;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import static org.broadinstitute.hellbender.tools.variantdb.ExtractCohortBQ.*;\n+\n+\n+public class ArrayExtractCohortEngine {\n+    private final DecimalFormat df = new DecimalFormat();\n+    private final String DOT = \".\";\n+    \n+    static {\n+    }\n+    \n+    private static final Logger logger = LogManager.getLogger(ExtractCohortEngine.class);\n+\n+    private final VariantContextWriter vcfWriter;\n+\n+    private final boolean useCompressedData;\n+    private final boolean printDebugInformation;\n+    private final int localSortMaxRecordsInRam;\n+    private final TableReference cohortTableRef;\n+    private final ReferenceDataSource refSource;\n+\n+    private final ProgressMeter progressMeter;\n+    private final String projectID;\n+\n+    /** List of sample names seen in the variant data from BigQuery. */\n+    private final Map<Integer, String> sampleIdMap;\n+    private final Set<String> sampleNames;\n+\n+    private final Map<Long, ProbeInfo> probeIdMap;\n+    private final ReferenceConfidenceVariantContextMerger variantContextMerger;\n+\n+    private int totalNumberOfVariants = 0;\n+    private int totalNumberOfSites = 0;\n+\n+    /**\n+     * The conf threshold above which variants are not included in the position tables.\n+     * This value is used to construct the genotype information of those missing samples\n+     * when they are merged together into a {@link VariantContext} object\n+     */\n+    public static int MISSING_CONF_THRESHOLD = 60;\n+\n+\n+    public ArrayExtractCohortEngine(final String projectID,\n+                                    final VariantContextWriter vcfWriter,\n+                                    final VCFHeader vcfHeader,\n+                                    final VariantAnnotatorEngine annotationEngine,\n+                                    final ReferenceDataSource refSource,\n+                                    final Map<Integer, String> sampleIdMap,\n+                                    final Map<Long, ProbeInfo> probeIdMap,\n+                                    final String cohortTableName,\n+                                    final int localSortMaxRecordsInRam,\n+                                    final boolean useCompressedData,\n+                                    final boolean printDebugInformation,\n+                                    final ProgressMeter progressMeter) {\n+\n+        this.df.setMaximumFractionDigits(3);\n+        this.df.setGroupingSize(0);\n+                                \n+        this.localSortMaxRecordsInRam = localSortMaxRecordsInRam;\n+\n+        this.projectID = projectID;\n+        this.vcfWriter = vcfWriter;\n+        this.refSource = refSource;\n+        this.sampleIdMap = sampleIdMap;\n+        this.sampleNames = new HashSet<>(sampleIdMap.values());\n+\n+        this.probeIdMap = probeIdMap;\n+\n+        this.cohortTableRef = new TableReference(cohortTableName, useCompressedData?SchemaUtils.RAW_ARRAY_COHORT_FIELDS_COMPRESSED:SchemaUtils.RAW_ARRAY_COHORT_FIELDS_UNCOMPRESSED);\n+\n+        this.useCompressedData = useCompressedData;\n+        this.printDebugInformation = printDebugInformation;\n+        this.progressMeter = progressMeter;\n+\n+        // KCIBUL: what is the right variant context merger for arrays?\n+        this.variantContextMerger = new ReferenceConfidenceVariantContextMerger(annotationEngine, vcfHeader);\n+\n+    }\n+\n+    int getTotalNumberOfVariants() { return totalNumberOfVariants; }\n+    int getTotalNumberOfSites() { return totalNumberOfSites; }\n+\n+    public void traverse() {\n+        if (printDebugInformation) {\n+            logger.debug(\"using storage api with local sort\");\n+        }\n+        final StorageAPIAvroReader storageAPIAvroReader = new StorageAPIAvroReader(cohortTableRef);\n+        createVariantsFromUngroupedTableResult(storageAPIAvroReader);\n+    }\n+\n+\n+    private void createVariantsFromUngroupedTableResult(final GATKAvroReader avroReader) {\n+\n+        // stream out the data and sort locally\n+        final org.apache.avro.Schema schema = avroReader.getSchema();\n+        final Set<String> columnNames = new HashSet<>();\n+        schema.getFields().forEach(field -> columnNames.add(field.name()));\n+\n+        Comparator<GenericRecord> comparator = this.useCompressedData ? COMPRESSED_PROBE_ID_COMPARATOR : UNCOMPRESSED_PROBE_ID_COMPARATOR;\n+        SortingCollection<GenericRecord> sortingCollection =  getAvroProbeIdSortingCollection(schema, localSortMaxRecordsInRam, comparator);\n+        for ( final GenericRecord queryRow : avroReader ) {\n+            sortingCollection.add(queryRow);\n+        }\n+\n+        sortingCollection.printTempFileStats();\n+\n+        // iterate through records and process them\n+        final List<GenericRecord> currentPositionRecords = new ArrayList<>(sampleIdMap.size() * 2);\n+        long currentProbeId = -1;\n+\n+        for ( final GenericRecord sortedRow : sortingCollection ) {\n+            long probeId;\n+            if (useCompressedData) {\n+                final long rawData = (Long) sortedRow.get(SchemaUtils.RAW_ARRAY_DATA_FIELD_NAME);\n+                RawArrayData data = RawArrayData.decode(rawData);\n+                probeId = data.probeId;\n+            } else {\n+                probeId = (Long) sortedRow.get(\"probe_id\");\n+            }\n+\n+            if ( probeId != currentProbeId && currentProbeId != -1 ) {\n+                ++totalNumberOfSites;\n+                processSampleRecordsForLocation(currentProbeId, currentPositionRecords, columnNames);\n+                currentPositionRecords.clear();\n+            }\n+\n+            currentPositionRecords.add(sortedRow);\n+            currentProbeId = probeId;\n+        }\n+\n+        if ( ! currentPositionRecords.isEmpty() ) {\n+            ++totalNumberOfSites;\n+            processSampleRecordsForLocation(currentProbeId, currentPositionRecords, columnNames);\n+        }\n+    }\n+\n+    private void processSampleRecordsForLocation(final long probeId, final Iterable<GenericRecord> sampleRecordsAtPosition, final Set<String> columnNames) {\n+        final List<VariantContext> unmergedCalls = new ArrayList<>();\n+        final Set<String> currentPositionSamplesSeen = new HashSet<>();\n+        boolean currentPositionHasVariant = false;\n+\n+        final ProbeInfo probeInfo = probeIdMap.get(probeId);\n+        if (probeInfo == null) {\n+            throw new RuntimeException(\"Unable to find probeInfo for \" + probeId);\n+        }\n+\n+        final String contig = probeInfo.contig;\n+        final long position = probeInfo.position;\n+        final Allele refAllele = Allele.create(refSource.queryAndPrefetch(contig, position, position).getBaseString(), true);\n+\n+        int numRecordsAtPosition = 0;\n+\n+        for ( final GenericRecord sampleRecord : sampleRecordsAtPosition ) {\n+            final long sampleId = (Long) sampleRecord.get(SchemaUtils.SAMPLE_ID_FIELD_NAME);\n+\n+            // TODO: handle missing values\n+            String sampleName = sampleIdMap.get((int) sampleId);            \n+            currentPositionSamplesSeen.add(sampleName);\n+\n+            ++numRecordsAtPosition;\n+\n+            if ( printDebugInformation ) {\n+                logger.info(\"\\t\" + contig + \":\" + position + \": found record for sample \" + sampleName + \": \" + sampleRecord);\n+            }\n+\n+            ++totalNumberOfVariants;\n+            unmergedCalls.add(createVariantContextFromSampleRecord(probeInfo, sampleRecord, columnNames, contig, position, sampleName));\n+\n+        }\n+\n+        if ( printDebugInformation ) {\n+            logger.info(contig + \":\" + position + \": processed \" + numRecordsAtPosition + \" total sample records\");\n+        }\n+\n+        finalizeCurrentVariant(unmergedCalls, currentPositionSamplesSeen, contig, position, refAllele);\n+    }\n+\n+    private void finalizeCurrentVariant(final List<VariantContext> unmergedCalls, final Set<String> currentVariantSamplesSeen, final String contig, final long start, final Allele refAllele) {\n+\n+        // TODO: this is where we infer missing data points... once we know what we want to drop\n+        // final Set<String> samplesNotEncountered = Sets.difference(sampleNames, currentVariantSamplesSeen);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk4OTg3OQ==", "bodyText": "maybe... we need to dig into more data, and when we get to imputed data I think we will", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442989879", "createdAt": "2020-06-19T18:32:50Z", "author": {"login": "kcibul"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package org.broadinstitute.hellbender.tools.variantdb;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import com.google.cloud.bigquery.TableResult;\n+import com.google.common.collect.Sets;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.variantcontext.VariantContextBuilder;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.variantdb.RawArrayData.ArrayGenotype;\n+import org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.bigquery.*;\n+import org.broadinstitute.hellbender.utils.localsort.AvroSortingCollectionCodec;\n+import org.broadinstitute.hellbender.utils.localsort.SortingCollection;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.text.DecimalFormat;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import static org.broadinstitute.hellbender.tools.variantdb.ExtractCohortBQ.*;\n+\n+\n+public class ArrayExtractCohortEngine {\n+    private final DecimalFormat df = new DecimalFormat();\n+    private final String DOT = \".\";\n+    \n+    static {\n+    }\n+    \n+    private static final Logger logger = LogManager.getLogger(ExtractCohortEngine.class);\n+\n+    private final VariantContextWriter vcfWriter;\n+\n+    private final boolean useCompressedData;\n+    private final boolean printDebugInformation;\n+    private final int localSortMaxRecordsInRam;\n+    private final TableReference cohortTableRef;\n+    private final ReferenceDataSource refSource;\n+\n+    private final ProgressMeter progressMeter;\n+    private final String projectID;\n+\n+    /** List of sample names seen in the variant data from BigQuery. */\n+    private final Map<Integer, String> sampleIdMap;\n+    private final Set<String> sampleNames;\n+\n+    private final Map<Long, ProbeInfo> probeIdMap;\n+    private final ReferenceConfidenceVariantContextMerger variantContextMerger;\n+\n+    private int totalNumberOfVariants = 0;\n+    private int totalNumberOfSites = 0;\n+\n+    /**\n+     * The conf threshold above which variants are not included in the position tables.\n+     * This value is used to construct the genotype information of those missing samples\n+     * when they are merged together into a {@link VariantContext} object\n+     */\n+    public static int MISSING_CONF_THRESHOLD = 60;\n+\n+\n+    public ArrayExtractCohortEngine(final String projectID,\n+                                    final VariantContextWriter vcfWriter,\n+                                    final VCFHeader vcfHeader,\n+                                    final VariantAnnotatorEngine annotationEngine,\n+                                    final ReferenceDataSource refSource,\n+                                    final Map<Integer, String> sampleIdMap,\n+                                    final Map<Long, ProbeInfo> probeIdMap,\n+                                    final String cohortTableName,\n+                                    final int localSortMaxRecordsInRam,\n+                                    final boolean useCompressedData,\n+                                    final boolean printDebugInformation,\n+                                    final ProgressMeter progressMeter) {\n+\n+        this.df.setMaximumFractionDigits(3);\n+        this.df.setGroupingSize(0);\n+                                \n+        this.localSortMaxRecordsInRam = localSortMaxRecordsInRam;\n+\n+        this.projectID = projectID;\n+        this.vcfWriter = vcfWriter;\n+        this.refSource = refSource;\n+        this.sampleIdMap = sampleIdMap;\n+        this.sampleNames = new HashSet<>(sampleIdMap.values());\n+\n+        this.probeIdMap = probeIdMap;\n+\n+        this.cohortTableRef = new TableReference(cohortTableName, useCompressedData?SchemaUtils.RAW_ARRAY_COHORT_FIELDS_COMPRESSED:SchemaUtils.RAW_ARRAY_COHORT_FIELDS_UNCOMPRESSED);\n+\n+        this.useCompressedData = useCompressedData;\n+        this.printDebugInformation = printDebugInformation;\n+        this.progressMeter = progressMeter;\n+\n+        // KCIBUL: what is the right variant context merger for arrays?\n+        this.variantContextMerger = new ReferenceConfidenceVariantContextMerger(annotationEngine, vcfHeader);\n+\n+    }\n+\n+    int getTotalNumberOfVariants() { return totalNumberOfVariants; }\n+    int getTotalNumberOfSites() { return totalNumberOfSites; }\n+\n+    public void traverse() {\n+        if (printDebugInformation) {\n+            logger.debug(\"using storage api with local sort\");\n+        }\n+        final StorageAPIAvroReader storageAPIAvroReader = new StorageAPIAvroReader(cohortTableRef);\n+        createVariantsFromUngroupedTableResult(storageAPIAvroReader);\n+    }\n+\n+\n+    private void createVariantsFromUngroupedTableResult(final GATKAvroReader avroReader) {\n+\n+        // stream out the data and sort locally\n+        final org.apache.avro.Schema schema = avroReader.getSchema();\n+        final Set<String> columnNames = new HashSet<>();\n+        schema.getFields().forEach(field -> columnNames.add(field.name()));\n+\n+        Comparator<GenericRecord> comparator = this.useCompressedData ? COMPRESSED_PROBE_ID_COMPARATOR : UNCOMPRESSED_PROBE_ID_COMPARATOR;\n+        SortingCollection<GenericRecord> sortingCollection =  getAvroProbeIdSortingCollection(schema, localSortMaxRecordsInRam, comparator);\n+        for ( final GenericRecord queryRow : avroReader ) {\n+            sortingCollection.add(queryRow);\n+        }\n+\n+        sortingCollection.printTempFileStats();\n+\n+        // iterate through records and process them\n+        final List<GenericRecord> currentPositionRecords = new ArrayList<>(sampleIdMap.size() * 2);\n+        long currentProbeId = -1;\n+\n+        for ( final GenericRecord sortedRow : sortingCollection ) {\n+            long probeId;\n+            if (useCompressedData) {\n+                final long rawData = (Long) sortedRow.get(SchemaUtils.RAW_ARRAY_DATA_FIELD_NAME);\n+                RawArrayData data = RawArrayData.decode(rawData);\n+                probeId = data.probeId;\n+            } else {\n+                probeId = (Long) sortedRow.get(\"probe_id\");\n+            }\n+\n+            if ( probeId != currentProbeId && currentProbeId != -1 ) {\n+                ++totalNumberOfSites;\n+                processSampleRecordsForLocation(currentProbeId, currentPositionRecords, columnNames);\n+                currentPositionRecords.clear();\n+            }\n+\n+            currentPositionRecords.add(sortedRow);\n+            currentProbeId = probeId;\n+        }\n+\n+        if ( ! currentPositionRecords.isEmpty() ) {\n+            ++totalNumberOfSites;\n+            processSampleRecordsForLocation(currentProbeId, currentPositionRecords, columnNames);\n+        }\n+    }\n+\n+    private void processSampleRecordsForLocation(final long probeId, final Iterable<GenericRecord> sampleRecordsAtPosition, final Set<String> columnNames) {\n+        final List<VariantContext> unmergedCalls = new ArrayList<>();\n+        final Set<String> currentPositionSamplesSeen = new HashSet<>();\n+        boolean currentPositionHasVariant = false;\n+\n+        final ProbeInfo probeInfo = probeIdMap.get(probeId);\n+        if (probeInfo == null) {\n+            throw new RuntimeException(\"Unable to find probeInfo for \" + probeId);\n+        }\n+\n+        final String contig = probeInfo.contig;\n+        final long position = probeInfo.position;\n+        final Allele refAllele = Allele.create(refSource.queryAndPrefetch(contig, position, position).getBaseString(), true);\n+\n+        int numRecordsAtPosition = 0;\n+\n+        for ( final GenericRecord sampleRecord : sampleRecordsAtPosition ) {\n+            final long sampleId = (Long) sampleRecord.get(SchemaUtils.SAMPLE_ID_FIELD_NAME);\n+\n+            // TODO: handle missing values\n+            String sampleName = sampleIdMap.get((int) sampleId);            \n+            currentPositionSamplesSeen.add(sampleName);\n+\n+            ++numRecordsAtPosition;\n+\n+            if ( printDebugInformation ) {\n+                logger.info(\"\\t\" + contig + \":\" + position + \": found record for sample \" + sampleName + \": \" + sampleRecord);\n+            }\n+\n+            ++totalNumberOfVariants;\n+            unmergedCalls.add(createVariantContextFromSampleRecord(probeInfo, sampleRecord, columnNames, contig, position, sampleName));\n+\n+        }\n+\n+        if ( printDebugInformation ) {\n+            logger.info(contig + \":\" + position + \": processed \" + numRecordsAtPosition + \" total sample records\");\n+        }\n+\n+        finalizeCurrentVariant(unmergedCalls, currentPositionSamplesSeen, contig, position, refAllele);\n+    }\n+\n+    private void finalizeCurrentVariant(final List<VariantContext> unmergedCalls, final Set<String> currentVariantSamplesSeen, final String contig, final long start, final Allele refAllele) {\n+\n+        // TODO: this is where we infer missing data points... once we know what we want to drop\n+        // final Set<String> samplesNotEncountered = Sets.difference(sampleNames, currentVariantSamplesSeen);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxMjU1Ng=="}, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 212}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NTcwODk2OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNTozNDoyMVrOGl06XA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNTozNDoyMVrOGl06XA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxNzQwNA==", "bodyText": "is no-call the same as missing? there is a enum value for MISSING.\nand if you mean how do you represent a missing allele in the genotype, there is a constant for that. i think it's Genotype.NO_CALL", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442317404", "createdAt": "2020-06-18T15:34:21Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package org.broadinstitute.hellbender.tools.variantdb;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import com.google.cloud.bigquery.TableResult;\n+import com.google.common.collect.Sets;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.variantcontext.VariantContextBuilder;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.variantdb.RawArrayData.ArrayGenotype;\n+import org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.bigquery.*;\n+import org.broadinstitute.hellbender.utils.localsort.AvroSortingCollectionCodec;\n+import org.broadinstitute.hellbender.utils.localsort.SortingCollection;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.text.DecimalFormat;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import static org.broadinstitute.hellbender.tools.variantdb.ExtractCohortBQ.*;\n+\n+\n+public class ArrayExtractCohortEngine {\n+    private final DecimalFormat df = new DecimalFormat();\n+    private final String DOT = \".\";\n+    \n+    static {\n+    }\n+    \n+    private static final Logger logger = LogManager.getLogger(ExtractCohortEngine.class);\n+\n+    private final VariantContextWriter vcfWriter;\n+\n+    private final boolean useCompressedData;\n+    private final boolean printDebugInformation;\n+    private final int localSortMaxRecordsInRam;\n+    private final TableReference cohortTableRef;\n+    private final ReferenceDataSource refSource;\n+\n+    private final ProgressMeter progressMeter;\n+    private final String projectID;\n+\n+    /** List of sample names seen in the variant data from BigQuery. */\n+    private final Map<Integer, String> sampleIdMap;\n+    private final Set<String> sampleNames;\n+\n+    private final Map<Long, ProbeInfo> probeIdMap;\n+    private final ReferenceConfidenceVariantContextMerger variantContextMerger;\n+\n+    private int totalNumberOfVariants = 0;\n+    private int totalNumberOfSites = 0;\n+\n+    /**\n+     * The conf threshold above which variants are not included in the position tables.\n+     * This value is used to construct the genotype information of those missing samples\n+     * when they are merged together into a {@link VariantContext} object\n+     */\n+    public static int MISSING_CONF_THRESHOLD = 60;\n+\n+\n+    public ArrayExtractCohortEngine(final String projectID,\n+                                    final VariantContextWriter vcfWriter,\n+                                    final VCFHeader vcfHeader,\n+                                    final VariantAnnotatorEngine annotationEngine,\n+                                    final ReferenceDataSource refSource,\n+                                    final Map<Integer, String> sampleIdMap,\n+                                    final Map<Long, ProbeInfo> probeIdMap,\n+                                    final String cohortTableName,\n+                                    final int localSortMaxRecordsInRam,\n+                                    final boolean useCompressedData,\n+                                    final boolean printDebugInformation,\n+                                    final ProgressMeter progressMeter) {\n+\n+        this.df.setMaximumFractionDigits(3);\n+        this.df.setGroupingSize(0);\n+                                \n+        this.localSortMaxRecordsInRam = localSortMaxRecordsInRam;\n+\n+        this.projectID = projectID;\n+        this.vcfWriter = vcfWriter;\n+        this.refSource = refSource;\n+        this.sampleIdMap = sampleIdMap;\n+        this.sampleNames = new HashSet<>(sampleIdMap.values());\n+\n+        this.probeIdMap = probeIdMap;\n+\n+        this.cohortTableRef = new TableReference(cohortTableName, useCompressedData?SchemaUtils.RAW_ARRAY_COHORT_FIELDS_COMPRESSED:SchemaUtils.RAW_ARRAY_COHORT_FIELDS_UNCOMPRESSED);\n+\n+        this.useCompressedData = useCompressedData;\n+        this.printDebugInformation = printDebugInformation;\n+        this.progressMeter = progressMeter;\n+\n+        // KCIBUL: what is the right variant context merger for arrays?\n+        this.variantContextMerger = new ReferenceConfidenceVariantContextMerger(annotationEngine, vcfHeader);\n+\n+    }\n+\n+    int getTotalNumberOfVariants() { return totalNumberOfVariants; }\n+    int getTotalNumberOfSites() { return totalNumberOfSites; }\n+\n+    public void traverse() {\n+        if (printDebugInformation) {\n+            logger.debug(\"using storage api with local sort\");\n+        }\n+        final StorageAPIAvroReader storageAPIAvroReader = new StorageAPIAvroReader(cohortTableRef);\n+        createVariantsFromUngroupedTableResult(storageAPIAvroReader);\n+    }\n+\n+\n+    private void createVariantsFromUngroupedTableResult(final GATKAvroReader avroReader) {\n+\n+        // stream out the data and sort locally\n+        final org.apache.avro.Schema schema = avroReader.getSchema();\n+        final Set<String> columnNames = new HashSet<>();\n+        schema.getFields().forEach(field -> columnNames.add(field.name()));\n+\n+        Comparator<GenericRecord> comparator = this.useCompressedData ? COMPRESSED_PROBE_ID_COMPARATOR : UNCOMPRESSED_PROBE_ID_COMPARATOR;\n+        SortingCollection<GenericRecord> sortingCollection =  getAvroProbeIdSortingCollection(schema, localSortMaxRecordsInRam, comparator);\n+        for ( final GenericRecord queryRow : avroReader ) {\n+            sortingCollection.add(queryRow);\n+        }\n+\n+        sortingCollection.printTempFileStats();\n+\n+        // iterate through records and process them\n+        final List<GenericRecord> currentPositionRecords = new ArrayList<>(sampleIdMap.size() * 2);\n+        long currentProbeId = -1;\n+\n+        for ( final GenericRecord sortedRow : sortingCollection ) {\n+            long probeId;\n+            if (useCompressedData) {\n+                final long rawData = (Long) sortedRow.get(SchemaUtils.RAW_ARRAY_DATA_FIELD_NAME);\n+                RawArrayData data = RawArrayData.decode(rawData);\n+                probeId = data.probeId;\n+            } else {\n+                probeId = (Long) sortedRow.get(\"probe_id\");\n+            }\n+\n+            if ( probeId != currentProbeId && currentProbeId != -1 ) {\n+                ++totalNumberOfSites;\n+                processSampleRecordsForLocation(currentProbeId, currentPositionRecords, columnNames);\n+                currentPositionRecords.clear();\n+            }\n+\n+            currentPositionRecords.add(sortedRow);\n+            currentProbeId = probeId;\n+        }\n+\n+        if ( ! currentPositionRecords.isEmpty() ) {\n+            ++totalNumberOfSites;\n+            processSampleRecordsForLocation(currentProbeId, currentPositionRecords, columnNames);\n+        }\n+    }\n+\n+    private void processSampleRecordsForLocation(final long probeId, final Iterable<GenericRecord> sampleRecordsAtPosition, final Set<String> columnNames) {\n+        final List<VariantContext> unmergedCalls = new ArrayList<>();\n+        final Set<String> currentPositionSamplesSeen = new HashSet<>();\n+        boolean currentPositionHasVariant = false;\n+\n+        final ProbeInfo probeInfo = probeIdMap.get(probeId);\n+        if (probeInfo == null) {\n+            throw new RuntimeException(\"Unable to find probeInfo for \" + probeId);\n+        }\n+\n+        final String contig = probeInfo.contig;\n+        final long position = probeInfo.position;\n+        final Allele refAllele = Allele.create(refSource.queryAndPrefetch(contig, position, position).getBaseString(), true);\n+\n+        int numRecordsAtPosition = 0;\n+\n+        for ( final GenericRecord sampleRecord : sampleRecordsAtPosition ) {\n+            final long sampleId = (Long) sampleRecord.get(SchemaUtils.SAMPLE_ID_FIELD_NAME);\n+\n+            // TODO: handle missing values\n+            String sampleName = sampleIdMap.get((int) sampleId);            \n+            currentPositionSamplesSeen.add(sampleName);\n+\n+            ++numRecordsAtPosition;\n+\n+            if ( printDebugInformation ) {\n+                logger.info(\"\\t\" + contig + \":\" + position + \": found record for sample \" + sampleName + \": \" + sampleRecord);\n+            }\n+\n+            ++totalNumberOfVariants;\n+            unmergedCalls.add(createVariantContextFromSampleRecord(probeInfo, sampleRecord, columnNames, contig, position, sampleName));\n+\n+        }\n+\n+        if ( printDebugInformation ) {\n+            logger.info(contig + \":\" + position + \": processed \" + numRecordsAtPosition + \" total sample records\");\n+        }\n+\n+        finalizeCurrentVariant(unmergedCalls, currentPositionSamplesSeen, contig, position, refAllele);\n+    }\n+\n+    private void finalizeCurrentVariant(final List<VariantContext> unmergedCalls, final Set<String> currentVariantSamplesSeen, final String contig, final long start, final Allele refAllele) {\n+\n+        // TODO: this is where we infer missing data points... once we know what we want to drop\n+        // final Set<String> samplesNotEncountered = Sets.difference(sampleNames, currentVariantSamplesSeen);\n+        // for ( final String missingSample : samplesNotEncountered ) {\n+        //         unmergedCalls.add(createRefSiteVariantContext(missingSample, contig, start, refAllele));\n+        // }\n+\n+        final VariantContext mergedVC = variantContextMerger.merge(\n+                unmergedCalls,\n+                new SimpleInterval(contig, (int) start, (int) start),\n+                refAllele.getBases()[0],\n+                true,\n+                false,\n+                true);\n+\n+\n+        final VariantContext finalVC = mergedVC;\n+\n+        // TODO: this was commented out... probably need to re-enable\n+//        final VariantContext annotatedVC = enableVariantAnnotator ?\n+//                variantAnnotator.annotateContext(finalizedVC, new FeatureContext(), null, null, a -> true): finalVC;\n+\n+//        if ( annotatedVC != null ) {\n+//            vcfWriter.add(annotatedVC);\n+//            progressMeter.update(annotatedVC);\n+//        }\n+\n+        if ( finalVC != null ) {\n+            vcfWriter.add(finalVC);\n+            progressMeter.update(finalVC);\n+        } else {\n+            // TODO should i print a warning here?\n+            vcfWriter.add(mergedVC);\n+            progressMeter.update(mergedVC);\n+        }\n+    }\n+\n+    private String formatFloatForVcf(final Float value) {\n+        if (value == null || Double.isNaN(value)) {\n+            return DOT;\n+        }\n+        return df.format(value);\n+    }\n+\n+    private Float getNullableFloatFromDouble(Object d) {\n+        return d == null ? null : (float)  ((Double) d).doubleValue();\n+    }\n+\n+    private VariantContext createVariantContextFromSampleRecord(final ProbeInfo probeInfo, final GenericRecord sampleRecord, final Set<String> columnNames, final String contig, final long startPosition, final String sample) {\n+        final VariantContextBuilder builder = new VariantContextBuilder();\n+        final GenotypeBuilder genotypeBuilder = new GenotypeBuilder();\n+\n+        builder.chr(contig);\n+        builder.start(startPosition);\n+\n+        \n+        final List<Allele> alleles = new ArrayList<>();\n+        Allele ref = Allele.create(probeInfo.ref, true);        \n+        alleles.add(ref);\n+\n+        Allele alleleA = Allele.create(probeInfo.alleleA, false);\n+        Allele alleleB = Allele.create(probeInfo.alleleB, false);\n+\n+        boolean alleleAisRef = probeInfo.ref.equals(probeInfo.alleleA);\n+        boolean alleleBisRef = probeInfo.ref.equals(probeInfo.alleleB);\n+\n+        if (alleleAisRef) {\n+            alleleA = ref;\n+        } else {\n+            alleles.add(alleleA);\n+        }\n+\n+        if (alleleBisRef) {\n+            alleleB = ref;\n+        } else {\n+            alleles.add(alleleB);\n+        }\n+\n+        builder.alleles(alleles);\n+        builder.stop(startPosition + alleles.get(0).length() - 1);\n+\n+        Float normx;\n+        Float normy;\n+        Float baf;\n+        Float lrr;\n+        List<Allele> genotypeAlleles = new ArrayList<Allele>();\n+\n+        if (this.useCompressedData) {\n+            final RawArrayData data = RawArrayData.decode((Long) sampleRecord.get(SchemaUtils.RAW_ARRAY_DATA_FIELD_NAME));\n+            normx = data.normx;\n+            normy = data.normy;\n+            lrr = data.lrr;\n+            baf = data.baf;\n+\n+            // Genotype -- what about no-call?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 304}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NTcxNzA3OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNTozNjoxMlrOGl0_Xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxODozNDo1M1rOGmeAIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxODY4Nw==", "bodyText": "i think you can use the constants from the RawArrayFieldEnum", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442318687", "createdAt": "2020-06-18T15:36:12Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package org.broadinstitute.hellbender.tools.variantdb;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import com.google.cloud.bigquery.TableResult;\n+import com.google.common.collect.Sets;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.variantcontext.VariantContextBuilder;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.variantdb.RawArrayData.ArrayGenotype;\n+import org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.bigquery.*;\n+import org.broadinstitute.hellbender.utils.localsort.AvroSortingCollectionCodec;\n+import org.broadinstitute.hellbender.utils.localsort.SortingCollection;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.text.DecimalFormat;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import static org.broadinstitute.hellbender.tools.variantdb.ExtractCohortBQ.*;\n+\n+\n+public class ArrayExtractCohortEngine {\n+    private final DecimalFormat df = new DecimalFormat();\n+    private final String DOT = \".\";\n+    \n+    static {\n+    }\n+    \n+    private static final Logger logger = LogManager.getLogger(ExtractCohortEngine.class);\n+\n+    private final VariantContextWriter vcfWriter;\n+\n+    private final boolean useCompressedData;\n+    private final boolean printDebugInformation;\n+    private final int localSortMaxRecordsInRam;\n+    private final TableReference cohortTableRef;\n+    private final ReferenceDataSource refSource;\n+\n+    private final ProgressMeter progressMeter;\n+    private final String projectID;\n+\n+    /** List of sample names seen in the variant data from BigQuery. */\n+    private final Map<Integer, String> sampleIdMap;\n+    private final Set<String> sampleNames;\n+\n+    private final Map<Long, ProbeInfo> probeIdMap;\n+    private final ReferenceConfidenceVariantContextMerger variantContextMerger;\n+\n+    private int totalNumberOfVariants = 0;\n+    private int totalNumberOfSites = 0;\n+\n+    /**\n+     * The conf threshold above which variants are not included in the position tables.\n+     * This value is used to construct the genotype information of those missing samples\n+     * when they are merged together into a {@link VariantContext} object\n+     */\n+    public static int MISSING_CONF_THRESHOLD = 60;\n+\n+\n+    public ArrayExtractCohortEngine(final String projectID,\n+                                    final VariantContextWriter vcfWriter,\n+                                    final VCFHeader vcfHeader,\n+                                    final VariantAnnotatorEngine annotationEngine,\n+                                    final ReferenceDataSource refSource,\n+                                    final Map<Integer, String> sampleIdMap,\n+                                    final Map<Long, ProbeInfo> probeIdMap,\n+                                    final String cohortTableName,\n+                                    final int localSortMaxRecordsInRam,\n+                                    final boolean useCompressedData,\n+                                    final boolean printDebugInformation,\n+                                    final ProgressMeter progressMeter) {\n+\n+        this.df.setMaximumFractionDigits(3);\n+        this.df.setGroupingSize(0);\n+                                \n+        this.localSortMaxRecordsInRam = localSortMaxRecordsInRam;\n+\n+        this.projectID = projectID;\n+        this.vcfWriter = vcfWriter;\n+        this.refSource = refSource;\n+        this.sampleIdMap = sampleIdMap;\n+        this.sampleNames = new HashSet<>(sampleIdMap.values());\n+\n+        this.probeIdMap = probeIdMap;\n+\n+        this.cohortTableRef = new TableReference(cohortTableName, useCompressedData?SchemaUtils.RAW_ARRAY_COHORT_FIELDS_COMPRESSED:SchemaUtils.RAW_ARRAY_COHORT_FIELDS_UNCOMPRESSED);\n+\n+        this.useCompressedData = useCompressedData;\n+        this.printDebugInformation = printDebugInformation;\n+        this.progressMeter = progressMeter;\n+\n+        // KCIBUL: what is the right variant context merger for arrays?\n+        this.variantContextMerger = new ReferenceConfidenceVariantContextMerger(annotationEngine, vcfHeader);\n+\n+    }\n+\n+    int getTotalNumberOfVariants() { return totalNumberOfVariants; }\n+    int getTotalNumberOfSites() { return totalNumberOfSites; }\n+\n+    public void traverse() {\n+        if (printDebugInformation) {\n+            logger.debug(\"using storage api with local sort\");\n+        }\n+        final StorageAPIAvroReader storageAPIAvroReader = new StorageAPIAvroReader(cohortTableRef);\n+        createVariantsFromUngroupedTableResult(storageAPIAvroReader);\n+    }\n+\n+\n+    private void createVariantsFromUngroupedTableResult(final GATKAvroReader avroReader) {\n+\n+        // stream out the data and sort locally\n+        final org.apache.avro.Schema schema = avroReader.getSchema();\n+        final Set<String> columnNames = new HashSet<>();\n+        schema.getFields().forEach(field -> columnNames.add(field.name()));\n+\n+        Comparator<GenericRecord> comparator = this.useCompressedData ? COMPRESSED_PROBE_ID_COMPARATOR : UNCOMPRESSED_PROBE_ID_COMPARATOR;\n+        SortingCollection<GenericRecord> sortingCollection =  getAvroProbeIdSortingCollection(schema, localSortMaxRecordsInRam, comparator);\n+        for ( final GenericRecord queryRow : avroReader ) {\n+            sortingCollection.add(queryRow);\n+        }\n+\n+        sortingCollection.printTempFileStats();\n+\n+        // iterate through records and process them\n+        final List<GenericRecord> currentPositionRecords = new ArrayList<>(sampleIdMap.size() * 2);\n+        long currentProbeId = -1;\n+\n+        for ( final GenericRecord sortedRow : sortingCollection ) {\n+            long probeId;\n+            if (useCompressedData) {\n+                final long rawData = (Long) sortedRow.get(SchemaUtils.RAW_ARRAY_DATA_FIELD_NAME);\n+                RawArrayData data = RawArrayData.decode(rawData);\n+                probeId = data.probeId;\n+            } else {\n+                probeId = (Long) sortedRow.get(\"probe_id\");\n+            }\n+\n+            if ( probeId != currentProbeId && currentProbeId != -1 ) {\n+                ++totalNumberOfSites;\n+                processSampleRecordsForLocation(currentProbeId, currentPositionRecords, columnNames);\n+                currentPositionRecords.clear();\n+            }\n+\n+            currentPositionRecords.add(sortedRow);\n+            currentProbeId = probeId;\n+        }\n+\n+        if ( ! currentPositionRecords.isEmpty() ) {\n+            ++totalNumberOfSites;\n+            processSampleRecordsForLocation(currentProbeId, currentPositionRecords, columnNames);\n+        }\n+    }\n+\n+    private void processSampleRecordsForLocation(final long probeId, final Iterable<GenericRecord> sampleRecordsAtPosition, final Set<String> columnNames) {\n+        final List<VariantContext> unmergedCalls = new ArrayList<>();\n+        final Set<String> currentPositionSamplesSeen = new HashSet<>();\n+        boolean currentPositionHasVariant = false;\n+\n+        final ProbeInfo probeInfo = probeIdMap.get(probeId);\n+        if (probeInfo == null) {\n+            throw new RuntimeException(\"Unable to find probeInfo for \" + probeId);\n+        }\n+\n+        final String contig = probeInfo.contig;\n+        final long position = probeInfo.position;\n+        final Allele refAllele = Allele.create(refSource.queryAndPrefetch(contig, position, position).getBaseString(), true);\n+\n+        int numRecordsAtPosition = 0;\n+\n+        for ( final GenericRecord sampleRecord : sampleRecordsAtPosition ) {\n+            final long sampleId = (Long) sampleRecord.get(SchemaUtils.SAMPLE_ID_FIELD_NAME);\n+\n+            // TODO: handle missing values\n+            String sampleName = sampleIdMap.get((int) sampleId);            \n+            currentPositionSamplesSeen.add(sampleName);\n+\n+            ++numRecordsAtPosition;\n+\n+            if ( printDebugInformation ) {\n+                logger.info(\"\\t\" + contig + \":\" + position + \": found record for sample \" + sampleName + \": \" + sampleRecord);\n+            }\n+\n+            ++totalNumberOfVariants;\n+            unmergedCalls.add(createVariantContextFromSampleRecord(probeInfo, sampleRecord, columnNames, contig, position, sampleName));\n+\n+        }\n+\n+        if ( printDebugInformation ) {\n+            logger.info(contig + \":\" + position + \": processed \" + numRecordsAtPosition + \" total sample records\");\n+        }\n+\n+        finalizeCurrentVariant(unmergedCalls, currentPositionSamplesSeen, contig, position, refAllele);\n+    }\n+\n+    private void finalizeCurrentVariant(final List<VariantContext> unmergedCalls, final Set<String> currentVariantSamplesSeen, final String contig, final long start, final Allele refAllele) {\n+\n+        // TODO: this is where we infer missing data points... once we know what we want to drop\n+        // final Set<String> samplesNotEncountered = Sets.difference(sampleNames, currentVariantSamplesSeen);\n+        // for ( final String missingSample : samplesNotEncountered ) {\n+        //         unmergedCalls.add(createRefSiteVariantContext(missingSample, contig, start, refAllele));\n+        // }\n+\n+        final VariantContext mergedVC = variantContextMerger.merge(\n+                unmergedCalls,\n+                new SimpleInterval(contig, (int) start, (int) start),\n+                refAllele.getBases()[0],\n+                true,\n+                false,\n+                true);\n+\n+\n+        final VariantContext finalVC = mergedVC;\n+\n+        // TODO: this was commented out... probably need to re-enable\n+//        final VariantContext annotatedVC = enableVariantAnnotator ?\n+//                variantAnnotator.annotateContext(finalizedVC, new FeatureContext(), null, null, a -> true): finalVC;\n+\n+//        if ( annotatedVC != null ) {\n+//            vcfWriter.add(annotatedVC);\n+//            progressMeter.update(annotatedVC);\n+//        }\n+\n+        if ( finalVC != null ) {\n+            vcfWriter.add(finalVC);\n+            progressMeter.update(finalVC);\n+        } else {\n+            // TODO should i print a warning here?\n+            vcfWriter.add(mergedVC);\n+            progressMeter.update(mergedVC);\n+        }\n+    }\n+\n+    private String formatFloatForVcf(final Float value) {\n+        if (value == null || Double.isNaN(value)) {\n+            return DOT;\n+        }\n+        return df.format(value);\n+    }\n+\n+    private Float getNullableFloatFromDouble(Object d) {\n+        return d == null ? null : (float)  ((Double) d).doubleValue();\n+    }\n+\n+    private VariantContext createVariantContextFromSampleRecord(final ProbeInfo probeInfo, final GenericRecord sampleRecord, final Set<String> columnNames, final String contig, final long startPosition, final String sample) {\n+        final VariantContextBuilder builder = new VariantContextBuilder();\n+        final GenotypeBuilder genotypeBuilder = new GenotypeBuilder();\n+\n+        builder.chr(contig);\n+        builder.start(startPosition);\n+\n+        \n+        final List<Allele> alleles = new ArrayList<>();\n+        Allele ref = Allele.create(probeInfo.ref, true);        \n+        alleles.add(ref);\n+\n+        Allele alleleA = Allele.create(probeInfo.alleleA, false);\n+        Allele alleleB = Allele.create(probeInfo.alleleB, false);\n+\n+        boolean alleleAisRef = probeInfo.ref.equals(probeInfo.alleleA);\n+        boolean alleleBisRef = probeInfo.ref.equals(probeInfo.alleleB);\n+\n+        if (alleleAisRef) {\n+            alleleA = ref;\n+        } else {\n+            alleles.add(alleleA);\n+        }\n+\n+        if (alleleBisRef) {\n+            alleleB = ref;\n+        } else {\n+            alleles.add(alleleB);\n+        }\n+\n+        builder.alleles(alleles);\n+        builder.stop(startPosition + alleles.get(0).length() - 1);\n+\n+        Float normx;\n+        Float normy;\n+        Float baf;\n+        Float lrr;\n+        List<Allele> genotypeAlleles = new ArrayList<Allele>();\n+\n+        if (this.useCompressedData) {\n+            final RawArrayData data = RawArrayData.decode((Long) sampleRecord.get(SchemaUtils.RAW_ARRAY_DATA_FIELD_NAME));\n+            normx = data.normx;\n+            normy = data.normy;\n+            lrr = data.lrr;\n+            baf = data.baf;\n+\n+            // Genotype -- what about no-call?\n+            if (data.genotype == ArrayGenotype.AA) {\n+                genotypeAlleles.add(alleleA);\n+                genotypeAlleles.add(alleleA);\n+            } else if (data.genotype == ArrayGenotype.AB) {\n+                genotypeAlleles.add(alleleA);\n+                genotypeAlleles.add(alleleB);\n+            } else if  (data.genotype == ArrayGenotype.BB) {\n+                genotypeAlleles.add(alleleB);\n+                genotypeAlleles.add(alleleB);\n+            }\n+        } else {\n+            // TODO: constantize\n+            try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 317}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5MDYyNw==", "bodyText": "yeah I think these might all go away once we're storing compressed data so I didn't think too much about it", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442990627", "createdAt": "2020-06-19T18:34:53Z", "author": {"login": "kcibul"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ArrayExtractCohortEngine.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package org.broadinstitute.hellbender.tools.variantdb;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import com.google.cloud.bigquery.TableResult;\n+import com.google.common.collect.Sets;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.variantcontext.VariantContextBuilder;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.variantdb.RawArrayData.ArrayGenotype;\n+import org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.bigquery.*;\n+import org.broadinstitute.hellbender.utils.localsort.AvroSortingCollectionCodec;\n+import org.broadinstitute.hellbender.utils.localsort.SortingCollection;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.text.DecimalFormat;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import static org.broadinstitute.hellbender.tools.variantdb.ExtractCohortBQ.*;\n+\n+\n+public class ArrayExtractCohortEngine {\n+    private final DecimalFormat df = new DecimalFormat();\n+    private final String DOT = \".\";\n+    \n+    static {\n+    }\n+    \n+    private static final Logger logger = LogManager.getLogger(ExtractCohortEngine.class);\n+\n+    private final VariantContextWriter vcfWriter;\n+\n+    private final boolean useCompressedData;\n+    private final boolean printDebugInformation;\n+    private final int localSortMaxRecordsInRam;\n+    private final TableReference cohortTableRef;\n+    private final ReferenceDataSource refSource;\n+\n+    private final ProgressMeter progressMeter;\n+    private final String projectID;\n+\n+    /** List of sample names seen in the variant data from BigQuery. */\n+    private final Map<Integer, String> sampleIdMap;\n+    private final Set<String> sampleNames;\n+\n+    private final Map<Long, ProbeInfo> probeIdMap;\n+    private final ReferenceConfidenceVariantContextMerger variantContextMerger;\n+\n+    private int totalNumberOfVariants = 0;\n+    private int totalNumberOfSites = 0;\n+\n+    /**\n+     * The conf threshold above which variants are not included in the position tables.\n+     * This value is used to construct the genotype information of those missing samples\n+     * when they are merged together into a {@link VariantContext} object\n+     */\n+    public static int MISSING_CONF_THRESHOLD = 60;\n+\n+\n+    public ArrayExtractCohortEngine(final String projectID,\n+                                    final VariantContextWriter vcfWriter,\n+                                    final VCFHeader vcfHeader,\n+                                    final VariantAnnotatorEngine annotationEngine,\n+                                    final ReferenceDataSource refSource,\n+                                    final Map<Integer, String> sampleIdMap,\n+                                    final Map<Long, ProbeInfo> probeIdMap,\n+                                    final String cohortTableName,\n+                                    final int localSortMaxRecordsInRam,\n+                                    final boolean useCompressedData,\n+                                    final boolean printDebugInformation,\n+                                    final ProgressMeter progressMeter) {\n+\n+        this.df.setMaximumFractionDigits(3);\n+        this.df.setGroupingSize(0);\n+                                \n+        this.localSortMaxRecordsInRam = localSortMaxRecordsInRam;\n+\n+        this.projectID = projectID;\n+        this.vcfWriter = vcfWriter;\n+        this.refSource = refSource;\n+        this.sampleIdMap = sampleIdMap;\n+        this.sampleNames = new HashSet<>(sampleIdMap.values());\n+\n+        this.probeIdMap = probeIdMap;\n+\n+        this.cohortTableRef = new TableReference(cohortTableName, useCompressedData?SchemaUtils.RAW_ARRAY_COHORT_FIELDS_COMPRESSED:SchemaUtils.RAW_ARRAY_COHORT_FIELDS_UNCOMPRESSED);\n+\n+        this.useCompressedData = useCompressedData;\n+        this.printDebugInformation = printDebugInformation;\n+        this.progressMeter = progressMeter;\n+\n+        // KCIBUL: what is the right variant context merger for arrays?\n+        this.variantContextMerger = new ReferenceConfidenceVariantContextMerger(annotationEngine, vcfHeader);\n+\n+    }\n+\n+    int getTotalNumberOfVariants() { return totalNumberOfVariants; }\n+    int getTotalNumberOfSites() { return totalNumberOfSites; }\n+\n+    public void traverse() {\n+        if (printDebugInformation) {\n+            logger.debug(\"using storage api with local sort\");\n+        }\n+        final StorageAPIAvroReader storageAPIAvroReader = new StorageAPIAvroReader(cohortTableRef);\n+        createVariantsFromUngroupedTableResult(storageAPIAvroReader);\n+    }\n+\n+\n+    private void createVariantsFromUngroupedTableResult(final GATKAvroReader avroReader) {\n+\n+        // stream out the data and sort locally\n+        final org.apache.avro.Schema schema = avroReader.getSchema();\n+        final Set<String> columnNames = new HashSet<>();\n+        schema.getFields().forEach(field -> columnNames.add(field.name()));\n+\n+        Comparator<GenericRecord> comparator = this.useCompressedData ? COMPRESSED_PROBE_ID_COMPARATOR : UNCOMPRESSED_PROBE_ID_COMPARATOR;\n+        SortingCollection<GenericRecord> sortingCollection =  getAvroProbeIdSortingCollection(schema, localSortMaxRecordsInRam, comparator);\n+        for ( final GenericRecord queryRow : avroReader ) {\n+            sortingCollection.add(queryRow);\n+        }\n+\n+        sortingCollection.printTempFileStats();\n+\n+        // iterate through records and process them\n+        final List<GenericRecord> currentPositionRecords = new ArrayList<>(sampleIdMap.size() * 2);\n+        long currentProbeId = -1;\n+\n+        for ( final GenericRecord sortedRow : sortingCollection ) {\n+            long probeId;\n+            if (useCompressedData) {\n+                final long rawData = (Long) sortedRow.get(SchemaUtils.RAW_ARRAY_DATA_FIELD_NAME);\n+                RawArrayData data = RawArrayData.decode(rawData);\n+                probeId = data.probeId;\n+            } else {\n+                probeId = (Long) sortedRow.get(\"probe_id\");\n+            }\n+\n+            if ( probeId != currentProbeId && currentProbeId != -1 ) {\n+                ++totalNumberOfSites;\n+                processSampleRecordsForLocation(currentProbeId, currentPositionRecords, columnNames);\n+                currentPositionRecords.clear();\n+            }\n+\n+            currentPositionRecords.add(sortedRow);\n+            currentProbeId = probeId;\n+        }\n+\n+        if ( ! currentPositionRecords.isEmpty() ) {\n+            ++totalNumberOfSites;\n+            processSampleRecordsForLocation(currentProbeId, currentPositionRecords, columnNames);\n+        }\n+    }\n+\n+    private void processSampleRecordsForLocation(final long probeId, final Iterable<GenericRecord> sampleRecordsAtPosition, final Set<String> columnNames) {\n+        final List<VariantContext> unmergedCalls = new ArrayList<>();\n+        final Set<String> currentPositionSamplesSeen = new HashSet<>();\n+        boolean currentPositionHasVariant = false;\n+\n+        final ProbeInfo probeInfo = probeIdMap.get(probeId);\n+        if (probeInfo == null) {\n+            throw new RuntimeException(\"Unable to find probeInfo for \" + probeId);\n+        }\n+\n+        final String contig = probeInfo.contig;\n+        final long position = probeInfo.position;\n+        final Allele refAllele = Allele.create(refSource.queryAndPrefetch(contig, position, position).getBaseString(), true);\n+\n+        int numRecordsAtPosition = 0;\n+\n+        for ( final GenericRecord sampleRecord : sampleRecordsAtPosition ) {\n+            final long sampleId = (Long) sampleRecord.get(SchemaUtils.SAMPLE_ID_FIELD_NAME);\n+\n+            // TODO: handle missing values\n+            String sampleName = sampleIdMap.get((int) sampleId);            \n+            currentPositionSamplesSeen.add(sampleName);\n+\n+            ++numRecordsAtPosition;\n+\n+            if ( printDebugInformation ) {\n+                logger.info(\"\\t\" + contig + \":\" + position + \": found record for sample \" + sampleName + \": \" + sampleRecord);\n+            }\n+\n+            ++totalNumberOfVariants;\n+            unmergedCalls.add(createVariantContextFromSampleRecord(probeInfo, sampleRecord, columnNames, contig, position, sampleName));\n+\n+        }\n+\n+        if ( printDebugInformation ) {\n+            logger.info(contig + \":\" + position + \": processed \" + numRecordsAtPosition + \" total sample records\");\n+        }\n+\n+        finalizeCurrentVariant(unmergedCalls, currentPositionSamplesSeen, contig, position, refAllele);\n+    }\n+\n+    private void finalizeCurrentVariant(final List<VariantContext> unmergedCalls, final Set<String> currentVariantSamplesSeen, final String contig, final long start, final Allele refAllele) {\n+\n+        // TODO: this is where we infer missing data points... once we know what we want to drop\n+        // final Set<String> samplesNotEncountered = Sets.difference(sampleNames, currentVariantSamplesSeen);\n+        // for ( final String missingSample : samplesNotEncountered ) {\n+        //         unmergedCalls.add(createRefSiteVariantContext(missingSample, contig, start, refAllele));\n+        // }\n+\n+        final VariantContext mergedVC = variantContextMerger.merge(\n+                unmergedCalls,\n+                new SimpleInterval(contig, (int) start, (int) start),\n+                refAllele.getBases()[0],\n+                true,\n+                false,\n+                true);\n+\n+\n+        final VariantContext finalVC = mergedVC;\n+\n+        // TODO: this was commented out... probably need to re-enable\n+//        final VariantContext annotatedVC = enableVariantAnnotator ?\n+//                variantAnnotator.annotateContext(finalizedVC, new FeatureContext(), null, null, a -> true): finalVC;\n+\n+//        if ( annotatedVC != null ) {\n+//            vcfWriter.add(annotatedVC);\n+//            progressMeter.update(annotatedVC);\n+//        }\n+\n+        if ( finalVC != null ) {\n+            vcfWriter.add(finalVC);\n+            progressMeter.update(finalVC);\n+        } else {\n+            // TODO should i print a warning here?\n+            vcfWriter.add(mergedVC);\n+            progressMeter.update(mergedVC);\n+        }\n+    }\n+\n+    private String formatFloatForVcf(final Float value) {\n+        if (value == null || Double.isNaN(value)) {\n+            return DOT;\n+        }\n+        return df.format(value);\n+    }\n+\n+    private Float getNullableFloatFromDouble(Object d) {\n+        return d == null ? null : (float)  ((Double) d).doubleValue();\n+    }\n+\n+    private VariantContext createVariantContextFromSampleRecord(final ProbeInfo probeInfo, final GenericRecord sampleRecord, final Set<String> columnNames, final String contig, final long startPosition, final String sample) {\n+        final VariantContextBuilder builder = new VariantContextBuilder();\n+        final GenotypeBuilder genotypeBuilder = new GenotypeBuilder();\n+\n+        builder.chr(contig);\n+        builder.start(startPosition);\n+\n+        \n+        final List<Allele> alleles = new ArrayList<>();\n+        Allele ref = Allele.create(probeInfo.ref, true);        \n+        alleles.add(ref);\n+\n+        Allele alleleA = Allele.create(probeInfo.alleleA, false);\n+        Allele alleleB = Allele.create(probeInfo.alleleB, false);\n+\n+        boolean alleleAisRef = probeInfo.ref.equals(probeInfo.alleleA);\n+        boolean alleleBisRef = probeInfo.ref.equals(probeInfo.alleleB);\n+\n+        if (alleleAisRef) {\n+            alleleA = ref;\n+        } else {\n+            alleles.add(alleleA);\n+        }\n+\n+        if (alleleBisRef) {\n+            alleleB = ref;\n+        } else {\n+            alleles.add(alleleB);\n+        }\n+\n+        builder.alleles(alleles);\n+        builder.stop(startPosition + alleles.get(0).length() - 1);\n+\n+        Float normx;\n+        Float normy;\n+        Float baf;\n+        Float lrr;\n+        List<Allele> genotypeAlleles = new ArrayList<Allele>();\n+\n+        if (this.useCompressedData) {\n+            final RawArrayData data = RawArrayData.decode((Long) sampleRecord.get(SchemaUtils.RAW_ARRAY_DATA_FIELD_NAME));\n+            normx = data.normx;\n+            normy = data.normy;\n+            lrr = data.lrr;\n+            baf = data.baf;\n+\n+            // Genotype -- what about no-call?\n+            if (data.genotype == ArrayGenotype.AA) {\n+                genotypeAlleles.add(alleleA);\n+                genotypeAlleles.add(alleleA);\n+            } else if (data.genotype == ArrayGenotype.AB) {\n+                genotypeAlleles.add(alleleA);\n+                genotypeAlleles.add(alleleB);\n+            } else if  (data.genotype == ArrayGenotype.BB) {\n+                genotypeAlleles.add(alleleB);\n+                genotypeAlleles.add(alleleB);\n+            }\n+        } else {\n+            // TODO: constantize\n+            try {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxODY4Nw=="}, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 317}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NTc2MDY5OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ExtractCohortBQ.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNTo0NjozOVrOGl1bUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNTo0NjozOVrOGl1bUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyNTg0MQ==", "bodyText": "we could add a constant in ProbeInfoSchema for the list of field to get for extract. still need to figure out a package structure that makes sense since right now that class is in ingest", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442325841", "createdAt": "2020-06-18T15:46:39Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ExtractCohortBQ.java", "diffHunk": "@@ -32,10 +47,84 @@\n \n         // Add our samples to our map:\n         for (final FieldValueList row : result.iterateAll()) {\n-            results.add(row.get(0).getStringValue());\n+            results.put((int) row.get(0).getLongValue(), row.get(1).getStringValue());\n+        }\n+\n+        return results;\n+    }\n+\n+    private static String getOptionalString(FieldValue v) {\n+        return (v == null || v.isNull()) ? null : v.getStringValue();\n+    }\n+\n+    public static Map<Long, ProbeInfo> getProbeIdMap(String fqProbeTableName, boolean printDebugInformation) {\n+    \n+        Map<Long, ProbeInfo> results = new HashMap<>();\n+\n+        // Get the query string:\n+        final String sampleListQueryString = \n+            \"SELECT probeId, Name, Chr, Position, Ref, AlleleA, AlleleB\" + \n+            \" FROM `\" + fqProbeTableName + \"`\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NTc3ODk0OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ProbeInfo.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNTo1MDo1MlrOGl1m0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNTo1MDo1MlrOGl1m0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyODc4Ng==", "bodyText": "i can merge the ProbeInfo class under ingest with this one", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442328786", "createdAt": "2020-06-18T15:50:52Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/ProbeInfo.java", "diffHunk": "@@ -0,0 +1,23 @@\n+package org.broadinstitute.hellbender.tools.variantdb;\n+\n+public class ProbeInfo {\n+    long probeId;\n+\n+    String contig;\n+    long position;\n+    String ref;\n+    String alleleA;\n+    String alleleB;\n+    String name;\n+\n+    public ProbeInfo(long probeId, String name, String contig, long position, String ref, String alleleA, String alleleB) {\n+        this.probeId = probeId;\n+        this.name = name;\n+        this.contig = contig;\n+        this.position = position;\n+        this.ref = ref;\n+        this.alleleA = alleleA;\n+        this.alleleB = alleleB;     \n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NTc4MzU5OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/RawArrayData.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNTo1MTo1OFrOGl1psg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxNTo1MTo1OFrOGl1psg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyOTUyMg==", "bodyText": "again, i think we should merge this with the enum in the ingest code (i can do this after the PR is merged)", "url": "https://github.com/broadinstitute/gatk/pull/6666#discussion_r442329522", "createdAt": "2020-06-18T15:51:58Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/variantdb/RawArrayData.java", "diffHunk": "@@ -0,0 +1,58 @@\n+package org.broadinstitute.hellbender.tools.variantdb;\n+\n+import static org.broadinstitute.hellbender.tools.variantdb.BinaryUtils.*;\n+\n+public class RawArrayData {\n+    public static enum ArrayGenotype {\n+        // Order is critical here, the ordinal is the int encoding\n+        AA,AB, BB, NO_CALL\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a624bb377170b30121a16da9f8c5eb6a0bb9b35e"}, "originalPosition": 10}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 993, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}