{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI2ODUzOTQy", "number": 6634, "title": "HaplotypeCaller DRAGEN Genotyper Improvements.", "bodyText": "In this branch are a number of improvements and changes that form the baseline for the current ongoing evaluation of the DRAGEN/GATK pipeline. This represents the joint work of both msyelf and @vruano. The major improvements in this branch are as follows:\n\nEstimateDragstrModelParameters tool for estimating the per-sample/per-STRType errors for use in the HMM gap open/gap close penalties as well as the necessary changes to the PairHMM loading code in order to adjust the model appropriately.\nSupport for using the DragstrParams and flat SNP priors to compute genotype posteriors and the support for using them in the selection of genotypes as well as for computing the QUAL score.\nBase Quality Dropout (BQD) model which penalizes variants with low average base quality scores among genotyped reads and reads that were otherwise excluded from the genotyper. A number of additional arguments to expose internal behaviors in the readThreadingAssembler and HaplotypeCaller have been made in order to support threading more lowBQ reads through to the genotyper.\nForeign Read Detection (FRD) model which uses an adjusted mapping quality score as well as read strandedness information to penalize reads that are likely to have originated from somewhere else on the genome. A number of additional arguments and behaviors have been exposed in order to preserve lower mapping quality reads in the HaplotypeCaller in service.\nDynamic Read Disqualification, allows for longer/lower base quality reads to be less likely to be rejected by eliminating the hard cap on quality scores and further adjusting the limit based on the average base quality for bases in the read.\n\nDesign decisions that I would direct the reviewers attention to as they correspond to potentially dangerous/controversial changes:\n\nBecause FRD/BQD require low quality ends to be included in the models for genotyping, I have added the option to softclipLowQualityEnds (as opposed to their current treatment which involves hardclipping). This has resulted in a lot of code revolving around handling soft reads and making sure that the correct bases get used in the correct places, which often manifests as simply re-clipping the soft-clipped bases where necessary. This might seem expensive but low quality ends are fairly rare and consequently this has a negligible effect on runtime.\n(NOTE: this might cause unintended consequences for annotations, which have not been extensively tested thus far)\nThe DRAGENGenotypeLikelihoodCalculator object is actually an instantiation of the regular GenotypeLikelihoodCalculator object that is called normally for the standard variant model calculation and then has its computed tables/values reused for the subsequent calculations. This means there is a risk if not careful of using the table values for the wrong reads/sties if we are not strict about the state of the cache.\nCurrently in order to lower the mapping quality threshold for HaplotypeCaller two separate arguments must be called. This is because the mapping-quality threshold is checked twice, once for the read filter plugin getToolDefaultArgumentCollections() which gets instantiated before the HaplotypeCaller arguments are populated, and again before assembly. While the functionality to be stricter about mapping quality for assembly compared to active region discovery might be important it is unclear if this matters and perhaps the latter check can be done away with?\nI have added a genotype debugging stream that closely matches the debug output stream from DRAGEN (which itself was a reflection of the GATK3 debug out stream). This involved a lot of threading output writers through the codebase and perhaps this is better handled by the \"--debug\" argument like it used to? Thoughts?\n\nNotes:\n\nIt should be noted that by design all of the added changes to HaplotypeCaller are opt-in, barring errors in implementation.\nThis code is measurably slower than vanilla HaplotypeCaller. In particular FRD is a very expensive step that corresponds to ~5-7% of the runtime. This is in part because it has to duplicate many of the steps in the genotyper based on the number of unique mapping qualities present at a site as well as the fact that it performs an O(n^2) number of operations at sites with many possible alleles. There are options to cut down on the cost of this algorithm that moderately impact the results relative to DRAGEN.\n\nThis implementation is intended to produce results close to the results on DRAGEN 3.4.12 without stripping away the major improvements made in GATK4, as a result there are a number of areas in which we know we are producing different results:\n\nIn GATK4 variants that overlap with an upstream deletion will have added to their alleles list a sybmolic '*' deletion alleles which are genotyped as part of the allele array in the genotyeper. This is not the case in DRAGEN and it interacts with FRD in such a way as to produce a number of variants that in DRAGEN would have been called as 0/1 heterozygous calls with capped QUAL scores, in gatk they are called as 1/2 calls with uncapped quality scores.\nWhile we have added the option to use the legacy assembly region creation code, it is not part of the expected pipeline for running DRAGEN. This includes a number of arguments that were done away with in the recent refactoring pass.\nI have added hooks to revert the assembly engine to approximately its state in gatk3. While I don't stand by that change I think we should bundle the minor assembly engine changes together with our other assembly engine work to try to make a more convincing case.", "createdAt": "2020-06-02T22:07:14Z", "url": "https://github.com/broadinstitute/gatk/pull/6634", "merged": true, "mergeCommit": {"oid": "48afe160c9cfba5a82e40a6be9c8a555066271d1"}, "closed": true, "closedAt": "2020-10-22T16:37:35Z", "author": {"login": "jamesemery"}, "timelineItems": {"totalCount": 29, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcqV2rjAFqTQyNjgwMDQyNQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdVEMOUgFqTUxNDg4ODU1OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2ODAwNDI1", "url": "https://github.com/broadinstitute/gatk/pull/6634#pullrequestreview-426800425", "createdAt": "2020-06-09T05:34:14Z", "commit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "state": "DISMISSED", "comments": {"totalCount": 117, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTozNDoxNFrOGg5W3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQyMTozNjoyNlrOGivhgw==", "hasNextPage": true, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0NzM1Nw==", "bodyText": "overlaps is part of Locatable, so you shouldn't need to convert to SimpleInterval, I think.  And, actually, can this whole method be replaced with Locatable::overlaps?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437147357", "createdAt": "2020-06-09T05:34:14Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/engine/AssemblyRegion.java", "diffHunk": "@@ -231,6 +231,20 @@ public AssemblyRegion trim(final SimpleInterval span, final SimpleInterval padde\n         return result;\n     }\n \n+    /**\n+     * Returns true if read would overlap the extended extent of this region\n+     * @param read the read we want to test\n+     * @return true if read can be added to this region, false otherwise\n+     */\n+    public boolean readOverlapsRegion(final GATKRead read) {\n+        if ( read.isEmpty() || read.getStart() > read.getEnd() ) {\n+            return false;\n+        }\n+\n+        final SimpleInterval readLoc = new SimpleInterval( read );", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0ODI0NA==", "bodyText": "If this only applies with legacy trimming, make a note in the javadoc so we can easily delete later.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437148244", "createdAt": "2020-06-09T05:37:09Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/engine/spark/AssemblyRegionArgumentCollection.java", "diffHunk": "@@ -108,6 +108,12 @@\n     @Argument(fullName= STR_PADDING_LONG_NAME, doc = \"Include at least this many bases around an event for calling STR indels\", optional = true)\n     public int strPaddingForGenotyping = 75;\n \n+    /**\n+     * the maximum extent into the full active region extension that we're willing to go in genotyping our events", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0ODQ2Mw==", "bodyText": "This seems to be an accident.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437148463", "createdAt": "2020-06-09T05:37:45Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/ClipReads.java", "diffHunk": "@@ -371,7 +371,7 @@ private void clipSequences(ReadClipperWithData clipper) {\n                 boolean found = true;   // go through at least once\n                 while (found) {\n                     found = match.find();\n-                    //System.out.printf(\"Matching %s against %s/%s => %b%n\", bases, stc.seq, stc.revSeq, found);\n+//                    System.out.printf(\"Matching %s against %s/%s => %b%n\", bases, stc.seq, stc.revSeq, found);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0OTE1Mw==", "bodyText": "Use MathArrays.ebeSubtract", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437149153", "createdAt": "2020-06-09T05:40:10Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/SimpleGenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SimpleGenotypePriorCalculator implements GenotypePriorCalculator {\n+\n+\n+\n+    private enum AlleleType {\n+        REF, SNP, INDEL, OTHER;\n+    }\n+\n+    private final double[] hetValues;\n+    private final double[] homValues;\n+    private final double[] diffValues;\n+\n+    private SimpleGenotypePriorCalculator(final double snpHet, final double snpHom,\n+                                          final double indelHet, final double indelHom,\n+                                          final double otherHet, final double otherHom) {\n+        hetValues = new double[4];\n+        homValues = new double[4];\n+        diffValues = new double[4];\n+\n+        // SNPs: // * 1/3 since there is three possible mutations for SNPs.\n+        hetValues[1] = snpHet - Math.log10(3);\n+        homValues[1] = snpHom - Math.log10(3);\n+        // INDELs:\n+        hetValues[2] = indelHet;\n+        homValues[2] = indelHom;\n+        // Others:\n+        hetValues[3] = otherHet;\n+        homValues[3] = otherHom;\n+\n+        for (int i = 0; i < 4; i++) {\n+            diffValues[i] = homValues[i] - hetValues[i];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0OTQwMg==", "bodyText": "Use the static constant defined in MathUtils.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437149402", "createdAt": "2020-06-09T05:40:55Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/SimpleGenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SimpleGenotypePriorCalculator implements GenotypePriorCalculator {\n+\n+\n+\n+    private enum AlleleType {\n+        REF, SNP, INDEL, OTHER;\n+    }\n+\n+    private final double[] hetValues;\n+    private final double[] homValues;\n+    private final double[] diffValues;\n+\n+    private SimpleGenotypePriorCalculator(final double snpHet, final double snpHom,\n+                                          final double indelHet, final double indelHom,\n+                                          final double otherHet, final double otherHom) {\n+        hetValues = new double[4];\n+        homValues = new double[4];\n+        diffValues = new double[4];\n+\n+        // SNPs: // * 1/3 since there is three possible mutations for SNPs.\n+        hetValues[1] = snpHet - Math.log10(3);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MDMwOA==", "bodyText": "het_home_ratio should be camel-cased here and elsewhere.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437150308", "createdAt": "2020-06-09T05:43:49Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/SimpleGenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SimpleGenotypePriorCalculator implements GenotypePriorCalculator {\n+\n+\n+\n+    private enum AlleleType {\n+        REF, SNP, INDEL, OTHER;\n+    }\n+\n+    private final double[] hetValues;\n+    private final double[] homValues;\n+    private final double[] diffValues;\n+\n+    private SimpleGenotypePriorCalculator(final double snpHet, final double snpHom,\n+                                          final double indelHet, final double indelHom,\n+                                          final double otherHet, final double otherHom) {\n+        hetValues = new double[4];\n+        homValues = new double[4];\n+        diffValues = new double[4];\n+\n+        // SNPs: // * 1/3 since there is three possible mutations for SNPs.\n+        hetValues[1] = snpHet - Math.log10(3);\n+        homValues[1] = snpHom - Math.log10(3);\n+        // INDELs:\n+        hetValues[2] = indelHet;\n+        homValues[2] = indelHom;\n+        // Others:\n+        hetValues[3] = otherHet;\n+        homValues[3] = otherHom;\n+\n+        for (int i = 0; i < 4; i++) {\n+            diffValues[i] = homValues[i] - hetValues[i];\n+        }\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenHetToHomRatio(final double snpHet, final double indelHet,\n+                                                                   final double otherHet, final double het_hom_ratio) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MTAzMA==", "bodyText": "These can be implemented as EnumMap<AlleleType, Double>, which I believe performs about the same as a List<Double>.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437151030", "createdAt": "2020-06-09T05:46:03Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/SimpleGenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SimpleGenotypePriorCalculator implements GenotypePriorCalculator {\n+\n+\n+\n+    private enum AlleleType {\n+        REF, SNP, INDEL, OTHER;\n+    }\n+\n+    private final double[] hetValues;\n+    private final double[] homValues;\n+    private final double[] diffValues;\n+\n+    private SimpleGenotypePriorCalculator(final double snpHet, final double snpHom,\n+                                          final double indelHet, final double indelHom,\n+                                          final double otherHet, final double otherHom) {\n+        hetValues = new double[4];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MjEzNQ==", "bodyText": "I thought Dragstr was for determining pair-HMM parameters \u2014 that is, for determining the likelihood of sequencing errors.  Why should that have anything to do with genotype priors, which are biological?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437152135", "createdAt": "2020-06-09T05:49:47Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/SimpleGenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SimpleGenotypePriorCalculator implements GenotypePriorCalculator {\n+\n+\n+\n+    private enum AlleleType {\n+        REF, SNP, INDEL, OTHER;\n+    }\n+\n+    private final double[] hetValues;\n+    private final double[] homValues;\n+    private final double[] diffValues;\n+\n+    private SimpleGenotypePriorCalculator(final double snpHet, final double snpHom,\n+                                          final double indelHet, final double indelHom,\n+                                          final double otherHet, final double otherHom) {\n+        hetValues = new double[4];\n+        homValues = new double[4];\n+        diffValues = new double[4];\n+\n+        // SNPs: // * 1/3 since there is three possible mutations for SNPs.\n+        hetValues[1] = snpHet - Math.log10(3);\n+        homValues[1] = snpHom - Math.log10(3);\n+        // INDELs:\n+        hetValues[2] = indelHet;\n+        homValues[2] = indelHom;\n+        // Others:\n+        hetValues[3] = otherHet;\n+        homValues[3] = otherHom;\n+\n+        for (int i = 0; i < 4; i++) {\n+            diffValues[i] = homValues[i] - hetValues[i];\n+        }\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenHetToHomRatio(final double snpHet, final double indelHet,\n+                                                                   final double otherHet, final double het_hom_ratio) {\n+        final double log10Ratio = Math.log10(het_hom_ratio);\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet - log10Ratio,\n+                                                  indelHet, indelHet - log10Ratio,\n+                                                  otherHet, otherHet - log10Ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenDragstrParams(final DragstrParams dragstrParams, final int period,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NTA4MQ==", "bodyText": "result[g] = gac.sumOverAlleleIndicesAndCounts( (idx, cnt) -> \n   cnt == 2 ? homValues[alleleTypes[idx]] : hetValues[alleleTypes[idx]] + diffValues[alleleTypes[idx]] * (cnt - 1)\n);", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437155081", "createdAt": "2020-06-09T05:58:26Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/SimpleGenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SimpleGenotypePriorCalculator implements GenotypePriorCalculator {\n+\n+\n+\n+    private enum AlleleType {\n+        REF, SNP, INDEL, OTHER;\n+    }\n+\n+    private final double[] hetValues;\n+    private final double[] homValues;\n+    private final double[] diffValues;\n+\n+    private SimpleGenotypePriorCalculator(final double snpHet, final double snpHom,\n+                                          final double indelHet, final double indelHom,\n+                                          final double otherHet, final double otherHom) {\n+        hetValues = new double[4];\n+        homValues = new double[4];\n+        diffValues = new double[4];\n+\n+        // SNPs: // * 1/3 since there is three possible mutations for SNPs.\n+        hetValues[1] = snpHet - Math.log10(3);\n+        homValues[1] = snpHom - Math.log10(3);\n+        // INDELs:\n+        hetValues[2] = indelHet;\n+        homValues[2] = indelHom;\n+        // Others:\n+        hetValues[3] = otherHet;\n+        homValues[3] = otherHom;\n+\n+        for (int i = 0; i < 4; i++) {\n+            diffValues[i] = homValues[i] - hetValues[i];\n+        }\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenHetToHomRatio(final double snpHet, final double indelHet,\n+                                                                   final double otherHet, final double het_hom_ratio) {\n+        final double log10Ratio = Math.log10(het_hom_ratio);\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet - log10Ratio,\n+                                                  indelHet, indelHet - log10Ratio,\n+                                                  otherHet, otherHet - log10Ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenDragstrParams(final DragstrParams dragstrParams, final int period,\n+                                                                   final int repeats, final double snpHeterozygosity,\n+                                                                   final double het_hom_ratio) {\n+        final double snpHet = snpHeterozygosity;\n+        final double indelHet = -.1 * dragstrParams.api(period, repeats);\n+        final double otherHet = Math.max(snpHet, indelHet);\n+        return givenHetToHomRatio(snpHet, indelHet, otherHet, het_hom_ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(final double snpHet, final double indelHet) {\n+        return assumingHW(snpHet, indelHet, Math.max(snpHet, indelHet));\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(final double snpHet, final double indelHet,\n+                                                                   final double otherHet) {\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet * 2,\n+                indelHet, indelHet * 2,\n+                otherHet, otherHet * 2);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(GenotypeCalculationArgumentCollection genotypeArgs) {\n+        return assumingHW(Math.log10(genotypeArgs.snpHeterozygosity),\n+                          Math.log10(genotypeArgs.indelHeterozygosity));\n+    }\n+\n+\n+    @Override\n+    public double[] getLog10Priors(final GenotypeLikelihoodCalculator lkCalculator, final List<Allele> alleles) {\n+        final int[] alleleTypes = calculateAlleleTypes(alleles);\n+        final int numberOfGenotypes = lkCalculator.genotypeCount();\n+        final double[] result = new double[numberOfGenotypes];\n+        // implied = result[0] = 0.0;\n+        for (int g = 1; g < numberOfGenotypes; g++) {\n+            final GenotypeAlleleCounts gac = lkCalculator.genotypeAlleleCountsAt(g);\n+            final int numberOfDistictAlleles = gac.distinctAlleleCount();\n+            double log10Sum = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NTI3MA==", "bodyText": "typo", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437155270", "createdAt": "2020-06-09T05:59:02Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/SimpleGenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SimpleGenotypePriorCalculator implements GenotypePriorCalculator {\n+\n+\n+\n+    private enum AlleleType {\n+        REF, SNP, INDEL, OTHER;\n+    }\n+\n+    private final double[] hetValues;\n+    private final double[] homValues;\n+    private final double[] diffValues;\n+\n+    private SimpleGenotypePriorCalculator(final double snpHet, final double snpHom,\n+                                          final double indelHet, final double indelHom,\n+                                          final double otherHet, final double otherHom) {\n+        hetValues = new double[4];\n+        homValues = new double[4];\n+        diffValues = new double[4];\n+\n+        // SNPs: // * 1/3 since there is three possible mutations for SNPs.\n+        hetValues[1] = snpHet - Math.log10(3);\n+        homValues[1] = snpHom - Math.log10(3);\n+        // INDELs:\n+        hetValues[2] = indelHet;\n+        homValues[2] = indelHom;\n+        // Others:\n+        hetValues[3] = otherHet;\n+        homValues[3] = otherHom;\n+\n+        for (int i = 0; i < 4; i++) {\n+            diffValues[i] = homValues[i] - hetValues[i];\n+        }\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenHetToHomRatio(final double snpHet, final double indelHet,\n+                                                                   final double otherHet, final double het_hom_ratio) {\n+        final double log10Ratio = Math.log10(het_hom_ratio);\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet - log10Ratio,\n+                                                  indelHet, indelHet - log10Ratio,\n+                                                  otherHet, otherHet - log10Ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenDragstrParams(final DragstrParams dragstrParams, final int period,\n+                                                                   final int repeats, final double snpHeterozygosity,\n+                                                                   final double het_hom_ratio) {\n+        final double snpHet = snpHeterozygosity;\n+        final double indelHet = -.1 * dragstrParams.api(period, repeats);\n+        final double otherHet = Math.max(snpHet, indelHet);\n+        return givenHetToHomRatio(snpHet, indelHet, otherHet, het_hom_ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(final double snpHet, final double indelHet) {\n+        return assumingHW(snpHet, indelHet, Math.max(snpHet, indelHet));\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(final double snpHet, final double indelHet,\n+                                                                   final double otherHet) {\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet * 2,\n+                indelHet, indelHet * 2,\n+                otherHet, otherHet * 2);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(GenotypeCalculationArgumentCollection genotypeArgs) {\n+        return assumingHW(Math.log10(genotypeArgs.snpHeterozygosity),\n+                          Math.log10(genotypeArgs.indelHeterozygosity));\n+    }\n+\n+\n+    @Override\n+    public double[] getLog10Priors(final GenotypeLikelihoodCalculator lkCalculator, final List<Allele> alleles) {\n+        final int[] alleleTypes = calculateAlleleTypes(alleles);\n+        final int numberOfGenotypes = lkCalculator.genotypeCount();\n+        final double[] result = new double[numberOfGenotypes];\n+        // implied = result[0] = 0.0;\n+        for (int g = 1; g < numberOfGenotypes; g++) {\n+            final GenotypeAlleleCounts gac = lkCalculator.genotypeAlleleCountsAt(g);\n+            final int numberOfDistictAlleles = gac.distinctAlleleCount();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NTU2MQ==", "bodyText": "Use Utils.validateArg", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437155561", "createdAt": "2020-06-09T05:59:55Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/SimpleGenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SimpleGenotypePriorCalculator implements GenotypePriorCalculator {\n+\n+\n+\n+    private enum AlleleType {\n+        REF, SNP, INDEL, OTHER;\n+    }\n+\n+    private final double[] hetValues;\n+    private final double[] homValues;\n+    private final double[] diffValues;\n+\n+    private SimpleGenotypePriorCalculator(final double snpHet, final double snpHom,\n+                                          final double indelHet, final double indelHom,\n+                                          final double otherHet, final double otherHom) {\n+        hetValues = new double[4];\n+        homValues = new double[4];\n+        diffValues = new double[4];\n+\n+        // SNPs: // * 1/3 since there is three possible mutations for SNPs.\n+        hetValues[1] = snpHet - Math.log10(3);\n+        homValues[1] = snpHom - Math.log10(3);\n+        // INDELs:\n+        hetValues[2] = indelHet;\n+        homValues[2] = indelHom;\n+        // Others:\n+        hetValues[3] = otherHet;\n+        homValues[3] = otherHom;\n+\n+        for (int i = 0; i < 4; i++) {\n+            diffValues[i] = homValues[i] - hetValues[i];\n+        }\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenHetToHomRatio(final double snpHet, final double indelHet,\n+                                                                   final double otherHet, final double het_hom_ratio) {\n+        final double log10Ratio = Math.log10(het_hom_ratio);\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet - log10Ratio,\n+                                                  indelHet, indelHet - log10Ratio,\n+                                                  otherHet, otherHet - log10Ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenDragstrParams(final DragstrParams dragstrParams, final int period,\n+                                                                   final int repeats, final double snpHeterozygosity,\n+                                                                   final double het_hom_ratio) {\n+        final double snpHet = snpHeterozygosity;\n+        final double indelHet = -.1 * dragstrParams.api(period, repeats);\n+        final double otherHet = Math.max(snpHet, indelHet);\n+        return givenHetToHomRatio(snpHet, indelHet, otherHet, het_hom_ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(final double snpHet, final double indelHet) {\n+        return assumingHW(snpHet, indelHet, Math.max(snpHet, indelHet));\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(final double snpHet, final double indelHet,\n+                                                                   final double otherHet) {\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet * 2,\n+                indelHet, indelHet * 2,\n+                otherHet, otherHet * 2);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(GenotypeCalculationArgumentCollection genotypeArgs) {\n+        return assumingHW(Math.log10(genotypeArgs.snpHeterozygosity),\n+                          Math.log10(genotypeArgs.indelHeterozygosity));\n+    }\n+\n+\n+    @Override\n+    public double[] getLog10Priors(final GenotypeLikelihoodCalculator lkCalculator, final List<Allele> alleles) {\n+        final int[] alleleTypes = calculateAlleleTypes(alleles);\n+        final int numberOfGenotypes = lkCalculator.genotypeCount();\n+        final double[] result = new double[numberOfGenotypes];\n+        // implied = result[0] = 0.0;\n+        for (int g = 1; g < numberOfGenotypes; g++) {\n+            final GenotypeAlleleCounts gac = lkCalculator.genotypeAlleleCountsAt(g);\n+            final int numberOfDistictAlleles = gac.distinctAlleleCount();\n+            double log10Sum = 0;\n+            for (int a = 0; a < numberOfDistictAlleles; a++) {\n+                final int idx = gac.alleleIndexAt(a);\n+                final int cnt = gac.alleleCountAt(a);\n+                if (cnt == 1) {\n+                    log10Sum += hetValues[alleleTypes[idx]];\n+                } else if (cnt == 2) {\n+                    log10Sum += homValues[alleleTypes[idx]];\n+                } else { // for plodies over 2 and allele counts over 2 then we use the het/hom ratio for the rest\n+                    log10Sum += hetValues[alleleTypes[idx]] + diffValues[alleleTypes[idx]] * (cnt - 1);\n+                }\n+            }\n+            result[g] = log10Sum;\n+        }\n+        return result;\n+    }\n+\n+    private int[] calculateAlleleTypes(final List<Allele> alleles) {\n+        if (alleles.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NTY5Nw==", "bodyText": "Utils.validateArg", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437155697", "createdAt": "2020-06-09T06:00:18Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/SimpleGenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SimpleGenotypePriorCalculator implements GenotypePriorCalculator {\n+\n+\n+\n+    private enum AlleleType {\n+        REF, SNP, INDEL, OTHER;\n+    }\n+\n+    private final double[] hetValues;\n+    private final double[] homValues;\n+    private final double[] diffValues;\n+\n+    private SimpleGenotypePriorCalculator(final double snpHet, final double snpHom,\n+                                          final double indelHet, final double indelHom,\n+                                          final double otherHet, final double otherHom) {\n+        hetValues = new double[4];\n+        homValues = new double[4];\n+        diffValues = new double[4];\n+\n+        // SNPs: // * 1/3 since there is three possible mutations for SNPs.\n+        hetValues[1] = snpHet - Math.log10(3);\n+        homValues[1] = snpHom - Math.log10(3);\n+        // INDELs:\n+        hetValues[2] = indelHet;\n+        homValues[2] = indelHom;\n+        // Others:\n+        hetValues[3] = otherHet;\n+        homValues[3] = otherHom;\n+\n+        for (int i = 0; i < 4; i++) {\n+            diffValues[i] = homValues[i] - hetValues[i];\n+        }\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenHetToHomRatio(final double snpHet, final double indelHet,\n+                                                                   final double otherHet, final double het_hom_ratio) {\n+        final double log10Ratio = Math.log10(het_hom_ratio);\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet - log10Ratio,\n+                                                  indelHet, indelHet - log10Ratio,\n+                                                  otherHet, otherHet - log10Ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenDragstrParams(final DragstrParams dragstrParams, final int period,\n+                                                                   final int repeats, final double snpHeterozygosity,\n+                                                                   final double het_hom_ratio) {\n+        final double snpHet = snpHeterozygosity;\n+        final double indelHet = -.1 * dragstrParams.api(period, repeats);\n+        final double otherHet = Math.max(snpHet, indelHet);\n+        return givenHetToHomRatio(snpHet, indelHet, otherHet, het_hom_ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(final double snpHet, final double indelHet) {\n+        return assumingHW(snpHet, indelHet, Math.max(snpHet, indelHet));\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(final double snpHet, final double indelHet,\n+                                                                   final double otherHet) {\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet * 2,\n+                indelHet, indelHet * 2,\n+                otherHet, otherHet * 2);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(GenotypeCalculationArgumentCollection genotypeArgs) {\n+        return assumingHW(Math.log10(genotypeArgs.snpHeterozygosity),\n+                          Math.log10(genotypeArgs.indelHeterozygosity));\n+    }\n+\n+\n+    @Override\n+    public double[] getLog10Priors(final GenotypeLikelihoodCalculator lkCalculator, final List<Allele> alleles) {\n+        final int[] alleleTypes = calculateAlleleTypes(alleles);\n+        final int numberOfGenotypes = lkCalculator.genotypeCount();\n+        final double[] result = new double[numberOfGenotypes];\n+        // implied = result[0] = 0.0;\n+        for (int g = 1; g < numberOfGenotypes; g++) {\n+            final GenotypeAlleleCounts gac = lkCalculator.genotypeAlleleCountsAt(g);\n+            final int numberOfDistictAlleles = gac.distinctAlleleCount();\n+            double log10Sum = 0;\n+            for (int a = 0; a < numberOfDistictAlleles; a++) {\n+                final int idx = gac.alleleIndexAt(a);\n+                final int cnt = gac.alleleCountAt(a);\n+                if (cnt == 1) {\n+                    log10Sum += hetValues[alleleTypes[idx]];\n+                } else if (cnt == 2) {\n+                    log10Sum += homValues[alleleTypes[idx]];\n+                } else { // for plodies over 2 and allele counts over 2 then we use the het/hom ratio for the rest\n+                    log10Sum += hetValues[alleleTypes[idx]] + diffValues[alleleTypes[idx]] * (cnt - 1);\n+                }\n+            }\n+            result[g] = log10Sum;\n+        }\n+        return result;\n+    }\n+\n+    private int[] calculateAlleleTypes(final List<Allele> alleles) {\n+        if (alleles.isEmpty()) {\n+            throw new IllegalArgumentException(\"there must be at least one allele (the reference)\");\n+        } else {\n+            final int[] result = new int[alleles.size()];\n+            Arrays.fill(result, AlleleType.OTHER.ordinal());\n+            final Allele refAllele = alleles.get(0);\n+            if (!refAllele.isReference()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NjQ0MA==", "bodyText": "two typos", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437156440", "createdAt": "2020-06-09T06:02:44Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeAssignmentMethod.java", "diffHunk": "@@ -41,5 +41,11 @@\n     /**\n      * do not even bother changing the GTs\n      */\n-    DO_NOT_ASSIGN_GENOTYPES\n+    DO_NOT_ASSIGN_GENOTYPES,\n+\n+    /**\n+     * Use posteriors probabilties:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NjU5NQ==", "bodyText": "doc?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437156595", "createdAt": "2020-06-09T06:03:15Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeCalculationArgumentCollection.java", "diffHunk": "@@ -26,6 +27,13 @@\n     public static final int DEFAULT_MAX_ALTERNATE_ALLELES = 6;\n     public static final int DEFAULT_MAX_GENOTYPE_COUNT = 1024;\n \n+    @Argument(fullName=\"dragstr-prior-scale\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1ODIxNA==", "bodyText": "typos", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437158214", "createdAt": "2020-06-09T06:07:54Z", "author": {"login": "davidbenjamin"}, "path": "src/test/java/org/broadinstitute/hellbender/utils/read/ReadUtilsUnitTest.java", "diffHunk": "@@ -244,6 +244,29 @@ public void testReadWithNsRefIndexInDeletion( ) throws FileNotFoundException {\n         Assert.assertEquals(ReadUtils.getReadIndexForReferenceCoordinate(read, 203).getLeft().intValue(), 3);  // the first D base\n     }\n \n+    @Test\n+    public void testReadsWithSoftclipsReadCorrdinatRefCoordinate( ) throws FileNotFoundException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE2NDA3Mw==", "bodyText": "call this resolve\u2026GenotypePriorCalculator", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437164073", "createdAt": "2020-06-09T06:24:06Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerGenotypingEngine.java", "diffHunk": "@@ -202,6 +233,44 @@ public CalledHaplotypes assignGenotypeLikelihoods(final List<Haplotype> haplotyp\n         return new CalledHaplotypes(phasedCalls, calledHaplotypes);\n     }\n \n+    /**\n+     * Confirms wether there is the need to analyze the region's reference sequence for the presence of STRs.\n+     * <p>\n+     *     This is only the case when DRAGstr is activate, we are going to use their priors and there is some indel\n+     *     amongst the haplotypes.\n+     * </p>\n+     */\n+    private boolean isDragstrSTRAnalyzerNecessary(SortedSet<Integer> startPosKeySet, List<Haplotype> haplotypes) {\n+        if (startPosKeySet.isEmpty()) {\n+            return false;\n+        } else if (hcArgs.likelihoodArgs.dragstrParams == null) {\n+            return false;\n+        } else if (hcArgs.standardArgs.genotypeArgs.dontUseDragstrPriors) {\n+            return false;\n+        } else {\n+            for (final Haplotype haplotype : haplotypes) {\n+                for (final VariantContext vc : haplotype.getEventMap().getVariantContexts()) {\n+                    if (GATKVariantContextUtils.containsInlineIndel(vc)) {\n+                        return true;\n+                    }\n+                }\n+            }\n+            return false;\n+        }\n+    }\n+\n+    private GenotypePriorCalculator resolveCustomAlleleFrequencyCalculator(final VariantContext vc, final DragstrReferenceSTRs strs, final int pos, final int ploidy, final double snpHeterozygosity, final double indelHeterozygosity) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgzODY2NA==", "bodyText": "typo", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437838664", "createdAt": "2020-06-10T03:24:37Z", "author": {"login": "davidbenjamin"}, "path": "src/test/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrReferenceSTRsTest.java", "diffHunk": "@@ -0,0 +1,161 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.broadinstitute.hellbender.utils.RandomDNA;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n+\n+\n+/**\n+ * Tool to figure out the period and repeat-length (in units) of STRs in a reference sequence.\n+ * <p>\n+ *    The period and repeat-length of a reference sequence position is determined as follow.\n+ *    The STR unit is solelly determined by the sequence from that base onwards.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgzODg0OA==", "bodyText": "sites' with apostrophe", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437838848", "createdAt": "2020-06-10T03:25:27Z", "author": {"login": "davidbenjamin"}, "path": "src/test/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrReferenceSTRsTest.java", "diffHunk": "@@ -0,0 +1,161 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.broadinstitute.hellbender.utils.RandomDNA;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n+\n+\n+/**\n+ * Tool to figure out the period and repeat-length (in units) of STRs in a reference sequence.\n+ * <p>\n+ *    The period and repeat-length of a reference sequence position is determined as follow.\n+ *    The STR unit is solelly determined by the sequence from that base onwards.\n+ * </p>\n+ * <p>\n+ *     If the backward base sequence contains additional copies of that unit these are added to the repeat-length.\n+ * </p>\n+ * <p>\n+ *     However a larger repeat-length for a different STR unit upstream would effectively being ignored.\n+ * </p>\n+ * <p>\n+ *     All sites period and forward repeat-length are determined in a single pass thru the sequence (O(L * MaxPeriod)).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgzOTA5OQ==", "bodyText": "Why is there so much javadoc in the unit test?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437839099", "createdAt": "2020-06-10T03:26:28Z", "author": {"login": "davidbenjamin"}, "path": "src/test/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrReferenceSTRsTest.java", "diffHunk": "@@ -0,0 +1,161 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.broadinstitute.hellbender.utils.RandomDNA;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n+\n+\n+/**\n+ * Tool to figure out the period and repeat-length (in units) of STRs in a reference sequence.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgzOTk0MQ==", "bodyText": "typo coherce", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437839941", "createdAt": "2020-06-10T03:30:09Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java", "diffHunk": "@@ -177,8 +189,66 @@ public VariantContext calculateGenotypes(final VariantContext vc, final List<Var\n         return builder.genotypes(genotypes).attributes(attributes).make();\n     }\n \n+    protected double nonVariantPresentLog10PosteriorProbability(final GenotypesContext gc) {\n+        return gc.stream()\n+                .map(gt -> gt.getExtendedAttribute(VCFConstants.GENOTYPE_POSTERIORS_KEY))\n+                .mapToDouble(v -> coherceToDouble(v, Double.NaN, true))\n+                .filter(v -> !Double.isNaN(v))\n+                .min().orElse(Double.NaN);\n+    }\n+\n+    private double coherceToDouble(final Object obj, final double defaultValue, final boolean takeFirstElement) {\n+        if (obj == null) {\n+            return defaultValue;\n+        } else if (obj instanceof CharSequence) {\n+            try {\n+                return Double.parseDouble(obj.toString());\n+            } catch (final NumberFormatException ex) {\n+                return defaultValue;\n+            }\n+        } else if (obj instanceof Number) {\n+            return ((Number) obj).doubleValue();\n+        } else if (takeFirstElement) {\n+            if (obj instanceof Collection) {\n+                if( ((Collection)obj).isEmpty()) {\n+                    return defaultValue;\n+                } else if (obj instanceof List) {\n+                    final List<?> asList = (List<?>) obj;\n+                    return coherceToDouble(asList.get(0), defaultValue, false);\n+                } else {\n+                    final Collection<?> collection = (Collection<?>) obj;\n+                    return coherceToDouble(collection.iterator().next(), defaultValue, false);\n+                }\n+            } else if (obj.getClass().isArray()) {\n+                if (obj.getClass().getComponentType().isPrimitive()) {\n+                    if (Array.getLength(obj) == 0) {\n+                        return defaultValue;\n+                    } else {\n+                        return coherceToDouble(Array.get(obj, 1), defaultValue, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0MDQzMg==", "bodyText": "What is this * -.1 about?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437840432", "createdAt": "2020-06-10T03:32:27Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java", "diffHunk": "@@ -168,7 +171,16 @@ public VariantContext calculateGenotypes(final VariantContext vc, final List<Var\n         // create the genotypes\n         //TODO: omit subsetting if output alleles is not a proper subset of vc.getAlleles\n         final GenotypesContext genotypes = outputAlleles.size() == 1 ? GATKVariantContextUtils.subsetToRefOnly(vc, defaultPloidy) :\n-                AlleleSubsettingUtils.subsetAlleles(vc.getGenotypes(), defaultPloidy, vc.getAlleles(), outputAlleles, GenotypeAssignmentMethod.USE_PLS_TO_ASSIGN, vc.getAttributeAsInt(VCFConstants.DEPTH_KEY, 0));\n+                AlleleSubsettingUtils.subsetAlleles(vc.getGenotypes(), defaultPloidy, vc.getAlleles(), outputAlleles, gpc, configuration.genotypeArgs.genotypeAssignmentMethod, vc.getAttributeAsInt(VCFConstants.DEPTH_KEY, 0));\n+\n+        if (configuration.genotypeArgs.usePosteriorProbabilitiesToCalculateQual && hasPosteriors(genotypes)) {\n+            final double log10NoVariantPosterior = nonVariantPresentLog10PosteriorProbability(genotypes) * -.1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0MDgzMw==", "bodyText": "typo coherce", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437840833", "createdAt": "2020-06-10T03:34:25Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java", "diffHunk": "@@ -177,8 +189,66 @@ public VariantContext calculateGenotypes(final VariantContext vc, final List<Var\n         return builder.genotypes(genotypes).attributes(attributes).make();\n     }\n \n+    protected double nonVariantPresentLog10PosteriorProbability(final GenotypesContext gc) {\n+        return gc.stream()\n+                .map(gt -> gt.getExtendedAttribute(VCFConstants.GENOTYPE_POSTERIORS_KEY))\n+                .mapToDouble(v -> coherceToDouble(v, Double.NaN, true))\n+                .filter(v -> !Double.isNaN(v))\n+                .min().orElse(Double.NaN);\n+    }\n+\n+    private double coherceToDouble(final Object obj, final double defaultValue, final boolean takeFirstElement) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0MTM5Ng==", "bodyText": "Are you sure this is needed?  It seems to duplicate a lot of the casting logic in VariantContextGetters::getAttributeAsDouble", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437841396", "createdAt": "2020-06-10T03:37:00Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java", "diffHunk": "@@ -177,8 +189,66 @@ public VariantContext calculateGenotypes(final VariantContext vc, final List<Var\n         return builder.genotypes(genotypes).attributes(attributes).make();\n     }\n \n+    protected double nonVariantPresentLog10PosteriorProbability(final GenotypesContext gc) {\n+        return gc.stream()\n+                .map(gt -> gt.getExtendedAttribute(VCFConstants.GENOTYPE_POSTERIORS_KEY))\n+                .mapToDouble(v -> coherceToDouble(v, Double.NaN, true))\n+                .filter(v -> !Double.isNaN(v))\n+                .min().orElse(Double.NaN);\n+    }\n+\n+    private double coherceToDouble(final Object obj, final double defaultValue, final boolean takeFirstElement) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0MTU4OA==", "bodyText": "Why is taking the min the correct thing to do here?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437841588", "createdAt": "2020-06-10T03:37:53Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java", "diffHunk": "@@ -177,8 +189,66 @@ public VariantContext calculateGenotypes(final VariantContext vc, final List<Var\n         return builder.genotypes(genotypes).attributes(attributes).make();\n     }\n \n+    protected double nonVariantPresentLog10PosteriorProbability(final GenotypesContext gc) {\n+        return gc.stream()\n+                .map(gt -> gt.getExtendedAttribute(VCFConstants.GENOTYPE_POSTERIORS_KEY))\n+                .mapToDouble(v -> coherceToDouble(v, Double.NaN, true))\n+                .filter(v -> !Double.isNaN(v))\n+                .min().orElse(Double.NaN);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0MTczNw==", "bodyText": "I think you could make this a one-liner with Stream.anyMatch.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437841737", "createdAt": "2020-06-10T03:38:34Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java", "diffHunk": "@@ -177,8 +189,66 @@ public VariantContext calculateGenotypes(final VariantContext vc, final List<Var\n         return builder.genotypes(genotypes).attributes(attributes).make();\n     }\n \n+    protected double nonVariantPresentLog10PosteriorProbability(final GenotypesContext gc) {\n+        return gc.stream()\n+                .map(gt -> gt.getExtendedAttribute(VCFConstants.GENOTYPE_POSTERIORS_KEY))\n+                .mapToDouble(v -> coherceToDouble(v, Double.NaN, true))\n+                .filter(v -> !Double.isNaN(v))\n+                .min().orElse(Double.NaN);\n+    }\n+\n+    private double coherceToDouble(final Object obj, final double defaultValue, final boolean takeFirstElement) {\n+        if (obj == null) {\n+            return defaultValue;\n+        } else if (obj instanceof CharSequence) {\n+            try {\n+                return Double.parseDouble(obj.toString());\n+            } catch (final NumberFormatException ex) {\n+                return defaultValue;\n+            }\n+        } else if (obj instanceof Number) {\n+            return ((Number) obj).doubleValue();\n+        } else if (takeFirstElement) {\n+            if (obj instanceof Collection) {\n+                if( ((Collection)obj).isEmpty()) {\n+                    return defaultValue;\n+                } else if (obj instanceof List) {\n+                    final List<?> asList = (List<?>) obj;\n+                    return coherceToDouble(asList.get(0), defaultValue, false);\n+                } else {\n+                    final Collection<?> collection = (Collection<?>) obj;\n+                    return coherceToDouble(collection.iterator().next(), defaultValue, false);\n+                }\n+            } else if (obj.getClass().isArray()) {\n+                if (obj.getClass().getComponentType().isPrimitive()) {\n+                    if (Array.getLength(obj) == 0) {\n+                        return defaultValue;\n+                    } else {\n+                        return coherceToDouble(Array.get(obj, 1), defaultValue, false);\n+                    }\n+                } else {\n+                    final Object[] array = (Object[]) obj;\n+                    return array.length != 0 ? coherceToDouble(array[0], defaultValue, false) : defaultValue;\n+                }\n+            } else {\n+                return defaultValue;\n+            }\n+        } else {\n+            return defaultValue;\n+        }\n+    }\n+\n+    private boolean hasPosteriors(final GenotypesContext gc) {\n+        for (final Genotype genotype : gc) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0MTkyNg==", "bodyText": "I think Genotyper or GenotypingModel would be a better name.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437841926", "createdAt": "2020-06-10T03:39:25Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypersModel.java", "diffHunk": "@@ -0,0 +1,19 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrUtils;\n+\n+import java.io.PrintStream;\n+\n+/**\n+ * A wrapping interface between the various versions of genotypers so as to keep them interchangeable.\n+ */\n+public interface GenotypersModel {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0MjM1NA==", "bodyText": "Round largeGenotypeCount to the nearest long.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437842354", "createdAt": "2020-06-10T03:41:27Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculators.java", "diffHunk": "@@ -270,6 +270,30 @@ public synchronized GenotypeLikelihoodCalculator getInstance(final int ploidy, f\n         return new GenotypeLikelihoodCalculator(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n     }\n \n+    /**\n+     * Returns an instance of the DRAGEN genotypeLikelihoodCalculator given its ploidy and the number of alleles.\n+     *\n+     * @param alleleCount the required allele-count.\n+     * @param ploidy the required ploidy-count.\n+     *\n+     * @throws IllegalArgumentException if either {@code ploidy} or {@code alleleCount} is negative, or the resulting number of genotypes is too large.\n+     *\n+     * @return never {@code null}.\n+     */\n+    public synchronized GenotypeLikelihoodCalculatorDRAGEN getInstanceDRAGEN(final int ploidy, final int alleleCount) {\n+        Utils.validate(ploidy == 2, \"DRAGEN genotyping mode currently only supports diploid samples\");\n+        checkPloidyAndMaximumAllele(ploidy, alleleCount);\n+\n+        if (calculateGenotypeCountUsingTables(ploidy, alleleCount) == GENOTYPE_COUNT_OVERFLOW) {\n+            final double largeGenotypeCount = Math.pow(10, MathUtils.log10BinomialCoefficient(ploidy + alleleCount - 1, alleleCount - 1));\n+            throw new IllegalArgumentException(String.format(\"the number of genotypes is too large for ploidy %d and allele %d: approx. %.0f\", ploidy, alleleCount, largeGenotypeCount));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0MjU5Mw==", "bodyText": "incomplete comment", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437842593", "createdAt": "2020-06-10T03:42:38Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0NDAwMw==", "bodyText": "My vote is for logger.debug instead of an extra, possibly-null, variable.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437844003", "createdAt": "2020-06-10T03:48:54Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1MTc3NQ==", "bodyText": "Can these be final?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437851775", "createdAt": "2020-06-10T04:22:33Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1MjIyMg==", "bodyText": "How about (readForSample.isReverseStrand() ? strandReverse : strandForward).add(. . .)", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437852222", "createdAt": "2020-06-10T04:24:27Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            List<DragenReadContainer> strandForward = new ArrayList<>();\n+            List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                if (readForSample.isReverseStrand()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1MjQ1Mw==", "bodyText": "typo Comparitor", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437852453", "createdAt": "2020-06-10T04:25:22Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            List<DragenReadContainer> strandForward = new ArrayList<>();\n+            List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                if (readForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                }\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                if (filteredReadForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                }\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparitor());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1MjQ5Mw==", "bodyText": "ditto about ternary", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437852493", "createdAt": "2020-06-10T04:25:31Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            List<DragenReadContainer> strandForward = new ArrayList<>();\n+            List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                if (readForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                }\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                if (filteredReadForSample.isReverseStrand()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1Mjg5NQ==", "bodyText": "typo liklihoods", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437852895", "createdAt": "2020-06-10T04:27:18Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            List<DragenReadContainer> strandForward = new ArrayList<>();\n+            List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                if (readForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                }\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                if (filteredReadForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                }\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparitor());\n+            strandReverse.sort(new ReadFeatherEndRevereseComparitor());\n+\n+            // Compute default liklihoods as normal (before we go ahead and alter the liklihoods for the call)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1MzA2Mg==", "bodyText": "more reasons to vote for logger.debug.  This is getting messy.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437853062", "createdAt": "2020-06-10T04:28:04Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            List<DragenReadContainer> strandForward = new ArrayList<>();\n+            List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                if (readForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                }\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                if (filteredReadForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                }\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparitor());\n+            strandReverse.sort(new ReadFeatherEndRevereseComparitor());\n+\n+            // Compute default liklihoods as normal (before we go ahead and alter the liklihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+            if (genotyperDebugStream != null) {\n+                likelihoodsCalculator.addGenotyperDebugOutputStream(genotyperDebugStream);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1MzE5Ng==", "bodyText": "typo ployidy", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437853196", "createdAt": "2020-06-10T04:28:39Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            List<DragenReadContainer> strandForward = new ArrayList<>();\n+            List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                if (readForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                }\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                if (filteredReadForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                }\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparitor());\n+            strandReverse.sort(new ReadFeatherEndRevereseComparitor());\n+\n+            // Compute default liklihoods as normal (before we go ahead and alter the liklihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+            if (genotyperDebugStream != null) {\n+                likelihoodsCalculator.addGenotyperDebugOutputStream(genotyperDebugStream);\n+            }\n+\n+            // this is the data array for the read liklihoods without any trouble\n+            final LikelihoodMatrix<GATKRead, A> sampleLikelihoods = alleleLikelihoodMatrixMapper.mapAlleles(data.readLikelihoods().sampleMatrix(sampleIndex));\n+            final double[] ployidyModelGenotypeLikelihoods = likelihoodsCalculator.rawGenotypeLikelihoods(sampleLikelihoods);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1MzUwMQ==", "bodyText": "typo ployidy", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437853501", "createdAt": "2020-06-10T04:29:56Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            List<DragenReadContainer> strandForward = new ArrayList<>();\n+            List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                if (readForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                }\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                if (filteredReadForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                }\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparitor());\n+            strandReverse.sort(new ReadFeatherEndRevereseComparitor());\n+\n+            // Compute default liklihoods as normal (before we go ahead and alter the liklihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+            if (genotyperDebugStream != null) {\n+                likelihoodsCalculator.addGenotyperDebugOutputStream(genotyperDebugStream);\n+            }\n+\n+            // this is the data array for the read liklihoods without any trouble\n+            final LikelihoodMatrix<GATKRead, A> sampleLikelihoods = alleleLikelihoodMatrixMapper.mapAlleles(data.readLikelihoods().sampleMatrix(sampleIndex));\n+            final double[] ployidyModelGenotypeLikelihoods = likelihoodsCalculator.rawGenotypeLikelihoods(sampleLikelihoods);\n+\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"\\n Standard Genotyping Resutls:\");\n+                genotyperDebugStream.println(Arrays.toString(ployidyModelGenotypeLikelihoods));\n+            }\n+\n+            double[] BQDCallResults = null;\n+            double[] FRDCallResults = null;\n+            if (computeBQD) {\n+                BQDCallResults = likelihoodsCalculator.calculateBQDLikelihoods(sampleLikelihoods, strandForward, strandReverse, paddedReference, offsetForRefIntoEvent, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"BQD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(BQDCallResults));\n+                }\n+            }\n+            if (computeFRD) {\n+                FRDCallResults = likelihoodsCalculator.calculateFRDLikelihoods(sampleLikelihoods, ployidyModelGenotypeLikelihoods,\n+                        Stream.of(strandForward, strandReverse).flatMap(Collection::stream).collect(Collectors.toList()), // We filter out the HMM filtered reads as they do not apply to FRD\n+                        FLAT_SNP_HET_PRIOR, api, maxEffectiveDepthAdjustment, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"FRD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(FRDCallResults));\n+                }\n+            }\n+\n+            //make synthesized likelihoods object (NOTE that we can do this since for invalid model GT fields we simply infinity out the result in the array)\n+            for (int gt = 0; gt < ployidyModelGenotypeLikelihoods.length; gt++) {\n+                if (computeBQD) {\n+                    ployidyModelGenotypeLikelihoods[gt] = Math.max(ployidyModelGenotypeLikelihoods[gt], BQDCallResults[gt]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1Mzg4OA==", "bodyText": "Given that AlleleLikelihoodsis mutable, do we have any safeguards against the brittleness of storing this index?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437853888", "createdAt": "2020-06-10T04:31:27Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            List<DragenReadContainer> strandForward = new ArrayList<>();\n+            List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                if (readForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                }\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                if (filteredReadForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                }\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparitor());\n+            strandReverse.sort(new ReadFeatherEndRevereseComparitor());\n+\n+            // Compute default liklihoods as normal (before we go ahead and alter the liklihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+            if (genotyperDebugStream != null) {\n+                likelihoodsCalculator.addGenotyperDebugOutputStream(genotyperDebugStream);\n+            }\n+\n+            // this is the data array for the read liklihoods without any trouble\n+            final LikelihoodMatrix<GATKRead, A> sampleLikelihoods = alleleLikelihoodMatrixMapper.mapAlleles(data.readLikelihoods().sampleMatrix(sampleIndex));\n+            final double[] ployidyModelGenotypeLikelihoods = likelihoodsCalculator.rawGenotypeLikelihoods(sampleLikelihoods);\n+\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"\\n Standard Genotyping Resutls:\");\n+                genotyperDebugStream.println(Arrays.toString(ployidyModelGenotypeLikelihoods));\n+            }\n+\n+            double[] BQDCallResults = null;\n+            double[] FRDCallResults = null;\n+            if (computeBQD) {\n+                BQDCallResults = likelihoodsCalculator.calculateBQDLikelihoods(sampleLikelihoods, strandForward, strandReverse, paddedReference, offsetForRefIntoEvent, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"BQD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(BQDCallResults));\n+                }\n+            }\n+            if (computeFRD) {\n+                FRDCallResults = likelihoodsCalculator.calculateFRDLikelihoods(sampleLikelihoods, ployidyModelGenotypeLikelihoods,\n+                        Stream.of(strandForward, strandReverse).flatMap(Collection::stream).collect(Collectors.toList()), // We filter out the HMM filtered reads as they do not apply to FRD\n+                        FLAT_SNP_HET_PRIOR, api, maxEffectiveDepthAdjustment, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"FRD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(FRDCallResults));\n+                }\n+            }\n+\n+            //make synthesized likelihoods object (NOTE that we can do this since for invalid model GT fields we simply infinity out the result in the array)\n+            for (int gt = 0; gt < ployidyModelGenotypeLikelihoods.length; gt++) {\n+                if (computeBQD) {\n+                    ployidyModelGenotypeLikelihoods[gt] = Math.max(ployidyModelGenotypeLikelihoods[gt], BQDCallResults[gt]);\n+                }\n+                if (computeFRD) {\n+                    ployidyModelGenotypeLikelihoods[gt] = Math.max(ployidyModelGenotypeLikelihoods[gt], FRDCallResults[gt]);\n+                }\n+            }\n+\n+            // this is what the work actually is, after we have computed a few things\n+            genotypeLikelihoods.add(GenotypeLikelihoods.fromLog10Likelihoods(ployidyModelGenotypeLikelihoods));\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"merged matrix:\");\n+                genotyperDebugStream.println(Arrays.toString(ployidyModelGenotypeLikelihoods));\n+            }\n+        }\n+        return new GenotypingLikelihoods<>(genotypingAlleles, ploidyModel, genotypeLikelihoods);\n+    }\n+\n+    // Adding the debug stream managed by the haplotype caller engine\n+    @Override\n+    public void addDebugOutStream(final PrintStream debugStream) {\n+        this.genotyperDebugStream = debugStream;\n+    }\n+\n+    private GenotypeLikelihoodCalculatorDRAGEN getLikelihoodsCalculator(final int samplePloidy, final int alleleCount) {\n+        if (samplePloidy >= cachePloidyCapacity || alleleCount >= cacheAlleleCountCapacity) {\n+            return calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+        }\n+        final GenotypeLikelihoodCalculatorDRAGEN result = likelihoodCalculators[samplePloidy][alleleCount];\n+        if (result != null) {\n+            return result;\n+        } else {\n+            final GenotypeLikelihoodCalculatorDRAGEN newOne = calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+            likelihoodCalculators[samplePloidy][alleleCount] = newOne;\n+            return newOne;\n+        }\n+    }\n+\n+    /**\n+     * This helper class is used to store the necessary data in order to sort a read based on its BQD \"feather end\" as\n+     * well as information relevant to re-associate the read with its position in the AlleleLikelihoods object arrays.\n+     */\n+    static class DragenReadContainer {\n+        final public GATKRead underlyingRead;\n+        final int offsetIntoReadForBaseQuality;\n+        final int unclippedEnd;\n+        final int indexInLikelihoodsObject;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 223}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NDA0Nw==", "bodyText": "String.format", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437854047", "createdAt": "2020-06-10T04:32:06Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            List<DragenReadContainer> strandForward = new ArrayList<>();\n+            List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                if (readForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                }\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                if (filteredReadForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                }\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparitor());\n+            strandReverse.sort(new ReadFeatherEndRevereseComparitor());\n+\n+            // Compute default liklihoods as normal (before we go ahead and alter the liklihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+            if (genotyperDebugStream != null) {\n+                likelihoodsCalculator.addGenotyperDebugOutputStream(genotyperDebugStream);\n+            }\n+\n+            // this is the data array for the read liklihoods without any trouble\n+            final LikelihoodMatrix<GATKRead, A> sampleLikelihoods = alleleLikelihoodMatrixMapper.mapAlleles(data.readLikelihoods().sampleMatrix(sampleIndex));\n+            final double[] ployidyModelGenotypeLikelihoods = likelihoodsCalculator.rawGenotypeLikelihoods(sampleLikelihoods);\n+\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"\\n Standard Genotyping Resutls:\");\n+                genotyperDebugStream.println(Arrays.toString(ployidyModelGenotypeLikelihoods));\n+            }\n+\n+            double[] BQDCallResults = null;\n+            double[] FRDCallResults = null;\n+            if (computeBQD) {\n+                BQDCallResults = likelihoodsCalculator.calculateBQDLikelihoods(sampleLikelihoods, strandForward, strandReverse, paddedReference, offsetForRefIntoEvent, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"BQD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(BQDCallResults));\n+                }\n+            }\n+            if (computeFRD) {\n+                FRDCallResults = likelihoodsCalculator.calculateFRDLikelihoods(sampleLikelihoods, ployidyModelGenotypeLikelihoods,\n+                        Stream.of(strandForward, strandReverse).flatMap(Collection::stream).collect(Collectors.toList()), // We filter out the HMM filtered reads as they do not apply to FRD\n+                        FLAT_SNP_HET_PRIOR, api, maxEffectiveDepthAdjustment, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"FRD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(FRDCallResults));\n+                }\n+            }\n+\n+            //make synthesized likelihoods object (NOTE that we can do this since for invalid model GT fields we simply infinity out the result in the array)\n+            for (int gt = 0; gt < ployidyModelGenotypeLikelihoods.length; gt++) {\n+                if (computeBQD) {\n+                    ployidyModelGenotypeLikelihoods[gt] = Math.max(ployidyModelGenotypeLikelihoods[gt], BQDCallResults[gt]);\n+                }\n+                if (computeFRD) {\n+                    ployidyModelGenotypeLikelihoods[gt] = Math.max(ployidyModelGenotypeLikelihoods[gt], FRDCallResults[gt]);\n+                }\n+            }\n+\n+            // this is what the work actually is, after we have computed a few things\n+            genotypeLikelihoods.add(GenotypeLikelihoods.fromLog10Likelihoods(ployidyModelGenotypeLikelihoods));\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"merged matrix:\");\n+                genotyperDebugStream.println(Arrays.toString(ployidyModelGenotypeLikelihoods));\n+            }\n+        }\n+        return new GenotypingLikelihoods<>(genotypingAlleles, ploidyModel, genotypeLikelihoods);\n+    }\n+\n+    // Adding the debug stream managed by the haplotype caller engine\n+    @Override\n+    public void addDebugOutStream(final PrintStream debugStream) {\n+        this.genotyperDebugStream = debugStream;\n+    }\n+\n+    private GenotypeLikelihoodCalculatorDRAGEN getLikelihoodsCalculator(final int samplePloidy, final int alleleCount) {\n+        if (samplePloidy >= cachePloidyCapacity || alleleCount >= cacheAlleleCountCapacity) {\n+            return calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+        }\n+        final GenotypeLikelihoodCalculatorDRAGEN result = likelihoodCalculators[samplePloidy][alleleCount];\n+        if (result != null) {\n+            return result;\n+        } else {\n+            final GenotypeLikelihoodCalculatorDRAGEN newOne = calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+            likelihoodCalculators[samplePloidy][alleleCount] = newOne;\n+            return newOne;\n+        }\n+    }\n+\n+    /**\n+     * This helper class is used to store the necessary data in order to sort a read based on its BQD \"feather end\" as\n+     * well as information relevant to re-associate the read with its position in the AlleleLikelihoods object arrays.\n+     */\n+    static class DragenReadContainer {\n+        final public GATKRead underlyingRead;\n+        final int offsetIntoReadForBaseQuality;\n+        final int unclippedEnd;\n+        final int indexInLikelihoodsObject;\n+\n+        // Transient value used to store thresholds for FRD\n+       double phredPFValue = 0;\n+\n+\n+        public DragenReadContainer(final GATKRead underlyingRead, final int offsetIntoReadForBaseQuality, final int unclippedEnd, final int indexInLikelihoodsObject) {\n+            this.underlyingRead = underlyingRead;\n+            this.offsetIntoReadForBaseQuality = offsetIntoReadForBaseQuality;\n+            this.unclippedEnd = unclippedEnd;\n+            this.indexInLikelihoodsObject = indexInLikelihoodsObject;\n+        }\n+\n+        public int getUnclippedPosition() {\n+            return unclippedEnd;\n+        }\n+\n+        public int getIndexInLikelihoodsObject() {\n+            return indexInLikelihoodsObject;\n+        }\n+\n+        public boolean wasFilteredByHMM() {\n+            return this.getIndexInLikelihoodsObject() == -1;\n+        }\n+\n+        public boolean hasValidBaseQuality() {\n+            return offsetIntoReadForBaseQuality != -1;\n+        }\n+\n+        public int getBaseQuality() {\n+            return underlyingRead.getBaseQuality(offsetIntoReadForBaseQuality);\n+        }\n+\n+        public double getPhredScaledMappingQuality() {\n+            return DRAGENMappingQualityReadTransformer.mapValue(underlyingRead.getMappingQuality());\n+        }\n+\n+        public double getPhredPFValue() {\n+            return phredPFValue;\n+        }\n+\n+        public void setPhredPFValue(double phredPFValue) {\n+            this.phredPFValue = phredPFValue;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"Read: \"+underlyingRead.toString()+\" index: \"+indexInLikelihoodsObject+\" at unclipped end: \"+unclippedEnd+\" with base quality \"+(hasValidBaseQuality() ? getBaseQuality() : -1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 270}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NDEyOQ==", "bodyText": "typo Comparitor", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437854129", "createdAt": "2020-06-10T04:32:27Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            List<DragenReadContainer> strandForward = new ArrayList<>();\n+            List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                if (readForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                }\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                if (filteredReadForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                }\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparitor());\n+            strandReverse.sort(new ReadFeatherEndRevereseComparitor());\n+\n+            // Compute default liklihoods as normal (before we go ahead and alter the liklihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+            if (genotyperDebugStream != null) {\n+                likelihoodsCalculator.addGenotyperDebugOutputStream(genotyperDebugStream);\n+            }\n+\n+            // this is the data array for the read liklihoods without any trouble\n+            final LikelihoodMatrix<GATKRead, A> sampleLikelihoods = alleleLikelihoodMatrixMapper.mapAlleles(data.readLikelihoods().sampleMatrix(sampleIndex));\n+            final double[] ployidyModelGenotypeLikelihoods = likelihoodsCalculator.rawGenotypeLikelihoods(sampleLikelihoods);\n+\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"\\n Standard Genotyping Resutls:\");\n+                genotyperDebugStream.println(Arrays.toString(ployidyModelGenotypeLikelihoods));\n+            }\n+\n+            double[] BQDCallResults = null;\n+            double[] FRDCallResults = null;\n+            if (computeBQD) {\n+                BQDCallResults = likelihoodsCalculator.calculateBQDLikelihoods(sampleLikelihoods, strandForward, strandReverse, paddedReference, offsetForRefIntoEvent, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"BQD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(BQDCallResults));\n+                }\n+            }\n+            if (computeFRD) {\n+                FRDCallResults = likelihoodsCalculator.calculateFRDLikelihoods(sampleLikelihoods, ployidyModelGenotypeLikelihoods,\n+                        Stream.of(strandForward, strandReverse).flatMap(Collection::stream).collect(Collectors.toList()), // We filter out the HMM filtered reads as they do not apply to FRD\n+                        FLAT_SNP_HET_PRIOR, api, maxEffectiveDepthAdjustment, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"FRD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(FRDCallResults));\n+                }\n+            }\n+\n+            //make synthesized likelihoods object (NOTE that we can do this since for invalid model GT fields we simply infinity out the result in the array)\n+            for (int gt = 0; gt < ployidyModelGenotypeLikelihoods.length; gt++) {\n+                if (computeBQD) {\n+                    ployidyModelGenotypeLikelihoods[gt] = Math.max(ployidyModelGenotypeLikelihoods[gt], BQDCallResults[gt]);\n+                }\n+                if (computeFRD) {\n+                    ployidyModelGenotypeLikelihoods[gt] = Math.max(ployidyModelGenotypeLikelihoods[gt], FRDCallResults[gt]);\n+                }\n+            }\n+\n+            // this is what the work actually is, after we have computed a few things\n+            genotypeLikelihoods.add(GenotypeLikelihoods.fromLog10Likelihoods(ployidyModelGenotypeLikelihoods));\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"merged matrix:\");\n+                genotyperDebugStream.println(Arrays.toString(ployidyModelGenotypeLikelihoods));\n+            }\n+        }\n+        return new GenotypingLikelihoods<>(genotypingAlleles, ploidyModel, genotypeLikelihoods);\n+    }\n+\n+    // Adding the debug stream managed by the haplotype caller engine\n+    @Override\n+    public void addDebugOutStream(final PrintStream debugStream) {\n+        this.genotyperDebugStream = debugStream;\n+    }\n+\n+    private GenotypeLikelihoodCalculatorDRAGEN getLikelihoodsCalculator(final int samplePloidy, final int alleleCount) {\n+        if (samplePloidy >= cachePloidyCapacity || alleleCount >= cacheAlleleCountCapacity) {\n+            return calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+        }\n+        final GenotypeLikelihoodCalculatorDRAGEN result = likelihoodCalculators[samplePloidy][alleleCount];\n+        if (result != null) {\n+            return result;\n+        } else {\n+            final GenotypeLikelihoodCalculatorDRAGEN newOne = calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+            likelihoodCalculators[samplePloidy][alleleCount] = newOne;\n+            return newOne;\n+        }\n+    }\n+\n+    /**\n+     * This helper class is used to store the necessary data in order to sort a read based on its BQD \"feather end\" as\n+     * well as information relevant to re-associate the read with its position in the AlleleLikelihoods object arrays.\n+     */\n+    static class DragenReadContainer {\n+        final public GATKRead underlyingRead;\n+        final int offsetIntoReadForBaseQuality;\n+        final int unclippedEnd;\n+        final int indexInLikelihoodsObject;\n+\n+        // Transient value used to store thresholds for FRD\n+       double phredPFValue = 0;\n+\n+\n+        public DragenReadContainer(final GATKRead underlyingRead, final int offsetIntoReadForBaseQuality, final int unclippedEnd, final int indexInLikelihoodsObject) {\n+            this.underlyingRead = underlyingRead;\n+            this.offsetIntoReadForBaseQuality = offsetIntoReadForBaseQuality;\n+            this.unclippedEnd = unclippedEnd;\n+            this.indexInLikelihoodsObject = indexInLikelihoodsObject;\n+        }\n+\n+        public int getUnclippedPosition() {\n+            return unclippedEnd;\n+        }\n+\n+        public int getIndexInLikelihoodsObject() {\n+            return indexInLikelihoodsObject;\n+        }\n+\n+        public boolean wasFilteredByHMM() {\n+            return this.getIndexInLikelihoodsObject() == -1;\n+        }\n+\n+        public boolean hasValidBaseQuality() {\n+            return offsetIntoReadForBaseQuality != -1;\n+        }\n+\n+        public int getBaseQuality() {\n+            return underlyingRead.getBaseQuality(offsetIntoReadForBaseQuality);\n+        }\n+\n+        public double getPhredScaledMappingQuality() {\n+            return DRAGENMappingQualityReadTransformer.mapValue(underlyingRead.getMappingQuality());\n+        }\n+\n+        public double getPhredPFValue() {\n+            return phredPFValue;\n+        }\n+\n+        public void setPhredPFValue(double phredPFValue) {\n+            this.phredPFValue = phredPFValue;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"Read: \"+underlyingRead.toString()+\" index: \"+indexInLikelihoodsObject+\" at unclipped end: \"+unclippedEnd+\" with base quality \"+(hasValidBaseQuality() ? getBaseQuality() : -1);\n+        }\n+\n+        public boolean isReverseStrand() {\n+            return underlyingRead.isReverseStrand();\n+        }\n+    }\n+\n+    //MAJOR TODO THIS IS CURRENTLY BASED OFF OF THE REFERENCE UNCLIPPED START AND NOT THE BASES IN THE READ CONSEQUENTLY AT SITES WITH\n+    //      TODO INDELS PRESENT WE ARE GOING TO BE LOOKING AT THE WRONG OFFSETS FOR THIS SORT... a minor issue but still...\n+\n+    // Orders the reads based on the number of bases there are to the left of the fatherEndComparisonLocation as aligned according to the cigar\n+    // NOTE: here we compare the un-hardclipped edges for these reads as the model itself cares about the cycle count of the sequencer, and\n+    //       importantly this saves us having the thread the original alignment of these reads to this level, since by this point we have trimmed\n+    //       the reads twice, once to the active region with padding and again to the callable region within the active window and in both of these\n+    //       cases we have deleted bases with hardclips.\n+    public class ReadFeatherEndForwardComparitor implements Comparator<DragenReadContainer>, Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NDI3NQ==", "bodyText": "typo Reverese", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437854275", "createdAt": "2020-06-10T04:33:00Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            List<DragenReadContainer> strandForward = new ArrayList<>();\n+            List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                if (readForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+                }\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                if (filteredReadForSample.isReverseStrand()) {\n+                    strandReverse.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                } else {\n+                    strandForward.add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+                }\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparitor());\n+            strandReverse.sort(new ReadFeatherEndRevereseComparitor());\n+\n+            // Compute default liklihoods as normal (before we go ahead and alter the liklihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+            if (genotyperDebugStream != null) {\n+                likelihoodsCalculator.addGenotyperDebugOutputStream(genotyperDebugStream);\n+            }\n+\n+            // this is the data array for the read liklihoods without any trouble\n+            final LikelihoodMatrix<GATKRead, A> sampleLikelihoods = alleleLikelihoodMatrixMapper.mapAlleles(data.readLikelihoods().sampleMatrix(sampleIndex));\n+            final double[] ployidyModelGenotypeLikelihoods = likelihoodsCalculator.rawGenotypeLikelihoods(sampleLikelihoods);\n+\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"\\n Standard Genotyping Resutls:\");\n+                genotyperDebugStream.println(Arrays.toString(ployidyModelGenotypeLikelihoods));\n+            }\n+\n+            double[] BQDCallResults = null;\n+            double[] FRDCallResults = null;\n+            if (computeBQD) {\n+                BQDCallResults = likelihoodsCalculator.calculateBQDLikelihoods(sampleLikelihoods, strandForward, strandReverse, paddedReference, offsetForRefIntoEvent, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"BQD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(BQDCallResults));\n+                }\n+            }\n+            if (computeFRD) {\n+                FRDCallResults = likelihoodsCalculator.calculateFRDLikelihoods(sampleLikelihoods, ployidyModelGenotypeLikelihoods,\n+                        Stream.of(strandForward, strandReverse).flatMap(Collection::stream).collect(Collectors.toList()), // We filter out the HMM filtered reads as they do not apply to FRD\n+                        FLAT_SNP_HET_PRIOR, api, maxEffectiveDepthAdjustment, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"FRD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(FRDCallResults));\n+                }\n+            }\n+\n+            //make synthesized likelihoods object (NOTE that we can do this since for invalid model GT fields we simply infinity out the result in the array)\n+            for (int gt = 0; gt < ployidyModelGenotypeLikelihoods.length; gt++) {\n+                if (computeBQD) {\n+                    ployidyModelGenotypeLikelihoods[gt] = Math.max(ployidyModelGenotypeLikelihoods[gt], BQDCallResults[gt]);\n+                }\n+                if (computeFRD) {\n+                    ployidyModelGenotypeLikelihoods[gt] = Math.max(ployidyModelGenotypeLikelihoods[gt], FRDCallResults[gt]);\n+                }\n+            }\n+\n+            // this is what the work actually is, after we have computed a few things\n+            genotypeLikelihoods.add(GenotypeLikelihoods.fromLog10Likelihoods(ployidyModelGenotypeLikelihoods));\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"merged matrix:\");\n+                genotyperDebugStream.println(Arrays.toString(ployidyModelGenotypeLikelihoods));\n+            }\n+        }\n+        return new GenotypingLikelihoods<>(genotypingAlleles, ploidyModel, genotypeLikelihoods);\n+    }\n+\n+    // Adding the debug stream managed by the haplotype caller engine\n+    @Override\n+    public void addDebugOutStream(final PrintStream debugStream) {\n+        this.genotyperDebugStream = debugStream;\n+    }\n+\n+    private GenotypeLikelihoodCalculatorDRAGEN getLikelihoodsCalculator(final int samplePloidy, final int alleleCount) {\n+        if (samplePloidy >= cachePloidyCapacity || alleleCount >= cacheAlleleCountCapacity) {\n+            return calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+        }\n+        final GenotypeLikelihoodCalculatorDRAGEN result = likelihoodCalculators[samplePloidy][alleleCount];\n+        if (result != null) {\n+            return result;\n+        } else {\n+            final GenotypeLikelihoodCalculatorDRAGEN newOne = calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+            likelihoodCalculators[samplePloidy][alleleCount] = newOne;\n+            return newOne;\n+        }\n+    }\n+\n+    /**\n+     * This helper class is used to store the necessary data in order to sort a read based on its BQD \"feather end\" as\n+     * well as information relevant to re-associate the read with its position in the AlleleLikelihoods object arrays.\n+     */\n+    static class DragenReadContainer {\n+        final public GATKRead underlyingRead;\n+        final int offsetIntoReadForBaseQuality;\n+        final int unclippedEnd;\n+        final int indexInLikelihoodsObject;\n+\n+        // Transient value used to store thresholds for FRD\n+       double phredPFValue = 0;\n+\n+\n+        public DragenReadContainer(final GATKRead underlyingRead, final int offsetIntoReadForBaseQuality, final int unclippedEnd, final int indexInLikelihoodsObject) {\n+            this.underlyingRead = underlyingRead;\n+            this.offsetIntoReadForBaseQuality = offsetIntoReadForBaseQuality;\n+            this.unclippedEnd = unclippedEnd;\n+            this.indexInLikelihoodsObject = indexInLikelihoodsObject;\n+        }\n+\n+        public int getUnclippedPosition() {\n+            return unclippedEnd;\n+        }\n+\n+        public int getIndexInLikelihoodsObject() {\n+            return indexInLikelihoodsObject;\n+        }\n+\n+        public boolean wasFilteredByHMM() {\n+            return this.getIndexInLikelihoodsObject() == -1;\n+        }\n+\n+        public boolean hasValidBaseQuality() {\n+            return offsetIntoReadForBaseQuality != -1;\n+        }\n+\n+        public int getBaseQuality() {\n+            return underlyingRead.getBaseQuality(offsetIntoReadForBaseQuality);\n+        }\n+\n+        public double getPhredScaledMappingQuality() {\n+            return DRAGENMappingQualityReadTransformer.mapValue(underlyingRead.getMappingQuality());\n+        }\n+\n+        public double getPhredPFValue() {\n+            return phredPFValue;\n+        }\n+\n+        public void setPhredPFValue(double phredPFValue) {\n+            this.phredPFValue = phredPFValue;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"Read: \"+underlyingRead.toString()+\" index: \"+indexInLikelihoodsObject+\" at unclipped end: \"+unclippedEnd+\" with base quality \"+(hasValidBaseQuality() ? getBaseQuality() : -1);\n+        }\n+\n+        public boolean isReverseStrand() {\n+            return underlyingRead.isReverseStrand();\n+        }\n+    }\n+\n+    //MAJOR TODO THIS IS CURRENTLY BASED OFF OF THE REFERENCE UNCLIPPED START AND NOT THE BASES IN THE READ CONSEQUENTLY AT SITES WITH\n+    //      TODO INDELS PRESENT WE ARE GOING TO BE LOOKING AT THE WRONG OFFSETS FOR THIS SORT... a minor issue but still...\n+\n+    // Orders the reads based on the number of bases there are to the left of the fatherEndComparisonLocation as aligned according to the cigar\n+    // NOTE: here we compare the un-hardclipped edges for these reads as the model itself cares about the cycle count of the sequencer, and\n+    //       importantly this saves us having the thread the original alignment of these reads to this level, since by this point we have trimmed\n+    //       the reads twice, once to the active region with padding and again to the callable region within the active window and in both of these\n+    //       cases we have deleted bases with hardclips.\n+    public class ReadFeatherEndForwardComparitor implements Comparator<DragenReadContainer>, Serializable {\n+        private static final long serialVersionUID = 1L;\n+        /**\n+         * Evaluate first the number of bases to the end of the read and follow that by the base quality\n+         */\n+        @Override\n+        public int compare(final DragenReadContainer read1, final DragenReadContainer read2) {\n+            //NOTE: here we want the reads to wind up in ascending order by unclipped position because the unclipped position should be on the left\n+            int diffVal = read1.getUnclippedPosition() - read2.getUnclippedPosition();\n+            if (diffVal == 0) {\n+                diffVal = (read1.hasValidBaseQuality() ? read1.getBaseQuality() : 0)\n+                        - (read2.hasValidBaseQuality() ? read2.getBaseQuality() : 0);\n+            }\n+            return diffVal;\n+        }\n+    }\n+\n+    // Orders the reads based on the number of bases in the read that occur before the fatherEndComparisonLocation as aligned according to the cigar\n+    // NOTE: here we compare the un-hardclipped edges for these reads as the model itself cares about the cycle count of the sequencer, and\n+    //       importantly this saves us having the thread the original alignment of these reads to this level, since by this point we have trimmed\n+    //       the reads twice, once to the active region with padding and again to the callable region within the active window and in both of these\n+    //       cases we have deleted bases with hardclips.\n+    public class ReadFeatherEndRevereseComparitor implements Comparator<DragenReadContainer>, Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 308}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NjA0NA==", "bodyText": "This doesn't seem performance-critical, so how about\nreturn 5.0 * IntStream.range(0,5).filter(length -> errorBase == paddedReference[offsetForRefIntoEvent - length]).max()", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437856044", "createdAt": "2020-06-10T04:40:36Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/FRDBQDUtils.java", "diffHunk": "@@ -0,0 +1,44 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+\n+public class FRDBQDUtils {\n+\n+    /**\n+     * These two methods are used to calculate the homopolymer base phred scaled adjustment in BQD. This code is taken from DRAGEN.\n+     *\n+     * @param paddedReference       reference to check for homopolymer span\n+     * @param offsetForRefIntoEvent offset of the base upon which to make a call\n+     */\n+    public static double computeForwardHomopolymerAdjustment(final byte[] paddedReference, final int offsetForRefIntoEvent, final byte errorBase) {\n+        int length = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NjA3MQ==", "bodyText": "magic constants", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437856071", "createdAt": "2020-06-10T04:40:43Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/FRDBQDUtils.java", "diffHunk": "@@ -0,0 +1,44 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+\n+public class FRDBQDUtils {\n+\n+    /**\n+     * These two methods are used to calculate the homopolymer base phred scaled adjustment in BQD. This code is taken from DRAGEN.\n+     *\n+     * @param paddedReference       reference to check for homopolymer span\n+     * @param offsetForRefIntoEvent offset of the base upon which to make a call\n+     */\n+    public static double computeForwardHomopolymerAdjustment(final byte[] paddedReference, final int offsetForRefIntoEvent, final byte errorBase) {\n+        int length = 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NjA0NA=="}, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NjE2Mg==", "bodyText": "ditto comments above", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437856162", "createdAt": "2020-06-10T04:41:05Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/FRDBQDUtils.java", "diffHunk": "@@ -0,0 +1,44 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+\n+public class FRDBQDUtils {\n+\n+    /**\n+     * These two methods are used to calculate the homopolymer base phred scaled adjustment in BQD. This code is taken from DRAGEN.\n+     *\n+     * @param paddedReference       reference to check for homopolymer span\n+     * @param offsetForRefIntoEvent offset of the base upon which to make a call\n+     */\n+    public static double computeForwardHomopolymerAdjustment(final byte[] paddedReference, final int offsetForRefIntoEvent, final byte errorBase) {\n+        int length = 1;\n+        while(length < 4) {\n+            if (errorBase != paddedReference[offsetForRefIntoEvent - length]) {\n+                length--;\n+                break;\n+            }\n+            length++;\n+        }\n+        return 5.0 * length;\n+    }\n+    public static double computeReverseHomopolymerAdjustment(final byte[] paddedReference, final int offsetForRefIntoEvent, final byte errorBase) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NjQ0OA==", "bodyText": "Where do these come from?  If it's a log or something, put it in the code.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437856448", "createdAt": "2020-06-10T04:42:14Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/afcalc/DragstrAlleleFrequencyCalculator.java", "diffHunk": "@@ -0,0 +1,51 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+public class DragstrAlleleFrequencyCalculator implements AlleleFrequencyCalculator {\n+\n+    private static final double HOM_REF_PRIOR = 0;\n+    private static final double SNP_SIMPLE_HET_PRIOR = 34.77;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NzE5OA==", "bodyText": "If this is legacy, make it clear in method name or javadoc for later deletion.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437857198", "createdAt": "2020-06-10T04:45:22Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyRegionTrimmer.java", "diffHunk": "@@ -187,7 +192,96 @@ public Result trim(final AssemblyRegion region, final SortedSet<VariantContext>\n \n         final SimpleInterval paddedVariantSpan = new SimpleInterval(region.getContig(), minStart, maxEnd).intersect(region.getPaddedSpan());\n \n+//        System.out.println(\"Padded and trimmed the region to this span: \"+ paddedVariantSpan);\n         return new Result(region, variantSpan, paddedVariantSpan);\n     }\n \n+\n+    /**\n+     * Returns a trimming result object from which the variant trimmed region and flanking non-variant sections\n+     * can be recovered latter.\n+     *\n+     * @param originalRegion the genome location range to trim.\n+     * @param allVariantsWithinExtendedRegion list of variants contained in the trimming location. Variants therein\n+     *                                        not overlapping with {@code originalRegion} are simply ignored.\n+     * @return never {@code null}.\n+     */\n+    public Result trimLegacy(final AssemblyRegion originalRegion,\n+                       final SortedSet<VariantContext> allVariantsWithinExtendedRegion) {\n+\n+        if ( allVariantsWithinExtendedRegion.isEmpty() ) // no variants,\n+        {\n+            return noVariation(originalRegion);\n+        }\n+\n+        final List<VariantContext> withinActiveRegion = new LinkedList<>();\n+        final SimpleInterval originalRegionRange = originalRegion.getSpan();\n+        boolean foundNonSnp = false;\n+        SimpleInterval variantSpan = null;\n+        for ( final VariantContext vc : allVariantsWithinExtendedRegion ) {\n+            final SimpleInterval vcLoc = new SimpleInterval(vc);\n+            if ( originalRegionRange.overlaps(vcLoc) ) {\n+                foundNonSnp = foundNonSnp || ! vc.isSNP();\n+                variantSpan = variantSpan == null ? vcLoc : variantSpan.spanWith(vcLoc);\n+                withinActiveRegion.add(vc);\n+            }\n+        }\n+        final int padding = foundNonSnp ? assemblyRegionArgs.indelPaddingForGenotyping : assemblyRegionArgs.snpPaddingForGenotyping;\n+\n+        // we don't actually have anything in the region after skipping out variants that don't overlap\n+        // the region's full location\n+        if ( variantSpan == null ) {\n+            return noVariation(originalRegion);\n+        }\n+\n+        final SimpleInterval maximumSpan = originalRegionRange.expandWithinContig(assemblyRegionArgs.maxExtensionIntoRegionPadding, sequenceDictionary);\n+        final SimpleInterval idealSpan = variantSpan.expandWithinContig(padding, sequenceDictionary);\n+        final SimpleInterval finalSpan = maximumSpan.intersect(idealSpan).mergeWithContiguous(variantSpan);\n+//      TODO disable this code with ERC\n+//        // Make double sure that, if we are emitting GVCF we won't call non-variable positions beyond the target active region span.\n+//        // In regular call we don't do so so we don't care and we want to maintain behavior, so the conditional.\n+//        final SimpleInterval callableSpan = emitReferenceConfidence ? variantSpan.intersect(originalRegionRange) : variantSpan;\n+        final SimpleInterval callableSpan = variantSpan;\n+\n+        final Pair<SimpleInterval, SimpleInterval> nonVariantRegions = nonVariantTargetRegions(originalRegion, callableSpan);\n+\n+\n+//        System.out.println(\"events       : \" + withinActiveRegion);\n+//        System.out.println(\"region       : \" + originalRegion);\n+//        System.out.println(\"callableSpan : \" + callableSpan);\n+//        System.out.println(\"padding      : \" + padding);\n+//        System.out.println(\"idealSpan    : \" + idealSpan);\n+//        System.out.println(\"maximumSpan  : \" + maximumSpan);\n+//        System.out.println(\"finalSpan    : \" + finalSpan);\n+\n+\n+        return new Result(originalRegion, variantSpan, finalSpan);\n+    }\n+\n+    /**\n+     * Calculates the list of region to trim away.\n+     * @param targetRegion region for which to generate the flanking regions.\n+     * @param variantSpan the span of the core region containing relevant variation and required padding.\n+     * @return never {@code null}; 0, 1 or 2 element list.\n+     */\n+    private Pair<SimpleInterval, SimpleInterval> nonVariantTargetRegions(final AssemblyRegion targetRegion, final SimpleInterval variantSpan) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NzY5Nw==", "bodyText": "typo in class name", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437857697", "createdAt": "2020-06-10T04:47:24Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/DragstrPairHMMInputScoreImputator.java", "diffHunk": "@@ -0,0 +1,57 @@\n+package org.broadinstitute.hellbender.tools.walkers.haplotypecaller;\n+\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReadSTRAnalizer;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+public class DragstrPairHMMInputScoreImputator implements PairHMMInputScoreImputator {\n+\n+    private final DragstrParams params;\n+\n+    public static DragstrPairHMMInputScoreImputator newInstance(final String path) {\n+        return new DragstrPairHMMInputScoreImputator(new DragstrParams(path));\n+    }\n+\n+    public static DragstrPairHMMInputScoreImputator newInstance(final DragstrParams params) {\n+        return new DragstrPairHMMInputScoreImputator(params);\n+    }\n+\n+    private DragstrPairHMMInputScoreImputator(final DragstrParams params) {\n+        this.params = params;\n+    }\n+\n+    @Override\n+    public PairHMMInputScoreImputation impute(final GATKRead read) {\n+        final byte[] bases = read.getBases();\n+        final DragstrReadSTRAnalizer analyzer = DragstrUtils.repeatPeriodAndCounts(read.getLength(), params.maximumPeriod());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1Nzk2MQ==", "bodyText": "magic constants", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437857961", "createdAt": "2020-06-10T04:48:26Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/DragstrPairHMMInputScoreImputator.java", "diffHunk": "@@ -0,0 +1,57 @@\n+package org.broadinstitute.hellbender.tools.walkers.haplotypecaller;\n+\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReadSTRAnalizer;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+public class DragstrPairHMMInputScoreImputator implements PairHMMInputScoreImputator {\n+\n+    private final DragstrParams params;\n+\n+    public static DragstrPairHMMInputScoreImputator newInstance(final String path) {\n+        return new DragstrPairHMMInputScoreImputator(new DragstrParams(path));\n+    }\n+\n+    public static DragstrPairHMMInputScoreImputator newInstance(final DragstrParams params) {\n+        return new DragstrPairHMMInputScoreImputator(params);\n+    }\n+\n+    private DragstrPairHMMInputScoreImputator(final DragstrParams params) {\n+        this.params = params;\n+    }\n+\n+    @Override\n+    public PairHMMInputScoreImputation impute(final GATKRead read) {\n+        final byte[] bases = read.getBases();\n+        final DragstrReadSTRAnalizer analyzer = DragstrUtils.repeatPeriodAndCounts(read.getLength(), params.maximumPeriod());\n+        analyzer.load(bases);\n+        final int length = bases.length;\n+        final byte[] gop = new byte[length];\n+        final byte[] gcp = new byte[length];\n+        for (int i = 0; i < length - 1; i++) {\n+            final int period = analyzer.mostRepeatedPeriod(i + 1);\n+            final int repeats = analyzer.numberOfMostRepeats(i + 1);\n+            gop[i] = (byte) params.gop(period, repeats);\n+            gcp[i] = (byte) params.gcp(period, repeats);\n+        }\n+        gop[length - 1] = 45;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2MjMxMA==", "bodyText": "needs javadoc", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437862310", "createdAt": "2020-06-10T05:06:10Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/MathUtils.java", "diffHunk": "@@ -152,6 +153,26 @@ public static double dotProduct(double[] a, double[] b){\n         return sum(MathArrays.ebeMultiply(Utils.nonNull(a), Utils.nonNull(b)));\n     }\n \n+    public static double[] doubles(final double start, final double end, final double step) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2NDE3MA==", "bodyText": "resolve TODO", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437864170", "createdAt": "2020-06-10T05:13:06Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/read/AlignmentUtils.java", "diffHunk": "@@ -97,37 +99,61 @@ public static GATKRead createReadAlignedToRef(final GATKRead originalRead,\n         // compute the read -> ref alignment by mapping read -> hap -> ref from the\n         // SW of read -> hap mapped through the given by hap -> ref\n \n+        //TODO need to be very carful here, go over later between branch/master and be sure everything is done correctly", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2NDkzMQ==", "bodyText": "Is this being deleted on purpose?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437864931", "createdAt": "2020-06-10T05:15:56Z", "author": {"login": "davidbenjamin"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/LeftAlignIndelsIntegrationTest.java", "diffHunk": "@@ -1,29 +0,0 @@\n-package org.broadinstitute.hellbender.tools;\n-\n-import org.broadinstitute.hellbender.CommandLineProgramTest;\n-import org.broadinstitute.hellbender.testutils.ArgumentsBuilder;\n-import org.broadinstitute.hellbender.testutils.IntegrationTestSpec;\n-import org.testng.annotations.Test;\n-\n-import java.io.File;\n-import java.io.IOException;\n-\n-public class LeftAlignIndelsIntegrationTest extends CommandLineProgramTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2NTI4MA==", "bodyText": "intentional?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r437865280", "createdAt": "2020-06-10T05:17:22Z", "author": {"login": "davidbenjamin"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/variantutils/LeftAlignAndTrimVariantsIntegrationTest.java", "diffHunk": "@@ -17,8 +17,6 @@\n public class LeftAlignAndTrimVariantsIntegrationTest extends CommandLineProgramTest {\n     final Path testDataDir = Paths.get(getToolTestDataDir());\n \n-    // note: this test file has one particularly tricky case of a deletion AAA->A at chr21:13255301\tthat left aligns to CAA->A at chr21:13255289 by", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0NTA0Mw==", "bodyText": "delete?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438545043", "createdAt": "2020-06-11T05:07:37Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerEngine.java", "diffHunk": "@@ -323,9 +342,10 @@ private void initializeActiveRegionEvaluationGenotyperEngine() {\n     /**\n      * @return the default set of read filters for use with the HaplotypeCaller\n      */\n-    public static List<ReadFilter> makeStandardHCReadFilters() {\n+    public static List<ReadFilter> makeStandardHCReadFilters(final int mappingQualityThreshold) {\n         List<ReadFilter> filters = new ArrayList<>();\n-        filters.add(new MappingQualityReadFilter(READ_QUALITY_FILTER_THRESHOLD));\n+        filters.add(new MappingQualityReadFilter(mappingQualityThreshold));\n+//        filters.add(new MappingQualityReadFilter());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0NTE3NQ==", "bodyText": "typo justio", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438545175", "createdAt": "2020-06-11T05:08:13Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerEngine.java", "diffHunk": "@@ -504,9 +533,12 @@ public ActivityProfileState isActive( final AlignmentContext context, final Refe\n      */\n     public List<VariantContext> callRegion(final AssemblyRegion region, final FeatureContext features, final ReferenceContext referenceContext) {\n         if ( hcArgs.justDetermineActiveRegions ) {\n-            // we're benchmarking ART and/or the active region determination code in the HC, just leave without doing any work\n+            // we're benchmarking ART and/or the active region determination code in the HC, justio leave without doing any work", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0NTIzNA==", "bodyText": "typo uncliped", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438545234", "createdAt": "2020-06-11T05:08:28Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerEngine.java", "diffHunk": "@@ -559,7 +591,7 @@ public ActivityProfileState isActive( final AlignmentContext context, final Refe\n \n         final AssemblyRegion regionForGenotyping = assemblyResult.getRegionForGenotyping();\n         final List<GATKRead> readStubs = regionForGenotyping.getReads().stream()\n-                .filter(r -> r.getLength() < AssemblyBasedCallerUtils.MINIMUM_READ_LENGTH_AFTER_TRIMMING).collect(Collectors.toList());\n+                .filter(r -> AlignmentUtils.unclipedReadLength(r)  < AssemblyBasedCallerUtils.MINIMUM_READ_LENGTH_AFTER_TRIMMING).collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0NTU0OQ==", "bodyText": "2 is magic constant", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438545549", "createdAt": "2020-06-11T05:09:49Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerGenotypingEngine.java", "diffHunk": "@@ -139,12 +150,17 @@ public CalledHaplotypes assignGenotypeLikelihoods(final List<Haplotype> haplotyp\n             AssemblyBasedCallerUtils.annotateReadLikelihoodsWithRegions(readLikelihoods, activeRegionWindow);\n         }\n \n+        final DragstrReferenceSTRs dragstrs = isDragstrSTRAnalyzerNecessary(startPosKeySet, haplotypes)\n+                ? DragstrReferenceSTRs.of(ref,startPosKeySet.first() - refLoc.getStart(),\n+                startPosKeySet.last() + 2 - refLoc.getStart(), hcArgs.likelihoodArgs.dragstrParams.maximumPeriod())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0NTYzMA==", "bodyText": "typo retian", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438545630", "createdAt": "2020-06-11T05:10:07Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerGenotypingEngine.java", "diffHunk": "@@ -163,36 +179,53 @@ public CalledHaplotypes assignGenotypeLikelihoods(final List<Haplotype> haplotyp\n                 logger.info(\"Genotyping event at \" + loc + \" with alleles = \" + mergedVC.getAlleles());\n             }\n \n+            //TODO this might need to be revisited given the extra genotyping code that will possibly exist in the future...\n             mergedVC = removeAltAllelesIfTooManyGenotypes(ploidy, alleleMapper, mergedVC);\n \n             AlleleLikelihoods<GATKRead, Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper);\n             final SAMSequenceDictionary sequenceDictionary = header.getSequenceDictionary();\n             final SimpleInterval variantCallingRelevantOverlap = new SimpleInterval(mergedVC).expandWithinContig(hcArgs.informativeReadOverlapMargin, sequenceDictionary);\n-            readAlleleLikelihoods.retainEvidence(variantCallingRelevantOverlap::overlaps);\n+            // We want to retian evidence that overlaps within its softclipping edges.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0NTg2Mw==", "bodyText": "exceptionally whimsical typo paramertramtrized", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438545863", "createdAt": "2020-06-11T05:11:03Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerGenotypingEngine.java", "diffHunk": "@@ -163,36 +179,53 @@ public CalledHaplotypes assignGenotypeLikelihoods(final List<Haplotype> haplotyp\n                 logger.info(\"Genotyping event at \" + loc + \" with alleles = \" + mergedVC.getAlleles());\n             }\n \n+            //TODO this might need to be revisited given the extra genotyping code that will possibly exist in the future...\n             mergedVC = removeAltAllelesIfTooManyGenotypes(ploidy, alleleMapper, mergedVC);\n \n             AlleleLikelihoods<GATKRead, Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper);\n             final SAMSequenceDictionary sequenceDictionary = header.getSequenceDictionary();\n             final SimpleInterval variantCallingRelevantOverlap = new SimpleInterval(mergedVC).expandWithinContig(hcArgs.informativeReadOverlapMargin, sequenceDictionary);\n-            readAlleleLikelihoods.retainEvidence(variantCallingRelevantOverlap::overlaps);\n+            // We want to retian evidence that overlaps within its softclipping edges.\n+            //TODO this will need to be paramertramtrized in the future.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0NjI3MA==", "bodyText": "typo wether", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438546270", "createdAt": "2020-06-11T05:12:35Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerGenotypingEngine.java", "diffHunk": "@@ -202,6 +235,44 @@ public CalledHaplotypes assignGenotypeLikelihoods(final List<Haplotype> haplotyp\n         return new CalledHaplotypes(phasedCalls, calledHaplotypes);\n     }\n \n+    /**\n+     * Confirms wether there is the need to analyze the region's reference sequence for the presence of STRs.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0NjkwMA==", "bodyText": "How about\nreturn !startPosKeySet.isEmpty() && hcArgs.likelihoodArgs.dragstrParams != null && !hcArgs.standardArgs.genotypeArgs.dontUseDragstrPriors &&\n    haplotypes.stream().anyMatch(h -> h.getEventMap().getVariantContexts().anyMatch(GATKVariantContextUtils::containsInlineIndel));", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438546900", "createdAt": "2020-06-11T05:15:10Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerGenotypingEngine.java", "diffHunk": "@@ -202,6 +235,44 @@ public CalledHaplotypes assignGenotypeLikelihoods(final List<Haplotype> haplotyp\n         return new CalledHaplotypes(phasedCalls, calledHaplotypes);\n     }\n \n+    /**\n+     * Confirms wether there is the need to analyze the region's reference sequence for the presence of STRs.\n+     * <p>\n+     *     This is only the case when DRAGstr is activate, we are going to use their priors and there is some indel\n+     *     amongst the haplotypes.\n+     * </p>\n+     */\n+    private boolean isDragstrSTRAnalyzerNecessary(SortedSet<Integer> startPosKeySet, List<Haplotype> haplotypes) {\n+        if (startPosKeySet.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0NzAzMA==", "bodyText": "extra whitespace", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438547030", "createdAt": "2020-06-11T05:15:42Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerGenotypingEngine.java", "diffHunk": "@@ -364,6 +441,8 @@ static protected VariantContext makeAnnotatedCall(byte[] ref, SimpleInterval ref\n \n         final VariantContext untrimmedResult =  annotationEngine.annotateContext(call, tracker, referenceContext, readAlleleLikelihoods, a -> true);\n \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0NzIyOQ==", "bodyText": "I would say \"even when\" as opposed to just \"when\" in the doc string", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438547229", "createdAt": "2020-06-11T05:16:34Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/LikelihoodEngineArgumentCollection.java", "diffHunk": "@@ -21,10 +22,23 @@\n     @Argument(fullName = \"base-quality-score-threshold\", doc = \"Base qualities below this threshold will be reduced to the minimum (\" + QualityUtils.MIN_USABLE_Q_SCORE + \")\", optional = true)\n     public byte BASE_QUALITY_SCORE_THRESHOLD = PairHMM.BASE_QUALITY_SCORE_THRESHOLD;\n \n+    @Argument(fullName=\"dragstr-params-path\", doc=\"location of the DRAGstr model parameters for STR error correction used in the Pair HMM. When provided, it overrides other PCR error correcting mechanisms\", optional = true)\n+    public DragstrParams dragstrParams = null;\n+\n+    @Argument(fullName=\"dragstr-het-hom-ratio\", doc=\"het to hom prior ratio use with DRAGstr on\", optional = true)\n+    public int dragstrHetHomRatio = 2;\n+\n+    @Argument(fullName=\"dont-use-dragstr-pair-hmm-scores\", doc=\"disable DRAGstr pair-hmm score when dragstr-params-path was provided\", optional = false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0NzMwMA==", "bodyText": "extract constants for these argument names for benefit of integration tests", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438547300", "createdAt": "2020-06-11T05:16:56Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/LikelihoodEngineArgumentCollection.java", "diffHunk": "@@ -21,10 +22,23 @@\n     @Argument(fullName = \"base-quality-score-threshold\", doc = \"Base qualities below this threshold will be reduced to the minimum (\" + QualityUtils.MIN_USABLE_Q_SCORE + \")\", optional = true)\n     public byte BASE_QUALITY_SCORE_THRESHOLD = PairHMM.BASE_QUALITY_SCORE_THRESHOLD;\n \n+    @Argument(fullName=\"dragstr-params-path\", doc=\"location of the DRAGstr model parameters for STR error correction used in the Pair HMM. When provided, it overrides other PCR error correcting mechanisms\", optional = true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0Nzc3Nw==", "bodyText": "\"expected-error-rate-per-base\" could apply so many places in the code that I would be verbose and explicit and call it something like \"expected-mismatch-rate-for-read-disqualification\"", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438547777", "createdAt": "2020-06-11T05:18:44Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/LikelihoodEngineArgumentCollection.java", "diffHunk": "@@ -21,10 +22,23 @@\n     @Argument(fullName = \"base-quality-score-threshold\", doc = \"Base qualities below this threshold will be reduced to the minimum (\" + QualityUtils.MIN_USABLE_Q_SCORE + \")\", optional = true)\n     public byte BASE_QUALITY_SCORE_THRESHOLD = PairHMM.BASE_QUALITY_SCORE_THRESHOLD;\n \n+    @Argument(fullName=\"dragstr-params-path\", doc=\"location of the DRAGstr model parameters for STR error correction used in the Pair HMM. When provided, it overrides other PCR error correcting mechanisms\", optional = true)\n+    public DragstrParams dragstrParams = null;\n+\n+    @Argument(fullName=\"dragstr-het-hom-ratio\", doc=\"het to hom prior ratio use with DRAGstr on\", optional = true)\n+    public int dragstrHetHomRatio = 2;\n+\n+    @Argument(fullName=\"dont-use-dragstr-pair-hmm-scores\", doc=\"disable DRAGstr pair-hmm score when dragstr-params-path was provided\", optional = false)\n+    public boolean dontUseDragstrPairHMMScores = false;\n+\n     @Advanced\n     @Argument(fullName=\"pair-hmm-gap-continuation-penalty\", doc=\"Flat gap continuation penalty for use in the Pair HMM\", optional = true)\n     public int gcpHMM = 10;\n \n+    @Advanced\n+    @Argument(fullName=\"expected-error-rate-per-base\", doc=\"Error rate used to set expectation for post HMM read disqualification based on mismatches\", optional = true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0NzgzOQ==", "bodyText": "delete?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438547839", "createdAt": "2020-06-11T05:19:00Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/LikelihoodEngineArgumentCollection.java", "diffHunk": "@@ -62,7 +75,32 @@\n     @Argument(fullName=\"phred-scaled-global-read-mismapping-rate\", doc=\"The global assumed mismapping rate for reads\", optional = true)\n     public int phredScaledGlobalReadMismappingRate = 45;\n \n+//    @Advanced\n+//    @Argument(fullName =\"mapping-quality-based-read-mismapping-rate\", doc= \"If true this will limit the phred scaled likelihood for a read based on its MQ score\", optional = true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0ODQ4Mw==", "bodyText": "Why do we cap base qualities to mapQ?  We already cap read-haplotype likelihoods to the mapQ, and it seems like doing it on a per-read level is meaningful while on a per-base level is just weird.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438548483", "createdAt": "2020-06-11T05:21:04Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/LikelihoodEngineArgumentCollection.java", "diffHunk": "@@ -62,7 +75,32 @@\n     @Argument(fullName=\"phred-scaled-global-read-mismapping-rate\", doc=\"The global assumed mismapping rate for reads\", optional = true)\n     public int phredScaledGlobalReadMismappingRate = 45;\n \n+//    @Advanced\n+//    @Argument(fullName =\"mapping-quality-based-read-mismapping-rate\", doc= \"If true this will limit the phred scaled likelihood for a read based on its MQ score\", optional = true)\n+//    public boolean mapQBasedReadMismappingRateAdjustment = false;\n+\n+    @Advanced\n+    @Argument(fullName = \"disable-symmetric-hmm-normalizing\", doc=\"Toggle to revive legacy behavior of asymmetrically normalizing the arguments to the reference haplotype\", optional = true)\n+    public boolean disableSymmetricallyNormalizeAllelesToReference = false;\n+\n+    @Advanced\n+    @Argument(fullName =\"disable-cap-base-qualities-to-map-quality\", doc= \"If false this disables capping of base qualities in the HMM to the mapping quality of the read\", optional = true)\n+    public boolean capReadQualitiesToMapQ = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0ODU2Mg==", "bodyText": "TODO?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438548562", "createdAt": "2020-06-11T05:21:15Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/LikelihoodEngineArgumentCollection.java", "diffHunk": "@@ -62,7 +75,32 @@\n     @Argument(fullName=\"phred-scaled-global-read-mismapping-rate\", doc=\"The global assumed mismapping rate for reads\", optional = true)\n     public int phredScaledGlobalReadMismappingRate = 45;\n \n+//    @Advanced\n+//    @Argument(fullName =\"mapping-quality-based-read-mismapping-rate\", doc= \"If true this will limit the phred scaled likelihood for a read based on its MQ score\", optional = true)\n+//    public boolean mapQBasedReadMismappingRateAdjustment = false;\n+\n+    @Advanced\n+    @Argument(fullName = \"disable-symmetric-hmm-normalizing\", doc=\"Toggle to revive legacy behavior of asymmetrically normalizing the arguments to the reference haplotype\", optional = true)\n+    public boolean disableSymmetricallyNormalizeAllelesToReference = false;\n+\n+    @Advanced\n+    @Argument(fullName =\"disable-cap-base-qualities-to-map-quality\", doc= \"If false this disables capping of base qualities in the HMM to the mapping quality of the read\", optional = true)\n+    public boolean capReadQualitiesToMapQ = false;\n+\n+    /**\n+     * TODO", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0ODY2MQ==", "bodyText": "needs to be kebab case", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438548661", "createdAt": "2020-06-11T05:21:39Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/LikelihoodEngineArgumentCollection.java", "diffHunk": "@@ -62,7 +75,32 @@\n     @Argument(fullName=\"phred-scaled-global-read-mismapping-rate\", doc=\"The global assumed mismapping rate for reads\", optional = true)\n     public int phredScaledGlobalReadMismappingRate = 45;\n \n+//    @Advanced\n+//    @Argument(fullName =\"mapping-quality-based-read-mismapping-rate\", doc= \"If true this will limit the phred scaled likelihood for a read based on its MQ score\", optional = true)\n+//    public boolean mapQBasedReadMismappingRateAdjustment = false;\n+\n+    @Advanced\n+    @Argument(fullName = \"disable-symmetric-hmm-normalizing\", doc=\"Toggle to revive legacy behavior of asymmetrically normalizing the arguments to the reference haplotype\", optional = true)\n+    public boolean disableSymmetricallyNormalizeAllelesToReference = false;\n+\n+    @Advanced\n+    @Argument(fullName =\"disable-cap-base-qualities-to-map-quality\", doc= \"If false this disables capping of base qualities in the HMM to the mapping quality of the read\", optional = true)\n+    public boolean capReadQualitiesToMapQ = false;\n+\n+    /**\n+     * TODO\n+     */\n+    @Argument(fullName=\"enable-dynamic-read-disqualification-for-genotyping\", doc=\"Will enable less strict read disqualification low base quality reads\")\n+    public boolean enableDynamicReadDisqualification = false;\n+\n+    /**\n+     * TODO\n+     */\n+    @Advanced\n+    @Hidden\n+    @Argument(fullName=\"dynamicReadDisqualificationThreshold\", doc=\"Constant used to scale the dynamic read disqualificaiton\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0ODc2OA==", "bodyText": "TODO?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438548768", "createdAt": "2020-06-11T05:22:03Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/LikelihoodEngineArgumentCollection.java", "diffHunk": "@@ -62,7 +75,32 @@\n     @Argument(fullName=\"phred-scaled-global-read-mismapping-rate\", doc=\"The global assumed mismapping rate for reads\", optional = true)\n     public int phredScaledGlobalReadMismappingRate = 45;\n \n+//    @Advanced\n+//    @Argument(fullName =\"mapping-quality-based-read-mismapping-rate\", doc= \"If true this will limit the phred scaled likelihood for a read based on its MQ score\", optional = true)\n+//    public boolean mapQBasedReadMismappingRateAdjustment = false;\n+\n+    @Advanced\n+    @Argument(fullName = \"disable-symmetric-hmm-normalizing\", doc=\"Toggle to revive legacy behavior of asymmetrically normalizing the arguments to the reference haplotype\", optional = true)\n+    public boolean disableSymmetricallyNormalizeAllelesToReference = false;\n+\n+    @Advanced\n+    @Argument(fullName =\"disable-cap-base-qualities-to-map-quality\", doc= \"If false this disables capping of base qualities in the HMM to the mapping quality of the read\", optional = true)\n+    public boolean capReadQualitiesToMapQ = false;\n+\n+    /**\n+     * TODO\n+     */\n+    @Argument(fullName=\"enable-dynamic-read-disqualification-for-genotyping\", doc=\"Will enable less strict read disqualification low base quality reads\")\n+    public boolean enableDynamicReadDisqualification = false;\n+\n+    /**\n+     * TODO", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0OTYxNg==", "bodyText": "javadoc needs to explain what the transformation is", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438549616", "createdAt": "2020-06-11T05:25:15Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/transformers/DRAGENMappingQualityReadTransformer.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.broadinstitute.hellbender.transformers;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+/**\n+ * Read transformer intended to replicate DRAGEN behavior for handling mapping qualities.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MDc5Mg==", "bodyText": "Are you leaving open the option to lazily initialize the filtered evidence as a performance optimization?  Is the one-time cost of allocating empty array lists for each sample worth it?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438550792", "createdAt": "2020-06-11T05:29:51Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -139,13 +150,21 @@ public AlleleLikelihoods(final SampleList samples,\n     AlleleLikelihoods(final AlleleList alleles,\n                       final SampleList samples,\n                       final List<List<EVIDENCE>> evidenceBySampleIndex,\n+                      final List<List<EVIDENCE>> filteredEvidenceBySampleIndex,\n                       final double[][][] values) {\n         this.samples = samples;\n         this.alleles = alleles;\n         this.evidenceBySampleIndex = evidenceBySampleIndex;\n         this.valuesBySampleIndex = values;\n         final int sampleCount = samples.numberOfSamples();\n-        evidenceIndexBySampleIndex = new ArrayList<>(Collections.nCopies(sampleCount, null));\n+\n+        this.evidenceIndexBySampleIndex = new ArrayList<>(Collections.nCopies(sampleCount, null));\n+        if (filteredEvidenceBySampleIndex != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MDg3MA==", "bodyText": "returns returns", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438550870", "createdAt": "2020-06-11T05:30:08Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -257,6 +276,17 @@ public A getAllele(final int alleleIndex) {\n         return Collections.unmodifiableList(evidenceBySampleIndex.get(sampleIndex));\n     }\n \n+    /**\n+     * Returns returns the units of evidence that have been removed by PairHMM error score filtering.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MTA4MA==", "bodyText": "and not evidence filtered for any other reason?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438551080", "createdAt": "2020-06-11T05:30:52Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -257,6 +276,17 @@ public A getAllele(final int alleleIndex) {\n         return Collections.unmodifiableList(evidenceBySampleIndex.get(sampleIndex));\n     }\n \n+    /**\n+     * Returns returns the units of evidence that have been removed by PairHMM error score filtering.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MTMzOA==", "bodyText": "this variable seems to have become vacuous due to the commenting-out below", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438551338", "createdAt": "2020-06-11T05:31:50Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java", "diffHunk": "@@ -345,25 +375,30 @@ public void normalizeLikelihoods(final double maximumLikelihoodDifferenceCap) {\n             final double[][] sampleValues = valuesBySampleIndex[s];\n             final int evidenceCount = evidenceBySampleIndex.get(s).size();\n             for (int r = 0; r < evidenceCount; r++) {\n-                normalizeLikelihoodsPerEvidence(maximumLikelihoodDifferenceCap, sampleValues, s, r);\n+                normalizeLikelihoodsPerEvidence(maximumLikelihoodDifferenceCap, sampleValues, s, r, symmetricallyNormalizeAllelesToReference);\n             }\n         }\n     }\n \n     // Does the normalizeLikelihoods job for each piece of evidence.\n     private void normalizeLikelihoodsPerEvidence(final double maximumBestAltLikelihoodDifference,\n-                                                 final double[][] sampleValues, final int sampleIndex, final int evidenceIndex) {\n+                                                 final double[][] sampleValues, final int sampleIndex, final int evidenceIndex, final boolean symmetricallyNormalizeAllelesToReference) {\n \n         //allow the best allele to be the reference because asymmetry leads to strange artifacts like het calls with >90% alt reads\n-        final BestAllele bestAllele = searchBestAllele(sampleIndex,evidenceIndex,true);\n+        final BestAllele bestAllele = searchBestAllele(sampleIndex,evidenceIndex,symmetricallyNormalizeAllelesToReference);\n \n         final double worstLikelihoodCap = bestAllele.likelihood + maximumBestAltLikelihoodDifference;\n \n         final int alleleCount = alleles.numberOfAlleles();\n+        boolean hasWarned = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MjAxMQ==", "bodyText": "Do we need to expose this?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438552011", "createdAt": "2020-06-11T05:34:39Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSamplerArgumentCollection.java", "diffHunk": "@@ -0,0 +1,59 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+\n+public class DragstrCasesSamplerArgumentCollection {\n+    @Argument(fullName = DragstrConstants.RANDOM_SEED_ARGUMENT_FULL_NAME,\n+            doc = \"random number generator base seed\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MjE0NA==", "bodyText": "full package path.  Missing import?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438552144", "createdAt": "2020-06-11T05:35:10Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSamplerArgumentCollection.java", "diffHunk": "@@ -0,0 +1,59 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+\n+public class DragstrCasesSamplerArgumentCollection {\n+    @Argument(fullName = DragstrConstants.RANDOM_SEED_ARGUMENT_FULL_NAME,\n+            doc = \"random number generator base seed\",\n+            optional = true)\n+    public int randomSeed;\n+    @Argument(fullName = DragstrConstants.SAMPLING_PADDING_ARGUMENT_FULL_NAME,\n+            doc = \"bases on either side of the repeat that are included in the STR pileup\",\n+            optional = true)\n+    public int pileupPadding;\n+    @Argument(fullName = DragstrConstants.SAMPLING_MIN_MQ_ARGUMENT_FULL_NAME,\n+            doc = \"the minimum read mapping quality allowed in sampled loci. Any read with a lower MQ will result in discarding that locus\",\n+            optional = true)\n+    public int samplingMinMQ;\n+    @Argument(fullName = DragstrConstants.SAMPLING_MAX_MQ_ARGUMENT_FULL_NAME,\n+            doc = \"the maximum number of sites to sample per period and repeat count combination\",\n+            optional = true)\n+    public int maxCount;\n+    @Argument(fullName = DragstrConstants.SAMPLING_MIN_BQ_THRESHOLD_ARGUMENT_FULL_NAME,\n+            doc = \"Base quality thershold for base-call. Reads that contain a number bases that map on the STR will not qualify for sampling\",\n+            optional = true)\n+    public int baseQualThreshold;\n+\n+    @Argument(fullName = org.broadinstitute.hellbender.utils.pairhmm.DragstrConstants.SAMPLING_MAX_BQ_EXCEPTIONS_ALLOWED_ARGUMENT_FULL_NAME,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MjE5OQ==", "bodyText": "spaces needed between arguments", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438552199", "createdAt": "2020-06-11T05:35:22Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSamplerArgumentCollection.java", "diffHunk": "@@ -0,0 +1,59 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+\n+public class DragstrCasesSamplerArgumentCollection {\n+    @Argument(fullName = DragstrConstants.RANDOM_SEED_ARGUMENT_FULL_NAME,\n+            doc = \"random number generator base seed\",\n+            optional = true)\n+    public int randomSeed;\n+    @Argument(fullName = DragstrConstants.SAMPLING_PADDING_ARGUMENT_FULL_NAME,\n+            doc = \"bases on either side of the repeat that are included in the STR pileup\",\n+            optional = true)\n+    public int pileupPadding;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MjI3MQ==", "bodyText": "some more full paths", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438552271", "createdAt": "2020-06-11T05:35:38Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSamplerArgumentCollection.java", "diffHunk": "@@ -0,0 +1,59 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+\n+public class DragstrCasesSamplerArgumentCollection {\n+    @Argument(fullName = DragstrConstants.RANDOM_SEED_ARGUMENT_FULL_NAME,\n+            doc = \"random number generator base seed\",\n+            optional = true)\n+    public int randomSeed;\n+    @Argument(fullName = DragstrConstants.SAMPLING_PADDING_ARGUMENT_FULL_NAME,\n+            doc = \"bases on either side of the repeat that are included in the STR pileup\",\n+            optional = true)\n+    public int pileupPadding;\n+    @Argument(fullName = DragstrConstants.SAMPLING_MIN_MQ_ARGUMENT_FULL_NAME,\n+            doc = \"the minimum read mapping quality allowed in sampled loci. Any read with a lower MQ will result in discarding that locus\",\n+            optional = true)\n+    public int samplingMinMQ;\n+    @Argument(fullName = DragstrConstants.SAMPLING_MAX_MQ_ARGUMENT_FULL_NAME,\n+            doc = \"the maximum number of sites to sample per period and repeat count combination\",\n+            optional = true)\n+    public int maxCount;\n+    @Argument(fullName = DragstrConstants.SAMPLING_MIN_BQ_THRESHOLD_ARGUMENT_FULL_NAME,\n+            doc = \"Base quality thershold for base-call. Reads that contain a number bases that map on the STR will not qualify for sampling\",\n+            optional = true)\n+    public int baseQualThreshold;\n+\n+    @Argument(fullName = org.broadinstitute.hellbender.utils.pairhmm.DragstrConstants.SAMPLING_MAX_BQ_EXCEPTIONS_ALLOWED_ARGUMENT_FULL_NAME,\n+        doc =\"Maximum number of STR overlapping base call with low quality allowed for any read to be considered for further analysis\",\n+        optional =true)\n+    public int baseQualExceptionsAllowed;\n+\n+    @Argument(fullName = org.broadinstitute.hellbender.utils.pairhmm.DragstrConstants.SAMPLING_MAX_CASE_COUNT_ARGUMENT_FULL_NAME,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MjQ4NA==", "bodyText": "can't this be implemented by assigning variables in the argument declarations?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438552484", "createdAt": "2020-06-11T05:36:20Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSamplerArgumentCollection.java", "diffHunk": "@@ -0,0 +1,59 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+\n+public class DragstrCasesSamplerArgumentCollection {\n+    @Argument(fullName = DragstrConstants.RANDOM_SEED_ARGUMENT_FULL_NAME,\n+            doc = \"random number generator base seed\",\n+            optional = true)\n+    public int randomSeed;\n+    @Argument(fullName = DragstrConstants.SAMPLING_PADDING_ARGUMENT_FULL_NAME,\n+            doc = \"bases on either side of the repeat that are included in the STR pileup\",\n+            optional = true)\n+    public int pileupPadding;\n+    @Argument(fullName = DragstrConstants.SAMPLING_MIN_MQ_ARGUMENT_FULL_NAME,\n+            doc = \"the minimum read mapping quality allowed in sampled loci. Any read with a lower MQ will result in discarding that locus\",\n+            optional = true)\n+    public int samplingMinMQ;\n+    @Argument(fullName = DragstrConstants.SAMPLING_MAX_MQ_ARGUMENT_FULL_NAME,\n+            doc = \"the maximum number of sites to sample per period and repeat count combination\",\n+            optional = true)\n+    public int maxCount;\n+    @Argument(fullName = DragstrConstants.SAMPLING_MIN_BQ_THRESHOLD_ARGUMENT_FULL_NAME,\n+            doc = \"Base quality thershold for base-call. Reads that contain a number bases that map on the STR will not qualify for sampling\",\n+            optional = true)\n+    public int baseQualThreshold;\n+\n+    @Argument(fullName = org.broadinstitute.hellbender.utils.pairhmm.DragstrConstants.SAMPLING_MAX_BQ_EXCEPTIONS_ALLOWED_ARGUMENT_FULL_NAME,\n+        doc =\"Maximum number of STR overlapping base call with low quality allowed for any read to be considered for further analysis\",\n+        optional =true)\n+    public int baseQualExceptionsAllowed;\n+\n+    @Argument(fullName = org.broadinstitute.hellbender.utils.pairhmm.DragstrConstants.SAMPLING_MAX_CASE_COUNT_ARGUMENT_FULL_NAME,\n+        doc =\"maximum number of sites sampled for each combination of period and repeat count\",\n+        optional =true)\n+    public int maximumNumberOfCases;\n+\n+    @Argument(fullName = org.broadinstitute.hellbender.utils.pairhmm.DragstrConstants.SAMPLING_MIN_NON_REF_CONTAING_CASE_COUNT_ARGUMENT_FULL_NAME,\n+        doc =\"targeted minimum number of sites sampled that contain non-ref reads for each combination of period and repeat count\",\n+        optional =true)\n+    public int targetMinimumNonRefCases;\n+\n+    @Argument(fullName = org.broadinstitute.hellbender.utils.pairhmm.DragstrConstants.SAMPLING_MIN_CASE_COUNT_ARGUMENT_FULL_NAME,\n+        doc =\"minimum number of sites sampled for each combination of period and repeat count\",\n+        optional =true)\n+    public int minimumNumberOfCases;\n+\n+    public DragstrCasesSamplerArgumentCollection() {\n+        this.randomSeed = DragstrConstants.DEFAULT_RANDOM_SEED;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MjY0Ng==", "bodyText": "why are these here and not in an ArgumentCollection?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438552646", "createdAt": "2020-06-11T05:36:58Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrConstants.java", "diffHunk": "@@ -0,0 +1,61 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.barclay.argparser.Argument;\n+\n+public final class DragstrConstants {\n+\n+    public static final String DRAGSTRINFO_KEY = \"DRAGstrInfo\";\n+    public static final String DRAGSTRPARAMS_KEY = \"DRAGstrParams\";\n+\n+    public static final String MAX_PERIOD_ARGUMENT_FULL_NAME = \"max-period\";\n+    public static final String MAX_REPEATS_ARGUMENT_FULL_NAME = \"max-repeats\";\n+    public static final String RANDOM_SEED_ARGUMENT_FULL_NAME = \"random-seed\";\n+    public static final String SAMPLING_LOCI_ARGUMENT_FULL_NAME = \"sampling-loci-zip\";\n+    public static final String SAMPLING_PADDING_ARGUMENT_FULL_NAME = \"pileup-padding\";\n+    public static final String SAMPLING_MIN_MQ_ARGUMENT_FULL_NAME = \"sampling-min-mq\";\n+    public static final String SAMPLING_MAX_MQ_ARGUMENT_FULL_NAME = \"sampling-max-count\";\n+    public static final String SAMPLING_MIN_BQ_THRESHOLD_ARGUMENT_FULL_NAME = \"sampling-high-bq-threshold\";\n+    public static final String SAMPLING_MAX_BQ_EXCEPTIONS_ALLOWED_ARGUMENT_FULL_NAME = \"sampling-max-low-bq-exceptions-per-read\";\n+    public static final String SAMPLING_MAX_CASE_COUNT_ARGUMENT_FULL_NAME = \"sampling-max-case-count\";\n+    public static final String SAMPLING_MIN_NON_REF_CONTAING_CASE_COUNT_ARGUMENT_FULL_NAME = \"sampling-min-variant-case-count\";\n+    public static final String SAMPLING_MIN_CASE_COUNT_ARGUMENT_FULL_NAME = \"sampling-min-case-count\";\n+\n+\n+\n+    public static final int DEFAULT_MAX_PERIOD = 8;\n+    public static final int DEFAULT_MAX_REPEATS = 20;\n+    public static final int DEFAULT_RANDOM_SEED = 23;\n+    public static final int DEFAULT_SAMPLING_PADDING = 5;\n+    public static final int DEFAULT_SAMPLING_MIN_MQ = 20;\n+    public static final int DEFAULT_SAMPLING_MAX_COUNT = 2000;\n+    public static final int DEFAULT_SAMPLING_BASE_QUAL_THRESHOLD = 10;\n+    public static final int DEFAULT_SAMPLING_MAX_BQ_EXCEPTIONS_ALLOWED = 2;\n+    public static final int DEFAULT_SAMPLING_MIN_NON_REF_CONTAINING_CASE_COUNT = 200;\n+    public static final int DEFAULT_SAMPLING_MIN_CASE_COUNT = 400;\n+\n+\n+    @Argument(fullName = SAMPLING_MIN_BQ_THRESHOLD_ARGUMENT_FULL_NAME,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MzE1Mw==", "bodyText": "a lot of variables to camel-case", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438553153", "createdAt": "2020-06-11T05:38:45Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrModelEstimator.java", "diffHunk": "@@ -0,0 +1,416 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.apache.commons.lang.math.IntRange;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+\n+import java.util.ArrayDeque;\n+import java.util.Arrays;\n+import java.util.Deque;\n+\n+\n+public class DragstrModelEstimator {\n+\n+    private static double[][] DEFAULT_GOP = {\n+            {45.00, 45.00, 45.00, 45.00, 45.00, 45.00, 40.50, 33.50, 28.00, 24.00, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75},\n+            {39.50, 39.50, 39.50, 39.50, 36.00, 30.00, 27.25, 25.00, 24.25, 24.75, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.75},\n+            {38.50, 41.00, 41.00, 41.00, 41.00, 37.50, 35.25, 34.75, 34.75, 33.25, 33.25, 33.25, 32.50, 30.75, 28.50, 29.00, 29.00, 29.00, 29.00, 29.00},\n+            {37.50, 39.00, 39.00, 37.75, 34.00, 34.00, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 31.75, 31.75, 31.75, 31.75, 31.75},\n+            {37.00, 40.00, 40.00, 40.00, 36.00, 35.00, 24.50, 24.50, 24.50, 24.50, 22.50, 22.50, 22.50, 23.50, 23.50, 23.50, 23.50, 23.50, 23.50, 23.50},\n+            {36.25, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00},\n+            {36.00, 40.50, 40.50, 40.50, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75},\n+            {36.25, 39.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75}};\n+\n+    private static double[][] DEFAULT_API = {\n+            {39.00, 39.00, 37.00, 35.00, 32.00, 26.00, 20.00, 16.00, 12.00, 10.00, 8.00, 7.00, 7.00, 6.00, 6.00, 5.00, 5.00, 4.00, 4.00, 4.00},\n+            {30.00, 30.00, 29.00, 22.00, 17.00, 14.00, 11.00, 8.00, 6.00, 5.00, 4.00, 4.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00},\n+            {27.00, 27.00, 25.00, 18.00, 14.00, 12.00, 9.00, 7.00, 5.00, 4.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {27.00, 27.00, 18.00, 9.00, 9.00, 9.00, 9.00, 3.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {29.00, 29.00, 18.00, 8.00, 8.00, 8.00, 4.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {25.00, 25.00, 10.00, 10.00, 10.00, 4.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00},\n+            {21.00, 21.00, 11.00, 11.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00},\n+            {18.00, 18.00, 10.00, 6.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00}};\n+\n+    private static final Logger logger = LogManager.getLogger(DragstrModelEstimator.class);\n+\n+    private final double[] phred_gp_range;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MzI1MQ==", "bodyText": "Is there no way to generate these in code?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438553251", "createdAt": "2020-06-11T05:39:05Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrModelEstimator.java", "diffHunk": "@@ -0,0 +1,416 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.apache.commons.lang.math.IntRange;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+\n+import java.util.ArrayDeque;\n+import java.util.Arrays;\n+import java.util.Deque;\n+\n+\n+public class DragstrModelEstimator {\n+\n+    private static double[][] DEFAULT_GOP = {\n+            {45.00, 45.00, 45.00, 45.00, 45.00, 45.00, 40.50, 33.50, 28.00, 24.00, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75},", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MzM1MA==", "bodyText": "this is in MathUtils", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438553350", "createdAt": "2020-06-11T05:39:28Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrModelEstimator.java", "diffHunk": "@@ -0,0 +1,416 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.apache.commons.lang.math.IntRange;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+\n+import java.util.ArrayDeque;\n+import java.util.Arrays;\n+import java.util.Deque;\n+\n+\n+public class DragstrModelEstimator {\n+\n+    private static double[][] DEFAULT_GOP = {\n+            {45.00, 45.00, 45.00, 45.00, 45.00, 45.00, 40.50, 33.50, 28.00, 24.00, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75},\n+            {39.50, 39.50, 39.50, 39.50, 36.00, 30.00, 27.25, 25.00, 24.25, 24.75, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.75},\n+            {38.50, 41.00, 41.00, 41.00, 41.00, 37.50, 35.25, 34.75, 34.75, 33.25, 33.25, 33.25, 32.50, 30.75, 28.50, 29.00, 29.00, 29.00, 29.00, 29.00},\n+            {37.50, 39.00, 39.00, 37.75, 34.00, 34.00, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 31.75, 31.75, 31.75, 31.75, 31.75},\n+            {37.00, 40.00, 40.00, 40.00, 36.00, 35.00, 24.50, 24.50, 24.50, 24.50, 22.50, 22.50, 22.50, 23.50, 23.50, 23.50, 23.50, 23.50, 23.50, 23.50},\n+            {36.25, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00},\n+            {36.00, 40.50, 40.50, 40.50, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75},\n+            {36.25, 39.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75}};\n+\n+    private static double[][] DEFAULT_API = {\n+            {39.00, 39.00, 37.00, 35.00, 32.00, 26.00, 20.00, 16.00, 12.00, 10.00, 8.00, 7.00, 7.00, 6.00, 6.00, 5.00, 5.00, 4.00, 4.00, 4.00},\n+            {30.00, 30.00, 29.00, 22.00, 17.00, 14.00, 11.00, 8.00, 6.00, 5.00, 4.00, 4.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00},\n+            {27.00, 27.00, 25.00, 18.00, 14.00, 12.00, 9.00, 7.00, 5.00, 4.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {27.00, 27.00, 18.00, 9.00, 9.00, 9.00, 9.00, 3.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {29.00, 29.00, 18.00, 8.00, 8.00, 8.00, 4.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {25.00, 25.00, 10.00, 10.00, 10.00, 4.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00},\n+            {21.00, 21.00, 11.00, 11.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00},\n+            {18.00, 18.00, 10.00, 6.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00}};\n+\n+    private static final Logger logger = LogManager.getLogger(DragstrModelEstimator.class);\n+\n+    private final double[] phred_gp_range;\n+    private final double[] phred_api_range;\n+    private final double[] log10_gp_range;\n+    private final double[] log10_api_range;\n+    private final double het_hom_ratio;\n+    private final double log10_het_hom_ratio;\n+    private final int min_loci_count;\n+    private static final double LOG_10_OF_2 = Math.log10(2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MzYxMw==", "bodyText": "8 and 20 are magic", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438553613", "createdAt": "2020-06-11T05:40:24Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrModelEstimator.java", "diffHunk": "@@ -0,0 +1,416 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.apache.commons.lang.math.IntRange;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+\n+import java.util.ArrayDeque;\n+import java.util.Arrays;\n+import java.util.Deque;\n+\n+\n+public class DragstrModelEstimator {\n+\n+    private static double[][] DEFAULT_GOP = {\n+            {45.00, 45.00, 45.00, 45.00, 45.00, 45.00, 40.50, 33.50, 28.00, 24.00, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75},\n+            {39.50, 39.50, 39.50, 39.50, 36.00, 30.00, 27.25, 25.00, 24.25, 24.75, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.75},\n+            {38.50, 41.00, 41.00, 41.00, 41.00, 37.50, 35.25, 34.75, 34.75, 33.25, 33.25, 33.25, 32.50, 30.75, 28.50, 29.00, 29.00, 29.00, 29.00, 29.00},\n+            {37.50, 39.00, 39.00, 37.75, 34.00, 34.00, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 31.75, 31.75, 31.75, 31.75, 31.75},\n+            {37.00, 40.00, 40.00, 40.00, 36.00, 35.00, 24.50, 24.50, 24.50, 24.50, 22.50, 22.50, 22.50, 23.50, 23.50, 23.50, 23.50, 23.50, 23.50, 23.50},\n+            {36.25, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00},\n+            {36.00, 40.50, 40.50, 40.50, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75},\n+            {36.25, 39.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75}};\n+\n+    private static double[][] DEFAULT_API = {\n+            {39.00, 39.00, 37.00, 35.00, 32.00, 26.00, 20.00, 16.00, 12.00, 10.00, 8.00, 7.00, 7.00, 6.00, 6.00, 5.00, 5.00, 4.00, 4.00, 4.00},\n+            {30.00, 30.00, 29.00, 22.00, 17.00, 14.00, 11.00, 8.00, 6.00, 5.00, 4.00, 4.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00},\n+            {27.00, 27.00, 25.00, 18.00, 14.00, 12.00, 9.00, 7.00, 5.00, 4.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {27.00, 27.00, 18.00, 9.00, 9.00, 9.00, 9.00, 3.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {29.00, 29.00, 18.00, 8.00, 8.00, 8.00, 4.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {25.00, 25.00, 10.00, 10.00, 10.00, 4.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00},\n+            {21.00, 21.00, 11.00, 11.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00},\n+            {18.00, 18.00, 10.00, 6.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00}};\n+\n+    private static final Logger logger = LogManager.getLogger(DragstrModelEstimator.class);\n+\n+    private final double[] phred_gp_range;\n+    private final double[] phred_api_range;\n+    private final double[] log10_gp_range;\n+    private final double[] log10_api_range;\n+    private final double het_hom_ratio;\n+    private final double log10_het_hom_ratio;\n+    private final int min_loci_count;\n+    private static final double LOG_10_OF_2 = Math.log10(2);\n+    private final double api_mono_threshold;\n+    private final double min_gop;\n+    private final double max_gop;\n+    private final int[] min_gp_idx_by_period;\n+    private final double[][][] log10_p_err_by_len;\n+    private final double[][][] log10_p_no_err_by_len;\n+    private final boolean dont_adjust_gop;\n+    private final int minimum_depth = 10;\n+\n+    public DragstrModelEstimator(final DragstrModelEstimatorArgumentCollection argumentCollection) {\n+        phred_gp_range = argumentCollection.phredGpValues.toDoubleArray();\n+        phred_api_range = argumentCollection.phredApiValues.toDoubleArray();\n+        log10_gp_range = MathUtils.applyToArray(phred_gp_range, d -> -.1 * d);\n+        log10_api_range = MathUtils.applyToArray(phred_api_range, d -> -0.1 * d);\n+        het_hom_ratio = argumentCollection.hetToHomRatio;\n+        log10_het_hom_ratio = Math.log10(het_hom_ratio);\n+        min_loci_count = argumentCollection.minLociCount;\n+        api_mono_threshold = argumentCollection.apiMonothresh;\n+        min_gop = argumentCollection.minGOP;\n+        max_gop = argumentCollection.maxGOP;\n+        log10_p_err_by_len = new double[phred_gp_range.length][8][20];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1NDQxNg==", "bodyText": "\"prior\" instead of \"a priori\" here and below", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438554416", "createdAt": "2020-06-11T05:43:25Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrModelEstimatorArgumentCollection.java", "diffHunk": "@@ -0,0 +1,92 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineException;\n+\n+public class DragstrModelEstimatorArgumentCollection {\n+\n+    public static final String GP_VALUES_ARGUMENT_FULL_NAME = \"gp-values\";\n+    public static final String API_VALUES_ARGUMENT_FULL_NAME = \"api-values\";\n+    public static final String HET_TO_HOM_RATIO_FULL_NAME = \"het-to-hom-ratio\";\n+    public static final String MIN_LOCI_COUNT_FULL_NAME = \"min-loci-count\";\n+    public static final String API_MONO_THRESHOD_FULL_NAME = \"api-mono-threshold\";\n+    public static final String MIN_GOP_FULL_NAME = \"min-gop\";\n+    public static final String MAX_GOP_FULL_NAME = \"max-gop\";\n+    public static final DoubleSequence DEFAULT_PHRED_GP_VALUES = new DoubleSequence(\"10:1.0:50\");\n+    public static final DoubleSequence DEFAULT_PHRED_API_VALUES = new DoubleSequence(\"0:1.0:40\");\n+    public static final double DEFAULT_HET_TO_HOM_RATIO = 2.0;\n+    public static final int DEFAULT_MIN_LOCI_COUNT = 50;\n+    public static final int DEFAULT_API_MONO_THRESHOLD = 3;\n+    public static final double DEFAULT_MIN_GOP = 10;\n+    public static final double DEFAULT_MAX_GOP = 50;\n+\n+    @Argument(doc = \"Possible Gap-Penalty values for the DRAGstr model parameter esimation. \" +\n+            \"These are expressed in Phred scaled values with the following format: start:step:end. For example the default '10:1.0:50' indicate the sequence starting at 10 finishing at 50 sampled at 1.0 intervals.\",\n+             optional = true,\n+             fullName = GP_VALUES_ARGUMENT_FULL_NAME)\n+    public DoubleSequence phredGpValues = DEFAULT_PHRED_GP_VALUES;\n+\n+    @Argument(doc = \"Possible a-priori probabilities for the heterozygous indel call for the DRAGstr model parameter esimation. \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1NDYwOQ==", "bodyText": "Huh?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438554609", "createdAt": "2020-06-11T05:44:03Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrModelEstimatorArgumentCollection.java", "diffHunk": "@@ -0,0 +1,92 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineException;\n+\n+public class DragstrModelEstimatorArgumentCollection {\n+\n+    public static final String GP_VALUES_ARGUMENT_FULL_NAME = \"gp-values\";\n+    public static final String API_VALUES_ARGUMENT_FULL_NAME = \"api-values\";\n+    public static final String HET_TO_HOM_RATIO_FULL_NAME = \"het-to-hom-ratio\";\n+    public static final String MIN_LOCI_COUNT_FULL_NAME = \"min-loci-count\";\n+    public static final String API_MONO_THRESHOD_FULL_NAME = \"api-mono-threshold\";\n+    public static final String MIN_GOP_FULL_NAME = \"min-gop\";\n+    public static final String MAX_GOP_FULL_NAME = \"max-gop\";\n+    public static final DoubleSequence DEFAULT_PHRED_GP_VALUES = new DoubleSequence(\"10:1.0:50\");\n+    public static final DoubleSequence DEFAULT_PHRED_API_VALUES = new DoubleSequence(\"0:1.0:40\");\n+    public static final double DEFAULT_HET_TO_HOM_RATIO = 2.0;\n+    public static final int DEFAULT_MIN_LOCI_COUNT = 50;\n+    public static final int DEFAULT_API_MONO_THRESHOLD = 3;\n+    public static final double DEFAULT_MIN_GOP = 10;\n+    public static final double DEFAULT_MAX_GOP = 50;\n+\n+    @Argument(doc = \"Possible Gap-Penalty values for the DRAGstr model parameter esimation. \" +\n+            \"These are expressed in Phred scaled values with the following format: start:step:end. For example the default '10:1.0:50' indicate the sequence starting at 10 finishing at 50 sampled at 1.0 intervals.\",\n+             optional = true,\n+             fullName = GP_VALUES_ARGUMENT_FULL_NAME)\n+    public DoubleSequence phredGpValues = DEFAULT_PHRED_GP_VALUES;\n+\n+    @Argument(doc = \"Possible a-priori probabilities for the heterozygous indel call for the DRAGstr model parameter esimation. \" +\n+            \"These are expressed in Phred scaled values with the following format: start:step:end. For example the default '10:1.0:50' indicate the sequence starting at 10 finishing at 50 sampled at 1.0 intervals.\",\n+            optional = true,\n+            fullName = API_VALUES_ARGUMENT_FULL_NAME)\n+    public DoubleSequence phredApiValues = DEFAULT_PHRED_API_VALUES;\n+\n+    @Argument(doc = \"Possible a-priori probabilities for the heterozygous indel call for the DRAGstr model parameter esimation. \" +\n+            \"These are expressed in Phred scaled values with the following format: start:step:end. For example the default '10:1.0:50' indicate the sequence starting at 10 finishing at 50 sampled at 1.0 intervals.\",\n+            optional = true,\n+            fullName = HET_TO_HOM_RATIO_FULL_NAME,\n+            minValue = 0.0,\n+            maxValue = Double.MAX_VALUE)\n+    public double hetToHomRatio = DEFAULT_HET_TO_HOM_RATIO;\n+\n+    @Argument(doc = \"Minimum number of sites for a repeat count and period length pair. We will combine pairs that have a smaller number of such cases (same period but +/- 1 repeat count)\",\n+              optional = true,\n+              fullName = MIN_LOCI_COUNT_FULL_NAME,\n+              minValue = 1.0)\n+    public int minLociCount = DEFAULT_MIN_LOCI_COUNT;\n+\n+    @Argument(doc = \"<Not quite understand this one yet>\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1NTQ4Mg==", "bodyText": "typo conseqcutive", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438555482", "createdAt": "2020-06-11T05:47:04Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrReferenceSTRs.java", "diffHunk": "@@ -0,0 +1,135 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.apache.commons.lang.ArrayUtils;\n+\n+import java.util.Arrays;\n+\n+public class DragstrReferenceSTRs {\n+    private final int start;\n+    private final int end;\n+    private final byte[] bases;\n+    private final int[] period;\n+    private final int[] forwardRepeats;\n+\n+    private DragstrReferenceSTRs(final byte[] bases, final int start, final int end, final int[] period, final int[] repeats) {\n+        this.start = start;\n+        this.end = end;\n+        this.bases = bases;\n+        this.period = period;\n+        this.forwardRepeats = repeats;\n+    }\n+\n+    public int repeatLength(final int position) {\n+        int result = lookup(position, forwardRepeats);\n+        final int period = this.period[position - start];\n+        for (int i = position - 1, j = position + period, k = period; i >= 0; i--) {\n+            if (bases[i] != bases[--j]) {\n+                break;\n+            } else if (--k == 0) { // here we count complete unit matches (k reaches 0).\n+                k = period;\n+                result++;\n+            }\n+        }\n+        return result;\n+    }\n+\n+    public byte[] repeatUnit(final int position) {\n+        final int length = lookup(position, period);\n+        return Arrays.copyOfRange(bases, position, position + length);\n+    }\n+\n+    public String repeatUnitAsString(final int position) {\n+        return new String(repeatUnit(position));\n+    }\n+\n+    public int period(final int position) {\n+        return lookup(position, period);\n+    }\n+\n+    private int lookup(final int position, final int[] array) {\n+        final int offset = position - start;\n+        if (offset >= 0 && position <= end) {\n+            return array[offset];\n+        } else {\n+            throw new IllegalArgumentException(\"postion \" + position + \" is outside bounds\");\n+        }\n+    }\n+\n+\n+    public static DragstrReferenceSTRs of(final byte[] sequence, final int start, final int end, final int maxPeriod) {\n+        if (end < start || start < 0 || end > sequence.length) {\n+            throw new IndexOutOfBoundsException(\"bad indexes \" + start  + \" \" + end + \" \" + sequence.length);\n+        } else if (start >= end) {\n+            return new DragstrReferenceSTRs(sequence, start, end, ArrayUtils.EMPTY_INT_ARRAY, ArrayUtils.EMPTY_INT_ARRAY);\n+        }\n+        final int[] repeats = processPeriodOne(sequence, start, end);\n+        final int[] periods = new int[end - start];\n+        Arrays.fill(periods, 1);\n+        for (int period = 2; period <= maxPeriod; period++) {\n+            if (sequence.length  < period << 1) {\n+                break;\n+            }\n+            int position, remainingToMatchUnit, carryBack, rightMargin, positionPlusPeriod, resultArrayOffset;\n+\n+            // We first find the right-margin enclosing the last repeat that would match\n+            // a prospective repeat unit within [start,end):\n+            rightMargin = Math.min(end + period, sequence.length) - 1;\n+            final int rightMostStart = position = rightMargin - period;\n+            remainingToMatchUnit = period;\n+            carryBack = 1;\n+            for (; rightMargin < sequence.length; rightMargin++) {\n+                if (sequence[position++] != sequence[rightMargin]) {\n+                    break;\n+                } else if (--remainingToMatchUnit == 0) {\n+                    carryBack++;\n+                    remainingToMatchUnit = period;\n+                }\n+            }\n+\n+            // then we work our way backwards carrying on number of conseqcutive matching units", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1NTYyNw==", "bodyText": "What's this?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438555627", "createdAt": "2020-06-11T05:47:38Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrSTRTable.java", "diffHunk": "@@ -0,0 +1,4 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+public class DragstrSTRTable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1NjQwNg==", "bodyText": "Seems like an ebeAdd of likelihoods and priors followed by an array max would be clearer.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438556406", "createdAt": "2020-06-11T05:50:27Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKVariantContextUtils.java", "diffHunk": "@@ -310,21 +314,89 @@ public static void makeGenotypeCall(final int ploidy,\n             for (final Allele originalAllele : originalGT) {\n                 best.add((allelesToUse.contains(originalAllele) || originalAllele.isNoCall()) ? originalAllele : ref);\n             }\n-            if (best.contains(Allele.NON_REF_ALLELE)) {\n-                gb.alleles(GATKVariantContextUtils.noCallAlleles(2));\n-                gb.PL(new int[genotypeLikelihoods.length]);\n+            gb.alleles(best);\n+        } else if (assignmentMethod == GenotypeAssignmentMethod.USE_POSTERIOR_PROBABILITIES) {\n+            if (gpc == null) {\n+                throw new GATKException(\"cannot uses posteriors without a allele frequency calculator present\");\n             } else {\n-                gb.alleles(best);\n+                final GenotypeLikelihoodCalculator glCalc = GL_CALCS.getInstance(ploidy, allelesToUse.size());\n+                final double[] log10Priors = gpc.getLog10Priors(glCalc, allelesToUse);\n+                final double[] log10Posteriors = new double[genotypeLikelihoods.length];\n+                double bestPosterior = Double.NEGATIVE_INFINITY;\n+                int bestGenotypeIndex = 0;\n+                for (int i = 0; i < genotypeLikelihoods.length; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1NjcxMQ==", "bodyText": "This is MathUtils.scaleLogSpaceArrayForNumericalStability", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438556711", "createdAt": "2020-06-11T05:51:32Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKVariantContextUtils.java", "diffHunk": "@@ -310,21 +314,89 @@ public static void makeGenotypeCall(final int ploidy,\n             for (final Allele originalAllele : originalGT) {\n                 best.add((allelesToUse.contains(originalAllele) || originalAllele.isNoCall()) ? originalAllele : ref);\n             }\n-            if (best.contains(Allele.NON_REF_ALLELE)) {\n-                gb.alleles(GATKVariantContextUtils.noCallAlleles(2));\n-                gb.PL(new int[genotypeLikelihoods.length]);\n+            gb.alleles(best);\n+        } else if (assignmentMethod == GenotypeAssignmentMethod.USE_POSTERIOR_PROBABILITIES) {\n+            if (gpc == null) {\n+                throw new GATKException(\"cannot uses posteriors without a allele frequency calculator present\");\n             } else {\n-                gb.alleles(best);\n+                final GenotypeLikelihoodCalculator glCalc = GL_CALCS.getInstance(ploidy, allelesToUse.size());\n+                final double[] log10Priors = gpc.getLog10Priors(glCalc, allelesToUse);\n+                final double[] log10Posteriors = new double[genotypeLikelihoods.length];\n+                double bestPosterior = Double.NEGATIVE_INFINITY;\n+                int bestGenotypeIndex = 0;\n+                for (int i = 0; i < genotypeLikelihoods.length; i++) {\n+                    final double log10Posterior = log10Posteriors[i] = log10Priors[i] + genotypeLikelihoods[i];\n+                    if (log10Posterior > bestPosterior) {\n+                        bestGenotypeIndex = i;\n+                        bestPosterior = log10Posterior;\n+                    }\n+                }\n+                for (int i = 0; i < log10Posteriors.length; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1NzAwMg==", "bodyText": "Multiplying by -10 to get a phred qual should be somewhere in QualityUtils", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438557002", "createdAt": "2020-06-11T05:52:33Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKVariantContextUtils.java", "diffHunk": "@@ -310,21 +314,89 @@ public static void makeGenotypeCall(final int ploidy,\n             for (final Allele originalAllele : originalGT) {\n                 best.add((allelesToUse.contains(originalAllele) || originalAllele.isNoCall()) ? originalAllele : ref);\n             }\n-            if (best.contains(Allele.NON_REF_ALLELE)) {\n-                gb.alleles(GATKVariantContextUtils.noCallAlleles(2));\n-                gb.PL(new int[genotypeLikelihoods.length]);\n+            gb.alleles(best);\n+        } else if (assignmentMethod == GenotypeAssignmentMethod.USE_POSTERIOR_PROBABILITIES) {\n+            if (gpc == null) {\n+                throw new GATKException(\"cannot uses posteriors without a allele frequency calculator present\");\n             } else {\n-                gb.alleles(best);\n+                final GenotypeLikelihoodCalculator glCalc = GL_CALCS.getInstance(ploidy, allelesToUse.size());\n+                final double[] log10Priors = gpc.getLog10Priors(glCalc, allelesToUse);\n+                final double[] log10Posteriors = new double[genotypeLikelihoods.length];\n+                double bestPosterior = Double.NEGATIVE_INFINITY;\n+                int bestGenotypeIndex = 0;\n+                for (int i = 0; i < genotypeLikelihoods.length; i++) {\n+                    final double log10Posterior = log10Posteriors[i] = log10Priors[i] + genotypeLikelihoods[i];\n+                    if (log10Posterior > bestPosterior) {\n+                        bestGenotypeIndex = i;\n+                        bestPosterior = log10Posterior;\n+                    }\n+                }\n+                for (int i = 0; i < log10Posteriors.length; i++) {\n+                    log10Posteriors[i] -= bestPosterior;\n+                }\n+                gb.alleles(glCalc.genotypeAlleleCountsAt(bestGenotypeIndex).asAlleleList(allelesToUse));\n+                if ( allelesToUse.size() > 0 ) {\n+                    gb.log10PError(getGQLog10FromPosteriors(bestGenotypeIndex, log10Posteriors));\n+                }\n+                gb.attribute(VCFConstants.GENOTYPE_POSTERIORS_KEY, Arrays.stream(log10Posteriors)\n+                        .map(v -> v == 0.0 ? 0.0 : v * -10) // the reason for the == 0.0 is to avoid a signed 0 output \"-0.0\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1NzU4MA==", "bodyText": "More concise, perhaps too cute, would be to express this as\nelse if (log10Posteriors.length == 3) {\n   return return Math.min(0.0, MathUtils.log10SumLog10(log10Posteriors[(bestGenotypeIndex + 1)%3], log10Posteriors[(bestGenotypeIndex - 1)%3]));\n}", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438557580", "createdAt": "2020-06-11T05:54:32Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKVariantContextUtils.java", "diffHunk": "@@ -310,21 +314,89 @@ public static void makeGenotypeCall(final int ploidy,\n             for (final Allele originalAllele : originalGT) {\n                 best.add((allelesToUse.contains(originalAllele) || originalAllele.isNoCall()) ? originalAllele : ref);\n             }\n-            if (best.contains(Allele.NON_REF_ALLELE)) {\n-                gb.alleles(GATKVariantContextUtils.noCallAlleles(2));\n-                gb.PL(new int[genotypeLikelihoods.length]);\n+            gb.alleles(best);\n+        } else if (assignmentMethod == GenotypeAssignmentMethod.USE_POSTERIOR_PROBABILITIES) {\n+            if (gpc == null) {\n+                throw new GATKException(\"cannot uses posteriors without a allele frequency calculator present\");\n             } else {\n-                gb.alleles(best);\n+                final GenotypeLikelihoodCalculator glCalc = GL_CALCS.getInstance(ploidy, allelesToUse.size());\n+                final double[] log10Priors = gpc.getLog10Priors(glCalc, allelesToUse);\n+                final double[] log10Posteriors = new double[genotypeLikelihoods.length];\n+                double bestPosterior = Double.NEGATIVE_INFINITY;\n+                int bestGenotypeIndex = 0;\n+                for (int i = 0; i < genotypeLikelihoods.length; i++) {\n+                    final double log10Posterior = log10Posteriors[i] = log10Priors[i] + genotypeLikelihoods[i];\n+                    if (log10Posterior > bestPosterior) {\n+                        bestGenotypeIndex = i;\n+                        bestPosterior = log10Posterior;\n+                    }\n+                }\n+                for (int i = 0; i < log10Posteriors.length; i++) {\n+                    log10Posteriors[i] -= bestPosterior;\n+                }\n+                gb.alleles(glCalc.genotypeAlleleCountsAt(bestGenotypeIndex).asAlleleList(allelesToUse));\n+                if ( allelesToUse.size() > 0 ) {\n+                    gb.log10PError(getGQLog10FromPosteriors(bestGenotypeIndex, log10Posteriors));\n+                }\n+                gb.attribute(VCFConstants.GENOTYPE_POSTERIORS_KEY, Arrays.stream(log10Posteriors)\n+                        .map(v -> v == 0.0 ? 0.0 : v * -10) // the reason for the == 0.0 is to avoid a signed 0 output \"-0.0\"\n+                        .mapToObj(GATKVariantContextUtils::formatGP).toArray());\n+//                System.out.println(\"After applying the prior: \"+ Arrays.toString(log10Posteriors));\n+                gb.attribute(GATKVCFConstants.GENOTYPE_PRIOR_KEY, Arrays.stream(log10Priors)\n+                        .map(v -> v == 0.0 ? 0.0 : v * -10)\n+                        .mapToObj(GATKVariantContextUtils::formatGP).toArray());\n             }\n         }\n     }\n \n+    private static double getGQLog10FromPosteriors(final int bestGenotypeIndex, final double[] log10Posteriors) {\n+        if (bestGenotypeIndex < 0) {\n+            return CommonInfo.NO_LOG10_PERROR;\n+        } else if (log10Posteriors.length == 3) { // most common case\n+            if (bestGenotypeIndex == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1Nzg5MQ==", "bodyText": "commented out?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438557891", "createdAt": "2020-06-11T05:55:36Z", "author": {"login": "davidbenjamin"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerGenotypingEngineUnitTest.java", "diffHunk": "@@ -58,6 +66,33 @@ public String toString() {\n         }\n     }\n \n+//    @Test", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1ODAxNA==", "bodyText": "extract constants for all arguments", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438558014", "createdAt": "2020-06-11T05:56:03Z", "author": {"login": "davidbenjamin"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerIntegrationTest.java", "diffHunk": "@@ -197,6 +196,46 @@ public void testVCFModeIsConcordantWithGATK3_8Results(final String inputFileName\n         Assert.assertTrue(concordance >= 0.99, \"Concordance with GATK 3.8 in VCF mode is < 99% (\" +  concordance + \")\");\n     }\n \n+    /*\n+     * Test that the this version of DRAGEN-GATK has not changed relative to the last version with the recommended arguments enabled\n+     */\n+    @Test(dataProvider=\"HaplotypeCallerTestInputs\")\n+    public void testDRAGENGATKModeIsConsistentWithPastResults(final String inputFileName, final String referenceFileName) throws Exception {\n+        Utils.resetRandomGenerator();\n+\n+        final File output = createTempFile(\"testDRAGENGATKModeIsConsistentWithPastResults\", \".vcf\");\n+        final File expected = new File(TEST_FILES_DIR + \"expected.testVCFMode.gatk4.DRAGEN.vcf\");\n+        final String outputPath = UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS ? expected.getAbsolutePath() : output.getAbsolutePath();\n+\n+        final String[] args = {\n+                \"-I\", inputFileName,\n+                \"-R\", referenceFileName,\n+                \"-L\", \"20:10000000-10100000\",\n+                \"-O\", outputPath,\n+                \"-pairHMM\", \"AVX_LOGLESS_CACHING\",\n+                // FRD arguments\n+                \"--apply-frd\", \"--transform-dragen-mapping-quality\", \"--mapping-quality-threshold\", \"1\", \"--disable-cap-base-qualities-to-map-quality\", \"--minimum-mapping-quality\", \"1\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1ODExOQ==", "bodyText": "commented out?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438558119", "createdAt": "2020-06-11T05:56:23Z", "author": {"login": "davidbenjamin"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerIntegrationTest.java", "diffHunk": "@@ -370,6 +409,85 @@ public void testGVCFModeIsConcordantWithGATK3_8Results(final String inputFileNam\n         Assert.assertTrue(concordance >= 0.99, \"Concordance with GATK 3.8 in GVCF mode is < 99% (\" +  concordance + \")\");\n     }\n \n+    /*\n+     * Test that the this version of DRAGEN-GATK has not changed relative to the last version with the recommended arguments enabled\n+     */\n+    @Test\n+    public void testFRDBQDDRAGENGATKOnDRAGENProcessedFile() throws Exception {\n+        Utils.resetRandomGenerator();\n+        final String inputFileName = largeFileTestDir + \"DRAGENExampleBamSites.bam\";\n+        final String intervals = TEST_FILES_DIR + \"DRAGENTestSites.bed\";\n+\n+        final File output = createTempFile(\"testFRDBQDDRAGENGATKOnDRAGENProcessedFile\", \".vcf\");\n+        final File expected = new File(TEST_FILES_DIR + \"expected.testVCFMode.gatk4.FRDBQD.vcf\");\n+        final String outputPath = UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS ? expected.getAbsolutePath() : output.getAbsolutePath();\n+\n+        final String[] args = {\n+                \"-I\", inputFileName,\n+                \"-R\", b37Reference,\n+                \"-L\", intervals,\n+                \"-O\", outputPath,\n+                \"-pairHMM\", \"AVX_LOGLESS_CACHING\",\n+                // FRD arguments\n+                \"--apply-frd\", \"--transform-dragen-mapping-quality\", \"--mapping-quality-threshold\", \"1\", \"--disable-cap-base-qualities-to-map-quality\", \"--minimum-mapping-quality\", \"1\",\n+                // BQD arguments\n+                \"--apply-bqd\",  \"--soft-clip-low-quality-ends\",\n+                // Dynamic read disqualification arguments\"\n+                \"--enable-dynamic-read-disqualification-for-genotyping\", \"--expected-error-rate-per-base\", \"0.03\",\n+                // misc arguments\n+                \"--enable-legacy-graph-cycle-detection\", \"--padding-around-indels\", \"150\",\n+                \"--\" + AssemblyBasedCallerArgumentCollection.ALLELE_EXTENSION_LONG_NAME, \"1\",\n+                \"--\" + StandardArgumentDefinitions.ADD_OUTPUT_VCF_COMMANDLINE, \"false\",\n+        };\n+\n+        runCommandLine(args);\n+        if ( ! UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS ) {\n+            IntegrationTestSpec.assertEqualTextFiles(output, expected);\n+        }\n+    }\n+\n+//    @Test\n+//    public void testFRDOnThisFileInDebugger() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1ODc2MQ==", "bodyText": "extract constants", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438558761", "createdAt": "2020-06-11T05:58:13Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerArgumentCollection.java", "diffHunk": "@@ -119,6 +119,32 @@ protected ReadThreadingAssemblerArgumentCollection getReadThreadingAssemblerArgu\n             optional = true)\n     public boolean disableOptimizations = false;\n \n+    /**\n+     * These arguments are associated with DRAGEN-GATK\n+     */\n+    @Advanced\n+    @Argument(fullName = \"apply-bqd\", doc = \"If enabled this argument will apply the DRAGEN-GATK BaseQualityDropout model to the genotyping model for filtering sites due to Linked Error mode.\", optional = true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA3ODYzMw==", "bodyText": "Can we say something more descriptive than adjustment?  Penalty perhaps?  And what is adjusted \u2014 the base quality, the read likelihood, the genotype quality etc?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r439078633", "createdAt": "2020-06-11T21:25:04Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,495 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_DEFAULT_ALPHA = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA3OTU4NQ==", "bodyText": "Here and below \"alpha\" is mysterious.  Why not rename to \"BQD_ERROR_RATE\"?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r439079585", "createdAt": "2020-06-11T21:27:16Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,495 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_DEFAULT_ALPHA = 0.5;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA3OTc4OQ==", "bodyText": "\"Object\" is redundant.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r439079789", "createdAt": "2020-06-11T21:27:44Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,495 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_DEFAULT_ALPHA = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoodsObject = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA4MzM5NQ==", "bodyText": "Inverse doesn't seem to be the right word.  Maybe \"log10NonErrorRate\", if I understand its purpose.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r439083395", "createdAt": "2020-06-11T21:36:26Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,495 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_DEFAULT_ALPHA = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoodsObject = null;\n+\n+    private final double cachedLog10Alpha;\n+    private final double cachedLog10AlphaInverse;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4MjIxNDY0", "url": "https://github.com/broadinstitute/gatk/pull/6634#pullrequestreview-428221464", "createdAt": "2020-06-10T16:16:50Z", "commit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "state": "COMMENTED", "comments": {"totalCount": 79, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNjoxNjo1MFrOGh8lfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMTozMDo1OFrOGqoWOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI0ODgyOA==", "bodyText": "Hmm... i'm not sure its strictly necessary making this interface here at this stage. At the very least add a javadoc", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438248828", "createdAt": "2020-06-10T16:16:50Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/GenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,12 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+\n+import java.util.List;\n+\n+public interface GenotypePriorCalculator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI1MDY0Ng==", "bodyText": "Should this code ever encounter an SV_SIMPLE_INS/DEL? Furthermore would an SV event even reasonably apply with the prior we calculate here?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438250646", "createdAt": "2020-06-10T16:19:30Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/SimpleGenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SimpleGenotypePriorCalculator implements GenotypePriorCalculator {\n+\n+\n+\n+    private enum AlleleType {\n+        REF, SNP, INDEL, OTHER;\n+    }\n+\n+    private final double[] hetValues;\n+    private final double[] homValues;\n+    private final double[] diffValues;\n+\n+    private SimpleGenotypePriorCalculator(final double snpHet, final double snpHom,\n+                                          final double indelHet, final double indelHom,\n+                                          final double otherHet, final double otherHom) {\n+        hetValues = new double[4];\n+        homValues = new double[4];\n+        diffValues = new double[4];\n+\n+        // SNPs: // * 1/3 since there is three possible mutations for SNPs.\n+        hetValues[1] = snpHet - Math.log10(3);\n+        homValues[1] = snpHom - Math.log10(3);\n+        // INDELs:\n+        hetValues[2] = indelHet;\n+        homValues[2] = indelHom;\n+        // Others:\n+        hetValues[3] = otherHet;\n+        homValues[3] = otherHom;\n+\n+        for (int i = 0; i < 4; i++) {\n+            diffValues[i] = homValues[i] - hetValues[i];\n+        }\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenHetToHomRatio(final double snpHet, final double indelHet,\n+                                                                   final double otherHet, final double het_hom_ratio) {\n+        final double log10Ratio = Math.log10(het_hom_ratio);\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet - log10Ratio,\n+                                                  indelHet, indelHet - log10Ratio,\n+                                                  otherHet, otherHet - log10Ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenDragstrParams(final DragstrParams dragstrParams, final int period,\n+                                                                   final int repeats, final double snpHeterozygosity,\n+                                                                   final double het_hom_ratio) {\n+        final double snpHet = snpHeterozygosity;\n+        final double indelHet = -.1 * dragstrParams.api(period, repeats);\n+        final double otherHet = Math.max(snpHet, indelHet);\n+        return givenHetToHomRatio(snpHet, indelHet, otherHet, het_hom_ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(final double snpHet, final double indelHet) {\n+        return assumingHW(snpHet, indelHet, Math.max(snpHet, indelHet));\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(final double snpHet, final double indelHet,\n+                                                                   final double otherHet) {\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet * 2,\n+                indelHet, indelHet * 2,\n+                otherHet, otherHet * 2);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(GenotypeCalculationArgumentCollection genotypeArgs) {\n+        return assumingHW(Math.log10(genotypeArgs.snpHeterozygosity),\n+                          Math.log10(genotypeArgs.indelHeterozygosity));\n+    }\n+\n+\n+    @Override\n+    public double[] getLog10Priors(final GenotypeLikelihoodCalculator lkCalculator, final List<Allele> alleles) {\n+        final int[] alleleTypes = calculateAlleleTypes(alleles);\n+        final int numberOfGenotypes = lkCalculator.genotypeCount();\n+        final double[] result = new double[numberOfGenotypes];\n+        // implied = result[0] = 0.0;\n+        for (int g = 1; g < numberOfGenotypes; g++) {\n+            final GenotypeAlleleCounts gac = lkCalculator.genotypeAlleleCountsAt(g);\n+            final int numberOfDistictAlleles = gac.distinctAlleleCount();\n+            double log10Sum = 0;\n+            for (int a = 0; a < numberOfDistictAlleles; a++) {\n+                final int idx = gac.alleleIndexAt(a);\n+                final int cnt = gac.alleleCountAt(a);\n+                if (cnt == 1) {\n+                    log10Sum += hetValues[alleleTypes[idx]];\n+                } else if (cnt == 2) {\n+                    log10Sum += homValues[alleleTypes[idx]];\n+                } else { // for plodies over 2 and allele counts over 2 then we use the het/hom ratio for the rest\n+                    log10Sum += hetValues[alleleTypes[idx]] + diffValues[alleleTypes[idx]] * (cnt - 1);\n+                }\n+            }\n+            result[g] = log10Sum;\n+        }\n+        return result;\n+    }\n+\n+    private int[] calculateAlleleTypes(final List<Allele> alleles) {\n+        if (alleles.isEmpty()) {\n+            throw new IllegalArgumentException(\"there must be at least one allele (the reference)\");\n+        } else {\n+            final int[] result = new int[alleles.size()];\n+            Arrays.fill(result, AlleleType.OTHER.ordinal());\n+            final Allele refAllele = alleles.get(0);\n+            if (!refAllele.isReference()) {\n+                throw new IllegalArgumentException(\"the first allele in the list must be the reference\");\n+            }\n+            final int refAlleleLength = refAllele.length();\n+            result[0] = AlleleType.REF.ordinal();\n+            for (int i = 1; i < result.length; i++) {\n+                final Allele allele = alleles.get(i);\n+                if (allele.isCalled()) {\n+                    if (!allele.isSymbolic()) {\n+                        result[i] = (allele.length() == refAlleleLength ? AlleleType.SNP : AlleleType.INDEL).ordinal();\n+                    } else if (allele.equals(Allele.SV_SIMPLE_INS) || allele.equals(Allele.SV_SIMPLE_DEL)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI1MzA0Mg==", "bodyText": "I would maybe rename this/make a comment to the effect that this is that DRAGEN genotype prior that is being calculated.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438253042", "createdAt": "2020-06-10T16:23:15Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/SimpleGenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SimpleGenotypePriorCalculator implements GenotypePriorCalculator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI1MzU0NA==", "bodyText": "Add comments explaining these two methods", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r438253544", "createdAt": "2020-06-10T16:23:58Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/SimpleGenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SimpleGenotypePriorCalculator implements GenotypePriorCalculator {\n+\n+\n+\n+    private enum AlleleType {\n+        REF, SNP, INDEL, OTHER;\n+    }\n+\n+    private final double[] hetValues;\n+    private final double[] homValues;\n+    private final double[] diffValues;\n+\n+    private SimpleGenotypePriorCalculator(final double snpHet, final double snpHom,\n+                                          final double indelHet, final double indelHom,\n+                                          final double otherHet, final double otherHom) {\n+        hetValues = new double[4];\n+        homValues = new double[4];\n+        diffValues = new double[4];\n+\n+        // SNPs: // * 1/3 since there is three possible mutations for SNPs.\n+        hetValues[1] = snpHet - Math.log10(3);\n+        homValues[1] = snpHom - Math.log10(3);\n+        // INDELs:\n+        hetValues[2] = indelHet;\n+        homValues[2] = indelHom;\n+        // Others:\n+        hetValues[3] = otherHet;\n+        homValues[3] = otherHom;\n+\n+        for (int i = 0; i < 4; i++) {\n+            diffValues[i] = homValues[i] - hetValues[i];\n+        }\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenHetToHomRatio(final double snpHet, final double indelHet,\n+                                                                   final double otherHet, final double het_hom_ratio) {\n+        final double log10Ratio = Math.log10(het_hom_ratio);\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet - log10Ratio,\n+                                                  indelHet, indelHet - log10Ratio,\n+                                                  otherHet, otherHet - log10Ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenDragstrParams(final DragstrParams dragstrParams, final int period,\n+                                                                   final int repeats, final double snpHeterozygosity,\n+                                                                   final double het_hom_ratio) {\n+        final double snpHet = snpHeterozygosity;\n+        final double indelHet = -.1 * dragstrParams.api(period, repeats);\n+        final double otherHet = Math.max(snpHet, indelHet);\n+        return givenHetToHomRatio(snpHet, indelHet, otherHet, het_hom_ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(final double snpHet, final double indelHet) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI0ODk0Nw==", "bodyText": "Add advanced, add doc, add comments.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r440248947", "createdAt": "2020-06-15T15:13:04Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeCalculationArgumentCollection.java", "diffHunk": "@@ -49,6 +57,16 @@ public GenotypeCalculationArgumentCollection( final GenotypeCalculationArgumentC\n         this.numRefIfMissing = other.numRefIfMissing;\n     }\n \n+    @Argument(fullName = \"dont-use-dragstr-priors\", optional = true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI0OTI5MA==", "bodyText": "Should this be depricated?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r440249290", "createdAt": "2020-06-15T15:13:29Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeCalculationArgumentCollection.java", "diffHunk": "@@ -49,6 +57,16 @@ public GenotypeCalculationArgumentCollection( final GenotypeCalculationArgumentC\n         this.numRefIfMissing = other.numRefIfMissing;\n     }\n \n+    @Argument(fullName = \"dont-use-dragstr-priors\", optional = true)\n+    public boolean dontUseDragstrPriors = false;\n+\n+    /**\n+     * As of version 4.1.0.0, this argument is no longer needed because the new qual score is now on by default. See GATK 3.3 release notes for more details.\n+     */\n+    @Deprecated", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU4MjE3OA==", "bodyText": "Self: restore this validation.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r441582178", "createdAt": "2020-06-17T14:19:35Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculator.java", "diffHunk": "@@ -137,22 +121,15 @@\n      */\n     private double[] readGenotypeLikelihoodComponents;\n \n-    /**\n-     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n-     */\n-    protected GenotypeLikelihoodCalculator(final int ploidy, final int alleleCount,\n-                                           final int[][] alleleFirstGenotypeOffsetByPloidy,\n-                                           final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n-        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3Mjg1Mg==", "bodyText": "@vruano", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447272852", "createdAt": "2020-06-29T21:46:12Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java", "diffHunk": "@@ -177,8 +189,66 @@ public VariantContext calculateGenotypes(final VariantContext vc, final List<Var\n         return builder.genotypes(genotypes).attributes(attributes).make();\n     }\n \n+    protected double nonVariantPresentLog10PosteriorProbability(final GenotypesContext gc) {\n+        return gc.stream()\n+                .map(gt -> gt.getExtendedAttribute(VCFConstants.GENOTYPE_POSTERIORS_KEY))\n+                .mapToDouble(v -> coherceToDouble(v, Double.NaN, true))\n+                .filter(v -> !Double.isNaN(v))\n+                .min().orElse(Double.NaN);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0MTU4OA=="}, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3NDcwNg==", "bodyText": "Make a method to compute this interval on dragstr params and add a test (in particular make sure its doing something sane arround interval boundaries).", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447274706", "createdAt": "2020-06-29T21:50:45Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFsEngine.java", "diffHunk": "@@ -118,7 +121,12 @@ public VariantContext callRegion(Locatable loc, List<VariantContext> variants, R\n     {\n         final List<VariantContext> variantsToProcess = getVariantSubsetToProcess(loc, variants);\n \n-        ref.setWindow(10, 10); //TODO this matches the gatk3 behavior but may be unnecessary\n+        if (dragStrParams == null || !genotypeArgs.dontUseDragstrPriors) {\n+            ref.setWindow(10, 10); //TODO this matches the gatk3 behavior but may be unnecessary\n+        } else {\n+            ref.setWindow(dragStrParams.maximumPeriod() * dragStrParams.maximumRepeats(), dragStrParams.maximumPeriod() * dragStrParams.maximumRepeats());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3NTEyMQ==", "bodyText": "Make a method for calculating this interval: (and add a test that its doing something sane (especially since i think this doesn't check for invalid contig ends).", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447275121", "createdAt": "2020-06-29T21:51:41Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/MinimalGenotypingEngine.java", "diffHunk": "@@ -42,5 +70,29 @@ protected boolean forceKeepAllele(final Allele allele) {\n     protected String callSourceString() {\n         return \"UG_call\";\n     }\n+\n+    @Override\n+    public VariantContext calculateGenotypes(final VariantContext vc) {\n+        if (dragstrParams == null || getConfiguration().genotypeArgs.dontUseDragstrPriors || !GATKVariantContextUtils.containsInlineIndel(vc) || referenceContext == null) {\n+            final SimpleGenotypePriorCalculator gpc = SimpleGenotypePriorCalculator.assumingHW(configuration.genotypeArgs);\n+            return calculateGenotypes(vc, gpc, Collections.emptyList());\n+        } else {\n+            final SimpleInterval interval = new SimpleInterval(vc.getContig(), Math.max(1, vc.getStart() - dragstrParams.maximumPeriod() * dragstrParams.maximumRepeats()), vc.getStart() - dragstrParams.maximumPeriod() * dragstrParams.maximumRepeats());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3NTQ0OQ==", "bodyText": "Since this is an interface now this needs documentation explaining what the expected method use cases are.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447275449", "createdAt": "2020-06-29T21:52:29Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/afcalc/AlleleFrequencyCalculator.java", "diffHunk": "@@ -1,227 +1,18 @@\n package org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc;\n \n import htsjdk.variant.variantcontext.Allele;\n-import htsjdk.variant.variantcontext.Genotype;\n import htsjdk.variant.variantcontext.VariantContext;\n-import it.unimi.dsi.fastutil.doubles.DoubleArrayList;\n-import it.unimi.dsi.fastutil.ints.Int2ObjectArrayMap;\n-import org.apache.commons.math3.special.Gamma;\n-import org.apache.commons.math3.util.MathArrays;\n-import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n-import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n-import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n-import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators;\n-import org.broadinstitute.hellbender.utils.Dirichlet;\n-import org.broadinstitute.hellbender.utils.IndexRange;\n-import org.broadinstitute.hellbender.utils.MathUtils;\n-import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n \n-import java.util.Arrays;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.stream.Collectors;\n-import java.util.stream.IntStream;\n+public interface AlleleFrequencyCalculator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3NjMyOA==", "bodyText": "They are from DRAGEN, it is phred of log(1/1000 * (1/3)) as a prior for SNPs.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447276328", "createdAt": "2020-06-29T21:54:38Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/afcalc/DragstrAlleleFrequencyCalculator.java", "diffHunk": "@@ -0,0 +1,51 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+public class DragstrAlleleFrequencyCalculator implements AlleleFrequencyCalculator {\n+\n+    private static final double HOM_REF_PRIOR = 0;\n+    private static final double SNP_SIMPLE_HET_PRIOR = 34.77;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NjQ0OA=="}, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3NjY1Mw==", "bodyText": "If these aren't supported operations for this AF calculator then you should thorw exceptions on these methods.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447276653", "createdAt": "2020-06-29T21:55:25Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/afcalc/DragstrAlleleFrequencyCalculator.java", "diffHunk": "@@ -0,0 +1,51 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+public class DragstrAlleleFrequencyCalculator implements AlleleFrequencyCalculator {\n+\n+    private static final double HOM_REF_PRIOR = 0;\n+    private static final double SNP_SIMPLE_HET_PRIOR = 34.77;\n+    private static final double SNP_COMPOSITE_HET_PRIOR = 69.54;\n+    private static final double SNP_HOMVAR_PRIOR = 37.77;\n+\n+    private final double api;\n+    private final int defaultPloidy;\n+\n+    private DragstrAlleleFrequencyCalculator(final double api, final int defaultPloidy) {\n+        this.api = api;\n+        this.defaultPloidy = defaultPloidy;\n+    }\n+\n+    public static DragstrAlleleFrequencyCalculator makeCalculator(final DragstrParams params, final int period, final int repeats, final int defaultPloidy) {\n+        return new DragstrAlleleFrequencyCalculator(params.api(period, repeats), defaultPloidy);\n+    }\n+\n+    @Override\n+    public int getPloidy() {\n+        return defaultPloidy;\n+    }\n+\n+    @Override\n+    public double[] getPriorFrequencies(AlleleList<Allele> alleleList) {\n+        return new double[0];\n+    }\n+\n+    @Override\n+    public AFCalculationResult calculate(VariantContext vc) {\n+        return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3ODA5MQ==", "bodyText": "Note to self: This code is a duplicate of #6661", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447278091", "createdAt": "2020-06-29T21:58:50Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyResultSet.java", "diffHunk": "@@ -519,13 +520,40 @@ public void regenerateVariationEvents(int maxMnpDistance) {\n     }\n \n     /**\n-     * Get all of the VariantContexts in the event maps for all haplotypes, sorted by their start position\n+     * Get all of the VariantContexts in the event maps for all haplotypes, sorted by their start position and then arbitrarily by indel length followed by bases\n      * @param haplotypes the set of haplotypes to grab the VCs from\n      * @return a sorted set of variant contexts\n      */\n     private static SortedSet<VariantContext> getAllVariantContexts( final List<Haplotype> haplotypes ) {\n         // Using the cigar from each called haplotype figure out what events need to be written out in a VCF file\n-        final TreeSet<VariantContext> vcs = new TreeSet<>(Comparator.comparingInt(VariantContext::getStart));\n+        final TreeSet<VariantContext> vcs = new TreeSet<>(\n+                Comparator.comparingInt(VariantContext::getStart)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3ODE5MA==", "bodyText": "Javadoc this class", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447278190", "createdAt": "2020-06-29T21:59:05Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/DragstrPairHMMInputScoreImputator.java", "diffHunk": "@@ -0,0 +1,57 @@\n+package org.broadinstitute.hellbender.tools.walkers.haplotypecaller;\n+\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReadSTRAnalizer;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+public class DragstrPairHMMInputScoreImputator implements PairHMMInputScoreImputator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3ODQ0Ng==", "bodyText": "Make an informative constant name and docuemnt this in the class", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447278446", "createdAt": "2020-06-29T21:59:44Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/DragstrPairHMMInputScoreImputator.java", "diffHunk": "@@ -0,0 +1,57 @@\n+package org.broadinstitute.hellbender.tools.walkers.haplotypecaller;\n+\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReadSTRAnalizer;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+public class DragstrPairHMMInputScoreImputator implements PairHMMInputScoreImputator {\n+\n+    private final DragstrParams params;\n+\n+    public static DragstrPairHMMInputScoreImputator newInstance(final String path) {\n+        return new DragstrPairHMMInputScoreImputator(new DragstrParams(path));\n+    }\n+\n+    public static DragstrPairHMMInputScoreImputator newInstance(final DragstrParams params) {\n+        return new DragstrPairHMMInputScoreImputator(params);\n+    }\n+\n+    private DragstrPairHMMInputScoreImputator(final DragstrParams params) {\n+        this.params = params;\n+    }\n+\n+    @Override\n+    public PairHMMInputScoreImputation impute(final GATKRead read) {\n+        final byte[] bases = read.getBases();\n+        final DragstrReadSTRAnalizer analyzer = DragstrUtils.repeatPeriodAndCounts(read.getLength(), params.maximumPeriod());\n+        analyzer.load(bases);\n+        final int length = bases.length;\n+        final byte[] gop = new byte[length];\n+        final byte[] gcp = new byte[length];\n+        for (int i = 0; i < length - 1; i++) {\n+            final int period = analyzer.mostRepeatedPeriod(i + 1);\n+            final int repeats = analyzer.numberOfMostRepeats(i + 1);\n+            gop[i] = (byte) params.gop(period, repeats);\n+            gcp[i] = (byte) params.gcp(period, repeats);\n+        }\n+        gop[length - 1] = 45;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1Nzk2MQ=="}, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3ODczMg==", "bodyText": "Document why +1 here", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447278732", "createdAt": "2020-06-29T22:00:24Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/DragstrPairHMMInputScoreImputator.java", "diffHunk": "@@ -0,0 +1,57 @@\n+package org.broadinstitute.hellbender.tools.walkers.haplotypecaller;\n+\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReadSTRAnalizer;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+public class DragstrPairHMMInputScoreImputator implements PairHMMInputScoreImputator {\n+\n+    private final DragstrParams params;\n+\n+    public static DragstrPairHMMInputScoreImputator newInstance(final String path) {\n+        return new DragstrPairHMMInputScoreImputator(new DragstrParams(path));\n+    }\n+\n+    public static DragstrPairHMMInputScoreImputator newInstance(final DragstrParams params) {\n+        return new DragstrPairHMMInputScoreImputator(params);\n+    }\n+\n+    private DragstrPairHMMInputScoreImputator(final DragstrParams params) {\n+        this.params = params;\n+    }\n+\n+    @Override\n+    public PairHMMInputScoreImputation impute(final GATKRead read) {\n+        final byte[] bases = read.getBases();\n+        final DragstrReadSTRAnalizer analyzer = DragstrUtils.repeatPeriodAndCounts(read.getLength(), params.maximumPeriod());\n+        analyzer.load(bases);\n+        final int length = bases.length;\n+        final byte[] gop = new byte[length];\n+        final byte[] gcp = new byte[length];\n+        for (int i = 0; i < length - 1; i++) {\n+            final int period = analyzer.mostRepeatedPeriod(i + 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3OTA1OQ==", "bodyText": "@vruano", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447279059", "createdAt": "2020-06-29T22:01:18Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/LikelihoodEngineArgumentCollection.java", "diffHunk": "@@ -21,10 +22,23 @@\n     @Argument(fullName = \"base-quality-score-threshold\", doc = \"Base qualities below this threshold will be reduced to the minimum (\" + QualityUtils.MIN_USABLE_Q_SCORE + \")\", optional = true)\n     public byte BASE_QUALITY_SCORE_THRESHOLD = PairHMM.BASE_QUALITY_SCORE_THRESHOLD;\n \n+    @Argument(fullName=\"dragstr-params-path\", doc=\"location of the DRAGstr model parameters for STR error correction used in the Pair HMM. When provided, it overrides other PCR error correcting mechanisms\", optional = true)\n+    public DragstrParams dragstrParams = null;\n+\n+    @Argument(fullName=\"dragstr-het-hom-ratio\", doc=\"het to hom prior ratio use with DRAGstr on\", optional = true)\n+    public int dragstrHetHomRatio = 2;\n+\n+    @Argument(fullName=\"dont-use-dragstr-pair-hmm-scores\", doc=\"disable DRAGstr pair-hmm score when dragstr-params-path was provided\", optional = false)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0NzIyOQ=="}, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4MDA2OQ==", "bodyText": "NOTE TO SELF: do this", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447280069", "createdAt": "2020-06-29T22:03:29Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/PairHMMLikelihoodCalculationEngine.java", "diffHunk": "@@ -164,14 +193,76 @@ public void close() {\n             computeReadLikelihoods(result.sampleMatrix(i));\n         }\n \n-        result.normalizeLikelihoods(log10globalReadMismappingRate);\n-        result.filterPoorlyModeledEvidence(log10MinTrueLikelihood(EXPECTED_ERROR_RATE_PER_BASE));\n+        result.normalizeLikelihoods(log10globalReadMismappingRate, symmetricallyNormalizeAllelesToReference);\n+\n+        if (dynamicDisqualification) {\n+            result.filterPoorlyModeledEvidence(daynamicLog10MinLiklihoodModel(readDisqualificationScale, log10MinTrueLikelihood(expectedErrorRatePerBase, false)), genotyperDebugOutStream);\n+        } else {\n+            result.filterPoorlyModeledEvidence(log10MinTrueLikelihood(expectedErrorRatePerBase, true), genotyperDebugOutStream);\n+        }\n         return result;\n     }\n \n-    private ToDoubleFunction<GATKRead> log10MinTrueLikelihood(final double maximumErrorPerBase) {\n+    private ToDoubleFunction<GATKRead> daynamicLog10MinLiklihoodModel(final double dynamicRadQualConstant, final ToDoubleFunction<GATKRead> log10MinTrueLikelihood) {\n+        return read -> {\n+            double dynamicThreshold = calculateDynamicThreshold(read, dynamicRadQualConstant);\n+            double log10MaxLikelihoodForTrueAllele = log10MinTrueLikelihood.applyAsDouble(read);\n+            if (dynamicThreshold < log10MaxLikelihoodForTrueAllele ) {\n+                if (genotyperDebugOutStream != null) {\n+                    genotyperDebugOutStream.println(\"For read \"+ read.getName() + \" replacing old threshold (\"+log10MaxLikelihoodForTrueAllele+\") with new threshold: \"+dynamicThreshold);\n+                }\n+                return dynamicThreshold;\n+            } else {\n+                return log10MaxLikelihoodForTrueAllele;\n+            }\n+        };\n+    }\n+\n+    static double calculateDynamicThreshold(final GATKRead read, final double dynamicRadQualConstant) {\n+        double sumMean = 0;\n+        double sumVariance = 0;\n+        byte[] baseQualities = read.getTransientAttribute(\"HMMQuals\") != null ?\n+                (byte[]) read.getTransientAttribute(\"HMMQuals\") : read.getBaseQualities();\n+\n+        for( int i = 0; i < baseQualities.length; i++) {\n+            int bq = baseQualities[i];\n+            int boundedBq = bq < 1 ? 1 : bq > 40 ? 40 : bq;\n+            sumMean +=      dynamicReadQualThreshLookupTable[ (boundedBq - 1) * 3 + 1];\n+            sumVariance +=  dynamicReadQualThreshLookupTable[ (boundedBq - 1) * 3 + 2];\n+        }\n+\n+        double threshold = sumMean + dynamicRadQualConstant * Math.sqrt(sumVariance);\n+        return threshold / -10;\n+    }\n+\n+\n+    // TODO i don't like having a lookup table be static like this, i would prefer this be computed at initialization (with the default values being saved as a test)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4MDQ0Mg==", "bodyText": "Add a javadoc explaining this class.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447280442", "createdAt": "2020-06-29T22:04:19Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/StandardPairHMMInputScoreImputator.java", "diffHunk": "@@ -0,0 +1,51 @@\n+package org.broadinstitute.hellbender.tools.walkers.haplotypecaller;\n+\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.util.Arrays;\n+\n+public class StandardPairHMMInputScoreImputator implements PairHMMInputScoreImputator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4MTQwMA==", "bodyText": "Sorry is this not recursively calling itself?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447281400", "createdAt": "2020-06-29T22:06:16Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/StandardPairHMMInputScoreImputator.java", "diffHunk": "@@ -0,0 +1,51 @@\n+package org.broadinstitute.hellbender.tools.walkers.haplotypecaller;\n+\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.util.Arrays;\n+\n+public class StandardPairHMMInputScoreImputator implements PairHMMInputScoreImputator {\n+\n+    private final byte constantGCP;\n+\n+    private final byte defaultGOP;\n+\n+    private StandardPairHMMInputScoreImputator(final byte defaultGOP, final byte constantGCP) {\n+        this.constantGCP = constantGCP;\n+        this.defaultGOP = defaultGOP;\n+    }\n+\n+    public static StandardPairHMMInputScoreImputator newInstance(final byte defaultGOP, final byte constantGCP) {\n+        return new StandardPairHMMInputScoreImputator(defaultGOP, constantGCP);\n+    }\n+\n+    @Override\n+    public PairHMMInputScoreImputation impute(final GATKRead read) {\n+        return new PairHMMInputScoreImputation() {\n+            @Override\n+            public byte[] delOpenPenalties() {\n+                final byte[] existing = ReadUtils.getExistingBaseDeletionQualities(read);\n+                return existing == null ? defaultGOPs(read) : existing;\n+            }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4MjM3Mw==", "bodyText": "This looks like it should infinitely loop if this lambda ends up calling any of these methods and doesn't have pre-filled qualities because it'll get stuck calling defaultGOPs(read) forever. You should add a test that this codepath works on its own.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447282373", "createdAt": "2020-06-29T22:08:26Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/StandardPairHMMInputScoreImputator.java", "diffHunk": "@@ -0,0 +1,51 @@\n+package org.broadinstitute.hellbender.tools.walkers.haplotypecaller;\n+\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.util.Arrays;\n+\n+public class StandardPairHMMInputScoreImputator implements PairHMMInputScoreImputator {\n+\n+    private final byte constantGCP;\n+\n+    private final byte defaultGOP;\n+\n+    private StandardPairHMMInputScoreImputator(final byte defaultGOP, final byte constantGCP) {\n+        this.constantGCP = constantGCP;\n+        this.defaultGOP = defaultGOP;\n+    }\n+\n+    public static StandardPairHMMInputScoreImputator newInstance(final byte defaultGOP, final byte constantGCP) {\n+        return new StandardPairHMMInputScoreImputator(defaultGOP, constantGCP);\n+    }\n+\n+    @Override\n+    public PairHMMInputScoreImputation impute(final GATKRead read) {\n+        return new PairHMMInputScoreImputation() {\n+            @Override\n+            public byte[] delOpenPenalties() {\n+                final byte[] existing = ReadUtils.getExistingBaseDeletionQualities(read);\n+                return existing == null ? defaultGOPs(read) : existing;\n+            }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4MTQwMA=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI4NTU3OQ==", "bodyText": "this class desperately needs extensive javadocs explaining its function in relation to the existing pileup class interface.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447285579", "createdAt": "2020-06-29T22:14:08Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/IntervalPileup.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package org.broadinstitute.hellbender.utils;\n+\n+import com.google.inject.ImplementedBy;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.util.Locatable;\n+import org.apache.commons.lang3.builder.EqualsExclude;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import javax.validation.OverridesAttribute;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+public interface IntervalPileup {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI5MDM5NQ==", "bodyText": "I don't think this refactoring was equivalent, there is a gap at the 0x80 base that is now being handled whereas before it was not. Can you explain why this change was made? Can it be pulled into a seperate PR with tests that can be reviewed by somebody else?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447290395", "createdAt": "2020-06-29T22:19:42Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/Nucleotide.java", "diffHunk": "@@ -284,10 +284,11 @@ public static Nucleotide decode(final byte base) {\n      * to a valid nucleotide specification.\n      */\n     public static Nucleotide decode(final char ch) {\n-        if ((ch & 0xFF00) != 0) {\n-            return INVALID;\n+        if ((ch & 0xFFFFFF80) == 0) { // all valid codes have ascii lower than 127 so we may just treat all", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMxMTM1Nw==", "bodyText": "Why is it necessary to shuffle the ordering in this test? Should it not be sufficient to test every combination of repeated count/periods?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447311357", "createdAt": "2020-06-29T23:14:25Z", "author": {"login": "jamesemery"}, "path": "src/test/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrUtilsTest.java", "diffHunk": "@@ -0,0 +1,177 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.broadinstitute.hellbender.utils.RandomDNA;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.IntStream;\n+\n+\n+public class DragstrUtilsTest {\n+\n+    @Test(dataProvider = \"testSequenceAndMaxPeriodData\")\n+    public void testRepeatPeriodAndCount(final String sequenceStr, final int maxPeriod, final boolean includeUpstream) {\n+        final byte[] sequence = sequenceStr.getBytes();\n+        final DragstrReadSTRAnalizer rpc = DragstrUtils.repeatPeriodAndCounts(sequence.length, maxPeriod);\n+        rpc.load(sequence);\n+        final Random rdn = new Random(Arrays.hashCode(sequence) * 31 + maxPeriod);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMxMTYyOQ==", "bodyText": "Have these link to global defaults that are labeled", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447311629", "createdAt": "2020-06-29T23:15:11Z", "author": {"login": "jamesemery"}, "path": "src/test/java/org/broadinstitute/hellbender/utils/pairhmm/PairHMMUnitTest.java", "diffHunk": "@@ -518,12 +520,13 @@ public void testLikelihoodsFromHaplotypes(final PairHMM hmm){\n \n         final byte[] readQuals= Utils.dupBytes(baseQual, readBases.length);\n         final List<GATKRead> reads = Arrays.asList(ArtificialReadUtils.createArtificialRead(readBases, readQuals, readBases.length + \"M\"));\n-        final Map<GATKRead, byte[]> gpcs = buildGapContinuationPenalties(reads, gcp);\n \n-        hmm.computeLog10Likelihoods(matrix(Arrays.asList(refH)), Collections.emptyList(), gpcs);\n+        final PairHMMInputScoreImputator inputScoreImputator = StandardPairHMMInputScoreImputator.newInstance((byte) 45, (byte) 100);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMxMzM5Mg==", "bodyText": "This calculate method is fairly complicated and seems somewhat error prone. I would suggest hard coding a battery of specific cases and arrays of expected values for each position into this to supplement the random tests. Its very difficult to have insight into random tests like this especially when they are also shuffling inside of the test itself. It seems to me that the order of operations should not matter and thus beyond randomly generating test strings this seems unnecessary.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447313392", "createdAt": "2020-06-29T23:20:41Z", "author": {"login": "jamesemery"}, "path": "src/test/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrUtilsTest.java", "diffHunk": "@@ -0,0 +1,177 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.broadinstitute.hellbender.utils.RandomDNA;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.IntStream;\n+\n+\n+public class DragstrUtilsTest {\n+\n+    @Test(dataProvider = \"testSequenceAndMaxPeriodData\")\n+    public void testRepeatPeriodAndCount(final String sequenceStr, final int maxPeriod, final boolean includeUpstream) {\n+        final byte[] sequence = sequenceStr.getBytes();\n+        final DragstrReadSTRAnalizer rpc = DragstrUtils.repeatPeriodAndCounts(sequence.length, maxPeriod);\n+        rpc.load(sequence);\n+        final Random rdn = new Random(Arrays.hashCode(sequence) * 31 + maxPeriod);\n+        final int[] positions = new int[sequence.length];\n+        for (int i = 0; i < positions.length; i++) {\n+            positions[i] = i;\n+        }\n+        ArrayUtils.shuffle(positions, rdn);\n+        for (int position : positions) {\n+            final int[] periods = IntStream.range(1, maxPeriod + 1).toArray();\n+            ArrayUtils.shuffle(periods, rdn);\n+            for (final int period : periods) {\n+                final int repeatCount = rpc.numberOfRepeats(position, period);\n+                final int expected = calculate(sequence, position, period);\n+                Assert.assertEquals(repeatCount, expected, new String(sequence) + \" \" + position + \" \" + period);\n+            }\n+        }\n+    }\n+\n+    @Test(dataProvider = \"testSequenceAndMaxPeriodData\")\n+    public void testRepeatBestPeriodAndCount(final String sequenceStr, final int maxPeriod) {\n+        final Random rdn = new Random(sequenceStr.hashCode() * 31 + maxPeriod);\n+\n+        testRepeatBestPeriodAndCount(sequenceStr.getBytes(), maxPeriod, 0, sequenceStr.length(), rdn);\n+        testRepeatBestPeriodAndCount(sequenceStr.getBytes(), maxPeriod, 0, 0, rdn);\n+        // random start and ends:\n+        final int randomTries =  Math.min(sequenceStr.length() * sequenceStr.length() * 4, 100);\n+        for (int i = 0; i < randomTries; i++) {\n+            final int start = rdn.nextInt(sequenceStr.length());\n+            final int end = rdn.nextInt(sequenceStr.length() - start) + start;\n+            testRepeatBestPeriodAndCount(sequenceStr.getBytes(), maxPeriod, start, end, rdn);\n+        }\n+    }\n+\n+    private void testRepeatBestPeriodAndCount(final byte[] sequence, final int maxPeriod, final int start, final int end, final Random rdn) {\n+        final DragstrReadSTRAnalizer rpc = DragstrUtils.repeatPeriodAndCounts(sequence.length, maxPeriod);\n+        if (start == 0 && end == sequence.length && rdn.nextDouble() <= 0.5) { // sometimes use the margin free method when applies to test it.\n+            rpc.load(sequence);\n+        } else {\n+            rpc.load(sequence);\n+        }\n+        final int[] positions = new int[end - start];\n+        for (int i = 0; i < positions.length; i++) {\n+            positions[i] = i + start;\n+        }\n+        ArrayUtils.shuffle(positions, rdn);\n+        for (int position : positions) {\n+                final int[] expected = calculateBestPeriodAndRepeat(sequence, position, maxPeriod);\n+                final int bestPeriod = rpc.mostRepeatedPeriod(position);\n+                final int bestRepeat = rpc.numberOfMostRepeats(position);\n+                try {\n+                    Assert.assertEquals(bestPeriod, expected[0], new String(sequence) + \" \" + position + \" \" + start + \" \" + end);\n+                } catch (final AssertionError err) {\n+                    rpc.load(sequence);\n+                    throw err;\n+                }\n+                Assert.assertEquals(bestRepeat, expected[1], new String(sequence) + \" \" + position);\n+        }\n+    }\n+\n+\n+    /**\n+     * Brute force approach to calcuate the \"best\" repeat unit length and repeat count. The best is the one\n+     * that has a maximum number of repeated units. In case of a tie, the smaller unit is considered a better\n+     * one.\n+     * @param sequence\n+     * @param position\n+     * @param maxPeriod\n+     * @return\n+     */\n+    public static int[] calculateBestPeriodAndRepeat(final byte[] sequence, final int position, final int maxPeriod) {\n+        final int[] result = new int[2];\n+        result[0] = 1;\n+        result[1] = calculate(sequence, position, 1);\n+        for (int period = 2; period <= maxPeriod; period++) {\n+            final int candidate = calculate(sequence, position, period);\n+            if (candidate > result[1]) {\n+                result[0] = period;\n+                result[1] = candidate;\n+            }\n+        }\n+        return result;\n+    }\n+\n+    public static int calculate(final byte[] sequence, final int position, final int period) {\n+        if (period > sequence.length) {\n+            return 0;\n+        }\n+        final int start = Math.max(0, position - period + 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMxNDI2OQ==", "bodyText": "This seems like a weak tests. Essentially asserting that the tests are not producing infinite values. I would recommend hardcoding this array into this test for this particular file", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447314269", "createdAt": "2020-06-29T23:23:07Z", "author": {"login": "jamesemery"}, "path": "src/test/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrParamsUnitTest.java", "diffHunk": "@@ -0,0 +1,41 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.testutils.BaseTest;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+/**\n+ * Unit test for class {@link DragstrParams}.\n+ */\n+public class DragstrParamsUnitTest extends BaseTest {\n+\n+    public static final String TEST_PARAMS_FILE = \"org/broadinstitute/hellbender/utils/dragstr/dna_nexus_novaseq_plus0_0_params.txt\";\n+\n+\n+    @Test\n+    public void testLoad() {\n+        final String filePath = (\"src/test/resources/\" +  TEST_PARAMS_FILE);\n+        final DragstrParams params = new DragstrParams(filePath);\n+        Assert.assertNotNull(params);\n+        Assert.assertEquals(params.maximumPeriod(), 8);\n+        Assert.assertEquals(params.maximumRepeats(), 20);\n+    }\n+\n+    @Test\n+    public void testQueries() {\n+        final String filePath = (\"src/test/resources/\" +  TEST_PARAMS_FILE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMxNTY1MA==", "bodyText": "Use try with resources syntax for writing tests that open file streams to ensure we aren't leaking file handles for the test suite try (TableWriter<DragstrLocus> textWriter = DragstrLocus.textWriter(new FileOutputStream(textFile));  BinaryTableWriter<DragstrLocus> binaryWriter = DragstrLocus.binaryWriter(binaryFile)) { ...", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447315650", "createdAt": "2020-06-29T23:27:14Z", "author": {"login": "jamesemery"}, "path": "src/test/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrLocusUnitTest.java", "diffHunk": "@@ -0,0 +1,69 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.utils.RandomDNA;\n+import org.broadinstitute.hellbender.utils.tsv.TableWriter;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+\n+public class DragstrLocusUnitTest {\n+\n+    @Test()\n+    private void testWriteRead() throws IOException {\n+        final File binaryFile = File.createTempFile(\"test-dl\", \".bin\");\n+        final File textFile = File.createTempFile(\"test-dl\", \".tab\");\n+        //binaryFile.deleteOnExit();\n+        //textFile.deleteOnExit();\n+        final BinaryTableWriter<DragstrLocus> binaryWriter = DragstrLocus.binaryWriter(binaryFile);\n+        final TableWriter<DragstrLocus> textWriter = DragstrLocus.textWriter(new FileOutputStream(textFile));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMxNTk4Mw==", "bodyText": "Also this test is never run because its marked as private.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447315983", "createdAt": "2020-06-29T23:28:27Z", "author": {"login": "jamesemery"}, "path": "src/test/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrLocusUnitTest.java", "diffHunk": "@@ -0,0 +1,69 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.utils.RandomDNA;\n+import org.broadinstitute.hellbender.utils.tsv.TableWriter;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+\n+public class DragstrLocusUnitTest {\n+\n+    @Test()\n+    private void testWriteRead() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMxNjk0Mw==", "bodyText": "You should annotate each of these cases with some indication of what is going on at the site in question and what the case is testing.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447316943", "createdAt": "2020-06-29T23:31:29Z", "author": {"login": "jamesemery"}, "path": "src/test/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCaseSamplerUnitTest.java", "diffHunk": "@@ -0,0 +1,55 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceFileSource;\n+import org.broadinstitute.hellbender.utils.IntervalPileup;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+\n+public class DragstrCaseSamplerUnitTest {\n+\n+\n+    private static final String TEST_BAM = \"/Users/valentin/Analysis/dragstr/NA12878.bam\";\n+    private static final String TEST_REF = \"/Users/valentin/Analysis/dragstr/download/DRAGstr/references/hg38_alt_aware.fa\";\n+\n+    @Test(dataProvider = \"bugCasesData\")\n+    public void testBugCases(final int chridx, final int pos, final String unit, final int repeats, final int expectedK, final int expectedN) {\n+        final ReferenceDataSource referenceDataSource = new ReferenceFileSource(new File(TEST_REF).toPath());\n+        final ReadsDataSource readsDataSource = new ReadsDataSource(new File(TEST_BAM).toPath());\n+        final DragstrModelEstimator estimator = new DragstrModelEstimator(new DragstrModelEstimatorArgumentCollection());\n+        final DragstrCasesSampler sampler = new DragstrCasesSampler(new DragstrCasesSamplerArgumentCollection(), referenceDataSource, readsDataSource);\n+        final DragstrLocus locus = DragstrLocus.make(chridx, pos, unit.getBytes(), repeats);\n+        final DragstrModelEstimator.RepeatCases cases = estimator.createPeriodCases(unit.length(), 20, 10).getRepeatCases(repeats);\n+        sampler.sample(cases, Collections.singletonList(locus));\n+        Assert.assertEquals(cases.size(), 1);\n+        Assert.assertEquals(cases.k[0], expectedK);\n+        Assert.assertEquals(cases.n[0], expectedN);\n+    }\n+\n+    @DataProvider\n+    public Object[][] bugCasesData() {\n+        return new Object[][] {\n+                new Object[] {18, 28368088, \"tttc\", 18, 0, 1},", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMxNzY3OQ==", "bodyText": "Obviously this test only works on your local machine. You should be able to use the hg38 reference in our test suite for the test but you should make a paired down file that includes these sites or possibly transpose these sites into another location to demonstrate what you wanted to demonstrate. Either way this references your hard drive.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447317679", "createdAt": "2020-06-29T23:33:27Z", "author": {"login": "jamesemery"}, "path": "src/test/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCaseSamplerUnitTest.java", "diffHunk": "@@ -0,0 +1,55 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceFileSource;\n+import org.broadinstitute.hellbender.utils.IntervalPileup;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+\n+public class DragstrCaseSamplerUnitTest {\n+\n+\n+    private static final String TEST_BAM = \"/Users/valentin/Analysis/dragstr/NA12878.bam\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyMDE2MQ==", "bodyText": "I really think rather than relying on a random number generator to select sites you should simply run over the entire region in question here (or space your intervals by 10000 or something like that). This is only going to lead to confusion", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447320161", "createdAt": "2020-06-29T23:40:54Z", "author": {"login": "jamesemery"}, "path": "src/test/java/org/broadinstitute/hellbender/utils/IntervalPileupUnitTest.java", "diffHunk": "@@ -0,0 +1,553 @@\n+package org.broadinstitute.hellbender.utils;\n+\n+import com.google.common.collect.Iterators;\n+import htsjdk.samtools.Cigar;\n+import htsjdk.samtools.CigarOperator;\n+import org.broadinstitute.hellbender.GATKBaseTest;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceFileSource;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.nio.file.Path;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Unit tests for {@link IntervalPileup} implementations.\n+ */\n+public class IntervalPileupUnitTest extends GATKBaseTest {\n+\n+    private static final String TEST_REFERENCE_FILE = b37_reference_20_21;\n+    private static final String TEST_ALIGNMENT_FILE = NA12878_20_21_WGS_bam;\n+    private static final SimpleInterval[] FIXED_TEST_INTERVALS = Arrays.stream(new String[] {\"20:9,999,900-10,000,100\", \"21:9,999,900-10,000,100\"})\n+            .map(SimpleInterval::new)\n+            .toArray(SimpleInterval[]::new);\n+\n+    @DataProvider(name=\"randomAndFixedIntervalData\")\n+    public Iterator<Object[]> randomAndFixedIntervalData() {\n+        final Iterator<Object[]> fixed = Arrays.stream(fixedIntervalData()).iterator();\n+        final Iterator<Object[]> random = randomIntervalData(29, 100, 99999,25);\n+        final Iterator<Object[]> random2 = randomIntervalData(31, 1, 99,25);\n+        final Iterator<Object[]> random3 = randomIntervalData(7, 0, 0,10);\n+\n+        return Iterators.concat(fixed, random, random2, random3);\n+    }\n+\n+    public Iterator<Object[]> randomIntervalData(final int seed, final int minReads, final int maxReads, final int size) {\n+        final int chr20Length = 63025520; // hardwired length values for convenience... better upload the dictionary.\n+        final int chr21Length = 48129895;\n+        final Path reference = IOUtils.getPath(TEST_REFERENCE_FILE);\n+        final ReferenceDataSource ref = new ReferenceFileSource(reference);\n+        final Path alignment = IOUtils.getPath(TEST_ALIGNMENT_FILE);\n+        final ReadsDataSource readsDataSource = new ReadsDataSource(alignment);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyMTE5Ng==", "bodyText": "javadoc", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447321196", "createdAt": "2020-06-29T23:44:27Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/Utils.java", "diffHunk": "@@ -1406,4 +1463,36 @@ public static void truncate(final List<?> list, final int maxLength) {\n             }\n         }\n     }\n+\n+    public static void flip(final byte[] array, final int from, final int to) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyMjgzOQ==", "bodyText": "If you need to delete a directory rely on fileUtils.deleteDirectory(...) for this functionality rather than adding this method here. I see in both cases you are using this method to clean up a temp dir i would say replace those calls and delete this method from here.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447322839", "createdAt": "2020-06-29T23:49:04Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/Utils.java", "diffHunk": "@@ -1406,4 +1463,36 @@ public static void truncate(final List<?> list, final int maxLength) {\n             }\n         }\n     }\n+\n+    public static void flip(final byte[] array, final int from, final int to) {\n+        for (int i = from, j = to -1; i < j; i++, j--) {\n+            final byte b = array[i];\n+            array[i] = array[j];\n+            array[j] = b;\n+        }\n+    }\n+\n+    public static boolean deleteFileTree(final File base) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyMzI4Nw==", "bodyText": "Javadocs.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447323287", "createdAt": "2020-06-29T23:50:25Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/collections/AutoCloseableList.java", "diffHunk": "@@ -0,0 +1,54 @@\n+package org.broadinstitute.hellbender.utils.collections;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.IntFunction;\n+\n+public class AutoCloseableList<E extends AutoCloseable> implements AutoCloseable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyNTg0OA==", "bodyText": "@lbergelson do you have thoughts on this method?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447325848", "createdAt": "2020-06-29T23:58:09Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/gcs/BucketUtils.java", "diffHunk": "@@ -203,6 +206,58 @@ public static void copyFile(String sourcePath, String destPath) throws IOExcepti\n         }\n     }\n \n+    /**\n+     * If a provided path makes reference to a remote resource, it copies it over to a temporary local file that\n+     * is returned.\n+     * <p>\n+     *     The returned temporary file will be marked for deletion on exit if so requested.\n+     * </p>\n+     * <p>\n+     *     If the source is actually a local file it is considered to be already staged, unless forceStaggingOfLocalFiles is true,\n+     *     and it simply returns the corresponding File object, neither the deletion\n+     *     on exit request nor the tempDir input have any effect.\n+     * </p>\n+     * <p>\n+     *     In order to distinguish between these two scenarios you must call {@link BucketUtils#isCloudStorageUrl(String)}\n+     *     independently.\n+     * </p>\n+     * @param sourcePath\n+     * @param tempDir if provided the temporary file is created under this directory, if needed. If {@code null} the system\n+     *                default temporary file location is used instead.\n+     * @return never {@code null}.\n+     * @throws IOException if such an exeception occurs.\n+     */\n+    public static File stageFile(String sourcePath, final File tempDir, final boolean markForDeletionOnExit, final boolean forceStaggingOfLocalFiles) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyNjQyMg==", "bodyText": "not used, delete", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447326422", "createdAt": "2020-06-29T23:59:55Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/io/IOUtils.java", "diffHunk": "@@ -413,6 +413,15 @@ public static Reader makeReaderMaybeGzipped(Path path) throws IOException {\n         return makeReaderMaybeGzipped(in, path.toString().endsWith(\".gz\"));\n     }\n \n+    public static InputStream makeInputStreamMaybeGzipped(final String path) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyNjczNQ==", "bodyText": "Why this change?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447326735", "createdAt": "2020-06-30T00:00:42Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/iterators/SamReaderQueryingIterator.java", "diffHunk": "@@ -90,7 +90,9 @@ public SamReaderQueryingIterator( final SamReader reader, final TraversalParamet\n         }\n \n         // This might take a while with large interval lists, so log a status message\n-        logger.debug(\"Preparing intervals for traversal\");\n+        if (rawIntervals.size() > 20) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyNzIwOQ==", "bodyText": "This class needs a java doc explaining how to use it as well as docs for all of the abstract methods you lay out.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447327209", "createdAt": "2020-06-30T00:01:54Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/BinaryTableReader.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+\n+import javax.swing.*;\n+import java.io.*;\n+import java.util.*;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+public abstract class BinaryTableReader<R> implements AutoCloseable, Iterator<R> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyNzQyMA==", "bodyText": "unused constructor for abstract class", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447327420", "createdAt": "2020-06-30T00:02:33Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/BinaryTableReader.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+\n+import javax.swing.*;\n+import java.io.*;\n+import java.util.*;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+public abstract class BinaryTableReader<R> implements AutoCloseable, Iterator<R> {\n+\n+    private final PushbackDataInput dtInput;\n+\n+    private final String inputName;\n+\n+    private final Runnable closeAction;\n+\n+    protected BinaryTableReader(final InputStream in, final String source) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyNzQ0NQ==", "bodyText": "unused", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447327445", "createdAt": "2020-06-30T00:02:37Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/BinaryTableReader.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+\n+import javax.swing.*;\n+import java.io.*;\n+import java.util.*;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+public abstract class BinaryTableReader<R> implements AutoCloseable, Iterator<R> {\n+\n+    private final PushbackDataInput dtInput;\n+\n+    private final String inputName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyODgzNQ==", "bodyText": "I would move this and the writer from the pair hmm package to, package org.broadinstitute.hellbender.utils.codecs.table;", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447328835", "createdAt": "2020-06-30T00:07:33Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/BinaryTableReader.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+\n+import javax.swing.*;\n+import java.io.*;\n+import java.util.*;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+public abstract class BinaryTableReader<R> implements AutoCloseable, Iterator<R> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyNzIwOQ=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyOTE3Mw==", "bodyText": "alternatively this should perhaps emulate the asciiTableCodec in terms of structure..", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447329173", "createdAt": "2020-06-30T00:08:47Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/BinaryTableReader.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+\n+import javax.swing.*;\n+import java.io.*;\n+import java.util.*;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+public abstract class BinaryTableReader<R> implements AutoCloseable, Iterator<R> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyNzIwOQ=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyOTUxMg==", "bodyText": "More javadoc", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447329512", "createdAt": "2020-06-30T00:09:52Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/BinaryTableWriter.java", "diffHunk": "@@ -0,0 +1,55 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.io.*;\n+\n+public abstract class BinaryTableWriter<R> implements AutoCloseable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzMzEyNw==", "bodyText": "Both of these classes seem brittle and un-gatk engine like. I think they can be left intact approximately but we should really at some point either tie them explicitly to the dragstr file formats that you are using them for our use the existing machinery AbstractFeatureCodec() to read them in. This format seems unlikely to be reused but there are benefits to using the feature codec that might be useful.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447333127", "createdAt": "2020-06-30T00:20:39Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/BinaryTableWriter.java", "diffHunk": "@@ -0,0 +1,55 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.io.*;\n+\n+public abstract class BinaryTableWriter<R> implements AutoCloseable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyOTUxMg=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzMzIzMw==", "bodyText": "Javadoc", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447333233", "createdAt": "2020-06-30T00:21:01Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/ByteBufferPushbackDataInput.java", "diffHunk": "@@ -0,0 +1,394 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.io.*;\n+import java.nio.BufferOverflowException;\n+import java.nio.ByteBuffer;\n+\n+public class ByteBufferPushbackDataInput implements PushbackDataInput {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNTQ0Nw==", "bodyText": "Can you explain why you needed to create this class from scratch for reading the dragstr file? Is this ported from the DRAGEN matlab code? This class has some rather non-trivial functionality and methods and there is no testing for this class. The java library has a PushbackInputStream that seems to handle the extra functionality you are layering onto this class. Perhaps you could use that to handle the byte operations and reistrict yourself to just doing the file reading operations?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447335447", "createdAt": "2020-06-30T00:26:22Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/ByteBufferPushbackDataInput.java", "diffHunk": "@@ -0,0 +1,394 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.io.*;\n+import java.nio.BufferOverflowException;\n+import java.nio.ByteBuffer;\n+\n+public class ByteBufferPushbackDataInput implements PushbackDataInput {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzMzIzMw=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNTY4NQ==", "bodyText": "This needs a test or to be offoaded onto an existing method", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447335685", "createdAt": "2020-06-30T00:26:52Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/ByteBufferPushbackDataInput.java", "diffHunk": "@@ -0,0 +1,394 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.io.*;\n+import java.nio.BufferOverflowException;\n+import java.nio.ByteBuffer;\n+\n+public class ByteBufferPushbackDataInput implements PushbackDataInput {\n+\n+    private final byte[] bufferArray;\n+    private final ByteBuffer buffer;\n+\n+    private final DataInput input;\n+\n+    public ByteBufferPushbackDataInput(final DataInput in, final int bufferSize) {\n+        input = Utils.nonNull(in);\n+        bufferArray = new byte[bufferSize];\n+        buffer = ByteBuffer.wrap(bufferArray);\n+    }\n+\n+    @Override\n+    public void unread(final byte b) throws IOException {\n+        try {\n+            buffer.put(b);\n+        } catch (final BufferOverflowException ex) {\n+            throw new IOException(\"buffer is full\", ex);\n+        }\n+    }\n+\n+    @Override\n+    public void unread(final byte[] bytes, final int offset, final int len) throws IOException {\n+        try {\n+            buffer.put(bytes, offset, len);\n+        } catch (final BufferOverflowException ex) {\n+            throw new IOException(\"buffer is full\", ex);\n+        }\n+    }\n+\n+    @Override\n+    public void unread(final short s) throws IOException {\n+        try {\n+            buffer.putShort(s);\n+        } catch (final BufferOverflowException ex) {\n+            throw new IOException(\"buffer is full\", ex);\n+        }\n+    }\n+\n+    @Override\n+    public void unread(final long l) throws IOException {\n+        try {\n+            buffer.putLong(l);\n+        } catch (final BufferOverflowException ex) {\n+            throw new IOException(\"buffer is full\", ex);\n+        }\n+    }\n+\n+    @Override\n+    public void unread(final int i) throws IOException {\n+        try {\n+            buffer.putInt(i);\n+        } catch (final BufferOverflowException ex) {\n+            throw new IOException(\"buffer is full\", ex);\n+        }\n+    }\n+\n+    @Override\n+    public void unread(final double d) throws IOException {\n+        try {\n+            buffer.putDouble(d);\n+        } catch (final BufferOverflowException ex) {\n+            throw new IOException(\"buffer is full\", ex);\n+        }\n+    }\n+\n+    @Override\n+    public void unread(final float f) throws IOException {\n+        try {\n+            buffer.putFloat(f);\n+        } catch (final BufferOverflowException ex) {\n+            throw new IOException(\"buffer is full\", ex);\n+        }\n+    }\n+\n+    @Override\n+    public void unread(final boolean b) throws IOException {\n+        try {\n+            buffer.put((byte) (b ? 0 : 1));\n+        } catch (final BufferOverflowException ex) {\n+            throw new IOException(\"buffer is full\", ex);\n+        }\n+    }\n+\n+    @Override\n+    public void unread(final char c) throws IOException {\n+        try {\n+            buffer.putChar(c);\n+        } catch (final BufferOverflowException ex) {\n+            throw new IOException(\"buffer is full\", ex);\n+        }\n+\n+    }\n+\n+    @Override\n+    public void unreadLine(final String str) throws IOException {\n+        unread('\\n');\n+        for (int i = str.length() - 1; i >= 0; i++) {\n+            unread(str.charAt(i));\n+        }\n+    }\n+\n+\n+    @Override\n+    public void readFully(final byte[] b) throws IOException {\n+        readFully(Utils.nonNull(b), 0, b.length);\n+    }\n+\n+    @Override\n+    public void readFully(final byte[] b, final int off, final int len) throws IOException {\n+        Utils.nonNull(b);\n+        final int position = buffer.position();\n+        if (position == 0) {\n+            input.readFully(b, off, len);\n+        } else if (position >= len) {\n+            final int offset = position - len;\n+            Utils.flip(bufferArray, offset, position);\n+            System.arraycopy(bufferArray, offset, b, off, len);\n+            buffer.position(offset);\n+        } else {\n+            System.arraycopy(bufferArray, 0, b, off, position);\n+            input.readFully(b, off + position, len - position);\n+            buffer.clear();\n+        }\n+    }\n+\n+    @Override\n+    public int skipBytes(final int n) throws IOException {\n+        if (n <= 0) {\n+            return 0;\n+        }\n+        final int position = buffer.position();\n+        if (position == 0) {\n+            return input.skipBytes(n);\n+        } else if (position >= n) {\n+            buffer.position(position - n);\n+            return n;\n+        } else {\n+            buffer.clear();\n+            return position + input.skipBytes(n - position);\n+        }\n+    }\n+\n+    @Override\n+    public boolean readBoolean() throws IOException {\n+        final int position = buffer.position();\n+        if (position == 0) {\n+            return input.readBoolean();\n+        } else {\n+            final int newPosition = position - 1;\n+            final boolean result = buffer.get(newPosition) != 0;\n+            buffer.position(newPosition);\n+            return result;\n+        }\n+    }\n+\n+    @Override\n+    public byte readByte() throws IOException {\n+        final int position = buffer.position();\n+        if (position == 0) {\n+            return input.readByte();\n+        } else {\n+            final int newPosition = position - 1;\n+            final byte result = buffer.get(newPosition);\n+            buffer.position(newPosition);\n+            return result;\n+        }\n+    }\n+\n+    @Override\n+    public int readUnsignedByte() throws IOException {\n+        return 0xFF & readByte();\n+    }\n+\n+    @Override\n+    public short readShort() throws IOException {\n+        final int position = buffer.position();\n+        if (position == 0) {\n+            return input.readShort();\n+        } else if (position == 1) {\n+            buffer.put(input.readByte());\n+        }\n+        final int newPosition = position - 1;\n+        final short result = buffer.getShort(newPosition);\n+        buffer.position(newPosition);\n+        return result;\n+    }\n+\n+    @Override\n+    public int readUnsignedShort() throws IOException {\n+        return 0xFFFF & readShort();\n+    }\n+\n+    @Override\n+    public char readChar() throws IOException {\n+        final int position = buffer.position();\n+        if (position == 0) {\n+            return input.readChar();\n+        } else if (position == 1) {\n+            buffer.put(input.readByte());\n+        }\n+        final int newPosition = position - 1;\n+        final char result = buffer.getChar(newPosition);\n+        buffer.position(newPosition);\n+        return result;\n+    }\n+\n+    @Override\n+    public int readInt() throws IOException {\n+        final int position = buffer.position();\n+        if (position == 0) {\n+            return input.readInt();\n+        }\n+        final int newPosition = ensureBufferHasBytes(position,4);\n+        final int result = buffer.getInt(newPosition);\n+        buffer.position(newPosition);\n+        return result;\n+    }\n+\n+    @Override\n+    public long readLong() throws IOException {\n+        final int position = buffer.position();\n+        if (position == 0) {\n+            return input.readLong();\n+        }\n+        final int newPosition = ensureBufferHasBytes(position,8);\n+        final long result = buffer.getLong(newPosition);\n+        buffer.position(newPosition);\n+        return result;\n+    }\n+\n+    private int ensureBufferHasBytes(final int position, final int nb) throws IOException {\n+        if (position < nb) {\n+            try {\n+                input.readFully(bufferArray, position, nb - position);\n+            } catch (final IndexOutOfBoundsException ex) {\n+                throw new IOException(\"buffer is full\", new BufferOverflowException());\n+            }\n+            buffer.position(nb);\n+            return 0;\n+        } else {\n+            return position - nb;\n+        }\n+    }\n+\n+    @Override\n+    public float readFloat() throws IOException {\n+        final int position = buffer.position();\n+        if (position == 0) {\n+            return input.readFloat();\n+        }\n+        final int newPosition = ensureBufferHasBytes(position, 4);\n+        final float result = buffer.getFloat(newPosition);\n+        buffer.position(newPosition);\n+        return result;\n+    }\n+\n+    @Override\n+    public double readDouble() throws IOException {\n+        final int position = buffer.position();\n+        if (position == 0) {\n+            return input.readFloat();\n+        }\n+        final int newPosition = ensureBufferHasBytes(position, 8);\n+        final double result = buffer.getDouble(newPosition);\n+        buffer.position(newPosition);\n+        return result;\n+    }\n+\n+    @Override\n+    public String readLine() throws IOException {\n+        final int position = buffer.position();\n+        if (position == 0) {\n+            return input.readLine();\n+        } else {\n+            String result = null;\n+            int i;\n+            for (i = position - 1; i >= 0; i--) {\n+                byte b = bufferArray[i];\n+                if (b == '\\n') {\n+                    Utils.flip(bufferArray, i + 1, position);\n+                    result = new String(bufferArray, i + 1, position - i - 1);\n+                    buffer.position(i);\n+                    break;\n+                } else if (b == '\\r') {\n+                    Utils.flip(bufferArray, i + 1, position);\n+                    result = new String(bufferArray, i + 1, position - i - 1);\n+                    if (i > 0) {\n+                        buffer.position(bufferArray[i - 1] == '\\n' ? i - 1 : i);\n+                    } else {\n+                        buffer.position(0);\n+                        try {\n+                            final byte next = input.readByte();\n+                            if (next != '\\n') {\n+                                buffer.put(next);\n+                            }\n+                        } catch (final EOFException ex) {\n+                            // nothing to do.\n+                        }\n+                    }\n+                    break;\n+                }\n+            }\n+            if (result == null) {\n+                Utils.flip(bufferArray, 0, position);\n+                final String head = new String(bufferArray, 0, position);\n+                final String tail = input.readLine();\n+                result = head + tail;\n+                buffer.position(0);\n+            }\n+            return result;\n+        }\n+    }\n+\n+    public void unreadUTF(final String str) throws IOException {\n+        final int len = str.length();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 325}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzNzQyOA==", "bodyText": "If you are going to add a new mechanism for parsing double lists on the command line you are going to need to at least have an integration test asserting that the pattern works.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447337428", "createdAt": "2020-06-30T00:32:29Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DoubleSequence.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import it.unimi.dsi.fastutil.doubles.AbstractDoubleList;\n+import it.unimi.dsi.fastutil.doubles.DoubleList;\n+import org.broadinstitute.barclay.argparser.CommandLineException;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * A sequence of doubles specificator that can be used as a user argument.\n+ */\n+public class DoubleSequence {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzODI4NQ==", "bodyText": "I would need to see a test that this works properly for scientific notations and regular doubles.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447338285", "createdAt": "2020-06-30T00:35:30Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DoubleSequence.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import it.unimi.dsi.fastutil.doubles.AbstractDoubleList;\n+import it.unimi.dsi.fastutil.doubles.DoubleList;\n+import org.broadinstitute.barclay.argparser.CommandLineException;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * A sequence of doubles specificator that can be used as a user argument.\n+ */\n+public class DoubleSequence {\n+\n+    private static final Pattern DOUBLE_PATTERN = Pattern.compile(\"[-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzODU3Mw==", "bodyText": "Add a test for parsing a bunch of mixed examples for this. As well as some negative tests for invalid cases like \":::\" or \"2:2:2:2:2:2\"", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447338573", "createdAt": "2020-06-30T00:36:22Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DoubleSequence.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import it.unimi.dsi.fastutil.doubles.AbstractDoubleList;\n+import it.unimi.dsi.fastutil.doubles.DoubleList;\n+import org.broadinstitute.barclay.argparser.CommandLineException;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * A sequence of doubles specificator that can be used as a user argument.\n+ */\n+public class DoubleSequence {\n+\n+    private static final Pattern DOUBLE_PATTERN = Pattern.compile(\"[-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?\");\n+\n+    private final String input;\n+\n+    private final String description;\n+\n+    private DoubleList values;\n+\n+    /**\n+     * Currently the only supported format for the string descriptor is:\n+     * start:step:end.\n+     */\n+    public static final Pattern STEP_DECRIPTION_PATTERN = Pattern.compile(\n+            String.format(\"^(%s):(%s):(%s)$\", DOUBLE_PATTERN, DOUBLE_PATTERN, DOUBLE_PATTERN));\n+\n+\n+    public DoubleSequence(final String decriptor) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzOTM0OA==", "bodyText": "So this class is only ever taking a subsample of the str sites and using those to test?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447339348", "createdAt": "2020-06-30T00:39:07Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSampler.java", "diffHunk": "@@ -0,0 +1,152 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.IntervalPileup;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+\n+public class DragstrCasesSampler {\n+\n+    private SAMSequenceDictionary dictionary;\n+    private ReadsDataSource readsSource;\n+    private ReferenceDataSource referenceSource;\n+    private static final Logger logger = LogManager.getLogger(EstimateDragstrModelParameters.class);\n+    private DragstrCasesSamplerArgumentCollection config;\n+\n+    public DragstrCasesSampler(final DragstrCasesSamplerArgumentCollection dragstrCasesSamplerArgumentCollection,\n+                               final ReferenceDataSource referenceSource,\n+                               final ReadsDataSource readsSource) {\n+        this.config = dragstrCasesSamplerArgumentCollection;\n+        this.readsSource = readsSource;\n+        this.referenceSource = referenceSource;\n+        this.dictionary = referenceSource.getSequenceDictionary();\n+    }\n+\n+    void sample(final DragstrModelEstimator.RepeatCases dest, final List<DragstrLocus> loci) {\n+        final int period = dest.getPeriod();\n+        final int repeats = dest.getRepeats();\n+        logger.info(\"Sampling period = \" + period + \" and repeat count = \" + repeats);\n+        final Random rdn = new Random(((config.randomSeed * 31) + period * 31) + repeats * 31);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzOTYzNg==", "bodyText": "I would recommend sorting the loci, the readsDataSource is optimized for sequential querying and this will probably result in very few cache hits and consequently a poor performance.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447339636", "createdAt": "2020-06-30T00:40:05Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSampler.java", "diffHunk": "@@ -0,0 +1,152 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.IntervalPileup;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+\n+public class DragstrCasesSampler {\n+\n+    private SAMSequenceDictionary dictionary;\n+    private ReadsDataSource readsSource;\n+    private ReferenceDataSource referenceSource;\n+    private static final Logger logger = LogManager.getLogger(EstimateDragstrModelParameters.class);\n+    private DragstrCasesSamplerArgumentCollection config;\n+\n+    public DragstrCasesSampler(final DragstrCasesSamplerArgumentCollection dragstrCasesSamplerArgumentCollection,\n+                               final ReferenceDataSource referenceSource,\n+                               final ReadsDataSource readsSource) {\n+        this.config = dragstrCasesSamplerArgumentCollection;\n+        this.readsSource = readsSource;\n+        this.referenceSource = referenceSource;\n+        this.dictionary = referenceSource.getSequenceDictionary();\n+    }\n+\n+    void sample(final DragstrModelEstimator.RepeatCases dest, final List<DragstrLocus> loci) {\n+        final int period = dest.getPeriod();\n+        final int repeats = dest.getRepeats();\n+        logger.info(\"Sampling period = \" + period + \" and repeat count = \" + repeats);\n+        final Random rdn = new Random(((config.randomSeed * 31) + period * 31) + repeats * 31);\n+        Collections.shuffle(loci, rdn);\n+        final int size = loci.size();\n+        int nonAllRefCases = 0; // number of cases that contain at least some non-ref read.\n+        int noReads = 0;\n+        int lowMinMQ = 0;\n+        for (int i = 0; i < size; i++) {\n+            final DragstrLocus locus = loci.get(i);\n+            final SimpleInterval startLocation = locus.getStartInterval(dictionary, 0);\n+            final SimpleInterval repeatInterval = locus.getRepeatInterval(dictionary, config.pileupPadding, config.pileupPadding + period);\n+            final List<GATKRead> reads = Utils.stream(readsSource", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MDMzOA==", "bodyText": "commented code.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447340338", "createdAt": "2020-06-30T00:42:34Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSampler.java", "diffHunk": "@@ -0,0 +1,152 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.IntervalPileup;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+\n+public class DragstrCasesSampler {\n+\n+    private SAMSequenceDictionary dictionary;\n+    private ReadsDataSource readsSource;\n+    private ReferenceDataSource referenceSource;\n+    private static final Logger logger = LogManager.getLogger(EstimateDragstrModelParameters.class);\n+    private DragstrCasesSamplerArgumentCollection config;\n+\n+    public DragstrCasesSampler(final DragstrCasesSamplerArgumentCollection dragstrCasesSamplerArgumentCollection,\n+                               final ReferenceDataSource referenceSource,\n+                               final ReadsDataSource readsSource) {\n+        this.config = dragstrCasesSamplerArgumentCollection;\n+        this.readsSource = readsSource;\n+        this.referenceSource = referenceSource;\n+        this.dictionary = referenceSource.getSequenceDictionary();\n+    }\n+\n+    void sample(final DragstrModelEstimator.RepeatCases dest, final List<DragstrLocus> loci) {\n+        final int period = dest.getPeriod();\n+        final int repeats = dest.getRepeats();\n+        logger.info(\"Sampling period = \" + period + \" and repeat count = \" + repeats);\n+        final Random rdn = new Random(((config.randomSeed * 31) + period * 31) + repeats * 31);\n+        Collections.shuffle(loci, rdn);\n+        final int size = loci.size();\n+        int nonAllRefCases = 0; // number of cases that contain at least some non-ref read.\n+        int noReads = 0;\n+        int lowMinMQ = 0;\n+        for (int i = 0; i < size; i++) {\n+            final DragstrLocus locus = loci.get(i);\n+            final SimpleInterval startLocation = locus.getStartInterval(dictionary, 0);\n+            final SimpleInterval repeatInterval = locus.getRepeatInterval(dictionary, config.pileupPadding, config.pileupPadding + period);\n+            final List<GATKRead> reads = Utils.stream(readsSource\n+                    .query(startLocation))\n+                    .filter(m -> (m.getFlags() & 0x0f04) == 0)\n+                    .filter(m -> m.getMappingQuality() != 0)\n+                    .filter(m -> !m.isUnmapped())\n+                    .collect(Collectors.toList());\n+            if (reads.isEmpty()) {\n+                if (logger.isDebugEnabled()) {\n+                    logger.debug(String.format(\"P=%d, L=%d, j=%d, %s: no alignments found\", period, locus.getRepeats(),\n+                            i + 1, startLocation));\n+                }\n+                noReads++;\n+            } else if (reads.stream().mapToInt(GATKRead::getMappingQuality).min().getAsInt() < config.samplingMinMQ) {\n+                if (logger.isDebugEnabled()) {\n+                    logger.debug(String.format(\"P=%d, L=%d, j=%d, %s: MAPQ check failed\", period, locus.getRepeats(),\n+                            i + 1, startLocation));\n+                }\n+                lowMinMQ++;\n+            } else {\n+                byte[] referenceSequence = referenceSource.queryAndPrefetch(repeatInterval).getBases();\n+                final ReferenceBases referenceBases = new ReferenceBases(referenceSequence, repeatInterval);\n+                final IntervalPileup pileup = IntervalPileup.of(reads, referenceBases);\n+                final int startColumn = (int) locus.getStart() - repeatInterval.getStart();\n+                int j, k, l;\n+                for (j = startColumn, k = j + locus.getRepeats() * period, l = 0; l < period && k < referenceSequence.length; l++, j++, k++) {\n+                    if (referenceSequence[j] != referenceSequence[k]) {\n+                        break;\n+                    }\n+                }\n+                final int endOfRepeatWithExtraBases = l + startColumn + locus.getRepeats() * period - 1;\n+                final int firstRelevantColumn = 0;\n+                final int lastRelevantColumn = Math.min(endOfRepeatWithExtraBases + config.pileupPadding, pileup.width() - 1);\n+                int total = 0;\n+                int nonRefTotal = 0;\n+                int readsWithNoBases = 0;\n+                int readsWithTooManyBadBQs = 0;\n+                int readsWithNoQuals = 0;\n+                row_for:\n+                for (int row = 0; row < pileup.height(); row++) {\n+                    if (!pileup.reads().get(row).hasBaseQualities()) {\n+                        readsWithNoBases++;\n+                        continue;\n+                    }\n+                    int disqualifyingBaseCalls = 0;\n+                    int indelLength = 0;\n+                    final IntervalPileup.Element element = pileup.element(row);\n+                    for (int column = firstRelevantColumn; column <= lastRelevantColumn; column++) {\n+                        final byte qual = pileup.qualAt(row, column);\n+                        final byte base = pileup.baseAt(row, column);\n+                        if (base == IntervalPileup.NO_BASE) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MTE2MQ==", "bodyText": "@vruano can you explain to me why you couldnt' use the existing pileup machinery in order to handle this querying? why make your own IntervalPileup class?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447341161", "createdAt": "2020-06-30T00:45:30Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSampler.java", "diffHunk": "@@ -0,0 +1,152 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.IntervalPileup;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+\n+public class DragstrCasesSampler {\n+\n+    private SAMSequenceDictionary dictionary;\n+    private ReadsDataSource readsSource;\n+    private ReferenceDataSource referenceSource;\n+    private static final Logger logger = LogManager.getLogger(EstimateDragstrModelParameters.class);\n+    private DragstrCasesSamplerArgumentCollection config;\n+\n+    public DragstrCasesSampler(final DragstrCasesSamplerArgumentCollection dragstrCasesSamplerArgumentCollection,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MjE1Mw==", "bodyText": "Doc this method inputs, rename loci to something more descriptive", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447342153", "createdAt": "2020-06-30T00:48:52Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSampler.java", "diffHunk": "@@ -0,0 +1,152 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.IntervalPileup;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+\n+public class DragstrCasesSampler {\n+\n+    private SAMSequenceDictionary dictionary;\n+    private ReadsDataSource readsSource;\n+    private ReferenceDataSource referenceSource;\n+    private static final Logger logger = LogManager.getLogger(EstimateDragstrModelParameters.class);\n+    private DragstrCasesSamplerArgumentCollection config;\n+\n+    public DragstrCasesSampler(final DragstrCasesSamplerArgumentCollection dragstrCasesSamplerArgumentCollection,\n+                               final ReferenceDataSource referenceSource,\n+                               final ReadsDataSource readsSource) {\n+        this.config = dragstrCasesSamplerArgumentCollection;\n+        this.readsSource = readsSource;\n+        this.referenceSource = referenceSource;\n+        this.dictionary = referenceSource.getSequenceDictionary();\n+    }\n+\n+    void sample(final DragstrModelEstimator.RepeatCases dest, final List<DragstrLocus> loci) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MjkwNQ==", "bodyText": "You should encapsulate everything beyond this point into a method that returns the numTotal and nonRefTotal to be added to dest rather than having one method with a spider web of if statements that handles the iteration over the locations, and the iteration over the bases in the ref at that location etc...\nIt would make this method much more readable.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447342905", "createdAt": "2020-06-30T00:51:35Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSampler.java", "diffHunk": "@@ -0,0 +1,152 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.IntervalPileup;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+\n+public class DragstrCasesSampler {\n+\n+    private SAMSequenceDictionary dictionary;\n+    private ReadsDataSource readsSource;\n+    private ReferenceDataSource referenceSource;\n+    private static final Logger logger = LogManager.getLogger(EstimateDragstrModelParameters.class);\n+    private DragstrCasesSamplerArgumentCollection config;\n+\n+    public DragstrCasesSampler(final DragstrCasesSamplerArgumentCollection dragstrCasesSamplerArgumentCollection,\n+                               final ReferenceDataSource referenceSource,\n+                               final ReadsDataSource readsSource) {\n+        this.config = dragstrCasesSamplerArgumentCollection;\n+        this.readsSource = readsSource;\n+        this.referenceSource = referenceSource;\n+        this.dictionary = referenceSource.getSequenceDictionary();\n+    }\n+\n+    void sample(final DragstrModelEstimator.RepeatCases dest, final List<DragstrLocus> loci) {\n+        final int period = dest.getPeriod();\n+        final int repeats = dest.getRepeats();\n+        logger.info(\"Sampling period = \" + period + \" and repeat count = \" + repeats);\n+        final Random rdn = new Random(((config.randomSeed * 31) + period * 31) + repeats * 31);\n+        Collections.shuffle(loci, rdn);\n+        final int size = loci.size();\n+        int nonAllRefCases = 0; // number of cases that contain at least some non-ref read.\n+        int noReads = 0;\n+        int lowMinMQ = 0;\n+        for (int i = 0; i < size; i++) {\n+            final DragstrLocus locus = loci.get(i);\n+            final SimpleInterval startLocation = locus.getStartInterval(dictionary, 0);\n+            final SimpleInterval repeatInterval = locus.getRepeatInterval(dictionary, config.pileupPadding, config.pileupPadding + period);\n+            final List<GATKRead> reads = Utils.stream(readsSource\n+                    .query(startLocation))\n+                    .filter(m -> (m.getFlags() & 0x0f04) == 0)\n+                    .filter(m -> m.getMappingQuality() != 0)\n+                    .filter(m -> !m.isUnmapped())\n+                    .collect(Collectors.toList());\n+            if (reads.isEmpty()) {\n+                if (logger.isDebugEnabled()) {\n+                    logger.debug(String.format(\"P=%d, L=%d, j=%d, %s: no alignments found\", period, locus.getRepeats(),\n+                            i + 1, startLocation));\n+                }\n+                noReads++;\n+            } else if (reads.stream().mapToInt(GATKRead::getMappingQuality).min().getAsInt() < config.samplingMinMQ) {\n+                if (logger.isDebugEnabled()) {\n+                    logger.debug(String.format(\"P=%d, L=%d, j=%d, %s: MAPQ check failed\", period, locus.getRepeats(),\n+                            i + 1, startLocation));\n+                }\n+                lowMinMQ++;\n+            } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MzMyNA==", "bodyText": "The jump statements here make me nervous. Could you refactor these so that this whole thing is nested with switch statements so its easier to read what is going on?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447343324", "createdAt": "2020-06-30T00:53:05Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSampler.java", "diffHunk": "@@ -0,0 +1,152 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.IntervalPileup;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+\n+public class DragstrCasesSampler {\n+\n+    private SAMSequenceDictionary dictionary;\n+    private ReadsDataSource readsSource;\n+    private ReferenceDataSource referenceSource;\n+    private static final Logger logger = LogManager.getLogger(EstimateDragstrModelParameters.class);\n+    private DragstrCasesSamplerArgumentCollection config;\n+\n+    public DragstrCasesSampler(final DragstrCasesSamplerArgumentCollection dragstrCasesSamplerArgumentCollection,\n+                               final ReferenceDataSource referenceSource,\n+                               final ReadsDataSource readsSource) {\n+        this.config = dragstrCasesSamplerArgumentCollection;\n+        this.readsSource = readsSource;\n+        this.referenceSource = referenceSource;\n+        this.dictionary = referenceSource.getSequenceDictionary();\n+    }\n+\n+    void sample(final DragstrModelEstimator.RepeatCases dest, final List<DragstrLocus> loci) {\n+        final int period = dest.getPeriod();\n+        final int repeats = dest.getRepeats();\n+        logger.info(\"Sampling period = \" + period + \" and repeat count = \" + repeats);\n+        final Random rdn = new Random(((config.randomSeed * 31) + period * 31) + repeats * 31);\n+        Collections.shuffle(loci, rdn);\n+        final int size = loci.size();\n+        int nonAllRefCases = 0; // number of cases that contain at least some non-ref read.\n+        int noReads = 0;\n+        int lowMinMQ = 0;\n+        for (int i = 0; i < size; i++) {\n+            final DragstrLocus locus = loci.get(i);\n+            final SimpleInterval startLocation = locus.getStartInterval(dictionary, 0);\n+            final SimpleInterval repeatInterval = locus.getRepeatInterval(dictionary, config.pileupPadding, config.pileupPadding + period);\n+            final List<GATKRead> reads = Utils.stream(readsSource\n+                    .query(startLocation))\n+                    .filter(m -> (m.getFlags() & 0x0f04) == 0)\n+                    .filter(m -> m.getMappingQuality() != 0)\n+                    .filter(m -> !m.isUnmapped())\n+                    .collect(Collectors.toList());\n+            if (reads.isEmpty()) {\n+                if (logger.isDebugEnabled()) {\n+                    logger.debug(String.format(\"P=%d, L=%d, j=%d, %s: no alignments found\", period, locus.getRepeats(),\n+                            i + 1, startLocation));\n+                }\n+                noReads++;\n+            } else if (reads.stream().mapToInt(GATKRead::getMappingQuality).min().getAsInt() < config.samplingMinMQ) {\n+                if (logger.isDebugEnabled()) {\n+                    logger.debug(String.format(\"P=%d, L=%d, j=%d, %s: MAPQ check failed\", period, locus.getRepeats(),\n+                            i + 1, startLocation));\n+                }\n+                lowMinMQ++;\n+            } else {\n+                byte[] referenceSequence = referenceSource.queryAndPrefetch(repeatInterval).getBases();\n+                final ReferenceBases referenceBases = new ReferenceBases(referenceSequence, repeatInterval);\n+                final IntervalPileup pileup = IntervalPileup.of(reads, referenceBases);\n+                final int startColumn = (int) locus.getStart() - repeatInterval.getStart();\n+                int j, k, l;\n+                for (j = startColumn, k = j + locus.getRepeats() * period, l = 0; l < period && k < referenceSequence.length; l++, j++, k++) {\n+                    if (referenceSequence[j] != referenceSequence[k]) {\n+                        break;\n+                    }\n+                }\n+                final int endOfRepeatWithExtraBases = l + startColumn + locus.getRepeats() * period - 1;\n+                final int firstRelevantColumn = 0;\n+                final int lastRelevantColumn = Math.min(endOfRepeatWithExtraBases + config.pileupPadding, pileup.width() - 1);\n+                int total = 0;\n+                int nonRefTotal = 0;\n+                int readsWithNoBases = 0;\n+                int readsWithTooManyBadBQs = 0;\n+                int readsWithNoQuals = 0;\n+                row_for:\n+                for (int row = 0; row < pileup.height(); row++) {\n+                    if (!pileup.reads().get(row).hasBaseQualities()) {\n+                        readsWithNoBases++;\n+                        continue;\n+                    }\n+                    int disqualifyingBaseCalls = 0;\n+                    int indelLength = 0;\n+                    final IntervalPileup.Element element = pileup.element(row);\n+                    for (int column = firstRelevantColumn; column <= lastRelevantColumn; column++) {\n+                        final byte qual = pileup.qualAt(row, column);\n+                        final byte base = pileup.baseAt(row, column);\n+                        if (base == IntervalPileup.NO_BASE) {\n+                            //if (++disqualifyingBaseCalls > config.baseQualExceptionsAllowed) {\n+                            //readsWithTooManyBadBQs++;\n+                            readsWithNoBases++;\n+                            continue row_for;\n+                            //}\n+                        } else if (base == IntervalPileup.GAP) {\n+                            if (column >= startColumn && column <= endOfRepeatWithExtraBases) {\n+                                indelLength--;\n+                            }\n+                        } else if (qual != IntervalPileup.NO_BQ) {\n+                            if (qual < config.baseQualThreshold\n+                                    && ++disqualifyingBaseCalls > config.baseQualExceptionsAllowed) {\n+                                readsWithTooManyBadBQs++;\n+                                continue row_for;\n+                            }\n+                        } else { //if (qual == IntervalPileup.NO_BQ) {\n+                            readsWithNoQuals++;\n+                            continue row_for;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MzU2NQ==", "bodyText": "Can you please break up the driving method in this class into more delegated methods so that it is easier to manage what is going on.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447343565", "createdAt": "2020-06-30T00:53:52Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSampler.java", "diffHunk": "@@ -0,0 +1,152 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.IntervalPileup;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+\n+public class DragstrCasesSampler {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA5OTgxMQ=="}, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NDMzMA==", "bodyText": "Javadoc", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447344330", "createdAt": "2020-06-30T00:56:24Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrLocus.java", "diffHunk": "@@ -0,0 +1,218 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+import org.broadinstitute.hellbender.utils.tsv.DataLine;\n+import org.broadinstitute.hellbender.utils.tsv.TableColumnCollection;\n+import org.broadinstitute.hellbender.utils.tsv.TableReader;\n+import org.broadinstitute.hellbender.utils.tsv.TableWriter;\n+\n+import java.io.*;\n+import java.util.Arrays;\n+\n+public class DragstrLocus {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NDc5OQ==", "bodyText": "unused", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447344799", "createdAt": "2020-06-30T00:57:49Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrLocus.java", "diffHunk": "@@ -0,0 +1,218 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+import org.broadinstitute.hellbender.utils.tsv.DataLine;\n+import org.broadinstitute.hellbender.utils.tsv.TableColumnCollection;\n+import org.broadinstitute.hellbender.utils.tsv.TableReader;\n+import org.broadinstitute.hellbender.utils.tsv.TableWriter;\n+\n+import java.io.*;\n+import java.util.Arrays;\n+\n+public class DragstrLocus {\n+\n+    private final int chromosomeIndex;\n+    private final long start;\n+    private final int repeats;\n+    private final byte[] unit;\n+\n+    private DragstrLocus(final int chrIdx, final long start, final byte[] unit, final int repeats) {\n+        chromosomeIndex = chrIdx;\n+        this.start = start;\n+        this.unit = unit;\n+        this.repeats = repeats;\n+    }\n+\n+    public static DragstrLocus make(final int chrIdx, final long start, final byte[] unit, final int repeatCount) {\n+        ParamUtils.isPositiveOrZero(chrIdx, \"chromosome index\");\n+        ParamUtils.isPositive(start, \"start position\");\n+        final byte[] unitCloned = Utils.nonNull(unit).clone();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NTIxOA==", "bodyText": "Tools like this one should have descriptive javadocs to explain what they are doing. Also add @DocumentedFeature @BetaFeature to this class.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447345218", "createdAt": "2020-06-30T00:59:25Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NjE0Nw==", "bodyText": "Perhaps we should declare the DragstrLocus as a feature for the feature reader? that means these methods can all be pushed in the feature codec machinery and consequently it should cut down on the amount of code added here..", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447346147", "createdAt": "2020-06-30T01:02:16Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrLocus.java", "diffHunk": "@@ -0,0 +1,218 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+import org.broadinstitute.hellbender.utils.tsv.DataLine;\n+import org.broadinstitute.hellbender.utils.tsv.TableColumnCollection;\n+import org.broadinstitute.hellbender.utils.tsv.TableReader;\n+import org.broadinstitute.hellbender.utils.tsv.TableWriter;\n+\n+import java.io.*;\n+import java.util.Arrays;\n+\n+public class DragstrLocus {\n+\n+    private final int chromosomeIndex;\n+    private final long start;\n+    private final int repeats;\n+    private final byte[] unit;\n+\n+    private DragstrLocus(final int chrIdx, final long start, final byte[] unit, final int repeats) {\n+        chromosomeIndex = chrIdx;\n+        this.start = start;\n+        this.unit = unit;\n+        this.repeats = repeats;\n+    }\n+\n+    public static DragstrLocus make(final int chrIdx, final long start, final byte[] unit, final int repeatCount) {\n+        ParamUtils.isPositiveOrZero(chrIdx, \"chromosome index\");\n+        ParamUtils.isPositive(start, \"start position\");\n+        final byte[] unitCloned = Utils.nonNull(unit).clone();\n+        for (final byte base : unit) {\n+            if (!Nucleotide.decode(base).isStandard()) {\n+                throw new IllegalArgumentException(\"bad bases in \" + new String(unit));\n+            }\n+        }\n+        ParamUtils.isPositive(repeatCount, \"repeat count\");\n+        return new DragstrLocus(chrIdx, start, unit.clone(), repeatCount);\n+    }\n+\n+    public long getStart() {\n+        return start;\n+    }\n+\n+    public long getEnd() {\n+        return start + unit.length * repeats - 1;\n+    }\n+\n+    public int getPeriod() {\n+        return unit.length;\n+    }\n+\n+    byte[] getUnitUnsafe() {\n+        return unit;\n+    }\n+\n+    public byte[] getUnit() {\n+        return unit.clone();\n+    }\n+\n+    public int getRepeats() {\n+        return repeats;\n+    }\n+\n+    public static BinaryTableWriter<DragstrLocus> binaryWriter(final OutputStream out, final String path) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NzY3NQ==", "bodyText": "move internal classes to the bottom of the file.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447347675", "createdAt": "2020-06-30T01:07:17Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NzcwNw==", "bodyText": "and javadoc them", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447347707", "createdAt": "2020-06-30T01:07:23Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NzY3NQ=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0Nzk1NA==", "bodyText": "You really need to add some documentation somewhere as to what \"decimation\" is.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447347954", "createdAt": "2020-06-30T01:08:18Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 204}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0ODg1Ng==", "bodyText": "Almost all of the code in this codepath is duplicated with SingleTextToDragstrLociZipBinary (which also doesn't have docs). You should only include one of these two classes and add a toggle to the main one in order to enable the binary intermediate files.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447348856", "createdAt": "2020-06-30T01:11:26Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private DecimationTable decimationTable = DecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(fullName=\"max-period\", doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = 8;\n+\n+    @Argument(fullName=\"max-repeats\", doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = 20;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private File tempDir;\n+\n+    private long[][] totalCounts;\n+\n+    private long[][] emittedCounts;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 233}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MDUzOQ==", "bodyText": "So far as I can tell the only places that these binary readers/writers are used internally for an intermediate output for SampleSitesForDRAGstrModel to store the outputs. It seems to me that the binary output formats are fairly heavyweight, perhaps we could investigate reducing our code exposure by simply using gzip output streams instead of these binary streams? They will probably not compress as neatly but will probably save us the risk of bugs in the long run.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447350539", "createdAt": "2020-06-30T01:17:23Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrLocus.java", "diffHunk": "@@ -0,0 +1,218 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+import org.broadinstitute.hellbender.utils.tsv.DataLine;\n+import org.broadinstitute.hellbender.utils.tsv.TableColumnCollection;\n+import org.broadinstitute.hellbender.utils.tsv.TableReader;\n+import org.broadinstitute.hellbender.utils.tsv.TableWriter;\n+\n+import java.io.*;\n+import java.util.Arrays;\n+\n+public class DragstrLocus {\n+\n+    private final int chromosomeIndex;\n+    private final long start;\n+    private final int repeats;\n+    private final byte[] unit;\n+\n+    private DragstrLocus(final int chrIdx, final long start, final byte[] unit, final int repeats) {\n+        chromosomeIndex = chrIdx;\n+        this.start = start;\n+        this.unit = unit;\n+        this.repeats = repeats;\n+    }\n+\n+    public static DragstrLocus make(final int chrIdx, final long start, final byte[] unit, final int repeatCount) {\n+        ParamUtils.isPositiveOrZero(chrIdx, \"chromosome index\");\n+        ParamUtils.isPositive(start, \"start position\");\n+        final byte[] unitCloned = Utils.nonNull(unit).clone();\n+        for (final byte base : unit) {\n+            if (!Nucleotide.decode(base).isStandard()) {\n+                throw new IllegalArgumentException(\"bad bases in \" + new String(unit));\n+            }\n+        }\n+        ParamUtils.isPositive(repeatCount, \"repeat count\");\n+        return new DragstrLocus(chrIdx, start, unit.clone(), repeatCount);\n+    }\n+\n+    public long getStart() {\n+        return start;\n+    }\n+\n+    public long getEnd() {\n+        return start + unit.length * repeats - 1;\n+    }\n+\n+    public int getPeriod() {\n+        return unit.length;\n+    }\n+\n+    byte[] getUnitUnsafe() {\n+        return unit;\n+    }\n+\n+    public byte[] getUnit() {\n+        return unit.clone();\n+    }\n+\n+    public int getRepeats() {\n+        return repeats;\n+    }\n+\n+    public static BinaryTableWriter<DragstrLocus> binaryWriter(final OutputStream out, final String path) {\n+        return new BinaryTableWriter<DragstrLocus>(out, path) {\n+            @Override\n+            protected void writeRecord(final DragstrLocus record, final DataOutput output) throws IOException {\n+                output.writeShort(record.chromosomeIndex);\n+                output.writeLong(record.start);\n+                output.writeByte(record.unit.length);\n+                output.write(record.unit);\n+                output.writeByte(record.repeats);\n+            }\n+        };\n+    }\n+\n+    public static BinaryTableWriter<DragstrLocus> binaryWriter(final File out)\n+        throws FileNotFoundException\n+    {\n+        return binaryWriter(new FileOutputStream(out), out.toString());\n+    }\n+\n+    public static BinaryTableReader<DragstrLocus> binaryReader(final InputStream in) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MDk2MQ==", "bodyText": "Use a system directory delete option here. Also add a check that the tmp dir was never full to begin with at the start.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447350961", "createdAt": "2020-06-30T01:19:02Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private DecimationTable decimationTable = DecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(fullName=\"max-period\", doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = 8;\n+\n+    @Argument(fullName=\"max-repeats\", doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = 20;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private File tempDir;\n+\n+    private long[][] totalCounts;\n+\n+    private long[][] emittedCounts;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+        totalCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+        emittedCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+    }\n+\n+    public void onShutdown() {\n+        Utils.deleteFileTree(tempDir);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 248}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MTM3Nw==", "bodyText": "I think perhaps if we are worried about the performance of this method it might be worth looking into using spark to paralellize this traversal.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447351377", "createdAt": "2020-06-30T01:20:23Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private DecimationTable decimationTable = DecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(fullName=\"max-period\", doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = 8;\n+\n+    @Argument(fullName=\"max-repeats\", doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = 20;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private File tempDir;\n+\n+    private long[][] totalCounts;\n+\n+    private long[][] emittedCounts;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+        totalCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+        emittedCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+    }\n+\n+    public void onShutdown() {\n+        Utils.deleteFileTree(tempDir);\n+        super.onShutdown();\n+    }\n+\n+    @Override\n+    public void traverse() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 253}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MTg2Ng==", "bodyText": "this should perhaps implement list no?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447351866", "createdAt": "2020-06-30T01:22:00Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/collections/AutoCloseableList.java", "diffHunk": "@@ -0,0 +1,54 @@\n+package org.broadinstitute.hellbender.utils.collections;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.IntFunction;\n+\n+public class AutoCloseableList<E extends AutoCloseable> implements AutoCloseable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyMzI4Nw=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MjQyOQ==", "bodyText": "How big is this list of loci internally? Is it small enough to fit into memory? Its just a list of sites correct?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447352429", "createdAt": "2020-06-30T01:24:10Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private DecimationTable decimationTable = DecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(fullName=\"max-period\", doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = 8;\n+\n+    @Argument(fullName=\"max-repeats\", doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = 20;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private File tempDir;\n+\n+    private long[][] totalCounts;\n+\n+    private long[][] emittedCounts;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+        totalCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+        emittedCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+    }\n+\n+    public void onShutdown() {\n+        Utils.deleteFileTree(tempDir);\n+        super.onShutdown();\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        final SAMSequenceDictionary dictionary = Utils.nonNull(getReferenceDictionary());\n+        final File tempDir = this.tempDir;\n+        try (final AutoCloseableList<BinaryTableWriter<DragstrLocus>> allSitesWriter\n+                     = AutoCloseableList.of(maxPeriod, i -> createDragstrLocusWriter(tempDir, \"period_\" + (i + 1) + \".bin\"))) {\n+            if (!intervalArgumentCollection.intervalsSpecified()) {\n+                for (final SAMSequenceRecord sequence : dictionary.getSequences()) {\n+                    traverse(sequence.getSequenceIndex(), sequence, 1, sequence.getSequenceLength(), allSitesWriter, decimationTable);\n+                }\n+            } else {\n+                for (final SimpleInterval interval : intervalArgumentCollection.getIntervals(dictionary)) {\n+                    final SAMSequenceRecord sequence = dictionary.getSequence(interval.getContig());\n+                    traverse(sequence.getSequenceIndex(), sequence, interval.getStart(), interval.getEnd(), allSitesWriter, decimationTable);\n+                }\n+            }\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(tempDir, ex);\n+        }\n+        progressMeter.stop();\n+        logger.info(\"Finishing reference sampling. Proceeding to splitting each period case by repeat count\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            progressMeter = new ProgressMeter(secondsBetweenProgressUpdates);\n+            progressMeter.setRecordLabel(\"Splitting cases with period \" + i + \" by repeat count\");\n+            progressMeter.start();\n+            splitPeriodLociByRepeat(i);\n+            progressMeter.stop();\n+            logger.info(\"Done with period \" + i);\n+        }\n+        logger.info(\"Composing output zip\");\n+        composeOutputZip();\n+    }\n+\n+    private void splitPeriodLociByRepeat(final int period) {\n+        final File lociFile = new File(tempDir, \"period_\" + period + \".bin\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MjU1MA==", "bodyText": "why not use a gzip output stream?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447352550", "createdAt": "2020-06-30T01:24:30Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private DecimationTable decimationTable = DecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(fullName=\"max-period\", doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = 8;\n+\n+    @Argument(fullName=\"max-repeats\", doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = 20;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private File tempDir;\n+\n+    private long[][] totalCounts;\n+\n+    private long[][] emittedCounts;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+        totalCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+        emittedCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+    }\n+\n+    public void onShutdown() {\n+        Utils.deleteFileTree(tempDir);\n+        super.onShutdown();\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        final SAMSequenceDictionary dictionary = Utils.nonNull(getReferenceDictionary());\n+        final File tempDir = this.tempDir;\n+        try (final AutoCloseableList<BinaryTableWriter<DragstrLocus>> allSitesWriter\n+                     = AutoCloseableList.of(maxPeriod, i -> createDragstrLocusWriter(tempDir, \"period_\" + (i + 1) + \".bin\"))) {\n+            if (!intervalArgumentCollection.intervalsSpecified()) {\n+                for (final SAMSequenceRecord sequence : dictionary.getSequences()) {\n+                    traverse(sequence.getSequenceIndex(), sequence, 1, sequence.getSequenceLength(), allSitesWriter, decimationTable);\n+                }\n+            } else {\n+                for (final SimpleInterval interval : intervalArgumentCollection.getIntervals(dictionary)) {\n+                    final SAMSequenceRecord sequence = dictionary.getSequence(interval.getContig());\n+                    traverse(sequence.getSequenceIndex(), sequence, interval.getStart(), interval.getEnd(), allSitesWriter, decimationTable);\n+                }\n+            }\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(tempDir, ex);\n+        }\n+        progressMeter.stop();\n+        logger.info(\"Finishing reference sampling. Proceeding to splitting each period case by repeat count\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            progressMeter = new ProgressMeter(secondsBetweenProgressUpdates);\n+            progressMeter.setRecordLabel(\"Splitting cases with period \" + i + \" by repeat count\");\n+            progressMeter.start();\n+            splitPeriodLociByRepeat(i);\n+            progressMeter.stop();\n+            logger.info(\"Done with period \" + i);\n+        }\n+        logger.info(\"Composing output zip\");\n+        composeOutputZip();\n+    }\n+\n+    private void splitPeriodLociByRepeat(final int period) {\n+        final File lociFile = new File(tempDir, \"period_\" + period + \".bin\");\n+        final File periodDir = new File(tempDir, \"\" + period);\n+        if (!periodDir.mkdir()) {\n+            throw new UserException.CouldNotCreateOutputFile(periodDir, \"create period loci split directory\");\n+        }\n+        try (final BinaryTableReader<DragstrLocus> reader = DragstrLocus.binaryReader(new FileInputStream(lociFile));\n+             final AutoCloseableList<BinaryTableWriter<DragstrLocus>> writers =\n+                     AutoCloseableList.of(maxRepeat, i -> createDragstrLocusWriter(periodDir, \"\" + (i+1) + \".bin\"))) {\n+            while (reader.hasNext()) {\n+                final DragstrLocus next = reader.next();\n+                final int effectiveRepeats = next.getRepeats() > maxRepeat ? maxRepeat : next.getRepeats();\n+                progressMeter.update(next.getStartInterval(getReferenceDictionary(), 0));\n+                final BinaryTableWriter<DragstrLocus> writer = writers.get(effectiveRepeats - 1);\n+                writer.write(next);\n+\n+            }\n+        } catch (final Exception ex) {\n+            throw new UserException.CouldNotCreateOutputFile(lociFile, \"temporary period loci splitting failed\");\n+        }\n+        if (!lociFile.delete()) {\n+            throw new GATKException(\"could not delete temporary file \" + lociFile);\n+        }\n+    }\n+\n+    private void composeOutputZip() {\n+        final byte[] buffer = new byte[1 << 16];\n+        try (final ZipArchiveOutputStream output = new JarArchiveOutputStream(BucketUtils.createFile(outputPath))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 312}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MzI3Mg==", "bodyText": "Can you document what all of thes steps of this traverese method are?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447353272", "createdAt": "2020-06-30T01:26:52Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private DecimationTable decimationTable = DecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(fullName=\"max-period\", doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = 8;\n+\n+    @Argument(fullName=\"max-repeats\", doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = 20;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private File tempDir;\n+\n+    private long[][] totalCounts;\n+\n+    private long[][] emittedCounts;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+        totalCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+        emittedCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+    }\n+\n+    public void onShutdown() {\n+        Utils.deleteFileTree(tempDir);\n+        super.onShutdown();\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        final SAMSequenceDictionary dictionary = Utils.nonNull(getReferenceDictionary());\n+        final File tempDir = this.tempDir;\n+        try (final AutoCloseableList<BinaryTableWriter<DragstrLocus>> allSitesWriter\n+                     = AutoCloseableList.of(maxPeriod, i -> createDragstrLocusWriter(tempDir, \"period_\" + (i + 1) + \".bin\"))) {\n+            if (!intervalArgumentCollection.intervalsSpecified()) {\n+                for (final SAMSequenceRecord sequence : dictionary.getSequences()) {\n+                    traverse(sequence.getSequenceIndex(), sequence, 1, sequence.getSequenceLength(), allSitesWriter, decimationTable);\n+                }\n+            } else {\n+                for (final SimpleInterval interval : intervalArgumentCollection.getIntervals(dictionary)) {\n+                    final SAMSequenceRecord sequence = dictionary.getSequence(interval.getContig());\n+                    traverse(sequence.getSequenceIndex(), sequence, interval.getStart(), interval.getEnd(), allSitesWriter, decimationTable);\n+                }\n+            }\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(tempDir, ex);\n+        }\n+        progressMeter.stop();\n+        logger.info(\"Finishing reference sampling. Proceeding to splitting each period case by repeat count\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            progressMeter = new ProgressMeter(secondsBetweenProgressUpdates);\n+            progressMeter.setRecordLabel(\"Splitting cases with period \" + i + \" by repeat count\");\n+            progressMeter.start();\n+            splitPeriodLociByRepeat(i);\n+            progressMeter.stop();\n+            logger.info(\"Done with period \" + i);\n+        }\n+        logger.info(\"Composing output zip\");\n+        composeOutputZip();\n+    }\n+\n+    private void splitPeriodLociByRepeat(final int period) {\n+        final File lociFile = new File(tempDir, \"period_\" + period + \".bin\");\n+        final File periodDir = new File(tempDir, \"\" + period);\n+        if (!periodDir.mkdir()) {\n+            throw new UserException.CouldNotCreateOutputFile(periodDir, \"create period loci split directory\");\n+        }\n+        try (final BinaryTableReader<DragstrLocus> reader = DragstrLocus.binaryReader(new FileInputStream(lociFile));\n+             final AutoCloseableList<BinaryTableWriter<DragstrLocus>> writers =\n+                     AutoCloseableList.of(maxRepeat, i -> createDragstrLocusWriter(periodDir, \"\" + (i+1) + \".bin\"))) {\n+            while (reader.hasNext()) {\n+                final DragstrLocus next = reader.next();\n+                final int effectiveRepeats = next.getRepeats() > maxRepeat ? maxRepeat : next.getRepeats();\n+                progressMeter.update(next.getStartInterval(getReferenceDictionary(), 0));\n+                final BinaryTableWriter<DragstrLocus> writer = writers.get(effectiveRepeats - 1);\n+                writer.write(next);\n+\n+            }\n+        } catch (final Exception ex) {\n+            throw new UserException.CouldNotCreateOutputFile(lociFile, \"temporary period loci splitting failed\");\n+        }\n+        if (!lociFile.delete()) {\n+            throw new GATKException(\"could not delete temporary file \" + lociFile);\n+        }\n+    }\n+\n+    private void composeOutputZip() {\n+        final byte[] buffer = new byte[1 << 16];\n+        try (final ZipArchiveOutputStream output = new JarArchiveOutputStream(BucketUtils.createFile(outputPath))) {\n+            composeSummaryText(output);\n+            saveReferenceDictionary(output);\n+            composeLociFiles(buffer, output);\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(outputPath, \"error composing the zip file\", ex);\n+        }\n+    }\n+\n+    private void composeLociFiles(byte[] buffer, ZipArchiveOutputStream output) throws IOException {\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            for (int j = 1; j <= maxRepeat; j++) {\n+                final File inFile = new File(new File(tempDir, \"\" + i), \"\" + j + \".bin\");\n+                output.putArchiveEntry(new ZipArchiveEntry(\"\" + i + \"/\" + j + \".bin\"));\n+                final InputStream inStream = new FileInputStream(inFile);\n+                int len;\n+                while ((len = inStream.read(buffer)) > 0) {\n+                    output.write(buffer, 0, len);\n+                }\n+                inStream.close();\n+                output.closeArchiveEntry();\n+            }\n+        }\n+    }\n+\n+    private void saveReferenceDictionary(final ZipArchiveOutputStream output) throws IOException {\n+        final SAMSequenceDictionary dictionary = getBestAvailableSequenceDictionary();\n+        output.putArchiveEntry(new ZipArchiveEntry(\"reference.dict\"));\n+        // need to prevent closing output if the writer's close is called. We simply\n+        // flush instead:\n+        try (final Writer writer = new OutputStreamWriter(output) {\n+                public void close() throws IOException {\n+                    flush();\n+                }\n+        }) {\n+            final SAMSequenceDictionaryCodec codec = new SAMSequenceDictionaryCodec(writer);\n+            codec.encode(dictionary);\n+        }\n+        output.closeArchiveEntry();\n+    }\n+\n+    private void composeSummaryText(ZipArchiveOutputStream output) throws IOException {\n+        output.putArchiveEntry(new ZipArchiveEntry(\"summary.txt\"));\n+        final PrintWriter writer = new PrintWriter(new OutputStreamWriter(output));\n+        writer.println(\"period\\trepeat\\ttotal\\temmitted\\tdecimation\\tactual_decimation\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            for (int j = 1; j <= maxRepeat; j++) {\n+                writer.println(String.format(\"%d\\t%d\\t%d\\t%d\\t%.2f\\t%.2f\",\n+                        i, j, totalCounts[i][j], emittedCounts[i][j],\n+                        Math.log(decimationTable.decimationMask(i,j) + 1) / Math.log(2),\n+                        (- Math.log(emittedCounts[i][j]) + Math.log(totalCounts[i][j])) / Math.log(2) ));\n+            }\n+        }\n+        writer.flush();\n+        output.closeArchiveEntry();\n+    }\n+\n+    private static BinaryTableWriter<DragstrLocus> createDragstrLocusWriter(final File outDir, final String name) {\n+        try {\n+            final File outFile = new File(outDir, name);\n+            return DragstrLocus.binaryWriter(outFile);\n+        } catch (final IOException  e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private void traverse(final int seqNumber, final SAMSequenceRecord sequence, final long seqStart, final long seqEnd, final AutoCloseableList<BinaryTableWriter<DragstrLocus>> output, final DecimationTable decimationTable)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 378}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MzU5Nw==", "bodyText": "This could be handled by the stream API, or a while loop. Even if not use {} brackets to encapsulate the beg++; and end--; code.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447353597", "createdAt": "2020-06-30T01:27:52Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private DecimationTable decimationTable = DecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(fullName=\"max-period\", doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = 8;\n+\n+    @Argument(fullName=\"max-repeats\", doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = 20;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private File tempDir;\n+\n+    private long[][] totalCounts;\n+\n+    private long[][] emittedCounts;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+        totalCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+        emittedCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+    }\n+\n+    public void onShutdown() {\n+        Utils.deleteFileTree(tempDir);\n+        super.onShutdown();\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        final SAMSequenceDictionary dictionary = Utils.nonNull(getReferenceDictionary());\n+        final File tempDir = this.tempDir;\n+        try (final AutoCloseableList<BinaryTableWriter<DragstrLocus>> allSitesWriter\n+                     = AutoCloseableList.of(maxPeriod, i -> createDragstrLocusWriter(tempDir, \"period_\" + (i + 1) + \".bin\"))) {\n+            if (!intervalArgumentCollection.intervalsSpecified()) {\n+                for (final SAMSequenceRecord sequence : dictionary.getSequences()) {\n+                    traverse(sequence.getSequenceIndex(), sequence, 1, sequence.getSequenceLength(), allSitesWriter, decimationTable);\n+                }\n+            } else {\n+                for (final SimpleInterval interval : intervalArgumentCollection.getIntervals(dictionary)) {\n+                    final SAMSequenceRecord sequence = dictionary.getSequence(interval.getContig());\n+                    traverse(sequence.getSequenceIndex(), sequence, interval.getStart(), interval.getEnd(), allSitesWriter, decimationTable);\n+                }\n+            }\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(tempDir, ex);\n+        }\n+        progressMeter.stop();\n+        logger.info(\"Finishing reference sampling. Proceeding to splitting each period case by repeat count\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            progressMeter = new ProgressMeter(secondsBetweenProgressUpdates);\n+            progressMeter.setRecordLabel(\"Splitting cases with period \" + i + \" by repeat count\");\n+            progressMeter.start();\n+            splitPeriodLociByRepeat(i);\n+            progressMeter.stop();\n+            logger.info(\"Done with period \" + i);\n+        }\n+        logger.info(\"Composing output zip\");\n+        composeOutputZip();\n+    }\n+\n+    private void splitPeriodLociByRepeat(final int period) {\n+        final File lociFile = new File(tempDir, \"period_\" + period + \".bin\");\n+        final File periodDir = new File(tempDir, \"\" + period);\n+        if (!periodDir.mkdir()) {\n+            throw new UserException.CouldNotCreateOutputFile(periodDir, \"create period loci split directory\");\n+        }\n+        try (final BinaryTableReader<DragstrLocus> reader = DragstrLocus.binaryReader(new FileInputStream(lociFile));\n+             final AutoCloseableList<BinaryTableWriter<DragstrLocus>> writers =\n+                     AutoCloseableList.of(maxRepeat, i -> createDragstrLocusWriter(periodDir, \"\" + (i+1) + \".bin\"))) {\n+            while (reader.hasNext()) {\n+                final DragstrLocus next = reader.next();\n+                final int effectiveRepeats = next.getRepeats() > maxRepeat ? maxRepeat : next.getRepeats();\n+                progressMeter.update(next.getStartInterval(getReferenceDictionary(), 0));\n+                final BinaryTableWriter<DragstrLocus> writer = writers.get(effectiveRepeats - 1);\n+                writer.write(next);\n+\n+            }\n+        } catch (final Exception ex) {\n+            throw new UserException.CouldNotCreateOutputFile(lociFile, \"temporary period loci splitting failed\");\n+        }\n+        if (!lociFile.delete()) {\n+            throw new GATKException(\"could not delete temporary file \" + lociFile);\n+        }\n+    }\n+\n+    private void composeOutputZip() {\n+        final byte[] buffer = new byte[1 << 16];\n+        try (final ZipArchiveOutputStream output = new JarArchiveOutputStream(BucketUtils.createFile(outputPath))) {\n+            composeSummaryText(output);\n+            saveReferenceDictionary(output);\n+            composeLociFiles(buffer, output);\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(outputPath, \"error composing the zip file\", ex);\n+        }\n+    }\n+\n+    private void composeLociFiles(byte[] buffer, ZipArchiveOutputStream output) throws IOException {\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            for (int j = 1; j <= maxRepeat; j++) {\n+                final File inFile = new File(new File(tempDir, \"\" + i), \"\" + j + \".bin\");\n+                output.putArchiveEntry(new ZipArchiveEntry(\"\" + i + \"/\" + j + \".bin\"));\n+                final InputStream inStream = new FileInputStream(inFile);\n+                int len;\n+                while ((len = inStream.read(buffer)) > 0) {\n+                    output.write(buffer, 0, len);\n+                }\n+                inStream.close();\n+                output.closeArchiveEntry();\n+            }\n+        }\n+    }\n+\n+    private void saveReferenceDictionary(final ZipArchiveOutputStream output) throws IOException {\n+        final SAMSequenceDictionary dictionary = getBestAvailableSequenceDictionary();\n+        output.putArchiveEntry(new ZipArchiveEntry(\"reference.dict\"));\n+        // need to prevent closing output if the writer's close is called. We simply\n+        // flush instead:\n+        try (final Writer writer = new OutputStreamWriter(output) {\n+                public void close() throws IOException {\n+                    flush();\n+                }\n+        }) {\n+            final SAMSequenceDictionaryCodec codec = new SAMSequenceDictionaryCodec(writer);\n+            codec.encode(dictionary);\n+        }\n+        output.closeArchiveEntry();\n+    }\n+\n+    private void composeSummaryText(ZipArchiveOutputStream output) throws IOException {\n+        output.putArchiveEntry(new ZipArchiveEntry(\"summary.txt\"));\n+        final PrintWriter writer = new PrintWriter(new OutputStreamWriter(output));\n+        writer.println(\"period\\trepeat\\ttotal\\temmitted\\tdecimation\\tactual_decimation\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            for (int j = 1; j <= maxRepeat; j++) {\n+                writer.println(String.format(\"%d\\t%d\\t%d\\t%d\\t%.2f\\t%.2f\",\n+                        i, j, totalCounts[i][j], emittedCounts[i][j],\n+                        Math.log(decimationTable.decimationMask(i,j) + 1) / Math.log(2),\n+                        (- Math.log(emittedCounts[i][j]) + Math.log(totalCounts[i][j])) / Math.log(2) ));\n+            }\n+        }\n+        writer.flush();\n+        output.closeArchiveEntry();\n+    }\n+\n+    private static BinaryTableWriter<DragstrLocus> createDragstrLocusWriter(final File outDir, final String name) {\n+        try {\n+            final File outFile = new File(outDir, name);\n+            return DragstrLocus.binaryWriter(outFile);\n+        } catch (final IOException  e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private void traverse(final int seqNumber, final SAMSequenceRecord sequence, final long seqStart, final long seqEnd, final AutoCloseableList<BinaryTableWriter<DragstrLocus>> output, final DecimationTable decimationTable)\n+        throws IOException\n+    {\n+        final String id = sequence.getSequenceName();\n+        final NucleotideSequence fullSequence = LazyLoadingReferenceNucleotideSequence.of(directlyAccessEngineReferenceDataSource(), sequence.getSequenceName(),10000);\n+        final int maxPeriod = this.maxPeriod;\n+        final int length = sequence.getSequenceLength();\n+        long pos = seqStart;\n+        utter: while (pos <= seqEnd) {\n+           final byte base = fullSequence.byteAt(pos);\n+           long beg, end, cmp;\n+           for (beg = pos - 1; beg >= 1 && fullSequence.byteAt(beg) == base; beg--);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 389}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MzY1MQ==", "bodyText": "bracket this line", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447353651", "createdAt": "2020-06-30T01:28:03Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private DecimationTable decimationTable = DecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(fullName=\"max-period\", doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = 8;\n+\n+    @Argument(fullName=\"max-repeats\", doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = 20;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private File tempDir;\n+\n+    private long[][] totalCounts;\n+\n+    private long[][] emittedCounts;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+        totalCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+        emittedCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+    }\n+\n+    public void onShutdown() {\n+        Utils.deleteFileTree(tempDir);\n+        super.onShutdown();\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        final SAMSequenceDictionary dictionary = Utils.nonNull(getReferenceDictionary());\n+        final File tempDir = this.tempDir;\n+        try (final AutoCloseableList<BinaryTableWriter<DragstrLocus>> allSitesWriter\n+                     = AutoCloseableList.of(maxPeriod, i -> createDragstrLocusWriter(tempDir, \"period_\" + (i + 1) + \".bin\"))) {\n+            if (!intervalArgumentCollection.intervalsSpecified()) {\n+                for (final SAMSequenceRecord sequence : dictionary.getSequences()) {\n+                    traverse(sequence.getSequenceIndex(), sequence, 1, sequence.getSequenceLength(), allSitesWriter, decimationTable);\n+                }\n+            } else {\n+                for (final SimpleInterval interval : intervalArgumentCollection.getIntervals(dictionary)) {\n+                    final SAMSequenceRecord sequence = dictionary.getSequence(interval.getContig());\n+                    traverse(sequence.getSequenceIndex(), sequence, interval.getStart(), interval.getEnd(), allSitesWriter, decimationTable);\n+                }\n+            }\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(tempDir, ex);\n+        }\n+        progressMeter.stop();\n+        logger.info(\"Finishing reference sampling. Proceeding to splitting each period case by repeat count\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            progressMeter = new ProgressMeter(secondsBetweenProgressUpdates);\n+            progressMeter.setRecordLabel(\"Splitting cases with period \" + i + \" by repeat count\");\n+            progressMeter.start();\n+            splitPeriodLociByRepeat(i);\n+            progressMeter.stop();\n+            logger.info(\"Done with period \" + i);\n+        }\n+        logger.info(\"Composing output zip\");\n+        composeOutputZip();\n+    }\n+\n+    private void splitPeriodLociByRepeat(final int period) {\n+        final File lociFile = new File(tempDir, \"period_\" + period + \".bin\");\n+        final File periodDir = new File(tempDir, \"\" + period);\n+        if (!periodDir.mkdir()) {\n+            throw new UserException.CouldNotCreateOutputFile(periodDir, \"create period loci split directory\");\n+        }\n+        try (final BinaryTableReader<DragstrLocus> reader = DragstrLocus.binaryReader(new FileInputStream(lociFile));\n+             final AutoCloseableList<BinaryTableWriter<DragstrLocus>> writers =\n+                     AutoCloseableList.of(maxRepeat, i -> createDragstrLocusWriter(periodDir, \"\" + (i+1) + \".bin\"))) {\n+            while (reader.hasNext()) {\n+                final DragstrLocus next = reader.next();\n+                final int effectiveRepeats = next.getRepeats() > maxRepeat ? maxRepeat : next.getRepeats();\n+                progressMeter.update(next.getStartInterval(getReferenceDictionary(), 0));\n+                final BinaryTableWriter<DragstrLocus> writer = writers.get(effectiveRepeats - 1);\n+                writer.write(next);\n+\n+            }\n+        } catch (final Exception ex) {\n+            throw new UserException.CouldNotCreateOutputFile(lociFile, \"temporary period loci splitting failed\");\n+        }\n+        if (!lociFile.delete()) {\n+            throw new GATKException(\"could not delete temporary file \" + lociFile);\n+        }\n+    }\n+\n+    private void composeOutputZip() {\n+        final byte[] buffer = new byte[1 << 16];\n+        try (final ZipArchiveOutputStream output = new JarArchiveOutputStream(BucketUtils.createFile(outputPath))) {\n+            composeSummaryText(output);\n+            saveReferenceDictionary(output);\n+            composeLociFiles(buffer, output);\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(outputPath, \"error composing the zip file\", ex);\n+        }\n+    }\n+\n+    private void composeLociFiles(byte[] buffer, ZipArchiveOutputStream output) throws IOException {\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            for (int j = 1; j <= maxRepeat; j++) {\n+                final File inFile = new File(new File(tempDir, \"\" + i), \"\" + j + \".bin\");\n+                output.putArchiveEntry(new ZipArchiveEntry(\"\" + i + \"/\" + j + \".bin\"));\n+                final InputStream inStream = new FileInputStream(inFile);\n+                int len;\n+                while ((len = inStream.read(buffer)) > 0) {\n+                    output.write(buffer, 0, len);\n+                }\n+                inStream.close();\n+                output.closeArchiveEntry();\n+            }\n+        }\n+    }\n+\n+    private void saveReferenceDictionary(final ZipArchiveOutputStream output) throws IOException {\n+        final SAMSequenceDictionary dictionary = getBestAvailableSequenceDictionary();\n+        output.putArchiveEntry(new ZipArchiveEntry(\"reference.dict\"));\n+        // need to prevent closing output if the writer's close is called. We simply\n+        // flush instead:\n+        try (final Writer writer = new OutputStreamWriter(output) {\n+                public void close() throws IOException {\n+                    flush();\n+                }\n+        }) {\n+            final SAMSequenceDictionaryCodec codec = new SAMSequenceDictionaryCodec(writer);\n+            codec.encode(dictionary);\n+        }\n+        output.closeArchiveEntry();\n+    }\n+\n+    private void composeSummaryText(ZipArchiveOutputStream output) throws IOException {\n+        output.putArchiveEntry(new ZipArchiveEntry(\"summary.txt\"));\n+        final PrintWriter writer = new PrintWriter(new OutputStreamWriter(output));\n+        writer.println(\"period\\trepeat\\ttotal\\temmitted\\tdecimation\\tactual_decimation\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            for (int j = 1; j <= maxRepeat; j++) {\n+                writer.println(String.format(\"%d\\t%d\\t%d\\t%d\\t%.2f\\t%.2f\",\n+                        i, j, totalCounts[i][j], emittedCounts[i][j],\n+                        Math.log(decimationTable.decimationMask(i,j) + 1) / Math.log(2),\n+                        (- Math.log(emittedCounts[i][j]) + Math.log(totalCounts[i][j])) / Math.log(2) ));\n+            }\n+        }\n+        writer.flush();\n+        output.closeArchiveEntry();\n+    }\n+\n+    private static BinaryTableWriter<DragstrLocus> createDragstrLocusWriter(final File outDir, final String name) {\n+        try {\n+            final File outFile = new File(outDir, name);\n+            return DragstrLocus.binaryWriter(outFile);\n+        } catch (final IOException  e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private void traverse(final int seqNumber, final SAMSequenceRecord sequence, final long seqStart, final long seqEnd, final AutoCloseableList<BinaryTableWriter<DragstrLocus>> output, final DecimationTable decimationTable)\n+        throws IOException\n+    {\n+        final String id = sequence.getSequenceName();\n+        final NucleotideSequence fullSequence = LazyLoadingReferenceNucleotideSequence.of(directlyAccessEngineReferenceDataSource(), sequence.getSequenceName(),10000);\n+        final int maxPeriod = this.maxPeriod;\n+        final int length = sequence.getSequenceLength();\n+        long pos = seqStart;\n+        utter: while (pos <= seqEnd) {\n+           final byte base = fullSequence.byteAt(pos);\n+           long beg, end, cmp;\n+           for (beg = pos - 1; beg >= 1 && fullSequence.byteAt(beg) == base; beg--);\n+           beg++;\n+           for (end = pos + 1; end <= length && fullSequence.byteAt(end) == base; end++);\n+           end--;\n+           int bestPeriod = 1;\n+           long bestPeriodRepeats = end - beg + 1;\n+           long bestEnd = end;\n+           long bestBeg = beg;\n+           for (int period = 2; period <= maxPeriod; period++) {\n+               for (beg = length - pos > period ? pos - 1 : length - period,\n+                    cmp = beg + period; beg >= 1 && fullSequence.byteAt(cmp) == fullSequence.byteAt(beg); beg--, cmp--);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 399}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MzY3OQ==", "bodyText": "and this one", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447353679", "createdAt": "2020-06-30T01:28:10Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private DecimationTable decimationTable = DecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(fullName=\"max-period\", doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = 8;\n+\n+    @Argument(fullName=\"max-repeats\", doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = 20;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private File tempDir;\n+\n+    private long[][] totalCounts;\n+\n+    private long[][] emittedCounts;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+        totalCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+        emittedCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+    }\n+\n+    public void onShutdown() {\n+        Utils.deleteFileTree(tempDir);\n+        super.onShutdown();\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        final SAMSequenceDictionary dictionary = Utils.nonNull(getReferenceDictionary());\n+        final File tempDir = this.tempDir;\n+        try (final AutoCloseableList<BinaryTableWriter<DragstrLocus>> allSitesWriter\n+                     = AutoCloseableList.of(maxPeriod, i -> createDragstrLocusWriter(tempDir, \"period_\" + (i + 1) + \".bin\"))) {\n+            if (!intervalArgumentCollection.intervalsSpecified()) {\n+                for (final SAMSequenceRecord sequence : dictionary.getSequences()) {\n+                    traverse(sequence.getSequenceIndex(), sequence, 1, sequence.getSequenceLength(), allSitesWriter, decimationTable);\n+                }\n+            } else {\n+                for (final SimpleInterval interval : intervalArgumentCollection.getIntervals(dictionary)) {\n+                    final SAMSequenceRecord sequence = dictionary.getSequence(interval.getContig());\n+                    traverse(sequence.getSequenceIndex(), sequence, interval.getStart(), interval.getEnd(), allSitesWriter, decimationTable);\n+                }\n+            }\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(tempDir, ex);\n+        }\n+        progressMeter.stop();\n+        logger.info(\"Finishing reference sampling. Proceeding to splitting each period case by repeat count\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            progressMeter = new ProgressMeter(secondsBetweenProgressUpdates);\n+            progressMeter.setRecordLabel(\"Splitting cases with period \" + i + \" by repeat count\");\n+            progressMeter.start();\n+            splitPeriodLociByRepeat(i);\n+            progressMeter.stop();\n+            logger.info(\"Done with period \" + i);\n+        }\n+        logger.info(\"Composing output zip\");\n+        composeOutputZip();\n+    }\n+\n+    private void splitPeriodLociByRepeat(final int period) {\n+        final File lociFile = new File(tempDir, \"period_\" + period + \".bin\");\n+        final File periodDir = new File(tempDir, \"\" + period);\n+        if (!periodDir.mkdir()) {\n+            throw new UserException.CouldNotCreateOutputFile(periodDir, \"create period loci split directory\");\n+        }\n+        try (final BinaryTableReader<DragstrLocus> reader = DragstrLocus.binaryReader(new FileInputStream(lociFile));\n+             final AutoCloseableList<BinaryTableWriter<DragstrLocus>> writers =\n+                     AutoCloseableList.of(maxRepeat, i -> createDragstrLocusWriter(periodDir, \"\" + (i+1) + \".bin\"))) {\n+            while (reader.hasNext()) {\n+                final DragstrLocus next = reader.next();\n+                final int effectiveRepeats = next.getRepeats() > maxRepeat ? maxRepeat : next.getRepeats();\n+                progressMeter.update(next.getStartInterval(getReferenceDictionary(), 0));\n+                final BinaryTableWriter<DragstrLocus> writer = writers.get(effectiveRepeats - 1);\n+                writer.write(next);\n+\n+            }\n+        } catch (final Exception ex) {\n+            throw new UserException.CouldNotCreateOutputFile(lociFile, \"temporary period loci splitting failed\");\n+        }\n+        if (!lociFile.delete()) {\n+            throw new GATKException(\"could not delete temporary file \" + lociFile);\n+        }\n+    }\n+\n+    private void composeOutputZip() {\n+        final byte[] buffer = new byte[1 << 16];\n+        try (final ZipArchiveOutputStream output = new JarArchiveOutputStream(BucketUtils.createFile(outputPath))) {\n+            composeSummaryText(output);\n+            saveReferenceDictionary(output);\n+            composeLociFiles(buffer, output);\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(outputPath, \"error composing the zip file\", ex);\n+        }\n+    }\n+\n+    private void composeLociFiles(byte[] buffer, ZipArchiveOutputStream output) throws IOException {\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            for (int j = 1; j <= maxRepeat; j++) {\n+                final File inFile = new File(new File(tempDir, \"\" + i), \"\" + j + \".bin\");\n+                output.putArchiveEntry(new ZipArchiveEntry(\"\" + i + \"/\" + j + \".bin\"));\n+                final InputStream inStream = new FileInputStream(inFile);\n+                int len;\n+                while ((len = inStream.read(buffer)) > 0) {\n+                    output.write(buffer, 0, len);\n+                }\n+                inStream.close();\n+                output.closeArchiveEntry();\n+            }\n+        }\n+    }\n+\n+    private void saveReferenceDictionary(final ZipArchiveOutputStream output) throws IOException {\n+        final SAMSequenceDictionary dictionary = getBestAvailableSequenceDictionary();\n+        output.putArchiveEntry(new ZipArchiveEntry(\"reference.dict\"));\n+        // need to prevent closing output if the writer's close is called. We simply\n+        // flush instead:\n+        try (final Writer writer = new OutputStreamWriter(output) {\n+                public void close() throws IOException {\n+                    flush();\n+                }\n+        }) {\n+            final SAMSequenceDictionaryCodec codec = new SAMSequenceDictionaryCodec(writer);\n+            codec.encode(dictionary);\n+        }\n+        output.closeArchiveEntry();\n+    }\n+\n+    private void composeSummaryText(ZipArchiveOutputStream output) throws IOException {\n+        output.putArchiveEntry(new ZipArchiveEntry(\"summary.txt\"));\n+        final PrintWriter writer = new PrintWriter(new OutputStreamWriter(output));\n+        writer.println(\"period\\trepeat\\ttotal\\temmitted\\tdecimation\\tactual_decimation\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            for (int j = 1; j <= maxRepeat; j++) {\n+                writer.println(String.format(\"%d\\t%d\\t%d\\t%d\\t%.2f\\t%.2f\",\n+                        i, j, totalCounts[i][j], emittedCounts[i][j],\n+                        Math.log(decimationTable.decimationMask(i,j) + 1) / Math.log(2),\n+                        (- Math.log(emittedCounts[i][j]) + Math.log(totalCounts[i][j])) / Math.log(2) ));\n+            }\n+        }\n+        writer.flush();\n+        output.closeArchiveEntry();\n+    }\n+\n+    private static BinaryTableWriter<DragstrLocus> createDragstrLocusWriter(final File outDir, final String name) {\n+        try {\n+            final File outFile = new File(outDir, name);\n+            return DragstrLocus.binaryWriter(outFile);\n+        } catch (final IOException  e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private void traverse(final int seqNumber, final SAMSequenceRecord sequence, final long seqStart, final long seqEnd, final AutoCloseableList<BinaryTableWriter<DragstrLocus>> output, final DecimationTable decimationTable)\n+        throws IOException\n+    {\n+        final String id = sequence.getSequenceName();\n+        final NucleotideSequence fullSequence = LazyLoadingReferenceNucleotideSequence.of(directlyAccessEngineReferenceDataSource(), sequence.getSequenceName(),10000);\n+        final int maxPeriod = this.maxPeriod;\n+        final int length = sequence.getSequenceLength();\n+        long pos = seqStart;\n+        utter: while (pos <= seqEnd) {\n+           final byte base = fullSequence.byteAt(pos);\n+           long beg, end, cmp;\n+           for (beg = pos - 1; beg >= 1 && fullSequence.byteAt(beg) == base; beg--);\n+           beg++;\n+           for (end = pos + 1; end <= length && fullSequence.byteAt(end) == base; end++);\n+           end--;\n+           int bestPeriod = 1;\n+           long bestPeriodRepeats = end - beg + 1;\n+           long bestEnd = end;\n+           long bestBeg = beg;\n+           for (int period = 2; period <= maxPeriod; period++) {\n+               for (beg = length - pos > period ? pos - 1 : length - period,\n+                    cmp = beg + period; beg >= 1 && fullSequence.byteAt(cmp) == fullSequence.byteAt(beg); beg--, cmp--);\n+               beg++;\n+               for (end = pos >= period ? pos + 1 : period + 1, cmp = end - period; end <= length && fullSequence.byteAt(end) == fullSequence.byteAt(cmp); end++, cmp++);\n+               end--;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 402}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1NDQyNw==", "bodyText": "The entirety of this inner loop could probably be encapsulated into a method to make readability easier.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447354427", "createdAt": "2020-06-30T01:30:58Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private DecimationTable decimationTable = DecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(fullName=\"max-period\", doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = 8;\n+\n+    @Argument(fullName=\"max-repeats\", doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = 20;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private File tempDir;\n+\n+    private long[][] totalCounts;\n+\n+    private long[][] emittedCounts;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+        totalCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+        emittedCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+    }\n+\n+    public void onShutdown() {\n+        Utils.deleteFileTree(tempDir);\n+        super.onShutdown();\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        final SAMSequenceDictionary dictionary = Utils.nonNull(getReferenceDictionary());\n+        final File tempDir = this.tempDir;\n+        try (final AutoCloseableList<BinaryTableWriter<DragstrLocus>> allSitesWriter\n+                     = AutoCloseableList.of(maxPeriod, i -> createDragstrLocusWriter(tempDir, \"period_\" + (i + 1) + \".bin\"))) {\n+            if (!intervalArgumentCollection.intervalsSpecified()) {\n+                for (final SAMSequenceRecord sequence : dictionary.getSequences()) {\n+                    traverse(sequence.getSequenceIndex(), sequence, 1, sequence.getSequenceLength(), allSitesWriter, decimationTable);\n+                }\n+            } else {\n+                for (final SimpleInterval interval : intervalArgumentCollection.getIntervals(dictionary)) {\n+                    final SAMSequenceRecord sequence = dictionary.getSequence(interval.getContig());\n+                    traverse(sequence.getSequenceIndex(), sequence, interval.getStart(), interval.getEnd(), allSitesWriter, decimationTable);\n+                }\n+            }\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(tempDir, ex);\n+        }\n+        progressMeter.stop();\n+        logger.info(\"Finishing reference sampling. Proceeding to splitting each period case by repeat count\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            progressMeter = new ProgressMeter(secondsBetweenProgressUpdates);\n+            progressMeter.setRecordLabel(\"Splitting cases with period \" + i + \" by repeat count\");\n+            progressMeter.start();\n+            splitPeriodLociByRepeat(i);\n+            progressMeter.stop();\n+            logger.info(\"Done with period \" + i);\n+        }\n+        logger.info(\"Composing output zip\");\n+        composeOutputZip();\n+    }\n+\n+    private void splitPeriodLociByRepeat(final int period) {\n+        final File lociFile = new File(tempDir, \"period_\" + period + \".bin\");\n+        final File periodDir = new File(tempDir, \"\" + period);\n+        if (!periodDir.mkdir()) {\n+            throw new UserException.CouldNotCreateOutputFile(periodDir, \"create period loci split directory\");\n+        }\n+        try (final BinaryTableReader<DragstrLocus> reader = DragstrLocus.binaryReader(new FileInputStream(lociFile));\n+             final AutoCloseableList<BinaryTableWriter<DragstrLocus>> writers =\n+                     AutoCloseableList.of(maxRepeat, i -> createDragstrLocusWriter(periodDir, \"\" + (i+1) + \".bin\"))) {\n+            while (reader.hasNext()) {\n+                final DragstrLocus next = reader.next();\n+                final int effectiveRepeats = next.getRepeats() > maxRepeat ? maxRepeat : next.getRepeats();\n+                progressMeter.update(next.getStartInterval(getReferenceDictionary(), 0));\n+                final BinaryTableWriter<DragstrLocus> writer = writers.get(effectiveRepeats - 1);\n+                writer.write(next);\n+\n+            }\n+        } catch (final Exception ex) {\n+            throw new UserException.CouldNotCreateOutputFile(lociFile, \"temporary period loci splitting failed\");\n+        }\n+        if (!lociFile.delete()) {\n+            throw new GATKException(\"could not delete temporary file \" + lociFile);\n+        }\n+    }\n+\n+    private void composeOutputZip() {\n+        final byte[] buffer = new byte[1 << 16];\n+        try (final ZipArchiveOutputStream output = new JarArchiveOutputStream(BucketUtils.createFile(outputPath))) {\n+            composeSummaryText(output);\n+            saveReferenceDictionary(output);\n+            composeLociFiles(buffer, output);\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(outputPath, \"error composing the zip file\", ex);\n+        }\n+    }\n+\n+    private void composeLociFiles(byte[] buffer, ZipArchiveOutputStream output) throws IOException {\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            for (int j = 1; j <= maxRepeat; j++) {\n+                final File inFile = new File(new File(tempDir, \"\" + i), \"\" + j + \".bin\");\n+                output.putArchiveEntry(new ZipArchiveEntry(\"\" + i + \"/\" + j + \".bin\"));\n+                final InputStream inStream = new FileInputStream(inFile);\n+                int len;\n+                while ((len = inStream.read(buffer)) > 0) {\n+                    output.write(buffer, 0, len);\n+                }\n+                inStream.close();\n+                output.closeArchiveEntry();\n+            }\n+        }\n+    }\n+\n+    private void saveReferenceDictionary(final ZipArchiveOutputStream output) throws IOException {\n+        final SAMSequenceDictionary dictionary = getBestAvailableSequenceDictionary();\n+        output.putArchiveEntry(new ZipArchiveEntry(\"reference.dict\"));\n+        // need to prevent closing output if the writer's close is called. We simply\n+        // flush instead:\n+        try (final Writer writer = new OutputStreamWriter(output) {\n+                public void close() throws IOException {\n+                    flush();\n+                }\n+        }) {\n+            final SAMSequenceDictionaryCodec codec = new SAMSequenceDictionaryCodec(writer);\n+            codec.encode(dictionary);\n+        }\n+        output.closeArchiveEntry();\n+    }\n+\n+    private void composeSummaryText(ZipArchiveOutputStream output) throws IOException {\n+        output.putArchiveEntry(new ZipArchiveEntry(\"summary.txt\"));\n+        final PrintWriter writer = new PrintWriter(new OutputStreamWriter(output));\n+        writer.println(\"period\\trepeat\\ttotal\\temmitted\\tdecimation\\tactual_decimation\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            for (int j = 1; j <= maxRepeat; j++) {\n+                writer.println(String.format(\"%d\\t%d\\t%d\\t%d\\t%.2f\\t%.2f\",\n+                        i, j, totalCounts[i][j], emittedCounts[i][j],\n+                        Math.log(decimationTable.decimationMask(i,j) + 1) / Math.log(2),\n+                        (- Math.log(emittedCounts[i][j]) + Math.log(totalCounts[i][j])) / Math.log(2) ));\n+            }\n+        }\n+        writer.flush();\n+        output.closeArchiveEntry();\n+    }\n+\n+    private static BinaryTableWriter<DragstrLocus> createDragstrLocusWriter(final File outDir, final String name) {\n+        try {\n+            final File outFile = new File(outDir, name);\n+            return DragstrLocus.binaryWriter(outFile);\n+        } catch (final IOException  e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private void traverse(final int seqNumber, final SAMSequenceRecord sequence, final long seqStart, final long seqEnd, final AutoCloseableList<BinaryTableWriter<DragstrLocus>> output, final DecimationTable decimationTable)\n+        throws IOException\n+    {\n+        final String id = sequence.getSequenceName();\n+        final NucleotideSequence fullSequence = LazyLoadingReferenceNucleotideSequence.of(directlyAccessEngineReferenceDataSource(), sequence.getSequenceName(),10000);\n+        final int maxPeriod = this.maxPeriod;\n+        final int length = sequence.getSequenceLength();\n+        long pos = seqStart;\n+        utter: while (pos <= seqEnd) {\n+           final byte base = fullSequence.byteAt(pos);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 387}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5NjA5MDQ1", "url": "https://github.com/broadinstitute/gatk/pull/6634#pullrequestreview-439609045", "createdAt": "2020-06-30T01:34:46Z", "commit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMTozNDo0NlrOGqoaaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMTozNzo0MlrOGqod8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1NTQ5Ng==", "bodyText": "This should probably have a seperate unit test", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447355496", "createdAt": "2020-06-30T01:34:46Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private DecimationTable decimationTable = DecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(fullName=\"max-period\", doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = 8;\n+\n+    @Argument(fullName=\"max-repeats\", doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = 20;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private File tempDir;\n+\n+    private long[][] totalCounts;\n+\n+    private long[][] emittedCounts;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+        totalCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+        emittedCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+    }\n+\n+    public void onShutdown() {\n+        Utils.deleteFileTree(tempDir);\n+        super.onShutdown();\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        final SAMSequenceDictionary dictionary = Utils.nonNull(getReferenceDictionary());\n+        final File tempDir = this.tempDir;\n+        try (final AutoCloseableList<BinaryTableWriter<DragstrLocus>> allSitesWriter\n+                     = AutoCloseableList.of(maxPeriod, i -> createDragstrLocusWriter(tempDir, \"period_\" + (i + 1) + \".bin\"))) {\n+            if (!intervalArgumentCollection.intervalsSpecified()) {\n+                for (final SAMSequenceRecord sequence : dictionary.getSequences()) {\n+                    traverse(sequence.getSequenceIndex(), sequence, 1, sequence.getSequenceLength(), allSitesWriter, decimationTable);\n+                }\n+            } else {\n+                for (final SimpleInterval interval : intervalArgumentCollection.getIntervals(dictionary)) {\n+                    final SAMSequenceRecord sequence = dictionary.getSequence(interval.getContig());\n+                    traverse(sequence.getSequenceIndex(), sequence, interval.getStart(), interval.getEnd(), allSitesWriter, decimationTable);\n+                }\n+            }\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(tempDir, ex);\n+        }\n+        progressMeter.stop();\n+        logger.info(\"Finishing reference sampling. Proceeding to splitting each period case by repeat count\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            progressMeter = new ProgressMeter(secondsBetweenProgressUpdates);\n+            progressMeter.setRecordLabel(\"Splitting cases with period \" + i + \" by repeat count\");\n+            progressMeter.start();\n+            splitPeriodLociByRepeat(i);\n+            progressMeter.stop();\n+            logger.info(\"Done with period \" + i);\n+        }\n+        logger.info(\"Composing output zip\");\n+        composeOutputZip();\n+    }\n+\n+    private void splitPeriodLociByRepeat(final int period) {\n+        final File lociFile = new File(tempDir, \"period_\" + period + \".bin\");\n+        final File periodDir = new File(tempDir, \"\" + period);\n+        if (!periodDir.mkdir()) {\n+            throw new UserException.CouldNotCreateOutputFile(periodDir, \"create period loci split directory\");\n+        }\n+        try (final BinaryTableReader<DragstrLocus> reader = DragstrLocus.binaryReader(new FileInputStream(lociFile));\n+             final AutoCloseableList<BinaryTableWriter<DragstrLocus>> writers =\n+                     AutoCloseableList.of(maxRepeat, i -> createDragstrLocusWriter(periodDir, \"\" + (i+1) + \".bin\"))) {\n+            while (reader.hasNext()) {\n+                final DragstrLocus next = reader.next();\n+                final int effectiveRepeats = next.getRepeats() > maxRepeat ? maxRepeat : next.getRepeats();\n+                progressMeter.update(next.getStartInterval(getReferenceDictionary(), 0));\n+                final BinaryTableWriter<DragstrLocus> writer = writers.get(effectiveRepeats - 1);\n+                writer.write(next);\n+\n+            }\n+        } catch (final Exception ex) {\n+            throw new UserException.CouldNotCreateOutputFile(lociFile, \"temporary period loci splitting failed\");\n+        }\n+        if (!lociFile.delete()) {\n+            throw new GATKException(\"could not delete temporary file \" + lociFile);\n+        }\n+    }\n+\n+    private void composeOutputZip() {\n+        final byte[] buffer = new byte[1 << 16];\n+        try (final ZipArchiveOutputStream output = new JarArchiveOutputStream(BucketUtils.createFile(outputPath))) {\n+            composeSummaryText(output);\n+            saveReferenceDictionary(output);\n+            composeLociFiles(buffer, output);\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(outputPath, \"error composing the zip file\", ex);\n+        }\n+    }\n+\n+    private void composeLociFiles(byte[] buffer, ZipArchiveOutputStream output) throws IOException {\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            for (int j = 1; j <= maxRepeat; j++) {\n+                final File inFile = new File(new File(tempDir, \"\" + i), \"\" + j + \".bin\");\n+                output.putArchiveEntry(new ZipArchiveEntry(\"\" + i + \"/\" + j + \".bin\"));\n+                final InputStream inStream = new FileInputStream(inFile);\n+                int len;\n+                while ((len = inStream.read(buffer)) > 0) {\n+                    output.write(buffer, 0, len);\n+                }\n+                inStream.close();\n+                output.closeArchiveEntry();\n+            }\n+        }\n+    }\n+\n+    private void saveReferenceDictionary(final ZipArchiveOutputStream output) throws IOException {\n+        final SAMSequenceDictionary dictionary = getBestAvailableSequenceDictionary();\n+        output.putArchiveEntry(new ZipArchiveEntry(\"reference.dict\"));\n+        // need to prevent closing output if the writer's close is called. We simply\n+        // flush instead:\n+        try (final Writer writer = new OutputStreamWriter(output) {\n+                public void close() throws IOException {\n+                    flush();\n+                }\n+        }) {\n+            final SAMSequenceDictionaryCodec codec = new SAMSequenceDictionaryCodec(writer);\n+            codec.encode(dictionary);\n+        }\n+        output.closeArchiveEntry();\n+    }\n+\n+    private void composeSummaryText(ZipArchiveOutputStream output) throws IOException {\n+        output.putArchiveEntry(new ZipArchiveEntry(\"summary.txt\"));\n+        final PrintWriter writer = new PrintWriter(new OutputStreamWriter(output));\n+        writer.println(\"period\\trepeat\\ttotal\\temmitted\\tdecimation\\tactual_decimation\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            for (int j = 1; j <= maxRepeat; j++) {\n+                writer.println(String.format(\"%d\\t%d\\t%d\\t%d\\t%.2f\\t%.2f\",\n+                        i, j, totalCounts[i][j], emittedCounts[i][j],\n+                        Math.log(decimationTable.decimationMask(i,j) + 1) / Math.log(2),\n+                        (- Math.log(emittedCounts[i][j]) + Math.log(totalCounts[i][j])) / Math.log(2) ));\n+            }\n+        }\n+        writer.flush();\n+        output.closeArchiveEntry();\n+    }\n+\n+    private static BinaryTableWriter<DragstrLocus> createDragstrLocusWriter(final File outDir, final String name) {\n+        try {\n+            final File outFile = new File(outDir, name);\n+            return DragstrLocus.binaryWriter(outFile);\n+        } catch (final IOException  e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private void traverse(final int seqNumber, final SAMSequenceRecord sequence, final long seqStart, final long seqEnd, final AutoCloseableList<BinaryTableWriter<DragstrLocus>> output, final DecimationTable decimationTable)\n+        throws IOException\n+    {\n+        final String id = sequence.getSequenceName();\n+        final NucleotideSequence fullSequence = LazyLoadingReferenceNucleotideSequence.of(directlyAccessEngineReferenceDataSource(), sequence.getSequenceName(),10000);\n+        final int maxPeriod = this.maxPeriod;\n+        final int length = sequence.getSequenceLength();\n+        long pos = seqStart;\n+        utter: while (pos <= seqEnd) {\n+           final byte base = fullSequence.byteAt(pos);\n+           long beg, end, cmp;\n+           for (beg = pos - 1; beg >= 1 && fullSequence.byteAt(beg) == base; beg--);\n+           beg++;\n+           for (end = pos + 1; end <= length && fullSequence.byteAt(end) == base; end++);\n+           end--;\n+           int bestPeriod = 1;\n+           long bestPeriodRepeats = end - beg + 1;\n+           long bestEnd = end;\n+           long bestBeg = beg;\n+           for (int period = 2; period <= maxPeriod; period++) {\n+               for (beg = length - pos > period ? pos - 1 : length - period,\n+                    cmp = beg + period; beg >= 1 && fullSequence.byteAt(cmp) == fullSequence.byteAt(beg); beg--, cmp--);\n+               beg++;\n+               for (end = pos >= period ? pos + 1 : period + 1, cmp = end - period; end <= length && fullSequence.byteAt(end) == fullSequence.byteAt(cmp); end++, cmp++);\n+               end--;\n+               final long strLength = end - beg + 1;\n+               final long repeats =  strLength / period;\n+               if (repeats > bestPeriodRepeats) {\n+                   bestPeriod = period;\n+                   bestPeriodRepeats = repeats;\n+                   bestEnd = end;\n+                   bestBeg = beg;\n+               }\n+           }\n+           final byte[] unit = fullSequence.bytesAt(bestBeg, bestPeriod);\n+           pos = bestEnd + 1;\n+\n+           for (int i = 0; i < bestPeriod; i++) {\n+               if (!Nucleotide.decode(unit[i]).isStandard()) {\n+                   continue utter;\n+               }\n+           }\n+           emitOrDecimateSTR(seqNumber, id, bestBeg, bestBeg + bestPeriod * bestPeriodRepeats - 1, bestPeriod, bestPeriodRepeats, output, decimationTable, unit);\n+        }\n+    }\n+\n+    private void emitOrDecimateSTR(final int seqNumber, final String seqId, long bestBeg, long bestEnd, int bestPeriod, long bestPeriodRepeats, final AutoCloseableList<BinaryTableWriter<DragstrLocus>> output, final DecimationTable decimationTable, final byte[] unit)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 424}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1NTg5OA==", "bodyText": "Java already supports pushback input streams https://docs.oracle.com/javase/7/docs/api/java/io/PushbackInputStream.html", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447355898", "createdAt": "2020-06-30T01:35:56Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/PushbackDataInput.java", "diffHunk": "@@ -0,0 +1,61 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.io.DataInput;\n+import java.io.EOFException;\n+import java.io.IOException;\n+\n+public interface PushbackDataInput extends DataInput {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1NjIwMg==", "bodyText": "I think this interface is unnecessary as its only used in one place.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447356202", "createdAt": "2020-06-30T01:36:56Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/NucleotideSequence.java", "diffHunk": "@@ -0,0 +1,31 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+\n+public interface NucleotideSequence {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1NjQwMQ==", "bodyText": "Complicated methods like this should have javadocs. Also add @DocumentedFeature", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447356401", "createdAt": "2020-06-30T01:37:42Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/EstimateDragstrModelParameters.java", "diffHunk": "@@ -0,0 +1,158 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.*;\n+import htsjdk.samtools.util.BufferedLineReader;\n+import htsjdk.samtools.util.LineReader;\n+import org.apache.commons.collections4.EnumerationUtils;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+\n+import static org.broadinstitute.hellbender.utils.pairhmm.DragstrConstants.*;\n+\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.ZipEntry;\n+import java.util.zip.ZipFile;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class EstimateDragstrModelParameters extends GATKTool {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwMDcxNDcw", "url": "https://github.com/broadinstitute/gatk/pull/6634#pullrequestreview-440071470", "createdAt": "2020-06-30T14:26:28Z", "commit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "state": "COMMENTED", "comments": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNDoyNjoyOFrOGq_BhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTozNDowNlrOGrCOyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcyNTk1Nw==", "bodyText": "At the very least can this not extend PileupElement?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447725957", "createdAt": "2020-06-30T14:26:28Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/IntervalPileup.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package org.broadinstitute.hellbender.utils;\n+\n+import com.google.inject.ImplementedBy;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.util.Locatable;\n+import org.apache.commons.lang3.builder.EqualsExclude;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import javax.validation.OverridesAttribute;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+public interface IntervalPileup {\n+\n+    byte NO_BQ = (byte) -1;\n+    byte GAP = (byte) '-';\n+    byte NO_BASE = (byte) -1;\n+\n+    List<GATKRead> reads();\n+    ReferenceBases reference();\n+    int width();\n+    int height();\n+    byte baseAt(final int row, final int column); // if overlapping and different calls, it returns the call for the \"first\" read.\n+    byte qualAt(final int row, final int column); // if overlapping and different quals, it return the qual for the \"first\" read.\n+    Insert insertAt(final int row, final int column);\n+\n+    GATKRead readAt(final int row, final int column); // if overlapping, it returns the \"first\" read (possibly random).\n+    List<GATKRead> readsAt(final int row, final int column); // if mates are overlapping and we force mates on the same row we may have more than one read.\n+\n+    Element element(final GATKRead read);\n+    default Element element(int row) {\n+        final GATKRead read = reads().get(row);\n+        return element(read);\n+    }\n+\n+    boolean hasInsertAt(int i, int j);\n+\n+    byte[] insertBasesAt(int i, int j);\n+\n+    byte[] insertQualsAt(int i, int j);\n+\n+    interface Element {\n+        GATKRead read();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcyNjM0OQ==", "bodyText": "Javadoc these internal classes", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447726349", "createdAt": "2020-06-30T14:27:01Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/IntervalPileup.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package org.broadinstitute.hellbender.utils;\n+\n+import com.google.inject.ImplementedBy;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.util.Locatable;\n+import org.apache.commons.lang3.builder.EqualsExclude;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import javax.validation.OverridesAttribute;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+public interface IntervalPileup {\n+\n+    byte NO_BQ = (byte) -1;\n+    byte GAP = (byte) '-';\n+    byte NO_BASE = (byte) -1;\n+\n+    List<GATKRead> reads();\n+    ReferenceBases reference();\n+    int width();\n+    int height();\n+    byte baseAt(final int row, final int column); // if overlapping and different calls, it returns the call for the \"first\" read.\n+    byte qualAt(final int row, final int column); // if overlapping and different quals, it return the qual for the \"first\" read.\n+    Insert insertAt(final int row, final int column);\n+\n+    GATKRead readAt(final int row, final int column); // if overlapping, it returns the \"first\" read (possibly random).\n+    List<GATKRead> readsAt(final int row, final int column); // if mates are overlapping and we force mates on the same row we may have more than one read.\n+\n+    Element element(final GATKRead read);\n+    default Element element(int row) {\n+        final GATKRead read = reads().get(row);\n+        return element(read);\n+    }\n+\n+    boolean hasInsertAt(int i, int j);\n+\n+    byte[] insertBasesAt(int i, int j);\n+\n+    byte[] insertQualsAt(int i, int j);\n+\n+    interface Element {\n+        GATKRead read();\n+        int row();\n+        int minColumn();\n+        int maxColumn();\n+        Insert insertAt(final int column);\n+        List<Insert> inserts();\n+        default List<Insert> inserts(final int firstColumn, final int lastColumn) {\n+            if (lastColumn < firstColumn) {\n+                return Collections.emptyList();\n+            }\n+            final List<Insert> all = inserts();\n+            if (all.isEmpty()) {\n+                return Collections.emptyList();\n+            } else if (all.size() == 1) {\n+                final int column = all.get(0).column();\n+                return column >= firstColumn && column <= lastColumn ? all : Collections.emptyList();\n+            } else {\n+                int i;\n+                for (i = 0; i < all.size(); i++) {\n+                    if (all.get(i).column() >= firstColumn) {\n+                        break;\n+                    }\n+                }\n+                int j, k;\n+                for (j = i, k = 0; j < all.size(); j++, k++) {\n+                    if (all.get(j).column() > lastColumn) {\n+                        break;\n+                    }\n+                }\n+                if (k == 0) {\n+                    return Collections.emptyList();\n+                } else if (k == 1) {\n+                    return Collections.singletonList(all.get(i));\n+                } else {\n+                    return all.subList(i, i + k);\n+                }\n+            }\n+        }\n+\n+        boolean hasInsertAt(final int column);\n+        int insertSize(final int column);\n+        int copyInsertBases(int column, final byte[] dest, final int offset, final int length);\n+        int copyInsertQuals(int column, byte[] dest, int offset, int maxLength);\n+        byte[] insertQualsAt(int column);\n+        byte[] insertBasesAt(final int column);\n+        byte baseAt(final int column);\n+        byte qualAt(final int column);\n+    }\n+\n+    interface Insert {\n+        int column();\n+        int length();\n+        byte[] bases();\n+        byte[] quals();\n+        /**\n+         * Returns true iff and only iff the {@code other} object is also an insert and\n+         * has exactly the same bases and qualities.\n+         * @param other\n+         * @return\n+         * @see #hashCode()\n+         */\n+        @Override\n+        boolean equals(final Object other);\n+\n+        /**\n+         * Must be overrided in agreement with equals.\n+         * @return\n+         */\n+        @Override\n+        int hashCode();\n+\n+        int copyBases(int offset, byte[] dest, int destOffset, final int maxLength);\n+\n+        default int copyBases(int offset, byte[] dest, int destOffset) {\n+            return copyBases(offset, dest, destOffset, Integer.MAX_VALUE);\n+        }\n+        default int copyBases(byte[] dest) {\n+            return copyBases(0, dest, 0, Integer.MAX_VALUE);\n+        }\n+\n+        int copyQuals(int offset, byte[] dest, int destOffset, final int maxLength);\n+\n+        default int copyQuals(int offset, byte[] dest, int destOffset) {\n+            return copyQuals(offset, dest, destOffset, Integer.MAX_VALUE);\n+        }\n+        default int copyQuals(byte[] dest) {\n+            return copyQuals(0, dest, 0, Integer.MAX_VALUE);\n+        }\n+    }\n+\n+\n+    static IntervalPileup of(final Locatable loc, final ReadsDataSource aln, final ReferenceDataSource ref) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcyODQzOA==", "bodyText": "If this is done to save time over using a Pileup object then you should add a comment about that and an explanation of how this differs from that.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447728438", "createdAt": "2020-06-30T14:29:41Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/ByteMapIntervalPileup.java", "diffHunk": "@@ -0,0 +1,468 @@\n+package org.broadinstitute.hellbender.utils;\n+\n+import htsjdk.samtools.Cigar;\n+import htsjdk.samtools.CigarElement;\n+import htsjdk.samtools.CigarOperator;\n+import it.unimi.dsi.fastutil.ints.*;\n+import org.apache.commons.lang.ArrayUtils;\n+import org.broadinstitute.hellbender.utils.read.CigarUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * An interval map where bases and qualities are explicitly stored in matrices", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzczMDE4Mw==", "bodyText": "Rename this to something more descriptive, like numreads.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447730183", "createdAt": "2020-06-30T14:31:45Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/ByteMapIntervalPileup.java", "diffHunk": "@@ -0,0 +1,468 @@\n+package org.broadinstitute.hellbender.utils;\n+\n+import htsjdk.samtools.Cigar;\n+import htsjdk.samtools.CigarElement;\n+import htsjdk.samtools.CigarOperator;\n+import it.unimi.dsi.fastutil.ints.*;\n+import org.apache.commons.lang.ArrayUtils;\n+import org.broadinstitute.hellbender.utils.read.CigarUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * An interval map where bases and qualities are explicitly stored in matrices\n+ * for fast look-up.\n+ */\n+class ByteMapIntervalPileup implements IntervalPileup {\n+\n+    private final ReferenceBases referenceBases;\n+    private final List<GATKRead> reads;\n+    private final List<IntervalPileup.Element> elements;\n+    private final Int2ObjectMap<IntervalPileup.Element> elementByIndex;\n+    private final Map<GATKRead, IntervalPileup.Element> elementByRead;\n+    private final int width;\n+    private final int height;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzczMDMxNQ==", "bodyText": "Rename this to somethign more descriptive", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447730315", "createdAt": "2020-06-30T14:31:55Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/ByteMapIntervalPileup.java", "diffHunk": "@@ -0,0 +1,468 @@\n+package org.broadinstitute.hellbender.utils;\n+\n+import htsjdk.samtools.Cigar;\n+import htsjdk.samtools.CigarElement;\n+import htsjdk.samtools.CigarOperator;\n+import it.unimi.dsi.fastutil.ints.*;\n+import org.apache.commons.lang.ArrayUtils;\n+import org.broadinstitute.hellbender.utils.read.CigarUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * An interval map where bases and qualities are explicitly stored in matrices\n+ * for fast look-up.\n+ */\n+class ByteMapIntervalPileup implements IntervalPileup {\n+\n+    private final ReferenceBases referenceBases;\n+    private final List<GATKRead> reads;\n+    private final List<IntervalPileup.Element> elements;\n+    private final Int2ObjectMap<IntervalPileup.Element> elementByIndex;\n+    private final Map<GATKRead, IntervalPileup.Element> elementByRead;\n+    private final int width;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzczMjEwOA==", "bodyText": "Please add javdocs to visually distinguish the internal classes here.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447732108", "createdAt": "2020-06-30T14:34:13Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/ByteMapIntervalPileup.java", "diffHunk": "@@ -0,0 +1,468 @@\n+package org.broadinstitute.hellbender.utils;\n+\n+import htsjdk.samtools.Cigar;\n+import htsjdk.samtools.CigarElement;\n+import htsjdk.samtools.CigarOperator;\n+import it.unimi.dsi.fastutil.ints.*;\n+import org.apache.commons.lang.ArrayUtils;\n+import org.broadinstitute.hellbender.utils.read.CigarUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * An interval map where bases and qualities are explicitly stored in matrices\n+ * for fast look-up.\n+ */\n+class ByteMapIntervalPileup implements IntervalPileup {\n+\n+    private final ReferenceBases referenceBases;\n+    private final List<GATKRead> reads;\n+    private final List<IntervalPileup.Element> elements;\n+    private final Int2ObjectMap<IntervalPileup.Element> elementByIndex;\n+    private final Map<GATKRead, IntervalPileup.Element> elementByRead;\n+    private final int width;\n+    private final int height;\n+    private byte[][] bases;\n+    private byte[][] quals;\n+    private final List<Insert> insertsBuffer = new ArrayList<>(10);\n+    private final IntList insertsBufferOffsets = new IntArrayList(10);\n+\n+\n+    ByteMapIntervalPileup(final ReferenceBases referenceBases, final List<GATKRead> reads) {\n+        this.referenceBases = referenceBases;\n+        this.width = referenceBases.getInterval().size();\n+        final SimpleInterval referenceInterval = referenceBases.getInterval();\n+        final int referenceStart = referenceInterval.getStart();\n+        final int referenceEnd = referenceInterval.getEnd();\n+        this.reads = Collections.unmodifiableList(reads.stream()\n+                .filter(read -> !read.isUnmapped() && read.getStart() <= referenceEnd && read.getEnd() >= referenceStart)\n+                .sorted(Comparator.comparingInt(GATKRead::getStart).thenComparing(GATKRead::getEnd).thenComparing(GATKRead::getName))\n+                .collect(Collectors.toList()));\n+        this.elements = new ArrayList<>(this.reads.size());\n+        this.elementByIndex = new Int2ObjectOpenHashMap<>(this.reads.size());\n+        this.elementByRead = new HashMap<>(this.reads.size());\n+        this.height = this.reads.size();\n+        bases = new byte[height][width];\n+        quals = new byte[height][width];\n+        for (int i = 0; i < height; i++) {\n+            final IntervalPileup.Element element = new Element(this, this.reads.get(i), i);\n+            elements.add(element);\n+            elementByIndex.put(i, element);\n+            elementByRead.put(this.reads.get(i), element);\n+        }\n+    }\n+\n+    @Override\n+    public List<GATKRead> reads() {\n+        return reads;\n+    }\n+\n+    @Override\n+    public ReferenceBases reference() {\n+        return referenceBases;\n+    }\n+\n+    @Override\n+    public int width() {\n+        return width;\n+    }\n+\n+    @Override\n+    public int height() {\n+        return height;\n+    }\n+\n+    @Override\n+    public byte baseAt(final int row, final int column) {\n+        return bases[row][column];\n+    }\n+\n+    @Override\n+    public byte qualAt(int row, int column) {\n+        return quals[row][column];\n+    }\n+\n+    @Override\n+    public IntervalPileup.Insert insertAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elementByIndex.get(row);\n+        return element != null ? element.insertAt(column) : null;\n+    }\n+\n+    @Override\n+    public GATKRead readAt(int row, int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        if (element.minColumn() > column || element.maxColumn() < column) {\n+            return null;\n+        } else {\n+            return element.read();\n+        }\n+    }\n+\n+    @Override\n+    public List<GATKRead> readsAt(int row, int column) {\n+        final GATKRead read = readAt(row, column);\n+        return read != null ? Collections.singletonList(read) : Collections.emptyList();\n+    }\n+\n+    @Override\n+    public IntervalPileup.Element element(final GATKRead read) {\n+        return elementByRead.get(read);\n+    }\n+\n+    @Override\n+    public boolean hasInsertAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        return element.hasInsertAt(column);\n+    }\n+\n+    @Override\n+    public byte[] insertBasesAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        return element.insertBasesAt(column);\n+    }\n+\n+    @Override\n+    public byte[] insertQualsAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        return element.insertQualsAt(column);\n+    }\n+\n+    static class Insert implements  IntervalPileup.Insert {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzczNDg0Nw==", "bodyText": "Do you not want to left align inserts? IT seems like this step would be necessary to ensure there is no strangeness from the aligner.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447734847", "createdAt": "2020-06-30T14:37:48Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/ByteMapIntervalPileup.java", "diffHunk": "@@ -0,0 +1,468 @@\n+package org.broadinstitute.hellbender.utils;\n+\n+import htsjdk.samtools.Cigar;\n+import htsjdk.samtools.CigarElement;\n+import htsjdk.samtools.CigarOperator;\n+import it.unimi.dsi.fastutil.ints.*;\n+import org.apache.commons.lang.ArrayUtils;\n+import org.broadinstitute.hellbender.utils.read.CigarUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * An interval map where bases and qualities are explicitly stored in matrices\n+ * for fast look-up.\n+ */\n+class ByteMapIntervalPileup implements IntervalPileup {\n+\n+    private final ReferenceBases referenceBases;\n+    private final List<GATKRead> reads;\n+    private final List<IntervalPileup.Element> elements;\n+    private final Int2ObjectMap<IntervalPileup.Element> elementByIndex;\n+    private final Map<GATKRead, IntervalPileup.Element> elementByRead;\n+    private final int width;\n+    private final int height;\n+    private byte[][] bases;\n+    private byte[][] quals;\n+    private final List<Insert> insertsBuffer = new ArrayList<>(10);\n+    private final IntList insertsBufferOffsets = new IntArrayList(10);\n+\n+\n+    ByteMapIntervalPileup(final ReferenceBases referenceBases, final List<GATKRead> reads) {\n+        this.referenceBases = referenceBases;\n+        this.width = referenceBases.getInterval().size();\n+        final SimpleInterval referenceInterval = referenceBases.getInterval();\n+        final int referenceStart = referenceInterval.getStart();\n+        final int referenceEnd = referenceInterval.getEnd();\n+        this.reads = Collections.unmodifiableList(reads.stream()\n+                .filter(read -> !read.isUnmapped() && read.getStart() <= referenceEnd && read.getEnd() >= referenceStart)\n+                .sorted(Comparator.comparingInt(GATKRead::getStart).thenComparing(GATKRead::getEnd).thenComparing(GATKRead::getName))\n+                .collect(Collectors.toList()));\n+        this.elements = new ArrayList<>(this.reads.size());\n+        this.elementByIndex = new Int2ObjectOpenHashMap<>(this.reads.size());\n+        this.elementByRead = new HashMap<>(this.reads.size());\n+        this.height = this.reads.size();\n+        bases = new byte[height][width];\n+        quals = new byte[height][width];\n+        for (int i = 0; i < height; i++) {\n+            final IntervalPileup.Element element = new Element(this, this.reads.get(i), i);\n+            elements.add(element);\n+            elementByIndex.put(i, element);\n+            elementByRead.put(this.reads.get(i), element);\n+        }\n+    }\n+\n+    @Override\n+    public List<GATKRead> reads() {\n+        return reads;\n+    }\n+\n+    @Override\n+    public ReferenceBases reference() {\n+        return referenceBases;\n+    }\n+\n+    @Override\n+    public int width() {\n+        return width;\n+    }\n+\n+    @Override\n+    public int height() {\n+        return height;\n+    }\n+\n+    @Override\n+    public byte baseAt(final int row, final int column) {\n+        return bases[row][column];\n+    }\n+\n+    @Override\n+    public byte qualAt(int row, int column) {\n+        return quals[row][column];\n+    }\n+\n+    @Override\n+    public IntervalPileup.Insert insertAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elementByIndex.get(row);\n+        return element != null ? element.insertAt(column) : null;\n+    }\n+\n+    @Override\n+    public GATKRead readAt(int row, int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        if (element.minColumn() > column || element.maxColumn() < column) {\n+            return null;\n+        } else {\n+            return element.read();\n+        }\n+    }\n+\n+    @Override\n+    public List<GATKRead> readsAt(int row, int column) {\n+        final GATKRead read = readAt(row, column);\n+        return read != null ? Collections.singletonList(read) : Collections.emptyList();\n+    }\n+\n+    @Override\n+    public IntervalPileup.Element element(final GATKRead read) {\n+        return elementByRead.get(read);\n+    }\n+\n+    @Override\n+    public boolean hasInsertAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        return element.hasInsertAt(column);\n+    }\n+\n+    @Override\n+    public byte[] insertBasesAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        return element.insertBasesAt(column);\n+    }\n+\n+    @Override\n+    public byte[] insertQualsAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        return element.insertQualsAt(column);\n+    }\n+\n+    static class Insert implements  IntervalPileup.Insert {\n+        private final GATKRead enclosingRead;\n+        private final int offset;\n+        private final int length;\n+        private final int column;\n+        private transient int hashCode = 0;\n+\n+        Insert(final GATKRead enclosingRead, final int offset, final int column, final int length) {\n+            this.enclosingRead = enclosingRead;\n+            this.offset = offset;\n+            this.length = length;\n+            this.column = column;\n+        }\n+\n+        public int column() {\n+            return column;\n+        }\n+\n+        @Override\n+        public int length() {\n+            return length;\n+        }\n+\n+        @Override\n+        public byte[] bases() {\n+            final byte[] result = new byte[length];\n+            final int copied = enclosingRead.copyBases(offset, result, 0, length);\n+            if (copied < length) {\n+                Arrays.fill(result, copied, result.length, (byte) 'N');\n+            }\n+            return result;\n+        }\n+\n+        @Override\n+        public byte[] quals() {\n+            final byte[] result = new byte[length];\n+            final int copied = enclosingRead.copyBaseQualities(offset, result, 0, length);\n+            if (copied < length) {\n+                Arrays.fill(result, copied, result.length, NO_BQ);\n+            }\n+            return result;\n+        }\n+\n+        @Override\n+        public int copyBases(int offset, byte[] dest, int destOffset, int maxLength) {\n+            final int actualMaxLength = Math.min(length, maxLength);\n+            final int copied = enclosingRead.copyBases(this.offset + offset, dest, destOffset, actualMaxLength);\n+            if (copied < actualMaxLength) {\n+                Arrays.fill(dest, destOffset + copied, destOffset + actualMaxLength, (byte) 'N');\n+            }\n+            return actualMaxLength;\n+        }\n+\n+        @Override\n+        public int copyQuals(int offset, byte[] dest, int destOffset, int maxLength) {\n+            final int actualMaxLength = Math.min(length, maxLength);\n+            final int copied = enclosingRead.copyBaseQualities(this.offset + offset, dest, destOffset, actualMaxLength);\n+            if (copied < actualMaxLength) {\n+                Arrays.fill(dest, destOffset + copied, destOffset + actualMaxLength, (byte) 'N');\n+            }\n+            return actualMaxLength;\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            if (hashCode == 0) {\n+                final byte[] readBases = enclosingRead.getBasesNoCopy();\n+                if (readBases != null && readBases.length > offset) {\n+                    final int to = offset + length;\n+                    final int to2 = to <= readBases.length ? to : readBases.length;\n+                    int i;\n+                    hashCode = 1;\n+                    for (i = offset; i < to2; i++) {\n+                        hashCode = hashCode * 31 + readBases[i];\n+                    }\n+                    for (; i < to; i++) {\n+                        hashCode = hashCode * 31 + 'N';\n+                    }\n+                }\n+            }\n+            return hashCode;\n+        }\n+\n+        @Override\n+        public boolean equals(final Object other) {\n+            return other == this || (other instanceof Insert && equals((IntervalPileup.Insert) other));\n+        }\n+\n+        private boolean equals(final IntervalPileup.Insert other) {\n+            return length == other.length() && hashCode() == other.hashCode() &&\n+                    equalBases(other) && equalQualities(other);\n+        }\n+\n+        private boolean equalBases(final IntervalPileup.Insert other) {\n+            if (other instanceof Insert) {\n+                equalBases((Insert) other);\n+            }\n+            final byte[] otherBases = other.bases();\n+            final byte[] readBases = enclosingRead.getBasesNoCopy();\n+            for (int i = 0; i < otherBases.length; i++) {\n+                if (otherBases[i] != readBases[offset + i]) {\n+                    return false;\n+                }\n+            }\n+            return true;\n+        }\n+\n+        private boolean equalBases(final Insert other) {\n+            final byte[] otherBases = other.enclosingRead.getBasesNoCopy();\n+            final byte[] thisBases = enclosingRead.getBasesNoCopy();\n+            for (int i = 0; i < length; i++) {\n+                if (otherBases[other.offset + i] != thisBases[offset + i]) {\n+                    return false;\n+                }\n+            }\n+            return true;\n+        }\n+\n+        private boolean equalQualities(final IntervalPileup.Insert other) {\n+            if (other instanceof Insert) {\n+                equalQualities((Insert) other);\n+            }\n+            final byte[] otherQuals = other.quals();\n+            final byte[] thisQuals = enclosingRead.getBaseQualitiesNoCopy();\n+            for (int i = 0; i < otherQuals.length; i++) {\n+                if (otherQuals[i] != thisQuals[offset + i]) {\n+                    return false;\n+                }\n+            }\n+            return true;\n+        }\n+\n+        private boolean equalQualities(final Insert other) {\n+            final byte[] otherQuals = other.enclosingRead.getBaseQualitiesNoCopy();\n+            final byte[] thisQuals = enclosingRead.getBaseQualitiesNoCopy();\n+            for (int i = 0; i < length; i++) {\n+                if (otherQuals[other.offset + i] != thisQuals[offset + i]) {\n+                    return false;\n+                }\n+            }\n+            return true;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return new String(bases()) + \"/\" + new String(quals());\n+        }\n+    }\n+\n+    static class Element implements IntervalPileup.Element {\n+\n+        private final GATKRead read;\n+        private final int row;\n+        private final int minColumn;\n+        private final int maxColumn;\n+        private final Int2ObjectMap<Insert> inserts;\n+        private final ByteMapIntervalPileup pileup;\n+\n+        private Element(final ByteMapIntervalPileup pileup, final GATKRead read, final int row) {\n+            this.pileup = pileup;\n+            this.row = row;\n+            this.read = read;\n+            final Cigar cigar = read.getCigar();\n+            final int referenceWidth = CigarUtils.countRefBases(cigar);\n+            int referenceOffset = read.getStart() - pileup.referenceBases.getInterval().getStart();\n+            int readOffset = 0;\n+            minColumn = Math.max(0, referenceOffset);\n+            maxColumn = Math.min(pileup.width - 1, referenceOffset + referenceWidth - 1);\n+            Arrays.fill(pileup.bases[row], 0, minColumn, NO_BASE);\n+            Arrays.fill(pileup.bases[row], maxColumn + 1, pileup.width, NO_BASE);\n+            Arrays.fill(pileup.quals[row], 0, minColumn, NO_BQ);\n+            Arrays.fill(pileup.quals[row], maxColumn + 1, pileup.width, NO_BQ);\n+            final List<CigarElement> cigarElements = cigar.getCigarElements();\n+            pileup.insertsBuffer.clear();\n+            pileup.insertsBufferOffsets.clear();\n+            int i;\n+            for (i = 0; referenceOffset <= maxColumn + 1 &&  i < cigarElements.size(); i++) {\n+                final CigarElement element = cigarElements.get(i);\n+                final CigarOperator op = element.getOperator();\n+                final int length = element.getLength();\n+                final int newReferenceOffset = referenceOffset + (op.consumesReferenceBases() ? length : 0);\n+                final int newReadOffset = readOffset + (op.consumesReadBases() ? length : 0);\n+                if (newReferenceOffset >= minColumn) {\n+                    if (op.isAlignment()) {\n+                        final int leftOverhang = referenceOffset < minColumn ? minColumn - referenceOffset : 0;\n+                        final int from = referenceOffset + leftOverhang;\n+                        final int len = newReferenceOffset > maxColumn ? maxColumn + 1 - from :\n+                                  length - leftOverhang;\n+                        final int readFrom = readOffset + leftOverhang;\n+                        final int copiedBases = read.copyBases(readFrom, pileup.bases[row], from, len);\n+                        final int copiedQuals = read.copyBaseQualities(readFrom, pileup.quals[row], from, len);\n+                        if (copiedBases < len) {\n+                            Arrays.fill(pileup.bases[row], from + copiedBases, from + len - copiedBases, (byte) 'N');\n+                        }\n+                        if (copiedQuals < len) {\n+                            Arrays.fill(pileup.quals[row], from + copiedQuals, from + len - copiedQuals, NO_BQ);\n+                        }\n+                    } else if (op == CigarOperator.I) {\n+                        pileup.insertsBuffer.add(new Insert(read, readOffset, referenceOffset - 1, length));\n+                        pileup.insertsBufferOffsets.add(referenceOffset - 1);\n+                    } else if (op == CigarOperator.D || op == CigarOperator.N) {\n+                        final int from = referenceOffset < minColumn ? minColumn : referenceOffset;\n+                        final int len = (newReferenceOffset > maxColumn ? maxColumn + 1 : newReferenceOffset) - from;\n+                        Arrays.fill(pileup.bases[row], from, from + len, GAP);\n+                        Arrays.fill(pileup.quals[row], from, from + len, NO_BQ);\n+                    }\n+                }\n+                readOffset = newReadOffset;\n+                referenceOffset = newReferenceOffset;\n+            }\n+            // merge adjacent inserts if any.\n+            mergeAdjacentInserts(read, pileup.insertsBuffer, pileup.insertsBufferOffsets);\n+            inserts = consolidateInserts(pileup.insertsBuffer, pileup.insertsBufferOffsets);\n+        }\n+\n+        private static Int2ObjectMap<Insert> consolidateInserts(final List<Insert> inserts, final IntList offsets) {\n+            final int size = inserts.size();\n+            if (size == 0) {\n+                return Int2ObjectMaps.emptyMap();\n+            } else if (size == 1) {\n+                return Int2ObjectMaps.singleton(offsets.get(0) , inserts.get(0));\n+            } else {\n+                final Int2ObjectMap<Insert> result = size < 5 ? new Int2ObjectArrayMap<>(size)\n+                        : new Int2ObjectOpenHashMap<>(size);\n+                for (int ins = 0; ins < size; ins++) {\n+                    result.put(offsets.get(ins) , inserts.get(ins));\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static void mergeAdjacentInserts(final GATKRead read, final List<Insert> inserts, final IntList offsets) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 364}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzczNjcxOQ==", "bodyText": "Replace this with \"consumes read bases\"", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447736719", "createdAt": "2020-06-30T14:40:01Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/ByteMapIntervalPileup.java", "diffHunk": "@@ -0,0 +1,468 @@\n+package org.broadinstitute.hellbender.utils;\n+\n+import htsjdk.samtools.Cigar;\n+import htsjdk.samtools.CigarElement;\n+import htsjdk.samtools.CigarOperator;\n+import it.unimi.dsi.fastutil.ints.*;\n+import org.apache.commons.lang.ArrayUtils;\n+import org.broadinstitute.hellbender.utils.read.CigarUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * An interval map where bases and qualities are explicitly stored in matrices\n+ * for fast look-up.\n+ */\n+class ByteMapIntervalPileup implements IntervalPileup {\n+\n+    private final ReferenceBases referenceBases;\n+    private final List<GATKRead> reads;\n+    private final List<IntervalPileup.Element> elements;\n+    private final Int2ObjectMap<IntervalPileup.Element> elementByIndex;\n+    private final Map<GATKRead, IntervalPileup.Element> elementByRead;\n+    private final int width;\n+    private final int height;\n+    private byte[][] bases;\n+    private byte[][] quals;\n+    private final List<Insert> insertsBuffer = new ArrayList<>(10);\n+    private final IntList insertsBufferOffsets = new IntArrayList(10);\n+\n+\n+    ByteMapIntervalPileup(final ReferenceBases referenceBases, final List<GATKRead> reads) {\n+        this.referenceBases = referenceBases;\n+        this.width = referenceBases.getInterval().size();\n+        final SimpleInterval referenceInterval = referenceBases.getInterval();\n+        final int referenceStart = referenceInterval.getStart();\n+        final int referenceEnd = referenceInterval.getEnd();\n+        this.reads = Collections.unmodifiableList(reads.stream()\n+                .filter(read -> !read.isUnmapped() && read.getStart() <= referenceEnd && read.getEnd() >= referenceStart)\n+                .sorted(Comparator.comparingInt(GATKRead::getStart).thenComparing(GATKRead::getEnd).thenComparing(GATKRead::getName))\n+                .collect(Collectors.toList()));\n+        this.elements = new ArrayList<>(this.reads.size());\n+        this.elementByIndex = new Int2ObjectOpenHashMap<>(this.reads.size());\n+        this.elementByRead = new HashMap<>(this.reads.size());\n+        this.height = this.reads.size();\n+        bases = new byte[height][width];\n+        quals = new byte[height][width];\n+        for (int i = 0; i < height; i++) {\n+            final IntervalPileup.Element element = new Element(this, this.reads.get(i), i);\n+            elements.add(element);\n+            elementByIndex.put(i, element);\n+            elementByRead.put(this.reads.get(i), element);\n+        }\n+    }\n+\n+    @Override\n+    public List<GATKRead> reads() {\n+        return reads;\n+    }\n+\n+    @Override\n+    public ReferenceBases reference() {\n+        return referenceBases;\n+    }\n+\n+    @Override\n+    public int width() {\n+        return width;\n+    }\n+\n+    @Override\n+    public int height() {\n+        return height;\n+    }\n+\n+    @Override\n+    public byte baseAt(final int row, final int column) {\n+        return bases[row][column];\n+    }\n+\n+    @Override\n+    public byte qualAt(int row, int column) {\n+        return quals[row][column];\n+    }\n+\n+    @Override\n+    public IntervalPileup.Insert insertAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elementByIndex.get(row);\n+        return element != null ? element.insertAt(column) : null;\n+    }\n+\n+    @Override\n+    public GATKRead readAt(int row, int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        if (element.minColumn() > column || element.maxColumn() < column) {\n+            return null;\n+        } else {\n+            return element.read();\n+        }\n+    }\n+\n+    @Override\n+    public List<GATKRead> readsAt(int row, int column) {\n+        final GATKRead read = readAt(row, column);\n+        return read != null ? Collections.singletonList(read) : Collections.emptyList();\n+    }\n+\n+    @Override\n+    public IntervalPileup.Element element(final GATKRead read) {\n+        return elementByRead.get(read);\n+    }\n+\n+    @Override\n+    public boolean hasInsertAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        return element.hasInsertAt(column);\n+    }\n+\n+    @Override\n+    public byte[] insertBasesAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        return element.insertBasesAt(column);\n+    }\n+\n+    @Override\n+    public byte[] insertQualsAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        return element.insertQualsAt(column);\n+    }\n+\n+    static class Insert implements  IntervalPileup.Insert {\n+        private final GATKRead enclosingRead;\n+        private final int offset;\n+        private final int length;\n+        private final int column;\n+        private transient int hashCode = 0;\n+\n+        Insert(final GATKRead enclosingRead, final int offset, final int column, final int length) {\n+            this.enclosingRead = enclosingRead;\n+            this.offset = offset;\n+            this.length = length;\n+            this.column = column;\n+        }\n+\n+        public int column() {\n+            return column;\n+        }\n+\n+        @Override\n+        public int length() {\n+            return length;\n+        }\n+\n+        @Override\n+        public byte[] bases() {\n+            final byte[] result = new byte[length];\n+            final int copied = enclosingRead.copyBases(offset, result, 0, length);\n+            if (copied < length) {\n+                Arrays.fill(result, copied, result.length, (byte) 'N');\n+            }\n+            return result;\n+        }\n+\n+        @Override\n+        public byte[] quals() {\n+            final byte[] result = new byte[length];\n+            final int copied = enclosingRead.copyBaseQualities(offset, result, 0, length);\n+            if (copied < length) {\n+                Arrays.fill(result, copied, result.length, NO_BQ);\n+            }\n+            return result;\n+        }\n+\n+        @Override\n+        public int copyBases(int offset, byte[] dest, int destOffset, int maxLength) {\n+            final int actualMaxLength = Math.min(length, maxLength);\n+            final int copied = enclosingRead.copyBases(this.offset + offset, dest, destOffset, actualMaxLength);\n+            if (copied < actualMaxLength) {\n+                Arrays.fill(dest, destOffset + copied, destOffset + actualMaxLength, (byte) 'N');\n+            }\n+            return actualMaxLength;\n+        }\n+\n+        @Override\n+        public int copyQuals(int offset, byte[] dest, int destOffset, int maxLength) {\n+            final int actualMaxLength = Math.min(length, maxLength);\n+            final int copied = enclosingRead.copyBaseQualities(this.offset + offset, dest, destOffset, actualMaxLength);\n+            if (copied < actualMaxLength) {\n+                Arrays.fill(dest, destOffset + copied, destOffset + actualMaxLength, (byte) 'N');\n+            }\n+            return actualMaxLength;\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            if (hashCode == 0) {\n+                final byte[] readBases = enclosingRead.getBasesNoCopy();\n+                if (readBases != null && readBases.length > offset) {\n+                    final int to = offset + length;\n+                    final int to2 = to <= readBases.length ? to : readBases.length;\n+                    int i;\n+                    hashCode = 1;\n+                    for (i = offset; i < to2; i++) {\n+                        hashCode = hashCode * 31 + readBases[i];\n+                    }\n+                    for (; i < to; i++) {\n+                        hashCode = hashCode * 31 + 'N';\n+                    }\n+                }\n+            }\n+            return hashCode;\n+        }\n+\n+        @Override\n+        public boolean equals(final Object other) {\n+            return other == this || (other instanceof Insert && equals((IntervalPileup.Insert) other));\n+        }\n+\n+        private boolean equals(final IntervalPileup.Insert other) {\n+            return length == other.length() && hashCode() == other.hashCode() &&\n+                    equalBases(other) && equalQualities(other);\n+        }\n+\n+        private boolean equalBases(final IntervalPileup.Insert other) {\n+            if (other instanceof Insert) {\n+                equalBases((Insert) other);\n+            }\n+            final byte[] otherBases = other.bases();\n+            final byte[] readBases = enclosingRead.getBasesNoCopy();\n+            for (int i = 0; i < otherBases.length; i++) {\n+                if (otherBases[i] != readBases[offset + i]) {\n+                    return false;\n+                }\n+            }\n+            return true;\n+        }\n+\n+        private boolean equalBases(final Insert other) {\n+            final byte[] otherBases = other.enclosingRead.getBasesNoCopy();\n+            final byte[] thisBases = enclosingRead.getBasesNoCopy();\n+            for (int i = 0; i < length; i++) {\n+                if (otherBases[other.offset + i] != thisBases[offset + i]) {\n+                    return false;\n+                }\n+            }\n+            return true;\n+        }\n+\n+        private boolean equalQualities(final IntervalPileup.Insert other) {\n+            if (other instanceof Insert) {\n+                equalQualities((Insert) other);\n+            }\n+            final byte[] otherQuals = other.quals();\n+            final byte[] thisQuals = enclosingRead.getBaseQualitiesNoCopy();\n+            for (int i = 0; i < otherQuals.length; i++) {\n+                if (otherQuals[i] != thisQuals[offset + i]) {\n+                    return false;\n+                }\n+            }\n+            return true;\n+        }\n+\n+        private boolean equalQualities(final Insert other) {\n+            final byte[] otherQuals = other.enclosingRead.getBaseQualitiesNoCopy();\n+            final byte[] thisQuals = enclosingRead.getBaseQualitiesNoCopy();\n+            for (int i = 0; i < length; i++) {\n+                if (otherQuals[other.offset + i] != thisQuals[offset + i]) {\n+                    return false;\n+                }\n+            }\n+            return true;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return new String(bases()) + \"/\" + new String(quals());\n+        }\n+    }\n+\n+    static class Element implements IntervalPileup.Element {\n+\n+        private final GATKRead read;\n+        private final int row;\n+        private final int minColumn;\n+        private final int maxColumn;\n+        private final Int2ObjectMap<Insert> inserts;\n+        private final ByteMapIntervalPileup pileup;\n+\n+        private Element(final ByteMapIntervalPileup pileup, final GATKRead read, final int row) {\n+            this.pileup = pileup;\n+            this.row = row;\n+            this.read = read;\n+            final Cigar cigar = read.getCigar();\n+            final int referenceWidth = CigarUtils.countRefBases(cigar);\n+            int referenceOffset = read.getStart() - pileup.referenceBases.getInterval().getStart();\n+            int readOffset = 0;\n+            minColumn = Math.max(0, referenceOffset);\n+            maxColumn = Math.min(pileup.width - 1, referenceOffset + referenceWidth - 1);\n+            Arrays.fill(pileup.bases[row], 0, minColumn, NO_BASE);\n+            Arrays.fill(pileup.bases[row], maxColumn + 1, pileup.width, NO_BASE);\n+            Arrays.fill(pileup.quals[row], 0, minColumn, NO_BQ);\n+            Arrays.fill(pileup.quals[row], maxColumn + 1, pileup.width, NO_BQ);\n+            final List<CigarElement> cigarElements = cigar.getCigarElements();\n+            pileup.insertsBuffer.clear();\n+            pileup.insertsBufferOffsets.clear();\n+            int i;\n+            for (i = 0; referenceOffset <= maxColumn + 1 &&  i < cigarElements.size(); i++) {\n+                final CigarElement element = cigarElements.get(i);\n+                final CigarOperator op = element.getOperator();\n+                final int length = element.getLength();\n+                final int newReferenceOffset = referenceOffset + (op.consumesReferenceBases() ? length : 0);\n+                final int newReadOffset = readOffset + (op.consumesReadBases() ? length : 0);\n+                if (newReferenceOffset >= minColumn) {\n+                    if (op.isAlignment()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 316}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc0MjMzNA==", "bodyText": "I think a lot of this machinery (this method in particular) is redundant with other machinery in the GATK and is duplicated effort. I think you can replace all of these offset computation operations by hooking into the AlignmentStateMachine which has a stepForwardOnGenome() operation that would handle most of the logic that you want here. I would advocate trying to reuse as much of that code as possible here or at least provide an explanation as to why we need to add another codepath for calculating offsets for a plie of reads over the genome", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447742334", "createdAt": "2020-06-30T14:47:15Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/ByteMapIntervalPileup.java", "diffHunk": "@@ -0,0 +1,468 @@\n+package org.broadinstitute.hellbender.utils;\n+\n+import htsjdk.samtools.Cigar;\n+import htsjdk.samtools.CigarElement;\n+import htsjdk.samtools.CigarOperator;\n+import it.unimi.dsi.fastutil.ints.*;\n+import org.apache.commons.lang.ArrayUtils;\n+import org.broadinstitute.hellbender.utils.read.CigarUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * An interval map where bases and qualities are explicitly stored in matrices\n+ * for fast look-up.\n+ */\n+class ByteMapIntervalPileup implements IntervalPileup {\n+\n+    private final ReferenceBases referenceBases;\n+    private final List<GATKRead> reads;\n+    private final List<IntervalPileup.Element> elements;\n+    private final Int2ObjectMap<IntervalPileup.Element> elementByIndex;\n+    private final Map<GATKRead, IntervalPileup.Element> elementByRead;\n+    private final int width;\n+    private final int height;\n+    private byte[][] bases;\n+    private byte[][] quals;\n+    private final List<Insert> insertsBuffer = new ArrayList<>(10);\n+    private final IntList insertsBufferOffsets = new IntArrayList(10);\n+\n+\n+    ByteMapIntervalPileup(final ReferenceBases referenceBases, final List<GATKRead> reads) {\n+        this.referenceBases = referenceBases;\n+        this.width = referenceBases.getInterval().size();\n+        final SimpleInterval referenceInterval = referenceBases.getInterval();\n+        final int referenceStart = referenceInterval.getStart();\n+        final int referenceEnd = referenceInterval.getEnd();\n+        this.reads = Collections.unmodifiableList(reads.stream()\n+                .filter(read -> !read.isUnmapped() && read.getStart() <= referenceEnd && read.getEnd() >= referenceStart)\n+                .sorted(Comparator.comparingInt(GATKRead::getStart).thenComparing(GATKRead::getEnd).thenComparing(GATKRead::getName))\n+                .collect(Collectors.toList()));\n+        this.elements = new ArrayList<>(this.reads.size());\n+        this.elementByIndex = new Int2ObjectOpenHashMap<>(this.reads.size());\n+        this.elementByRead = new HashMap<>(this.reads.size());\n+        this.height = this.reads.size();\n+        bases = new byte[height][width];\n+        quals = new byte[height][width];\n+        for (int i = 0; i < height; i++) {\n+            final IntervalPileup.Element element = new Element(this, this.reads.get(i), i);\n+            elements.add(element);\n+            elementByIndex.put(i, element);\n+            elementByRead.put(this.reads.get(i), element);\n+        }\n+    }\n+\n+    @Override\n+    public List<GATKRead> reads() {\n+        return reads;\n+    }\n+\n+    @Override\n+    public ReferenceBases reference() {\n+        return referenceBases;\n+    }\n+\n+    @Override\n+    public int width() {\n+        return width;\n+    }\n+\n+    @Override\n+    public int height() {\n+        return height;\n+    }\n+\n+    @Override\n+    public byte baseAt(final int row, final int column) {\n+        return bases[row][column];\n+    }\n+\n+    @Override\n+    public byte qualAt(int row, int column) {\n+        return quals[row][column];\n+    }\n+\n+    @Override\n+    public IntervalPileup.Insert insertAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elementByIndex.get(row);\n+        return element != null ? element.insertAt(column) : null;\n+    }\n+\n+    @Override\n+    public GATKRead readAt(int row, int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        if (element.minColumn() > column || element.maxColumn() < column) {\n+            return null;\n+        } else {\n+            return element.read();\n+        }\n+    }\n+\n+    @Override\n+    public List<GATKRead> readsAt(int row, int column) {\n+        final GATKRead read = readAt(row, column);\n+        return read != null ? Collections.singletonList(read) : Collections.emptyList();\n+    }\n+\n+    @Override\n+    public IntervalPileup.Element element(final GATKRead read) {\n+        return elementByRead.get(read);\n+    }\n+\n+    @Override\n+    public boolean hasInsertAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        return element.hasInsertAt(column);\n+    }\n+\n+    @Override\n+    public byte[] insertBasesAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        return element.insertBasesAt(column);\n+    }\n+\n+    @Override\n+    public byte[] insertQualsAt(final int row, final int column) {\n+        final IntervalPileup.Element element = elements.get(row);\n+        return element.insertQualsAt(column);\n+    }\n+\n+    static class Insert implements  IntervalPileup.Insert {\n+        private final GATKRead enclosingRead;\n+        private final int offset;\n+        private final int length;\n+        private final int column;\n+        private transient int hashCode = 0;\n+\n+        Insert(final GATKRead enclosingRead, final int offset, final int column, final int length) {\n+            this.enclosingRead = enclosingRead;\n+            this.offset = offset;\n+            this.length = length;\n+            this.column = column;\n+        }\n+\n+        public int column() {\n+            return column;\n+        }\n+\n+        @Override\n+        public int length() {\n+            return length;\n+        }\n+\n+        @Override\n+        public byte[] bases() {\n+            final byte[] result = new byte[length];\n+            final int copied = enclosingRead.copyBases(offset, result, 0, length);\n+            if (copied < length) {\n+                Arrays.fill(result, copied, result.length, (byte) 'N');\n+            }\n+            return result;\n+        }\n+\n+        @Override\n+        public byte[] quals() {\n+            final byte[] result = new byte[length];\n+            final int copied = enclosingRead.copyBaseQualities(offset, result, 0, length);\n+            if (copied < length) {\n+                Arrays.fill(result, copied, result.length, NO_BQ);\n+            }\n+            return result;\n+        }\n+\n+        @Override\n+        public int copyBases(int offset, byte[] dest, int destOffset, int maxLength) {\n+            final int actualMaxLength = Math.min(length, maxLength);\n+            final int copied = enclosingRead.copyBases(this.offset + offset, dest, destOffset, actualMaxLength);\n+            if (copied < actualMaxLength) {\n+                Arrays.fill(dest, destOffset + copied, destOffset + actualMaxLength, (byte) 'N');\n+            }\n+            return actualMaxLength;\n+        }\n+\n+        @Override\n+        public int copyQuals(int offset, byte[] dest, int destOffset, int maxLength) {\n+            final int actualMaxLength = Math.min(length, maxLength);\n+            final int copied = enclosingRead.copyBaseQualities(this.offset + offset, dest, destOffset, actualMaxLength);\n+            if (copied < actualMaxLength) {\n+                Arrays.fill(dest, destOffset + copied, destOffset + actualMaxLength, (byte) 'N');\n+            }\n+            return actualMaxLength;\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            if (hashCode == 0) {\n+                final byte[] readBases = enclosingRead.getBasesNoCopy();\n+                if (readBases != null && readBases.length > offset) {\n+                    final int to = offset + length;\n+                    final int to2 = to <= readBases.length ? to : readBases.length;\n+                    int i;\n+                    hashCode = 1;\n+                    for (i = offset; i < to2; i++) {\n+                        hashCode = hashCode * 31 + readBases[i];\n+                    }\n+                    for (; i < to; i++) {\n+                        hashCode = hashCode * 31 + 'N';\n+                    }\n+                }\n+            }\n+            return hashCode;\n+        }\n+\n+        @Override\n+        public boolean equals(final Object other) {\n+            return other == this || (other instanceof Insert && equals((IntervalPileup.Insert) other));\n+        }\n+\n+        private boolean equals(final IntervalPileup.Insert other) {\n+            return length == other.length() && hashCode() == other.hashCode() &&\n+                    equalBases(other) && equalQualities(other);\n+        }\n+\n+        private boolean equalBases(final IntervalPileup.Insert other) {\n+            if (other instanceof Insert) {\n+                equalBases((Insert) other);\n+            }\n+            final byte[] otherBases = other.bases();\n+            final byte[] readBases = enclosingRead.getBasesNoCopy();\n+            for (int i = 0; i < otherBases.length; i++) {\n+                if (otherBases[i] != readBases[offset + i]) {\n+                    return false;\n+                }\n+            }\n+            return true;\n+        }\n+\n+        private boolean equalBases(final Insert other) {\n+            final byte[] otherBases = other.enclosingRead.getBasesNoCopy();\n+            final byte[] thisBases = enclosingRead.getBasesNoCopy();\n+            for (int i = 0; i < length; i++) {\n+                if (otherBases[other.offset + i] != thisBases[offset + i]) {\n+                    return false;\n+                }\n+            }\n+            return true;\n+        }\n+\n+        private boolean equalQualities(final IntervalPileup.Insert other) {\n+            if (other instanceof Insert) {\n+                equalQualities((Insert) other);\n+            }\n+            final byte[] otherQuals = other.quals();\n+            final byte[] thisQuals = enclosingRead.getBaseQualitiesNoCopy();\n+            for (int i = 0; i < otherQuals.length; i++) {\n+                if (otherQuals[i] != thisQuals[offset + i]) {\n+                    return false;\n+                }\n+            }\n+            return true;\n+        }\n+\n+        private boolean equalQualities(final Insert other) {\n+            final byte[] otherQuals = other.enclosingRead.getBaseQualitiesNoCopy();\n+            final byte[] thisQuals = enclosingRead.getBaseQualitiesNoCopy();\n+            for (int i = 0; i < length; i++) {\n+                if (otherQuals[other.offset + i] != thisQuals[offset + i]) {\n+                    return false;\n+                }\n+            }\n+            return true;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return new String(bases()) + \"/\" + new String(quals());\n+        }\n+    }\n+\n+    static class Element implements IntervalPileup.Element {\n+\n+        private final GATKRead read;\n+        private final int row;\n+        private final int minColumn;\n+        private final int maxColumn;\n+        private final Int2ObjectMap<Insert> inserts;\n+        private final ByteMapIntervalPileup pileup;\n+\n+        private Element(final ByteMapIntervalPileup pileup, final GATKRead read, final int row) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 291}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc0NjgzNA==", "bodyText": "Put inner classes at the bottom of the class.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447746834", "createdAt": "2020-06-30T14:52:50Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrModelEstimator.java", "diffHunk": "@@ -0,0 +1,416 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.apache.commons.lang.math.IntRange;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+\n+import java.util.ArrayDeque;\n+import java.util.Arrays;\n+import java.util.Deque;\n+\n+\n+public class DragstrModelEstimator {\n+\n+    private static double[][] DEFAULT_GOP = {\n+            {45.00, 45.00, 45.00, 45.00, 45.00, 45.00, 40.50, 33.50, 28.00, 24.00, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75},\n+            {39.50, 39.50, 39.50, 39.50, 36.00, 30.00, 27.25, 25.00, 24.25, 24.75, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.75},\n+            {38.50, 41.00, 41.00, 41.00, 41.00, 37.50, 35.25, 34.75, 34.75, 33.25, 33.25, 33.25, 32.50, 30.75, 28.50, 29.00, 29.00, 29.00, 29.00, 29.00},\n+            {37.50, 39.00, 39.00, 37.75, 34.00, 34.00, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 31.75, 31.75, 31.75, 31.75, 31.75},\n+            {37.00, 40.00, 40.00, 40.00, 36.00, 35.00, 24.50, 24.50, 24.50, 24.50, 22.50, 22.50, 22.50, 23.50, 23.50, 23.50, 23.50, 23.50, 23.50, 23.50},\n+            {36.25, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00},\n+            {36.00, 40.50, 40.50, 40.50, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75},\n+            {36.25, 39.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75}};\n+\n+    private static double[][] DEFAULT_API = {\n+            {39.00, 39.00, 37.00, 35.00, 32.00, 26.00, 20.00, 16.00, 12.00, 10.00, 8.00, 7.00, 7.00, 6.00, 6.00, 5.00, 5.00, 4.00, 4.00, 4.00},\n+            {30.00, 30.00, 29.00, 22.00, 17.00, 14.00, 11.00, 8.00, 6.00, 5.00, 4.00, 4.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00},\n+            {27.00, 27.00, 25.00, 18.00, 14.00, 12.00, 9.00, 7.00, 5.00, 4.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {27.00, 27.00, 18.00, 9.00, 9.00, 9.00, 9.00, 3.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {29.00, 29.00, 18.00, 8.00, 8.00, 8.00, 4.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {25.00, 25.00, 10.00, 10.00, 10.00, 4.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00},\n+            {21.00, 21.00, 11.00, 11.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00},\n+            {18.00, 18.00, 10.00, 6.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00}};\n+\n+    private static final Logger logger = LogManager.getLogger(DragstrModelEstimator.class);\n+\n+    private final double[] phred_gp_range;\n+    private final double[] phred_api_range;\n+    private final double[] log10_gp_range;\n+    private final double[] log10_api_range;\n+    private final double het_hom_ratio;\n+    private final double log10_het_hom_ratio;\n+    private final int min_loci_count;\n+    private static final double LOG_10_OF_2 = Math.log10(2);\n+    private final double api_mono_threshold;\n+    private final double min_gop;\n+    private final double max_gop;\n+    private final int[] min_gp_idx_by_period;\n+    private final double[][][] log10_p_err_by_len;\n+    private final double[][][] log10_p_no_err_by_len;\n+    private final boolean dont_adjust_gop;\n+    private final int minimum_depth = 10;\n+\n+    public DragstrModelEstimator(final DragstrModelEstimatorArgumentCollection argumentCollection) {\n+        phred_gp_range = argumentCollection.phredGpValues.toDoubleArray();\n+        phred_api_range = argumentCollection.phredApiValues.toDoubleArray();\n+        log10_gp_range = MathUtils.applyToArray(phred_gp_range, d -> -.1 * d);\n+        log10_api_range = MathUtils.applyToArray(phred_api_range, d -> -0.1 * d);\n+        het_hom_ratio = argumentCollection.hetToHomRatio;\n+        log10_het_hom_ratio = Math.log10(het_hom_ratio);\n+        min_loci_count = argumentCollection.minLociCount;\n+        api_mono_threshold = argumentCollection.apiMonothresh;\n+        min_gop = argumentCollection.minGOP;\n+        max_gop = argumentCollection.maxGOP;\n+        log10_p_err_by_len = new double[phred_gp_range.length][8][20];\n+        log10_p_no_err_by_len = new double[phred_gp_range.length][8][20];\n+        min_gp_idx_by_period = new int[8];\n+        for (int i = 0; i < phred_gp_range.length; i++) {\n+            for (int k = 0; k < 8; k++) {\n+                final int period = k + 1;\n+                final double log10_p_no_err_per_pos = MathUtils.log10OneMinusPow10(LOG_10_OF_2 + log10_gp_range[i]);\n+                for (int j = 0; j < 20; j++) {\n+                    final int repeats = j + 1;\n+                    final int lengthInBases = repeats * period;\n+                    log10_p_no_err_by_len[i][k][j] = lengthInBases * log10_p_no_err_per_pos;\n+                    log10_p_err_by_len[i][k][j] = MathUtils.log10OneMinusPow10(log10_p_no_err_by_len[i][k][j]);\n+                }\n+            }\n+        }\n+        for (int i = 0; i < min_gp_idx_by_period.length; i++) {\n+            final int period = i + 1;\n+            final double gp_min = Math.ceil(-10 * Math.log10((1 - Math.pow(0.5, (1.0/ (20.0 * period)) / 2.0))));\n+            final int index = Arrays.binarySearch(phred_gp_range, gp_min);\n+            // since we are looking for a double, we have to be a bit tolerant in terms of differences,\n+            // so if no exact was found we look the position before the insertion position in case that value\n+            // is close enough (less than 0.001 way).\n+            min_gp_idx_by_period[i] = index >= 0 ? index :\n+                    (index < -1 && Math.abs(gp_min - phred_api_range[-index - 2]) < 0.001) ? -index - 2 : -index - 1;\n+        }\n+        dont_adjust_gop = argumentCollection.dontPostAdjustmentOfGOP;\n+    }\n+\n+    public Estimate createEstimate(final int maxPeriod, final int maxRepeats) {\n+        return new Estimate(maxPeriod, maxRepeats);\n+    }\n+\n+    public PeriodCases createPeriodCases(final int period, final int maxRepeats, final int totalCases) {\n+        return new PeriodCases(period, maxRepeats, totalCases);\n+    }\n+\n+    public static class PeriodCases {\n+        public final int period;\n+        public final RepeatCases[] repeatCases;\n+\n+        public PeriodCases(final int period, final int maxRepeats, final int casesPerRepeatCapacity) {\n+            this.period = period;\n+            this.repeatCases = new RepeatCases[maxRepeats];\n+            for (int i = 0; i < maxRepeats; i++) {\n+                this.repeatCases[i] = new RepeatCases(period,i + 1, casesPerRepeatCapacity);\n+            }\n+        }\n+\n+        /**\n+         * Returns the {@link RepeatCases} instance that holds the cases for this period an a particular number of\n+         * repeat units. Notice that repeats counts over the maximum are collapsed into that maximum.\n+         * @param repeats\n+         * @return never {@code null}.\n+         * @throws IllegalArgumentException if {@code repeats} is 0 or less.\n+         */\n+        public RepeatCases getRepeatCases(final int repeats) {\n+            if (repeats > repeatCases.length) {\n+                return repeatCases[repeatCases.length - 1];\n+            } else {\n+                return repeatCases[repeats - 1];\n+            }\n+        }\n+\n+        /**\n+         * The maximum number of repeats str.\n+         * @return 1 or greater.\n+         */\n+        public int getMaxRepeats() {\n+            return repeatCases.length;\n+        }\n+\n+        public void removeLowDepth(final int minimum_depth) {\n+            for (final RepeatCases rc : repeatCases) {\n+                rc.removeLowDepth(minimum_depth);\n+            }\n+        }\n+    }\n+\n+    public static class RepeatCases {\n+        final int period;\n+        final int repeat; // number of repeat units in this case.\n+        int[] n; // total number of reads/fragments considered.\n+        int[] k; // total number of reads/fragments that have a non-zero total indel sum within the str.\n+        int size;\n+\n+\n+        public int removeLowDepth(final int minDepth) {\n+            int i, newSize;\n+            for (i = 0, newSize = 0; i < size; newSize++) {\n+                if (n[i++] < minDepth) {\n+                    break;\n+                }\n+            }\n+            for (; i < size; i++) {\n+                if (n[i] >= minDepth) {\n+                   n[newSize] = n[i];\n+                   k[newSize++] = k[i];\n+                }\n+            }\n+            return size = newSize;\n+        }\n+\n+        public int effectiveSampleSize(final int minDepth) {\n+            int i, result;\n+            for (i = 0, result = 0; i < size; result++) {\n+                if (n[i++] < minDepth) {\n+                    break;\n+                }\n+            }\n+            for (; i < size; i++) {\n+                if (n[i] >= minDepth) {\n+                    result++;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public RepeatCases(final int period, final int repeat, final int capacity) {\n+            this.period = period;\n+            this.repeat = repeat;\n+            final int initialCapacity = capacity < 10 ? 10 : capacity;\n+            n = new int[initialCapacity];\n+            k = new int[initialCapacity];\n+            size = 0;\n+        }\n+\n+        public int getPeriod() {\n+            return period;\n+        }\n+\n+        public int getRepeats() {\n+            return repeat;\n+        }\n+\n+        public int size() {\n+            return size;\n+        }\n+\n+        public void add(final int n, final int k) {\n+            if (this.n.length == size) {\n+                this.n = Arrays.copyOf(this.n, size << 1);\n+                this.k = Arrays.copyOf(this.k, this.n.length);\n+            }\n+            this.n[size] = n;\n+            this.k[size++] = k;\n+        }\n+    }\n+\n+    public class Estimate {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc0ODEzMg==", "bodyText": "javadoc", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447748132", "createdAt": "2020-06-30T14:54:23Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrParams.java", "diffHunk": "@@ -0,0 +1,245 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.VariationalAlleleFrequencyCalculator;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+public class DragstrParams {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc0OTY0MA==", "bodyText": "If you care abbout performance for this class (perhaps not?) String.format commands are very slow compared to the alternatives.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447749640", "createdAt": "2020-06-30T14:56:11Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrParams.java", "diffHunk": "@@ -0,0 +1,245 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.VariationalAlleleFrequencyCalculator;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+public class DragstrParams {\n+\n+    private final int maxPeriod;\n+    private final int maxRepeats;\n+    private final double[][] gop;\n+    private final double[][] gcp;\n+    private final double[][] api;\n+    private final Map<Object, VariationalAlleleFrequencyCalculator> afcs;\n+\n+    public DragstrParams(final String path) {\n+        this(openBufferedReader(path), path);\n+    }\n+\n+    private static BufferedReader openBufferedReader(String path) {\n+        try {\n+            return Files.newBufferedReader(Paths.get(path));\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotReadInputFile(path, ex);\n+        }\n+    }\n+\n+    private BufferedWriter openBufferedWriter(final String path) {\n+        try {\n+            return Files.newBufferedWriter(Paths.get(path));\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(path, ex);\n+        }\n+    }\n+\n+    public static DragstrParams fromEstimate(final DragstrModelEstimator.Estimate estimate) {\n+        Utils.nonNull(estimate);\n+        final int maxPeriod = estimate.gp.length;\n+        final int maxRepeats = estimate.gp[0].length;\n+        final double[][] gop = new double[maxPeriod][maxRepeats];\n+        final double[][] gcp = new double[maxPeriod][maxRepeats];\n+        final double[][] api = new double[maxPeriod][maxRepeats];\n+        for (int i = 0; i < maxPeriod; i++) {\n+            for (int j = 0; j < maxRepeats; j++) {\n+                gop[i][j] = estimate.gop(i +1, j + 1);\n+                gcp[i][j] = estimate.gcp(i + 1, j+ 1);\n+                api[i][j] = estimate.api( i + 1, j + 1);\n+            }\n+        }\n+        return new DragstrParams(maxPeriod, maxRepeats, gop, gcp, api);\n+    }\n+\n+    private DragstrParams(final BufferedReader reader, final String path) {\n+        try {\n+            final String header = reader.readLine();\n+            final String[] headerParts = header.split(\"\\\\s+\");\n+            final int[] repeats = Arrays.stream(headerParts)\n+                    .filter(str -> !str.isEmpty())\n+                    .mapToInt(str -> {\n+                        try {\n+                            return Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw new UserException.BadInput(\"bad format for an integer\", ex);\n+                        }\n+                    })\n+                    .toArray();\n+            final int maxRepeats = repeats.length;\n+            for (int i = 0; i < repeats.length; i++) {\n+                if (repeats[i] != i + 1) {\n+                    throw new UserException.BadInput(\"the DRAGstr parameter file header line must contain integers starting at 1 \" + Arrays.toString(repeats));\n+                }\n+            }\n+            final Map<String, double[][]> tables = new HashMap<>();\n+            String line = reader.readLine();\n+            if (line == null) {\n+                throw new UserException.BadInput(\"end of table list before expected\");\n+            }\n+            String tableName = line.replaceAll(\":$\", \"\");\n+            List<String> tableLines = new ArrayList<>();\n+            while ((line = reader.readLine()) != null) {\n+                if (line.charAt(line.length() - 1) == ':') {\n+                    tables.put(tableName, linesToMatrix(tableLines, repeats.length));\n+                    tableName = line.replaceAll(\":$\", \"\");\n+                    tableLines.clear();\n+                } else {\n+                    tableLines.add(line);\n+                }\n+            }\n+            if (tableName == null) {\n+                throw new UserException.BadInput(\"table with no name\");\n+            }\n+            tables.put(tableName, linesToMatrix(tableLines, repeats.length));\n+            final double[][] gopMatrix = mandatoryMatrix(tables, \"GOP\");\n+            final double[][] gcpMatrix = mandatoryMatrix(tables, \"GCP\");\n+            final double[][] apiMatrix = mandatoryMatrix(tables, \"API\");\n+            final int maxPeriod = gopMatrix.length;\n+            checkMatricesAreValid(maxPeriod, maxRepeats, gopMatrix, gcpMatrix, apiMatrix);\n+\n+            this.maxPeriod = maxPeriod;\n+            this.maxRepeats = maxRepeats;\n+            this.gop = gopMatrix;\n+            this.gcp = gcpMatrix;\n+            this.api = apiMatrix;\n+            this.afcs = new HashMap<>(maxPeriod * maxRepeats);\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotReadInputFile(path, ex);\n+        }\n+    }\n+\n+    public void print(final String path) {\n+        try (final BufferedWriter writer = openBufferedWriter(path);\n+             final PrintWriter printWriter = new PrintWriter(writer)) {\n+            final StringBuilder lineBuilder = new StringBuilder(1024);\n+            lineBuilder.append(String.format(\"%5s\", \"1\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1MzUzMQ==", "bodyText": "I'm torn, i think there is some sense in pulling out a DragstrParamsTableCodec for this file type as the majority of the methods in this class are IO methods. I think if you did that you could keep a very lightweight lookup table that is disentangled from its reading/writing given how simple this class is without having to worry about the parsing.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447753531", "createdAt": "2020-06-30T15:01:05Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrParams.java", "diffHunk": "@@ -0,0 +1,245 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.VariationalAlleleFrequencyCalculator;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+public class DragstrParams {\n+\n+    private final int maxPeriod;\n+    private final int maxRepeats;\n+    private final double[][] gop;\n+    private final double[][] gcp;\n+    private final double[][] api;\n+    private final Map<Object, VariationalAlleleFrequencyCalculator> afcs;\n+\n+    public DragstrParams(final String path) {\n+        this(openBufferedReader(path), path);\n+    }\n+\n+    private static BufferedReader openBufferedReader(String path) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1NjQ2Nw==", "bodyText": "rename this \"sequence\" to \"bases\" to make it clearer this is intended to be run on read bases", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447756467", "createdAt": "2020-06-30T15:04:56Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrReadSTRAnalizer.java", "diffHunk": "@@ -0,0 +1,182 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+\n+import java.util.Arrays;\n+\n+public class DragstrReadSTRAnalizer {\n+    private final int[][] repeatsByPeriodAndPosition;\n+    private final int[] periodWithMostRepeats;\n+    private final int maxPeriod;\n+    private int seqLength;\n+\n+    DragstrReadSTRAnalizer(final int maxSequenceLength, final int maxPeriod) {\n+        repeatsByPeriodAndPosition = new int[maxPeriod][maxSequenceLength];\n+        this.maxPeriod = maxPeriod;\n+        this.periodWithMostRepeats = new int[maxSequenceLength];\n+    }\n+\n+    public int numberOfRepeats(final int position, final int period) {\n+        if (period <= 0 || period > maxPeriod) {\n+            return 0;\n+        } else if (position < 0 || position >= seqLength) {\n+            throw new IllegalArgumentException(\"cannot query outside requested boundaries\");\n+        } else {\n+            return repeatsByPeriodAndPosition[period - 1][position];\n+        }\n+    }\n+\n+    public int mostRepeatedPeriod(final int position) {\n+        if (position >= 0 && position < seqLength) {\n+            return periodWithMostRepeats[position];\n+        } else {\n+            throw new IllegalArgumentException(\"cannot query outside requested boundaries\");\n+        }\n+    }\n+\n+    public int numberOfMostRepeats(final int position) {\n+        if (position >= 0 && position < seqLength) {\n+            return repeatsByPeriodAndPosition[periodWithMostRepeats[position] - 1][position];\n+        } else {\n+            throw new IllegalArgumentException(\"cannot query outside requested boundaries\");\n+        }\n+    }\n+\n+    public void load(final byte[] sequence) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1Nzc3Ng==", "bodyText": "I think you should add a javadoc explainaing what exactly is in the tables that you pull here. A concise description somewhere that explains how the forwards and reverse looking works and what exactly \"numberOfMostRepeats()\" is for a given position would be very helpful i think.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447757776", "createdAt": "2020-06-30T15:06:34Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrReadSTRAnalizer.java", "diffHunk": "@@ -0,0 +1,182 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+\n+import java.util.Arrays;\n+\n+public class DragstrReadSTRAnalizer {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1OTQzOA==", "bodyText": "rename to \"annotateVariantContextWithDragstrParametersUsed\"", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447759438", "createdAt": "2020-06-30T15:08:54Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrUtils.java", "diffHunk": "@@ -0,0 +1,41 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.variantcontext.VariantContextBuilder;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFHeaderLineType;\n+import htsjdk.variant.vcf.VCFInfoHeaderLine;\n+import org.aeonbits.owner.util.Collections;\n+\n+import java.util.Collection;\n+\n+public class DragstrUtils {\n+\n+    public static DragstrReadSTRAnalizer repeatPeriodAndCounts(final int maxSequenceLength, final int maxPeriod) {\n+        return new DragstrReadSTRAnalizer(maxSequenceLength, maxPeriod);\n+    }\n+\n+    public static DragstrReadSTRAnalizer repeatPeriodAndCounts(final byte[] sequence, final int maxPeriod, final boolean considerUpstream) {\n+        final DragstrReadSTRAnalizer result = new DragstrReadSTRAnalizer(sequence.length, maxPeriod);\n+        result.load(sequence);\n+        return result;\n+    }\n+\n+\n+    public static Collection<? extends VCFHeaderLine> vcfHeaderLines() {\n+        return Collections.list(new VCFInfoHeaderLine(DragstrConstants.DRAGSTRINFO_KEY, 2, VCFHeaderLineType.Integer, \"Indicates the period and repeat count\"),\n+                                new VCFInfoHeaderLine(DragstrConstants.DRAGSTRPARAMS_KEY, 3, VCFHeaderLineType.Float, \"Parameeters used (GOP, GCP, API)\"));\n+\n+    }\n+\n+    public static VariantContext annotate(VariantContext annotatedCall, final DragstrParams dragstrParams, final int period, final int repeats) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc2MDAyMg==", "bodyText": "This should also make explicit that this is immutable", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447760022", "createdAt": "2020-06-30T15:09:40Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrParams.java", "diffHunk": "@@ -0,0 +1,245 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.VariationalAlleleFrequencyCalculator;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+public class DragstrParams {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc0ODEzMg=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc2MDc3Mw==", "bodyText": "I think this would be a good place to put a description of the GOP/GCP/API tables and what they contain.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447760773", "createdAt": "2020-06-30T15:10:36Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrParams.java", "diffHunk": "@@ -0,0 +1,245 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.VariationalAlleleFrequencyCalculator;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+public class DragstrParams {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc0ODEzMg=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc2MjExOA==", "bodyText": "doc", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447762118", "createdAt": "2020-06-30T15:12:29Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrModelEstimator.java", "diffHunk": "@@ -0,0 +1,416 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.apache.commons.lang.math.IntRange;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+\n+import java.util.ArrayDeque;\n+import java.util.Arrays;\n+import java.util.Deque;\n+\n+\n+public class DragstrModelEstimator {\n+\n+    private static double[][] DEFAULT_GOP = {\n+            {45.00, 45.00, 45.00, 45.00, 45.00, 45.00, 40.50, 33.50, 28.00, 24.00, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75},\n+            {39.50, 39.50, 39.50, 39.50, 36.00, 30.00, 27.25, 25.00, 24.25, 24.75, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.75},\n+            {38.50, 41.00, 41.00, 41.00, 41.00, 37.50, 35.25, 34.75, 34.75, 33.25, 33.25, 33.25, 32.50, 30.75, 28.50, 29.00, 29.00, 29.00, 29.00, 29.00},\n+            {37.50, 39.00, 39.00, 37.75, 34.00, 34.00, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 31.75, 31.75, 31.75, 31.75, 31.75},\n+            {37.00, 40.00, 40.00, 40.00, 36.00, 35.00, 24.50, 24.50, 24.50, 24.50, 22.50, 22.50, 22.50, 23.50, 23.50, 23.50, 23.50, 23.50, 23.50, 23.50},\n+            {36.25, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00},\n+            {36.00, 40.50, 40.50, 40.50, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75},\n+            {36.25, 39.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75}};\n+\n+    private static double[][] DEFAULT_API = {\n+            {39.00, 39.00, 37.00, 35.00, 32.00, 26.00, 20.00, 16.00, 12.00, 10.00, 8.00, 7.00, 7.00, 6.00, 6.00, 5.00, 5.00, 4.00, 4.00, 4.00},\n+            {30.00, 30.00, 29.00, 22.00, 17.00, 14.00, 11.00, 8.00, 6.00, 5.00, 4.00, 4.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00},\n+            {27.00, 27.00, 25.00, 18.00, 14.00, 12.00, 9.00, 7.00, 5.00, 4.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {27.00, 27.00, 18.00, 9.00, 9.00, 9.00, 9.00, 3.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {29.00, 29.00, 18.00, 8.00, 8.00, 8.00, 4.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {25.00, 25.00, 10.00, 10.00, 10.00, 4.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00},\n+            {21.00, 21.00, 11.00, 11.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00},\n+            {18.00, 18.00, 10.00, 6.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00}};\n+\n+    private static final Logger logger = LogManager.getLogger(DragstrModelEstimator.class);\n+\n+    private final double[] phred_gp_range;\n+    private final double[] phred_api_range;\n+    private final double[] log10_gp_range;\n+    private final double[] log10_api_range;\n+    private final double het_hom_ratio;\n+    private final double log10_het_hom_ratio;\n+    private final int min_loci_count;\n+    private static final double LOG_10_OF_2 = Math.log10(2);\n+    private final double api_mono_threshold;\n+    private final double min_gop;\n+    private final double max_gop;\n+    private final int[] min_gp_idx_by_period;\n+    private final double[][][] log10_p_err_by_len;\n+    private final double[][][] log10_p_no_err_by_len;\n+    private final boolean dont_adjust_gop;\n+    private final int minimum_depth = 10;\n+\n+    public DragstrModelEstimator(final DragstrModelEstimatorArgumentCollection argumentCollection) {\n+        phred_gp_range = argumentCollection.phredGpValues.toDoubleArray();\n+        phred_api_range = argumentCollection.phredApiValues.toDoubleArray();\n+        log10_gp_range = MathUtils.applyToArray(phred_gp_range, d -> -.1 * d);\n+        log10_api_range = MathUtils.applyToArray(phred_api_range, d -> -0.1 * d);\n+        het_hom_ratio = argumentCollection.hetToHomRatio;\n+        log10_het_hom_ratio = Math.log10(het_hom_ratio);\n+        min_loci_count = argumentCollection.minLociCount;\n+        api_mono_threshold = argumentCollection.apiMonothresh;\n+        min_gop = argumentCollection.minGOP;\n+        max_gop = argumentCollection.maxGOP;\n+        log10_p_err_by_len = new double[phred_gp_range.length][8][20];\n+        log10_p_no_err_by_len = new double[phred_gp_range.length][8][20];\n+        min_gp_idx_by_period = new int[8];\n+        for (int i = 0; i < phred_gp_range.length; i++) {\n+            for (int k = 0; k < 8; k++) {\n+                final int period = k + 1;\n+                final double log10_p_no_err_per_pos = MathUtils.log10OneMinusPow10(LOG_10_OF_2 + log10_gp_range[i]);\n+                for (int j = 0; j < 20; j++) {\n+                    final int repeats = j + 1;\n+                    final int lengthInBases = repeats * period;\n+                    log10_p_no_err_by_len[i][k][j] = lengthInBases * log10_p_no_err_per_pos;\n+                    log10_p_err_by_len[i][k][j] = MathUtils.log10OneMinusPow10(log10_p_no_err_by_len[i][k][j]);\n+                }\n+            }\n+        }\n+        for (int i = 0; i < min_gp_idx_by_period.length; i++) {\n+            final int period = i + 1;\n+            final double gp_min = Math.ceil(-10 * Math.log10((1 - Math.pow(0.5, (1.0/ (20.0 * period)) / 2.0))));\n+            final int index = Arrays.binarySearch(phred_gp_range, gp_min);\n+            // since we are looking for a double, we have to be a bit tolerant in terms of differences,\n+            // so if no exact was found we look the position before the insertion position in case that value\n+            // is close enough (less than 0.001 way).\n+            min_gp_idx_by_period[i] = index >= 0 ? index :\n+                    (index < -1 && Math.abs(gp_min - phred_api_range[-index - 2]) < 0.001) ? -index - 2 : -index - 1;\n+        }\n+        dont_adjust_gop = argumentCollection.dontPostAdjustmentOfGOP;\n+    }\n+\n+    public Estimate createEstimate(final int maxPeriod, final int maxRepeats) {\n+        return new Estimate(maxPeriod, maxRepeats);\n+    }\n+\n+    public PeriodCases createPeriodCases(final int period, final int maxRepeats, final int totalCases) {\n+        return new PeriodCases(period, maxRepeats, totalCases);\n+    }\n+\n+    public static class PeriodCases {\n+        public final int period;\n+        public final RepeatCases[] repeatCases;\n+\n+        public PeriodCases(final int period, final int maxRepeats, final int casesPerRepeatCapacity) {\n+            this.period = period;\n+            this.repeatCases = new RepeatCases[maxRepeats];\n+            for (int i = 0; i < maxRepeats; i++) {\n+                this.repeatCases[i] = new RepeatCases(period,i + 1, casesPerRepeatCapacity);\n+            }\n+        }\n+\n+        /**\n+         * Returns the {@link RepeatCases} instance that holds the cases for this period an a particular number of\n+         * repeat units. Notice that repeats counts over the maximum are collapsed into that maximum.\n+         * @param repeats\n+         * @return never {@code null}.\n+         * @throws IllegalArgumentException if {@code repeats} is 0 or less.\n+         */\n+        public RepeatCases getRepeatCases(final int repeats) {\n+            if (repeats > repeatCases.length) {\n+                return repeatCases[repeatCases.length - 1];\n+            } else {\n+                return repeatCases[repeats - 1];\n+            }\n+        }\n+\n+        /**\n+         * The maximum number of repeats str.\n+         * @return 1 or greater.\n+         */\n+        public int getMaxRepeats() {\n+            return repeatCases.length;\n+        }\n+\n+        public void removeLowDepth(final int minimum_depth) {\n+            for (final RepeatCases rc : repeatCases) {\n+                rc.removeLowDepth(minimum_depth);\n+            }\n+        }\n+    }\n+\n+    public static class RepeatCases {\n+        final int period;\n+        final int repeat; // number of repeat units in this case.\n+        int[] n; // total number of reads/fragments considered.\n+        int[] k; // total number of reads/fragments that have a non-zero total indel sum within the str.\n+        int size;\n+\n+\n+        public int removeLowDepth(final int minDepth) {\n+            int i, newSize;\n+            for (i = 0, newSize = 0; i < size; newSize++) {\n+                if (n[i++] < minDepth) {\n+                    break;\n+                }\n+            }\n+            for (; i < size; i++) {\n+                if (n[i] >= minDepth) {\n+                   n[newSize] = n[i];\n+                   k[newSize++] = k[i];\n+                }\n+            }\n+            return size = newSize;\n+        }\n+\n+        public int effectiveSampleSize(final int minDepth) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc2MjU4OQ==", "bodyText": "You should add an argument for the sample size rather than hard coding it here.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447762589", "createdAt": "2020-06-30T15:13:08Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/EstimateDragstrModelParameters.java", "diffHunk": "@@ -0,0 +1,158 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.*;\n+import htsjdk.samtools.util.BufferedLineReader;\n+import htsjdk.samtools.util.LineReader;\n+import org.apache.commons.collections4.EnumerationUtils;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+\n+import static org.broadinstitute.hellbender.utils.pairhmm.DragstrConstants.*;\n+\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.ZipEntry;\n+import java.util.zip.ZipFile;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class EstimateDragstrModelParameters extends GATKTool {\n+\n+    @ArgumentCollection\n+    private final DragstrCasesSamplerArgumentCollection dragstrCasesSamplerArgumentCollection = new DragstrCasesSamplerArgumentCollection();\n+    private Pattern ZIP_ENTRY_NAME_REGEXP = Pattern.compile(\"^(\\\\d+)/(\\\\d+).bin$\");\n+\n+    @ArgumentCollection\n+    public DragstrModelEstimatorArgumentCollection dragstrModelEstimatorArgumentCollection = new DragstrModelEstimatorArgumentCollection();\n+\n+    @Argument(doc = \"location of the .zip file containing the locations to sample from. This zip file is to be generated using SampleSitesForDRAGstrModel tool \",\n+              fullName = SAMPLING_LOCI_ARGUMENT_FULL_NAME)\n+    private String samplingLoci = null;\n+\n+    @Argument(fullName= MAX_PERIOD_ARGUMENT_FULL_NAME,\n+            doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = DEFAULT_MAX_PERIOD;\n+\n+    @Argument(fullName= MAX_REPEATS_ARGUMENT_FULL_NAME,\n+            doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = DEFAULT_MAX_REPEATS;\n+\n+\n+    @Argument(fullName = StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+              doc = \"name of the output parameter file\")\n+    private String outputPath;\n+\n+    @Override\n+    public boolean requiresReads() {\n+        return true;\n+    }\n+\n+\n+\n+    @Override\n+    public void traverse() {\n+        final DragstrModelEstimator estimator = new DragstrModelEstimator(dragstrModelEstimatorArgumentCollection);\n+        final DragstrCasesSampler sampler = new DragstrCasesSampler(dragstrCasesSamplerArgumentCollection, directlyAccessEngineReferenceDataSource(), directlyAccessEngineReadsDataSource());\n+        final DragstrModelEstimator.Estimate estimate = estimator.createEstimate(maxPeriod, maxRepeat);\n+        try (final ZipFile zipIn = stageZipInput()) {\n+            checkSameSampleDictionary(zipIn);\n+            final List<ZipEntry> zipEntries = EnumerationUtils.toList(zipIn.entries());\n+            for (int period = 1; period <= maxPeriod; period++) {\n+                final DragstrModelEstimator.PeriodCases periodCases = estimator.createPeriodCases(period, maxRepeat, 10000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc2MzE0Mw==", "bodyText": "typo", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447763143", "createdAt": "2020-06-30T15:13:53Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/EstimateDragstrModelParameters.java", "diffHunk": "@@ -0,0 +1,158 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.*;\n+import htsjdk.samtools.util.BufferedLineReader;\n+import htsjdk.samtools.util.LineReader;\n+import org.apache.commons.collections4.EnumerationUtils;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+\n+import static org.broadinstitute.hellbender.utils.pairhmm.DragstrConstants.*;\n+\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.ZipEntry;\n+import java.util.zip.ZipFile;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class EstimateDragstrModelParameters extends GATKTool {\n+\n+    @ArgumentCollection\n+    private final DragstrCasesSamplerArgumentCollection dragstrCasesSamplerArgumentCollection = new DragstrCasesSamplerArgumentCollection();\n+    private Pattern ZIP_ENTRY_NAME_REGEXP = Pattern.compile(\"^(\\\\d+)/(\\\\d+).bin$\");\n+\n+    @ArgumentCollection\n+    public DragstrModelEstimatorArgumentCollection dragstrModelEstimatorArgumentCollection = new DragstrModelEstimatorArgumentCollection();\n+\n+    @Argument(doc = \"location of the .zip file containing the locations to sample from. This zip file is to be generated using SampleSitesForDRAGstrModel tool \",\n+              fullName = SAMPLING_LOCI_ARGUMENT_FULL_NAME)\n+    private String samplingLoci = null;\n+\n+    @Argument(fullName= MAX_PERIOD_ARGUMENT_FULL_NAME,\n+            doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = DEFAULT_MAX_PERIOD;\n+\n+    @Argument(fullName= MAX_REPEATS_ARGUMENT_FULL_NAME,\n+            doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = DEFAULT_MAX_REPEATS;\n+\n+\n+    @Argument(fullName = StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+              doc = \"name of the output parameter file\")\n+    private String outputPath;\n+\n+    @Override\n+    public boolean requiresReads() {\n+        return true;\n+    }\n+\n+\n+\n+    @Override\n+    public void traverse() {\n+        final DragstrModelEstimator estimator = new DragstrModelEstimator(dragstrModelEstimatorArgumentCollection);\n+        final DragstrCasesSampler sampler = new DragstrCasesSampler(dragstrCasesSamplerArgumentCollection, directlyAccessEngineReferenceDataSource(), directlyAccessEngineReadsDataSource());\n+        final DragstrModelEstimator.Estimate estimate = estimator.createEstimate(maxPeriod, maxRepeat);\n+        try (final ZipFile zipIn = stageZipInput()) {\n+            checkSameSampleDictionary(zipIn);\n+            final List<ZipEntry> zipEntries = EnumerationUtils.toList(zipIn.entries());\n+            for (int period = 1; period <= maxPeriod; period++) {\n+                final DragstrModelEstimator.PeriodCases periodCases = estimator.createPeriodCases(period, maxRepeat, 10000);\n+                final String periodEntryPrefix = \"\" + period + \"/\";\n+                for (final ZipEntry entry : zipEntries) {\n+                    final String entryName = entry.getName();\n+                    if (entryName.startsWith(periodEntryPrefix)) {\n+                        final Matcher matcher = ZIP_ENTRY_NAME_REGEXP.matcher(entryName);\n+                        if (matcher.matches()) {\n+                            final int repeat = Integer.parseInt(matcher.group(2));\n+                            try (final BinaryTableReader<DragstrLocus> locusReader = DragstrLocus.binaryReader(zipIn.getInputStream(entry))) {\n+                                final DragstrModelEstimator.RepeatCases repeatCases = periodCases.getRepeatCases(repeat);\n+                                final List<DragstrLocus> loci = locusReader.readAll();\n+                                sampler.sample(repeatCases, loci);\n+                            } catch (final IOException ex) {\n+                                throw new UserException.CouldNotReadInputFile(samplingLoci, \"could not read entry \" + entryName + \" from \" + samplingLoci, ex);\n+                            }\n+                        }\n+                    }\n+                }\n+                logger.info(\"Estimating gap penalty and het prior for period \" + period);\n+                final long start = System.currentTimeMillis();\n+                estimator.estimate(estimate, periodCases);\n+                logger.info(\"Estimate done in \" + ((System.currentTimeMillis() - start) / 1000) + \" seconds\");\n+                logger.debug(\"GP: \" + Arrays.toString(estimate.gp[period - 1]));\n+                logger.debug(\"Pr: \" + Arrays.toString(estimate.ph_het_variant[period - 1]));\n+            }\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotReadInputFile(samplingLoci, \"could not open zip file \" + samplingLoci, ex);\n+        }\n+        final DragstrParams params = DragstrParams.fromEstimate(estimate);\n+        params.print(outputPath);\n+    }\n+\n+    private void checkSameSampleDictionary(final ZipFile zipIn) {\n+        final ZipEntry zipEntry = zipIn.getEntry(\"reference.dict\");\n+        if (zipEntry != null) {\n+            try (final LineReader reader = new BufferedLineReader(zipIn.getInputStream(zipEntry))) {\n+                final SAMTextHeaderCodec codec = new SAMTextHeaderCodec();\n+                final SAMSequenceDictionary zipDictionary = codec.decode(reader, zipIn.getName() + \"/reference.dict\").getSequenceDictionary();\n+                final SAMSequenceDictionary currentDictionary = getBestAvailableSequenceDictionary();\n+                if (!areThemCompatibleDictionaries(zipDictionary, currentDictionary)) {\n+                   throw new UserException(\"the input reference's dictionary does not match the loci zip dictionary\");\n+                };\n+                logger.info(\"Loci zip file reference dictionary check was successful\");\n+            } catch (final IOException ex) {\n+                throw new GATKException(\"could not open the reference dictionary inside the input loci zip \", ex);\n+            }\n+        } else {\n+            logger.warn(\"Missing reference dicitonary input sites zip file, we will proceed without checkin that\" +\n+                    \" that the reference provided is the same that was used to create this loci zip\");\n+        }\n+    }\n+\n+    private boolean areThemCompatibleDictionaries(final SAMSequenceDictionary a, final SAMSequenceDictionary b) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc2MzY4NQ==", "bodyText": "use DictionaryUtils.validateDictionaries(). for this operation", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447763685", "createdAt": "2020-06-30T15:14:39Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/EstimateDragstrModelParameters.java", "diffHunk": "@@ -0,0 +1,158 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.*;\n+import htsjdk.samtools.util.BufferedLineReader;\n+import htsjdk.samtools.util.LineReader;\n+import org.apache.commons.collections4.EnumerationUtils;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+\n+import static org.broadinstitute.hellbender.utils.pairhmm.DragstrConstants.*;\n+\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.ZipEntry;\n+import java.util.zip.ZipFile;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class EstimateDragstrModelParameters extends GATKTool {\n+\n+    @ArgumentCollection\n+    private final DragstrCasesSamplerArgumentCollection dragstrCasesSamplerArgumentCollection = new DragstrCasesSamplerArgumentCollection();\n+    private Pattern ZIP_ENTRY_NAME_REGEXP = Pattern.compile(\"^(\\\\d+)/(\\\\d+).bin$\");\n+\n+    @ArgumentCollection\n+    public DragstrModelEstimatorArgumentCollection dragstrModelEstimatorArgumentCollection = new DragstrModelEstimatorArgumentCollection();\n+\n+    @Argument(doc = \"location of the .zip file containing the locations to sample from. This zip file is to be generated using SampleSitesForDRAGstrModel tool \",\n+              fullName = SAMPLING_LOCI_ARGUMENT_FULL_NAME)\n+    private String samplingLoci = null;\n+\n+    @Argument(fullName= MAX_PERIOD_ARGUMENT_FULL_NAME,\n+            doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = DEFAULT_MAX_PERIOD;\n+\n+    @Argument(fullName= MAX_REPEATS_ARGUMENT_FULL_NAME,\n+            doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = DEFAULT_MAX_REPEATS;\n+\n+\n+    @Argument(fullName = StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+              doc = \"name of the output parameter file\")\n+    private String outputPath;\n+\n+    @Override\n+    public boolean requiresReads() {\n+        return true;\n+    }\n+\n+\n+\n+    @Override\n+    public void traverse() {\n+        final DragstrModelEstimator estimator = new DragstrModelEstimator(dragstrModelEstimatorArgumentCollection);\n+        final DragstrCasesSampler sampler = new DragstrCasesSampler(dragstrCasesSamplerArgumentCollection, directlyAccessEngineReferenceDataSource(), directlyAccessEngineReadsDataSource());\n+        final DragstrModelEstimator.Estimate estimate = estimator.createEstimate(maxPeriod, maxRepeat);\n+        try (final ZipFile zipIn = stageZipInput()) {\n+            checkSameSampleDictionary(zipIn);\n+            final List<ZipEntry> zipEntries = EnumerationUtils.toList(zipIn.entries());\n+            for (int period = 1; period <= maxPeriod; period++) {\n+                final DragstrModelEstimator.PeriodCases periodCases = estimator.createPeriodCases(period, maxRepeat, 10000);\n+                final String periodEntryPrefix = \"\" + period + \"/\";\n+                for (final ZipEntry entry : zipEntries) {\n+                    final String entryName = entry.getName();\n+                    if (entryName.startsWith(periodEntryPrefix)) {\n+                        final Matcher matcher = ZIP_ENTRY_NAME_REGEXP.matcher(entryName);\n+                        if (matcher.matches()) {\n+                            final int repeat = Integer.parseInt(matcher.group(2));\n+                            try (final BinaryTableReader<DragstrLocus> locusReader = DragstrLocus.binaryReader(zipIn.getInputStream(entry))) {\n+                                final DragstrModelEstimator.RepeatCases repeatCases = periodCases.getRepeatCases(repeat);\n+                                final List<DragstrLocus> loci = locusReader.readAll();\n+                                sampler.sample(repeatCases, loci);\n+                            } catch (final IOException ex) {\n+                                throw new UserException.CouldNotReadInputFile(samplingLoci, \"could not read entry \" + entryName + \" from \" + samplingLoci, ex);\n+                            }\n+                        }\n+                    }\n+                }\n+                logger.info(\"Estimating gap penalty and het prior for period \" + period);\n+                final long start = System.currentTimeMillis();\n+                estimator.estimate(estimate, periodCases);\n+                logger.info(\"Estimate done in \" + ((System.currentTimeMillis() - start) / 1000) + \" seconds\");\n+                logger.debug(\"GP: \" + Arrays.toString(estimate.gp[period - 1]));\n+                logger.debug(\"Pr: \" + Arrays.toString(estimate.ph_het_variant[period - 1]));\n+            }\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotReadInputFile(samplingLoci, \"could not open zip file \" + samplingLoci, ex);\n+        }\n+        final DragstrParams params = DragstrParams.fromEstimate(estimate);\n+        params.print(outputPath);\n+    }\n+\n+    private void checkSameSampleDictionary(final ZipFile zipIn) {\n+        final ZipEntry zipEntry = zipIn.getEntry(\"reference.dict\");\n+        if (zipEntry != null) {\n+            try (final LineReader reader = new BufferedLineReader(zipIn.getInputStream(zipEntry))) {\n+                final SAMTextHeaderCodec codec = new SAMTextHeaderCodec();\n+                final SAMSequenceDictionary zipDictionary = codec.decode(reader, zipIn.getName() + \"/reference.dict\").getSequenceDictionary();\n+                final SAMSequenceDictionary currentDictionary = getBestAvailableSequenceDictionary();\n+                if (!areThemCompatibleDictionaries(zipDictionary, currentDictionary)) {\n+                   throw new UserException(\"the input reference's dictionary does not match the loci zip dictionary\");\n+                };\n+                logger.info(\"Loci zip file reference dictionary check was successful\");\n+            } catch (final IOException ex) {\n+                throw new GATKException(\"could not open the reference dictionary inside the input loci zip \", ex);\n+            }\n+        } else {\n+            logger.warn(\"Missing reference dicitonary input sites zip file, we will proceed without checkin that\" +\n+                    \" that the reference provided is the same that was used to create this loci zip\");\n+        }\n+    }\n+\n+    private boolean areThemCompatibleDictionaries(final SAMSequenceDictionary a, final SAMSequenceDictionary b) {\n+        if (a == b) {\n+            return true;\n+        } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc2Mzk4OQ==", "bodyText": "javadoc", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447763989", "createdAt": "2020-06-30T15:15:02Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/LazyLoadingReferenceNucleotideSequence.java", "diffHunk": "@@ -0,0 +1,79 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceRecord;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.util.concurrent.*;\n+\n+public class LazyLoadingReferenceNucleotideSequence implements NucleotideSequence {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc3NTY2Ng==", "bodyText": "Looking at the places that you use this class (just in traverse() of SampleSitesForDRAGstrModel I don't think you are actually gaining very much by making this a lazy loader that sits on top of the reference datasource. As its written you are almost never saving yourself the loads. Indeed it looks like for each site you are instantiating a new one of these objects and immediately filling the first 10000 bases from the reference data source. There is already a cache in the reference data source that is designed to handle exactly this use case (traversing over the whole reference and grabbing bases ~in order).", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447775666", "createdAt": "2020-06-30T15:30:06Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/LazyLoadingReferenceNucleotideSequence.java", "diffHunk": "@@ -0,0 +1,79 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceRecord;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.util.concurrent.*;\n+\n+public class LazyLoadingReferenceNucleotideSequence implements NucleotideSequence {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc2Mzk4OQ=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc3NjYxNw==", "bodyText": "Rename the second traverse to something more descriptive.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447776617", "createdAt": "2020-06-30T15:31:22Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private DecimationTable decimationTable = DecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(fullName=\"max-period\", doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = 8;\n+\n+    @Argument(fullName=\"max-repeats\", doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = 20;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private File tempDir;\n+\n+    private long[][] totalCounts;\n+\n+    private long[][] emittedCounts;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+        totalCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+        emittedCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+    }\n+\n+    public void onShutdown() {\n+        Utils.deleteFileTree(tempDir);\n+        super.onShutdown();\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        final SAMSequenceDictionary dictionary = Utils.nonNull(getReferenceDictionary());\n+        final File tempDir = this.tempDir;\n+        try (final AutoCloseableList<BinaryTableWriter<DragstrLocus>> allSitesWriter\n+                     = AutoCloseableList.of(maxPeriod, i -> createDragstrLocusWriter(tempDir, \"period_\" + (i + 1) + \".bin\"))) {\n+            if (!intervalArgumentCollection.intervalsSpecified()) {\n+                for (final SAMSequenceRecord sequence : dictionary.getSequences()) {\n+                    traverse(sequence.getSequenceIndex(), sequence, 1, sequence.getSequenceLength(), allSitesWriter, decimationTable);\n+                }\n+            } else {\n+                for (final SimpleInterval interval : intervalArgumentCollection.getIntervals(dictionary)) {\n+                    final SAMSequenceRecord sequence = dictionary.getSequence(interval.getContig());\n+                    traverse(sequence.getSequenceIndex(), sequence, interval.getStart(), interval.getEnd(), allSitesWriter, decimationTable);\n+                }\n+            }\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(tempDir, ex);\n+        }\n+        progressMeter.stop();\n+        logger.info(\"Finishing reference sampling. Proceeding to splitting each period case by repeat count\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            progressMeter = new ProgressMeter(secondsBetweenProgressUpdates);\n+            progressMeter.setRecordLabel(\"Splitting cases with period \" + i + \" by repeat count\");\n+            progressMeter.start();\n+            splitPeriodLociByRepeat(i);\n+            progressMeter.stop();\n+            logger.info(\"Done with period \" + i);\n+        }\n+        logger.info(\"Composing output zip\");\n+        composeOutputZip();\n+    }\n+\n+    private void splitPeriodLociByRepeat(final int period) {\n+        final File lociFile = new File(tempDir, \"period_\" + period + \".bin\");\n+        final File periodDir = new File(tempDir, \"\" + period);\n+        if (!periodDir.mkdir()) {\n+            throw new UserException.CouldNotCreateOutputFile(periodDir, \"create period loci split directory\");\n+        }\n+        try (final BinaryTableReader<DragstrLocus> reader = DragstrLocus.binaryReader(new FileInputStream(lociFile));\n+             final AutoCloseableList<BinaryTableWriter<DragstrLocus>> writers =\n+                     AutoCloseableList.of(maxRepeat, i -> createDragstrLocusWriter(periodDir, \"\" + (i+1) + \".bin\"))) {\n+            while (reader.hasNext()) {\n+                final DragstrLocus next = reader.next();\n+                final int effectiveRepeats = next.getRepeats() > maxRepeat ? maxRepeat : next.getRepeats();\n+                progressMeter.update(next.getStartInterval(getReferenceDictionary(), 0));\n+                final BinaryTableWriter<DragstrLocus> writer = writers.get(effectiveRepeats - 1);\n+                writer.write(next);\n+\n+            }\n+        } catch (final Exception ex) {\n+            throw new UserException.CouldNotCreateOutputFile(lociFile, \"temporary period loci splitting failed\");\n+        }\n+        if (!lociFile.delete()) {\n+            throw new GATKException(\"could not delete temporary file \" + lociFile);\n+        }\n+    }\n+\n+    private void composeOutputZip() {\n+        final byte[] buffer = new byte[1 << 16];\n+        try (final ZipArchiveOutputStream output = new JarArchiveOutputStream(BucketUtils.createFile(outputPath))) {\n+            composeSummaryText(output);\n+            saveReferenceDictionary(output);\n+            composeLociFiles(buffer, output);\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(outputPath, \"error composing the zip file\", ex);\n+        }\n+    }\n+\n+    private void composeLociFiles(byte[] buffer, ZipArchiveOutputStream output) throws IOException {\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            for (int j = 1; j <= maxRepeat; j++) {\n+                final File inFile = new File(new File(tempDir, \"\" + i), \"\" + j + \".bin\");\n+                output.putArchiveEntry(new ZipArchiveEntry(\"\" + i + \"/\" + j + \".bin\"));\n+                final InputStream inStream = new FileInputStream(inFile);\n+                int len;\n+                while ((len = inStream.read(buffer)) > 0) {\n+                    output.write(buffer, 0, len);\n+                }\n+                inStream.close();\n+                output.closeArchiveEntry();\n+            }\n+        }\n+    }\n+\n+    private void saveReferenceDictionary(final ZipArchiveOutputStream output) throws IOException {\n+        final SAMSequenceDictionary dictionary = getBestAvailableSequenceDictionary();\n+        output.putArchiveEntry(new ZipArchiveEntry(\"reference.dict\"));\n+        // need to prevent closing output if the writer's close is called. We simply\n+        // flush instead:\n+        try (final Writer writer = new OutputStreamWriter(output) {\n+                public void close() throws IOException {\n+                    flush();\n+                }\n+        }) {\n+            final SAMSequenceDictionaryCodec codec = new SAMSequenceDictionaryCodec(writer);\n+            codec.encode(dictionary);\n+        }\n+        output.closeArchiveEntry();\n+    }\n+\n+    private void composeSummaryText(ZipArchiveOutputStream output) throws IOException {\n+        output.putArchiveEntry(new ZipArchiveEntry(\"summary.txt\"));\n+        final PrintWriter writer = new PrintWriter(new OutputStreamWriter(output));\n+        writer.println(\"period\\trepeat\\ttotal\\temmitted\\tdecimation\\tactual_decimation\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            for (int j = 1; j <= maxRepeat; j++) {\n+                writer.println(String.format(\"%d\\t%d\\t%d\\t%d\\t%.2f\\t%.2f\",\n+                        i, j, totalCounts[i][j], emittedCounts[i][j],\n+                        Math.log(decimationTable.decimationMask(i,j) + 1) / Math.log(2),\n+                        (- Math.log(emittedCounts[i][j]) + Math.log(totalCounts[i][j])) / Math.log(2) ));\n+            }\n+        }\n+        writer.flush();\n+        output.closeArchiveEntry();\n+    }\n+\n+    private static BinaryTableWriter<DragstrLocus> createDragstrLocusWriter(final File outDir, final String name) {\n+        try {\n+            final File outFile = new File(outDir, name);\n+            return DragstrLocus.binaryWriter(outFile);\n+        } catch (final IOException  e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    private void traverse(final int seqNumber, final SAMSequenceRecord sequence, final long seqStart, final long seqEnd, final AutoCloseableList<BinaryTableWriter<DragstrLocus>> output, final DecimationTable decimationTable)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MzI3Mg=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 378}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc3ODI5Nw==", "bodyText": "You should look at the ReferenceWalker class. It seems that the machinery in that class handles most of the traversal/caching code that you have implemented here and this class fundamentally coudl be implemented as simply a walker over the refeference without too much issue. That would help cut down on code duplication and help cleanup this class to just represent the important parts.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447778297", "createdAt": "2020-06-30T15:33:48Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NTIxOA=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc3ODUwNg==", "bodyText": "I would also pull this out of the tool.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447778506", "createdAt": "2020-06-30T15:34:06Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NzY3NQ=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 44}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzNTU0NzA5", "url": "https://github.com/broadinstitute/gatk/pull/6634#pullrequestreview-423554709", "createdAt": "2020-06-03T13:32:19Z", "commit": {"oid": "9304613a6c6d056c45e72981c3f97c3cd200283e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 96, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxMzozMjoyMFrOGeb9Mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxOTo0MDoyN1rOG3wzww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU2ODQ5OA==", "bodyText": "Not in use, please delete.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r434568498", "createdAt": "2020-06-03T13:32:20Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/engine/AssemblyRegion.java", "diffHunk": "@@ -231,6 +231,20 @@ public AssemblyRegion trim(final SimpleInterval span, final SimpleInterval padde\n         return result;\n     }\n \n+    /**\n+     * Returns true if read would overlap the extended extent of this region\n+     * @param read the read we want to test\n+     * @return true if read can be added to this region, false otherwise\n+     */\n+    public boolean readOverlapsRegion(final GATKRead read) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9304613a6c6d056c45e72981c3f97c3cd200283e"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU3MTQ4Ng==", "bodyText": "This change was done (by me) in order to be able to stop and create another progressMeter. ... in a way this is a difficiency in the progressMeter that is imposed to every GATKTool... instead of making this less opaque I should just add additional methods to GATKTool to enact the same required functionality.  Keeping it encapsulated.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r434571486", "createdAt": "2020-06-03T13:36:38Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/engine/GATKTool.java", "diffHunk": "@@ -66,8 +66,9 @@\n     private String masterSequenceDictionaryFilename = null;\n \n     public static final String SECONDS_BETWEEN_PROGRESS_UPDATES_NAME = \"seconds-between-progress-updates\";\n+\n     @Argument(fullName = SECONDS_BETWEEN_PROGRESS_UPDATES_NAME, shortName = SECONDS_BETWEEN_PROGRESS_UPDATES_NAME, doc = \"Output traversal statistics every time this many seconds elapse\", optional = true, common = true)\n-    private double secondsBetweenProgressUpdates = ProgressMeter.DEFAULT_SECONDS_BETWEEN_UPDATES;\n+    protected double secondsBetweenProgressUpdates = ProgressMeter.DEFAULT_SECONDS_BETWEEN_UPDATES;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9304613a6c6d056c45e72981c3f97c3cd200283e"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU3MjAyOA==", "bodyText": "Perhaps the progressMeter should tolerate to be stopped more than once without throwing an exception.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r434572028", "createdAt": "2020-06-03T13:37:23Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/engine/GATKTool.java", "diffHunk": "@@ -1046,7 +1047,9 @@ protected final Object doWork() {\n             onTraversalStart();\n             progressMeter.start();\n             traverse();\n-            progressMeter.stop();\n+            if (!progressMeter.stopped()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9304613a6c6d056c45e72981c3f97c3cd200283e"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU3MzE1Ng==", "bodyText": "what was the default before this was added 0 or full extension? We should make the default behavior match the original before the change and this probrably does not.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r434573156", "createdAt": "2020-06-03T13:39:00Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/engine/spark/AssemblyRegionArgumentCollection.java", "diffHunk": "@@ -108,6 +108,12 @@\n     @Argument(fullName= STR_PADDING_LONG_NAME, doc = \"Include at least this many bases around an event for calling STR indels\", optional = true)\n     public int strPaddingForGenotyping = 75;\n \n+    /**\n+     * the maximum extent into the full active region extension that we're willing to go in genotyping our events\n+     */\n+    @Hidden\n+    @Argument(fullName=\"max-extension\", doc = \"the maximum extent into the full active region extension that we're willing to go in genotyping (-1 to disable)\", optional = true)\n+    public int maxExtensionIntoRegionPadding = 25;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9304613a6c6d056c45e72981c3f97c3cd200283e"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU3Mzc0OA==", "bodyText": "Also how we can say \"full extension\" does the user have to put a ridiculous high number here to ensure that the full extension is taken?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r434573748", "createdAt": "2020-06-03T13:39:42Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/engine/spark/AssemblyRegionArgumentCollection.java", "diffHunk": "@@ -108,6 +108,12 @@\n     @Argument(fullName= STR_PADDING_LONG_NAME, doc = \"Include at least this many bases around an event for calling STR indels\", optional = true)\n     public int strPaddingForGenotyping = 75;\n \n+    /**\n+     * the maximum extent into the full active region extension that we're willing to go in genotyping our events\n+     */\n+    @Hidden\n+    @Argument(fullName=\"max-extension\", doc = \"the maximum extent into the full active region extension that we're willing to go in genotyping (-1 to disable)\", optional = true)\n+    public int maxExtensionIntoRegionPadding = 25;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU3MzE1Ng=="}, "originalCommit": {"oid": "9304613a6c6d056c45e72981c3f97c3cd200283e"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU3NTU5OQ==", "bodyText": "Remove this debugging comments before merge.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r434575599", "createdAt": "2020-06-03T13:41:51Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/ClipReads.java", "diffHunk": "@@ -371,7 +371,7 @@ private void clipSequences(ReadClipperWithData clipper) {\n                 boolean found = true;   // go through at least once\n                 while (found) {\n                     found = match.find();\n-                    //System.out.printf(\"Matching %s against %s/%s => %b%n\", bases, stc.seq, stc.revSeq, found);\n+//                    System.out.printf(\"Matching %s against %s/%s => %b%n\", bases, stc.seq, stc.revSeq, found);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9304613a6c6d056c45e72981c3f97c3cd200283e"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQxNzQxOQ==", "bodyText": "GATKException not used", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447417419", "createdAt": "2020-06-30T05:25:40Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/engine/AssemblyRegion.java", "diffHunk": "@@ -4,6 +4,7 @@\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.samtools.util.Locatable;\n import org.broadinstitute.hellbender.exceptions.GATKException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQzNTIzNg==", "bodyText": "Seems that some if not all the imports are unnecessary.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447435236", "createdAt": "2020-06-30T06:18:17Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/FRDBQDUtils.java", "diffHunk": "@@ -0,0 +1,44 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQzNTkyNw==", "bodyText": "second that, why 4?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447435927", "createdAt": "2020-06-30T06:19:58Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/FRDBQDUtils.java", "diffHunk": "@@ -0,0 +1,44 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+\n+public class FRDBQDUtils {\n+\n+    /**\n+     * These two methods are used to calculate the homopolymer base phred scaled adjustment in BQD. This code is taken from DRAGEN.\n+     *\n+     * @param paddedReference       reference to check for homopolymer span\n+     * @param offsetForRefIntoEvent offset of the base upon which to make a call\n+     */\n+    public static double computeForwardHomopolymerAdjustment(final byte[] paddedReference, final int offsetForRefIntoEvent, final byte errorBase) {\n+        int length = 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NjA0NA=="}, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ2NDI4MA==", "bodyText": "what about:\nfor (int i = 0, j = offsetBlahblah; i < 3; i++) {\n    if (paddedReference[++j] != errorBase) {\n       return 5.0 * i;\n    } \n} \nreturn 20.0; // 5.0 * 4;", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447464280", "createdAt": "2020-06-30T07:20:40Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/FRDBQDUtils.java", "diffHunk": "@@ -0,0 +1,44 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.Serializable;\n+import java.util.Comparator;\n+\n+public class FRDBQDUtils {\n+\n+    /**\n+     * These two methods are used to calculate the homopolymer base phred scaled adjustment in BQD. This code is taken from DRAGEN.\n+     *\n+     * @param paddedReference       reference to check for homopolymer span\n+     * @param offsetForRefIntoEvent offset of the base upon which to make a call\n+     */\n+    public static double computeForwardHomopolymerAdjustment(final byte[] paddedReference, final int offsetForRefIntoEvent, final byte errorBase) {\n+        int length = 1;\n+        while(length < 4) {\n+            if (errorBase != paddedReference[offsetForRefIntoEvent - length]) {\n+                length--;\n+                break;\n+            }\n+            length++;\n+        }\n+        return 5.0 * length;\n+    }\n+    public static double computeReverseHomopolymerAdjustment(final byte[] paddedReference, final int offsetForRefIntoEvent, final byte errorBase) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NjE2Mg=="}, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ2NjE5OQ==", "bodyText": "actually this one should be going out.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447466199", "createdAt": "2020-06-30T07:24:08Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeCalculationArgumentCollection.java", "diffHunk": "@@ -26,6 +27,13 @@\n     public static final int DEFAULT_MAX_ALTERNATE_ALLELES = 6;\n     public static final int DEFAULT_MAX_GENOTYPE_COUNT = 1024;\n \n+    @Argument(fullName=\"dragstr-prior-scale\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NjU5NQ=="}, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc4MzE0MQ==", "bodyText": "These are estimated jointly as they are \"opposite forces\". Based in their observations genuine indels are actually more likely in STRs so they wanted to have a varying prior for indels in STRs. Their pair-hmm and indel prior estimation is done in one go.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447783141", "createdAt": "2020-06-30T15:40:33Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/SimpleGenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SimpleGenotypePriorCalculator implements GenotypePriorCalculator {\n+\n+\n+\n+    private enum AlleleType {\n+        REF, SNP, INDEL, OTHER;\n+    }\n+\n+    private final double[] hetValues;\n+    private final double[] homValues;\n+    private final double[] diffValues;\n+\n+    private SimpleGenotypePriorCalculator(final double snpHet, final double snpHom,\n+                                          final double indelHet, final double indelHom,\n+                                          final double otherHet, final double otherHom) {\n+        hetValues = new double[4];\n+        homValues = new double[4];\n+        diffValues = new double[4];\n+\n+        // SNPs: // * 1/3 since there is three possible mutations for SNPs.\n+        hetValues[1] = snpHet - Math.log10(3);\n+        homValues[1] = snpHom - Math.log10(3);\n+        // INDELs:\n+        hetValues[2] = indelHet;\n+        homValues[2] = indelHom;\n+        // Others:\n+        hetValues[3] = otherHet;\n+        homValues[3] = otherHom;\n+\n+        for (int i = 0; i < 4; i++) {\n+            diffValues[i] = homValues[i] - hetValues[i];\n+        }\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenHetToHomRatio(final double snpHet, final double indelHet,\n+                                                                   final double otherHet, final double het_hom_ratio) {\n+        final double log10Ratio = Math.log10(het_hom_ratio);\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet - log10Ratio,\n+                                                  indelHet, indelHet - log10Ratio,\n+                                                  otherHet, otherHet - log10Ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenDragstrParams(final DragstrParams dragstrParams, final int period,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MjEzNQ=="}, "originalCommit": {"oid": "3a1ae67a69f7e58b1fc271c24902947924aafa5a"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc4ODk3NA==", "bodyText": "Perhaps not in practice but potentially could be so as it could be passed any allele, do you want it to throw an exception instead?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447788974", "createdAt": "2020-06-30T15:48:43Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/haplotypecaller/SimpleGenotypePriorCalculator.java", "diffHunk": "@@ -0,0 +1,131 @@\n+package org.broadinstitute.hellbender.tools.haplotypecaller;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeAlleleCounts;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeCalculationArgumentCollection;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SimpleGenotypePriorCalculator implements GenotypePriorCalculator {\n+\n+\n+\n+    private enum AlleleType {\n+        REF, SNP, INDEL, OTHER;\n+    }\n+\n+    private final double[] hetValues;\n+    private final double[] homValues;\n+    private final double[] diffValues;\n+\n+    private SimpleGenotypePriorCalculator(final double snpHet, final double snpHom,\n+                                          final double indelHet, final double indelHom,\n+                                          final double otherHet, final double otherHom) {\n+        hetValues = new double[4];\n+        homValues = new double[4];\n+        diffValues = new double[4];\n+\n+        // SNPs: // * 1/3 since there is three possible mutations for SNPs.\n+        hetValues[1] = snpHet - Math.log10(3);\n+        homValues[1] = snpHom - Math.log10(3);\n+        // INDELs:\n+        hetValues[2] = indelHet;\n+        homValues[2] = indelHom;\n+        // Others:\n+        hetValues[3] = otherHet;\n+        homValues[3] = otherHom;\n+\n+        for (int i = 0; i < 4; i++) {\n+            diffValues[i] = homValues[i] - hetValues[i];\n+        }\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenHetToHomRatio(final double snpHet, final double indelHet,\n+                                                                   final double otherHet, final double het_hom_ratio) {\n+        final double log10Ratio = Math.log10(het_hom_ratio);\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet - log10Ratio,\n+                                                  indelHet, indelHet - log10Ratio,\n+                                                  otherHet, otherHet - log10Ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator givenDragstrParams(final DragstrParams dragstrParams, final int period,\n+                                                                   final int repeats, final double snpHeterozygosity,\n+                                                                   final double het_hom_ratio) {\n+        final double snpHet = snpHeterozygosity;\n+        final double indelHet = -.1 * dragstrParams.api(period, repeats);\n+        final double otherHet = Math.max(snpHet, indelHet);\n+        return givenHetToHomRatio(snpHet, indelHet, otherHet, het_hom_ratio);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(final double snpHet, final double indelHet) {\n+        return assumingHW(snpHet, indelHet, Math.max(snpHet, indelHet));\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(final double snpHet, final double indelHet,\n+                                                                   final double otherHet) {\n+        return new SimpleGenotypePriorCalculator(snpHet, snpHet * 2,\n+                indelHet, indelHet * 2,\n+                otherHet, otherHet * 2);\n+    }\n+\n+    public static SimpleGenotypePriorCalculator assumingHW(GenotypeCalculationArgumentCollection genotypeArgs) {\n+        return assumingHW(Math.log10(genotypeArgs.snpHeterozygosity),\n+                          Math.log10(genotypeArgs.indelHeterozygosity));\n+    }\n+\n+\n+    @Override\n+    public double[] getLog10Priors(final GenotypeLikelihoodCalculator lkCalculator, final List<Allele> alleles) {\n+        final int[] alleleTypes = calculateAlleleTypes(alleles);\n+        final int numberOfGenotypes = lkCalculator.genotypeCount();\n+        final double[] result = new double[numberOfGenotypes];\n+        // implied = result[0] = 0.0;\n+        for (int g = 1; g < numberOfGenotypes; g++) {\n+            final GenotypeAlleleCounts gac = lkCalculator.genotypeAlleleCountsAt(g);\n+            final int numberOfDistictAlleles = gac.distinctAlleleCount();\n+            double log10Sum = 0;\n+            for (int a = 0; a < numberOfDistictAlleles; a++) {\n+                final int idx = gac.alleleIndexAt(a);\n+                final int cnt = gac.alleleCountAt(a);\n+                if (cnt == 1) {\n+                    log10Sum += hetValues[alleleTypes[idx]];\n+                } else if (cnt == 2) {\n+                    log10Sum += homValues[alleleTypes[idx]];\n+                } else { // for plodies over 2 and allele counts over 2 then we use the het/hom ratio for the rest\n+                    log10Sum += hetValues[alleleTypes[idx]] + diffValues[alleleTypes[idx]] * (cnt - 1);\n+                }\n+            }\n+            result[g] = log10Sum;\n+        }\n+        return result;\n+    }\n+\n+    private int[] calculateAlleleTypes(final List<Allele> alleles) {\n+        if (alleles.isEmpty()) {\n+            throw new IllegalArgumentException(\"there must be at least one allele (the reference)\");\n+        } else {\n+            final int[] result = new int[alleles.size()];\n+            Arrays.fill(result, AlleleType.OTHER.ordinal());\n+            final Allele refAllele = alleles.get(0);\n+            if (!refAllele.isReference()) {\n+                throw new IllegalArgumentException(\"the first allele in the list must be the reference\");\n+            }\n+            final int refAlleleLength = refAllele.length();\n+            result[0] = AlleleType.REF.ordinal();\n+            for (int i = 1; i < result.length; i++) {\n+                final Allele allele = alleles.get(i);\n+                if (allele.isCalled()) {\n+                    if (!allele.isSymbolic()) {\n+                        result[i] = (allele.length() == refAlleleLength ? AlleleType.SNP : AlleleType.INDEL).ordinal();\n+                    } else if (allele.equals(Allele.SV_SIMPLE_INS) || allele.equals(Allele.SV_SIMPLE_DEL)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI1MDY0Ng=="}, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc5NjMyNw==", "bodyText": "Break the line into more that one for easier code inspection.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447796327", "createdAt": "2020-06-30T15:58:21Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,341 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgwNTA2NA==", "bodyText": "If public you need to check that arguments are valid (e.g. negative capacities).", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447805064", "createdAt": "2020-06-30T16:11:21Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,341 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgwNjQ0OQ==", "bodyText": "Same, break the line in several.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447806449", "createdAt": "2020-06-30T16:13:03Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,341 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyMTc5OQ==", "bodyText": "This warning suppressions are not necessary with a simple change down below....", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447821799", "createdAt": "2020-06-30T16:33:12Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,341 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyMjMyMQ==", "bodyText": "To remove warning suppressions change new AlleleLikelihoodMatrixMapper(permutation) for\nnew AlleleLikelihoodMatrixMapper<>(permutation)", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447822321", "createdAt": "2020-06-30T16:34:00Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,341 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"No API from DRAGStrs found, falling back on snp het prior for indels\");\n+            }\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgyNDk5MA==", "bodyText": "I would write it like:\nfor (final GATKRead readForSample : readsForSample) {\n    final int indexForSnp = ... ;\n    ...", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r447824990", "createdAt": "2020-06-30T16:37:38Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,341 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"No API from DRAGStrs found, falling back on snp het prior for indels\");\n+            }\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            List<DragenReadContainer> strandForward = new ArrayList<>();\n+            List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ4MzQxMw==", "bodyText": "As per the comment above it should be deprected.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r448483413", "createdAt": "2020-07-01T16:33:23Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeCalculationArgumentCollection.java", "diffHunk": "@@ -49,6 +57,16 @@ public GenotypeCalculationArgumentCollection( final GenotypeCalculationArgumentC\n         this.numRefIfMissing = other.numRefIfMissing;\n     }\n \n+    @Argument(fullName = \"dont-use-dragstr-priors\", optional = true)\n+    public boolean dontUseDragstrPriors = false;\n+\n+    /**\n+     * As of version 4.1.0.0, this argument is no longer needed because the new qual score is now on by default. See GATK 3.3 release notes for more details.\n+     */\n+    @Deprecated", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI0OTI5MA=="}, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ4OTI2Nw==", "bodyText": "Break long argument line.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r448489267", "createdAt": "2020-07-01T16:44:03Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculator.java", "diffHunk": "@@ -137,22 +121,15 @@\n      */\n     private double[] readGenotypeLikelihoodComponents;\n \n-    /**\n-     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n-     */\n-    protected GenotypeLikelihoodCalculator(final int ploidy, final int alleleCount,\n-                                           final int[][] alleleFirstGenotypeOffsetByPloidy,\n-                                           final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n-        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU4MjE3OA=="}, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIyNDU4OQ==", "bodyText": "If this class is not mean to be used if both computeBQD and computeFRD are false; here you must fail with an exception if this is the case.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r449224589", "createdAt": "2020-07-02T19:26:48Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,336 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypingModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+    public static final double BQD_HOMOPOLYMER_PHRED_ADJUSTMENT_FACTOR = 5.0;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79d1ce71b8ed106b332f4405f86dc03a27ebce78"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIyNTA1MA==", "bodyText": "Is this actually FLAT? Perhaps just a DEFAULT?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r449225050", "createdAt": "2020-07-02T19:27:52Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,336 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypingModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79d1ce71b8ed106b332f4405f86dc03a27ebce78"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIyNTQ5Mg==", "bodyText": "Is this supposed to have an @Override annotation?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r449225492", "createdAt": "2020-07-02T19:28:57Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,341 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgwNjQ0OQ=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIyNzg5Mg==", "bodyText": "Me too... why is this debug is gone to a custom output as supposed to use the standard logger? perhaps will become more clear  when I review the invoking code.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r449227892", "createdAt": "2020-07-02T19:34:27Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,325 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypersModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior for the\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0NDAwMw=="}, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIyODY4NQ==", "bodyText": "Same... you could use a \"for-each\" rather than a for", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r449228685", "createdAt": "2020-07-02T19:36:15Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,336 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypingModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+    public static final double BQD_HOMOPOLYMER_PHRED_ADJUSTMENT_FACTOR = 5.0;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior to use for the alternate allele if it is an indel\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"No API from DRAGStrs found, falling back on snp het prior for indels\");\n+            }\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            final List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            final List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            final List<DragenReadContainer> strandForward = new ArrayList<>();\n+            final List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                (readForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79d1ce71b8ed106b332f4405f86dc03a27ebce78"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTExNTYxNQ==", "bodyText": "calculateGenotypeCountUsingTable is syncronized and since the rest of the code does not depend on the class state there is no nedd to syncronize this method.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451115615", "createdAt": "2020-07-07T20:11:58Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculators.java", "diffHunk": "@@ -337,7 +363,7 @@ private static void checkOffsetTableCapacity(final int[][] offsetTable, final in\n      *\n      * @return the number of genotypes given ploidy and allele count (0 or greater).\n      */\n-    public int genotypeCount(final int ploidy, final int alleleCount) {\n+    public synchronized int genotypeCount(final int ploidy, final int alleleCount) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4OTE1MA==", "bodyText": "I would rewrite the code below to remove repetitions and some unecessary ifs and non-final local vars.\nBQD and FRD sections seem to be quite similar with the difference of the likelihood calculation code.\nyou can create a method apply them to the likelihoods.\nprivate void applyAdjustment(double[] likelihoods, name, double[] adjustmentLikelihoods) {\n    logger.debug(name + \" adjusted lks: \");\n    logger.debug(Arrays.toString(adjustmentLikelihoods);\n    for (int gt = 0; gt < initialLikelihoods.length; gt++) {\n          likelihoods[gt] = Math.max(likelihoods[gt], adjustmentLikelihoods[gt];\n    }\n}\n// ...\n\nif (computeBQD) {\n    applyAdjustment(gtLikelihoods, \"BQD\", likelihoosCalculator.calculateBQDLikelihoods(...);\n} \nif (computeFRD) {\n    applyAdjustment(gtLikelihoods, \"FRD\", likihoodsCalculator.calculateFRDLikelihoods(...);\n}", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451289150", "createdAt": "2020-07-08T05:20:31Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,336 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypingModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+    public static final double BQD_HOMOPOLYMER_PHRED_ADJUSTMENT_FACTOR = 5.0;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior to use for the alternate allele if it is an indel\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"No API from DRAGStrs found, falling back on snp het prior for indels\");\n+            }\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            final List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            final List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            final List<DragenReadContainer> strandForward = new ArrayList<>();\n+            final List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                (readForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                (filteredReadForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparator());\n+            strandReverse.sort(new ReadFeatherEndReverseComparator());\n+\n+            // Compute default likelihoods as normal (before we go ahead and alter the likelihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+            if (genotyperDebugStream != null) {\n+                likelihoodsCalculator.addGenotyperDebugOutputStream(genotyperDebugStream);\n+            }\n+\n+            // this is the data array for the read likelihoods without any trouble\n+            final LikelihoodMatrix<GATKRead, A> sampleLikelihoods = alleleLikelihoodMatrixMapper.mapAlleles(data.readLikelihoods().sampleMatrix(sampleIndex));\n+            final double[] ploidyModelGenotypeLikelihoods = likelihoodsCalculator.rawGenotypeLikelihoods(sampleLikelihoods);\n+\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"\\n Standard Genotyping Resutls:\");\n+                genotyperDebugStream.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+\n+            double[] BQDCallResults = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5MDcwMw==", "bodyText": "I would call result cached instead as is not going to have the actual result sometimes.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451290703", "createdAt": "2020-07-08T05:25:36Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,336 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypingModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+    public static final double BQD_HOMOPOLYMER_PHRED_ADJUSTMENT_FACTOR = 5.0;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior to use for the alternate allele if it is an indel\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"No API from DRAGStrs found, falling back on snp het prior for indels\");\n+            }\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            final List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            final List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            final List<DragenReadContainer> strandForward = new ArrayList<>();\n+            final List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                (readForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                (filteredReadForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparator());\n+            strandReverse.sort(new ReadFeatherEndReverseComparator());\n+\n+            // Compute default likelihoods as normal (before we go ahead and alter the likelihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+            if (genotyperDebugStream != null) {\n+                likelihoodsCalculator.addGenotyperDebugOutputStream(genotyperDebugStream);\n+            }\n+\n+            // this is the data array for the read likelihoods without any trouble\n+            final LikelihoodMatrix<GATKRead, A> sampleLikelihoods = alleleLikelihoodMatrixMapper.mapAlleles(data.readLikelihoods().sampleMatrix(sampleIndex));\n+            final double[] ploidyModelGenotypeLikelihoods = likelihoodsCalculator.rawGenotypeLikelihoods(sampleLikelihoods);\n+\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"\\n Standard Genotyping Resutls:\");\n+                genotyperDebugStream.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+\n+            double[] BQDCallResults = null;\n+            double[] FRDCallResults = null;\n+            if (computeBQD) {\n+                BQDCallResults = likelihoodsCalculator.calculateBQDLikelihoods(sampleLikelihoods, strandForward, strandReverse, paddedReference, offsetForRefIntoEvent, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"BQD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(BQDCallResults));\n+                }\n+            }\n+            if (computeFRD) {\n+                FRDCallResults = likelihoodsCalculator.calculateFRDLikelihoods(sampleLikelihoods, ploidyModelGenotypeLikelihoods,\n+                        Stream.of(strandForward, strandReverse).flatMap(Collection::stream).collect(Collectors.toList()), // We filter out the HMM filtered reads as they do not apply to FRD\n+                        FLAT_SNP_HET_PRIOR, api, maxEffectiveDepthAdjustment, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"FRD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(FRDCallResults));\n+                }\n+            }\n+\n+            //make synthesized likelihoods object (NOTE that we can do this since for invalid model GT fields we simply infinity out the result in the array)\n+            for (int gt = 0; gt < ploidyModelGenotypeLikelihoods.length; gt++) {\n+                if (computeBQD) {\n+                    ploidyModelGenotypeLikelihoods[gt] = Math.max(ploidyModelGenotypeLikelihoods[gt], BQDCallResults[gt]);\n+                }\n+                if (computeFRD) {\n+                    ploidyModelGenotypeLikelihoods[gt] = Math.max(ploidyModelGenotypeLikelihoods[gt], FRDCallResults[gt]);\n+                }\n+            }\n+\n+            // this is what the work actually is, after we have computed a few things\n+            genotypeLikelihoods.add(GenotypeLikelihoods.fromLog10Likelihoods(ploidyModelGenotypeLikelihoods));\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"merged matrix:\");\n+                genotyperDebugStream.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+        }\n+        return new GenotypingLikelihoods<>(genotypingAlleles, ploidyModel, genotypeLikelihoods);\n+    }\n+\n+    // Adding the debug stream managed by the haplotype caller engine\n+    @Override\n+    public void addDebugOutStream(final PrintStream debugStream) {\n+        this.genotyperDebugStream = debugStream;\n+    }\n+\n+    private GenotypeLikelihoodCalculatorDRAGEN getLikelihoodsCalculator(final int samplePloidy, final int alleleCount) {\n+        if (samplePloidy >= cachePloidyCapacity || alleleCount >= cacheAlleleCountCapacity) {\n+            return calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+        }\n+        final GenotypeLikelihoodCalculatorDRAGEN result = likelihoodCalculators[samplePloidy][alleleCount];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5MjU0NA==", "bodyText": "Rather than a \"container\" is a \"wrapper\"", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451292544", "createdAt": "2020-07-08T05:32:00Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,336 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypingModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+    public static final double BQD_HOMOPOLYMER_PHRED_ADJUSTMENT_FACTOR = 5.0;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior to use for the alternate allele if it is an indel\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"No API from DRAGStrs found, falling back on snp het prior for indels\");\n+            }\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            final List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            final List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            final List<DragenReadContainer> strandForward = new ArrayList<>();\n+            final List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                (readForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                (filteredReadForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparator());\n+            strandReverse.sort(new ReadFeatherEndReverseComparator());\n+\n+            // Compute default likelihoods as normal (before we go ahead and alter the likelihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+            if (genotyperDebugStream != null) {\n+                likelihoodsCalculator.addGenotyperDebugOutputStream(genotyperDebugStream);\n+            }\n+\n+            // this is the data array for the read likelihoods without any trouble\n+            final LikelihoodMatrix<GATKRead, A> sampleLikelihoods = alleleLikelihoodMatrixMapper.mapAlleles(data.readLikelihoods().sampleMatrix(sampleIndex));\n+            final double[] ploidyModelGenotypeLikelihoods = likelihoodsCalculator.rawGenotypeLikelihoods(sampleLikelihoods);\n+\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"\\n Standard Genotyping Resutls:\");\n+                genotyperDebugStream.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+\n+            double[] BQDCallResults = null;\n+            double[] FRDCallResults = null;\n+            if (computeBQD) {\n+                BQDCallResults = likelihoodsCalculator.calculateBQDLikelihoods(sampleLikelihoods, strandForward, strandReverse, paddedReference, offsetForRefIntoEvent, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"BQD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(BQDCallResults));\n+                }\n+            }\n+            if (computeFRD) {\n+                FRDCallResults = likelihoodsCalculator.calculateFRDLikelihoods(sampleLikelihoods, ploidyModelGenotypeLikelihoods,\n+                        Stream.of(strandForward, strandReverse).flatMap(Collection::stream).collect(Collectors.toList()), // We filter out the HMM filtered reads as they do not apply to FRD\n+                        FLAT_SNP_HET_PRIOR, api, maxEffectiveDepthAdjustment, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"FRD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(FRDCallResults));\n+                }\n+            }\n+\n+            //make synthesized likelihoods object (NOTE that we can do this since for invalid model GT fields we simply infinity out the result in the array)\n+            for (int gt = 0; gt < ploidyModelGenotypeLikelihoods.length; gt++) {\n+                if (computeBQD) {\n+                    ploidyModelGenotypeLikelihoods[gt] = Math.max(ploidyModelGenotypeLikelihoods[gt], BQDCallResults[gt]);\n+                }\n+                if (computeFRD) {\n+                    ploidyModelGenotypeLikelihoods[gt] = Math.max(ploidyModelGenotypeLikelihoods[gt], FRDCallResults[gt]);\n+                }\n+            }\n+\n+            // this is what the work actually is, after we have computed a few things\n+            genotypeLikelihoods.add(GenotypeLikelihoods.fromLog10Likelihoods(ploidyModelGenotypeLikelihoods));\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"merged matrix:\");\n+                genotyperDebugStream.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+        }\n+        return new GenotypingLikelihoods<>(genotypingAlleles, ploidyModel, genotypeLikelihoods);\n+    }\n+\n+    // Adding the debug stream managed by the haplotype caller engine\n+    @Override\n+    public void addDebugOutStream(final PrintStream debugStream) {\n+        this.genotyperDebugStream = debugStream;\n+    }\n+\n+    private GenotypeLikelihoodCalculatorDRAGEN getLikelihoodsCalculator(final int samplePloidy, final int alleleCount) {\n+        if (samplePloidy >= cachePloidyCapacity || alleleCount >= cacheAlleleCountCapacity) {\n+            return calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+        }\n+        final GenotypeLikelihoodCalculatorDRAGEN result = likelihoodCalculators[samplePloidy][alleleCount];\n+        if (result != null) {\n+            return result;\n+        } else {\n+            final GenotypeLikelihoodCalculatorDRAGEN newOne = calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+            likelihoodCalculators[samplePloidy][alleleCount] = newOne;\n+            return newOne;\n+        }\n+    }\n+\n+    /**\n+     * This helper class is used to store the necessary data in order to sort a read based on its BQD \"feather end\" as\n+     * well as information relevant to re-associate the read with its position in the AlleleLikelihoods object arrays.\n+     */\n+    static class DragenReadContainer {\n+        final GATKRead underlyingRead;\n+        final int offsetIntoReadForBaseQuality;\n+        final int unclippedEnd;\n+        final int indexInLikelihoodsObject;\n+\n+        // Transient value used to store thresholds for FRD\n+       double phredPFValue = 0;\n+\n+\n+        public DragenReadContainer(final GATKRead underlyingRead, final int offsetIntoReadForBaseQuality, final int unclippedEnd, final int indexInLikelihoodsObject) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 227}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NjExNQ==", "bodyText": "This should be a singleton... no need to declare a class explicitly you can use. a lambda:\npublic static final Comparator<DRC> READ_FEATHER_END_FORWARD_CMP = (read1, read2) -> {\n   // your compare(read1, read2) code.\n};", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451296115", "createdAt": "2020-07-08T05:44:15Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,336 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypingModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+    public static final double BQD_HOMOPOLYMER_PHRED_ADJUSTMENT_FACTOR = 5.0;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior to use for the alternate allele if it is an indel\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"No API from DRAGStrs found, falling back on snp het prior for indels\");\n+            }\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            final List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            final List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            final List<DragenReadContainer> strandForward = new ArrayList<>();\n+            final List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                (readForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                (filteredReadForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparator());\n+            strandReverse.sort(new ReadFeatherEndReverseComparator());\n+\n+            // Compute default likelihoods as normal (before we go ahead and alter the likelihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+            if (genotyperDebugStream != null) {\n+                likelihoodsCalculator.addGenotyperDebugOutputStream(genotyperDebugStream);\n+            }\n+\n+            // this is the data array for the read likelihoods without any trouble\n+            final LikelihoodMatrix<GATKRead, A> sampleLikelihoods = alleleLikelihoodMatrixMapper.mapAlleles(data.readLikelihoods().sampleMatrix(sampleIndex));\n+            final double[] ploidyModelGenotypeLikelihoods = likelihoodsCalculator.rawGenotypeLikelihoods(sampleLikelihoods);\n+\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"\\n Standard Genotyping Resutls:\");\n+                genotyperDebugStream.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+\n+            double[] BQDCallResults = null;\n+            double[] FRDCallResults = null;\n+            if (computeBQD) {\n+                BQDCallResults = likelihoodsCalculator.calculateBQDLikelihoods(sampleLikelihoods, strandForward, strandReverse, paddedReference, offsetForRefIntoEvent, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"BQD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(BQDCallResults));\n+                }\n+            }\n+            if (computeFRD) {\n+                FRDCallResults = likelihoodsCalculator.calculateFRDLikelihoods(sampleLikelihoods, ploidyModelGenotypeLikelihoods,\n+                        Stream.of(strandForward, strandReverse).flatMap(Collection::stream).collect(Collectors.toList()), // We filter out the HMM filtered reads as they do not apply to FRD\n+                        FLAT_SNP_HET_PRIOR, api, maxEffectiveDepthAdjustment, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"FRD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(FRDCallResults));\n+                }\n+            }\n+\n+            //make synthesized likelihoods object (NOTE that we can do this since for invalid model GT fields we simply infinity out the result in the array)\n+            for (int gt = 0; gt < ploidyModelGenotypeLikelihoods.length; gt++) {\n+                if (computeBQD) {\n+                    ploidyModelGenotypeLikelihoods[gt] = Math.max(ploidyModelGenotypeLikelihoods[gt], BQDCallResults[gt]);\n+                }\n+                if (computeFRD) {\n+                    ploidyModelGenotypeLikelihoods[gt] = Math.max(ploidyModelGenotypeLikelihoods[gt], FRDCallResults[gt]);\n+                }\n+            }\n+\n+            // this is what the work actually is, after we have computed a few things\n+            genotypeLikelihoods.add(GenotypeLikelihoods.fromLog10Likelihoods(ploidyModelGenotypeLikelihoods));\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"merged matrix:\");\n+                genotyperDebugStream.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+        }\n+        return new GenotypingLikelihoods<>(genotypingAlleles, ploidyModel, genotypeLikelihoods);\n+    }\n+\n+    // Adding the debug stream managed by the haplotype caller engine\n+    @Override\n+    public void addDebugOutStream(final PrintStream debugStream) {\n+        this.genotyperDebugStream = debugStream;\n+    }\n+\n+    private GenotypeLikelihoodCalculatorDRAGEN getLikelihoodsCalculator(final int samplePloidy, final int alleleCount) {\n+        if (samplePloidy >= cachePloidyCapacity || alleleCount >= cacheAlleleCountCapacity) {\n+            return calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+        }\n+        final GenotypeLikelihoodCalculatorDRAGEN result = likelihoodCalculators[samplePloidy][alleleCount];\n+        if (result != null) {\n+            return result;\n+        } else {\n+            final GenotypeLikelihoodCalculatorDRAGEN newOne = calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+            likelihoodCalculators[samplePloidy][alleleCount] = newOne;\n+            return newOne;\n+        }\n+    }\n+\n+    /**\n+     * This helper class is used to store the necessary data in order to sort a read based on its BQD \"feather end\" as\n+     * well as information relevant to re-associate the read with its position in the AlleleLikelihoods object arrays.\n+     */\n+    static class DragenReadContainer {\n+        final GATKRead underlyingRead;\n+        final int offsetIntoReadForBaseQuality;\n+        final int unclippedEnd;\n+        final int indexInLikelihoodsObject;\n+\n+        // Transient value used to store thresholds for FRD\n+       double phredPFValue = 0;\n+\n+\n+        public DragenReadContainer(final GATKRead underlyingRead, final int offsetIntoReadForBaseQuality, final int unclippedEnd, final int indexInLikelihoodsObject) {\n+            this.underlyingRead = underlyingRead;\n+            this.offsetIntoReadForBaseQuality = offsetIntoReadForBaseQuality;\n+            this.unclippedEnd = unclippedEnd;\n+            this.indexInLikelihoodsObject = indexInLikelihoodsObject;\n+        }\n+\n+        public int getUnclippedPosition() {\n+            return unclippedEnd;\n+        }\n+\n+        public int getIndexInLikelihoodsObject() {\n+            return indexInLikelihoodsObject;\n+        }\n+\n+        public boolean wasFilteredByHMM() {\n+            return this.getIndexInLikelihoodsObject() == -1;\n+        }\n+\n+        public boolean hasValidBaseQuality() {\n+            return offsetIntoReadForBaseQuality != -1;\n+        }\n+\n+        public int getBaseQuality() {\n+            return underlyingRead.getBaseQuality(offsetIntoReadForBaseQuality);\n+        }\n+\n+        public int getForwardsFeatherEnd() {\n+            return (underlyingRead.getSoftStart() - underlyingRead.getUnclippedStart()) + offsetIntoReadForBaseQuality;\n+        }\n+\n+        public int getReverseFeatherEnd() {\n+            return (underlyingRead.getUnclippedEnd() - underlyingRead.getSoftEnd()) + (underlyingRead.getLength() - offsetIntoReadForBaseQuality);\n+        }\n+\n+        public double getPhredScaledMappingQuality() {\n+            return DRAGENMappingQualityReadTransformer.mapMappingQualityToPhredLikelihoodScore(underlyingRead.getMappingQuality());\n+        }\n+\n+        public double getPhredPFValue() {\n+            return phredPFValue;\n+        }\n+\n+        public void setPhredPFValue(double phredPFValue) {\n+            this.phredPFValue = phredPFValue;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return String.format(\"Read: %s index: %d at unclipped end: %d with base quality %d\", underlyingRead.toString(), indexInLikelihoodsObject, unclippedEnd, (hasValidBaseQuality() ? getBaseQuality() : -1));\n+        }\n+\n+        public boolean isReverseStrand() {\n+            return underlyingRead.isReverseStrand();\n+        }\n+    }\n+\n+    //MAJOR TODO THIS IS CURRENTLY BASED OFF OF THE REFERENCE UNCLIPPED START AND NOT THE BASES IN THE READ CONSEQUENTLY AT SITES WITH\n+    //      TODO INDELS PRESENT WE ARE GOING TO BE LOOKING AT THE WRONG OFFSETS FOR THIS SORT... a minor issue but still...\n+    //UPDATE:APPARENTLY DRAGEN ONLY USES THE ORIGINAL BASE ALIGNMENT OFFSETS HERE (NO HARDCLIPPED BASES OR INDEL SITE HANDLING)\n+\n+    // Orders the reads based on the number of bases there are to the left of the fatherEndComparisonLocation as aligned according to the cigar\n+    // NOTE: here we compare the un-hardclipped edges for these reads as the model itself cares about the cycle count of the sequencer, and\n+    //       importantly this saves us having the thread the original alignment of these reads to this level, since by this point we have trimmed\n+    //       the reads twice, once to the active region with padding and again to the callable region within the active window and in both of these\n+    //       cases we have deleted bases with hardclips.\n+    public class ReadFeatherEndForwardComparator implements Comparator<DragenReadContainer>, Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 293}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NzAzMw==", "bodyText": "Is necessary that is serializable? if so there is a trick to get the compiler to generate a class that is serializable. Not sure bout it may need to go like this:\npublic static final Comparator<DRC> READ_FEATHER_END_FORWARD_CMP = (Comparator<DRC> & Serializable) (read1, read2) -> {\n   // your compare(read1, read2) code.\n};", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451297033", "createdAt": "2020-07-08T05:47:01Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,336 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypingModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+    public static final double BQD_HOMOPOLYMER_PHRED_ADJUSTMENT_FACTOR = 5.0;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior to use for the alternate allele if it is an indel\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"No API from DRAGStrs found, falling back on snp het prior for indels\");\n+            }\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            final List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            final List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            final List<DragenReadContainer> strandForward = new ArrayList<>();\n+            final List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                (readForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                (filteredReadForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparator());\n+            strandReverse.sort(new ReadFeatherEndReverseComparator());\n+\n+            // Compute default likelihoods as normal (before we go ahead and alter the likelihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+            if (genotyperDebugStream != null) {\n+                likelihoodsCalculator.addGenotyperDebugOutputStream(genotyperDebugStream);\n+            }\n+\n+            // this is the data array for the read likelihoods without any trouble\n+            final LikelihoodMatrix<GATKRead, A> sampleLikelihoods = alleleLikelihoodMatrixMapper.mapAlleles(data.readLikelihoods().sampleMatrix(sampleIndex));\n+            final double[] ploidyModelGenotypeLikelihoods = likelihoodsCalculator.rawGenotypeLikelihoods(sampleLikelihoods);\n+\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"\\n Standard Genotyping Resutls:\");\n+                genotyperDebugStream.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+\n+            double[] BQDCallResults = null;\n+            double[] FRDCallResults = null;\n+            if (computeBQD) {\n+                BQDCallResults = likelihoodsCalculator.calculateBQDLikelihoods(sampleLikelihoods, strandForward, strandReverse, paddedReference, offsetForRefIntoEvent, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"BQD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(BQDCallResults));\n+                }\n+            }\n+            if (computeFRD) {\n+                FRDCallResults = likelihoodsCalculator.calculateFRDLikelihoods(sampleLikelihoods, ploidyModelGenotypeLikelihoods,\n+                        Stream.of(strandForward, strandReverse).flatMap(Collection::stream).collect(Collectors.toList()), // We filter out the HMM filtered reads as they do not apply to FRD\n+                        FLAT_SNP_HET_PRIOR, api, maxEffectiveDepthAdjustment, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"FRD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(FRDCallResults));\n+                }\n+            }\n+\n+            //make synthesized likelihoods object (NOTE that we can do this since for invalid model GT fields we simply infinity out the result in the array)\n+            for (int gt = 0; gt < ploidyModelGenotypeLikelihoods.length; gt++) {\n+                if (computeBQD) {\n+                    ploidyModelGenotypeLikelihoods[gt] = Math.max(ploidyModelGenotypeLikelihoods[gt], BQDCallResults[gt]);\n+                }\n+                if (computeFRD) {\n+                    ploidyModelGenotypeLikelihoods[gt] = Math.max(ploidyModelGenotypeLikelihoods[gt], FRDCallResults[gt]);\n+                }\n+            }\n+\n+            // this is what the work actually is, after we have computed a few things\n+            genotypeLikelihoods.add(GenotypeLikelihoods.fromLog10Likelihoods(ploidyModelGenotypeLikelihoods));\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"merged matrix:\");\n+                genotyperDebugStream.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+        }\n+        return new GenotypingLikelihoods<>(genotypingAlleles, ploidyModel, genotypeLikelihoods);\n+    }\n+\n+    // Adding the debug stream managed by the haplotype caller engine\n+    @Override\n+    public void addDebugOutStream(final PrintStream debugStream) {\n+        this.genotyperDebugStream = debugStream;\n+    }\n+\n+    private GenotypeLikelihoodCalculatorDRAGEN getLikelihoodsCalculator(final int samplePloidy, final int alleleCount) {\n+        if (samplePloidy >= cachePloidyCapacity || alleleCount >= cacheAlleleCountCapacity) {\n+            return calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+        }\n+        final GenotypeLikelihoodCalculatorDRAGEN result = likelihoodCalculators[samplePloidy][alleleCount];\n+        if (result != null) {\n+            return result;\n+        } else {\n+            final GenotypeLikelihoodCalculatorDRAGEN newOne = calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+            likelihoodCalculators[samplePloidy][alleleCount] = newOne;\n+            return newOne;\n+        }\n+    }\n+\n+    /**\n+     * This helper class is used to store the necessary data in order to sort a read based on its BQD \"feather end\" as\n+     * well as information relevant to re-associate the read with its position in the AlleleLikelihoods object arrays.\n+     */\n+    static class DragenReadContainer {\n+        final GATKRead underlyingRead;\n+        final int offsetIntoReadForBaseQuality;\n+        final int unclippedEnd;\n+        final int indexInLikelihoodsObject;\n+\n+        // Transient value used to store thresholds for FRD\n+       double phredPFValue = 0;\n+\n+\n+        public DragenReadContainer(final GATKRead underlyingRead, final int offsetIntoReadForBaseQuality, final int unclippedEnd, final int indexInLikelihoodsObject) {\n+            this.underlyingRead = underlyingRead;\n+            this.offsetIntoReadForBaseQuality = offsetIntoReadForBaseQuality;\n+            this.unclippedEnd = unclippedEnd;\n+            this.indexInLikelihoodsObject = indexInLikelihoodsObject;\n+        }\n+\n+        public int getUnclippedPosition() {\n+            return unclippedEnd;\n+        }\n+\n+        public int getIndexInLikelihoodsObject() {\n+            return indexInLikelihoodsObject;\n+        }\n+\n+        public boolean wasFilteredByHMM() {\n+            return this.getIndexInLikelihoodsObject() == -1;\n+        }\n+\n+        public boolean hasValidBaseQuality() {\n+            return offsetIntoReadForBaseQuality != -1;\n+        }\n+\n+        public int getBaseQuality() {\n+            return underlyingRead.getBaseQuality(offsetIntoReadForBaseQuality);\n+        }\n+\n+        public int getForwardsFeatherEnd() {\n+            return (underlyingRead.getSoftStart() - underlyingRead.getUnclippedStart()) + offsetIntoReadForBaseQuality;\n+        }\n+\n+        public int getReverseFeatherEnd() {\n+            return (underlyingRead.getUnclippedEnd() - underlyingRead.getSoftEnd()) + (underlyingRead.getLength() - offsetIntoReadForBaseQuality);\n+        }\n+\n+        public double getPhredScaledMappingQuality() {\n+            return DRAGENMappingQualityReadTransformer.mapMappingQualityToPhredLikelihoodScore(underlyingRead.getMappingQuality());\n+        }\n+\n+        public double getPhredPFValue() {\n+            return phredPFValue;\n+        }\n+\n+        public void setPhredPFValue(double phredPFValue) {\n+            this.phredPFValue = phredPFValue;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return String.format(\"Read: %s index: %d at unclipped end: %d with base quality %d\", underlyingRead.toString(), indexInLikelihoodsObject, unclippedEnd, (hasValidBaseQuality() ? getBaseQuality() : -1));\n+        }\n+\n+        public boolean isReverseStrand() {\n+            return underlyingRead.isReverseStrand();\n+        }\n+    }\n+\n+    //MAJOR TODO THIS IS CURRENTLY BASED OFF OF THE REFERENCE UNCLIPPED START AND NOT THE BASES IN THE READ CONSEQUENTLY AT SITES WITH\n+    //      TODO INDELS PRESENT WE ARE GOING TO BE LOOKING AT THE WRONG OFFSETS FOR THIS SORT... a minor issue but still...\n+    //UPDATE:APPARENTLY DRAGEN ONLY USES THE ORIGINAL BASE ALIGNMENT OFFSETS HERE (NO HARDCLIPPED BASES OR INDEL SITE HANDLING)\n+\n+    // Orders the reads based on the number of bases there are to the left of the fatherEndComparisonLocation as aligned according to the cigar\n+    // NOTE: here we compare the un-hardclipped edges for these reads as the model itself cares about the cycle count of the sequencer, and\n+    //       importantly this saves us having the thread the original alignment of these reads to this level, since by this point we have trimmed\n+    //       the reads twice, once to the active region with padding and again to the callable region within the active window and in both of these\n+    //       cases we have deleted bases with hardclips.\n+    public class ReadFeatherEndForwardComparator implements Comparator<DragenReadContainer>, Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NjExNQ=="}, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 293}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NzE4NA==", "bodyText": "Same as above.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451297184", "createdAt": "2020-07-08T05:47:31Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,336 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypingModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+    public static final double BQD_HOMOPOLYMER_PHRED_ADJUSTMENT_FACTOR = 5.0;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior to use for the alternate allele if it is an indel\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"No API from DRAGStrs found, falling back on snp het prior for indels\");\n+            }\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            final List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            final List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            final List<DragenReadContainer> strandForward = new ArrayList<>();\n+            final List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                (readForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                (filteredReadForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparator());\n+            strandReverse.sort(new ReadFeatherEndReverseComparator());\n+\n+            // Compute default likelihoods as normal (before we go ahead and alter the likelihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+            if (genotyperDebugStream != null) {\n+                likelihoodsCalculator.addGenotyperDebugOutputStream(genotyperDebugStream);\n+            }\n+\n+            // this is the data array for the read likelihoods without any trouble\n+            final LikelihoodMatrix<GATKRead, A> sampleLikelihoods = alleleLikelihoodMatrixMapper.mapAlleles(data.readLikelihoods().sampleMatrix(sampleIndex));\n+            final double[] ploidyModelGenotypeLikelihoods = likelihoodsCalculator.rawGenotypeLikelihoods(sampleLikelihoods);\n+\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"\\n Standard Genotyping Resutls:\");\n+                genotyperDebugStream.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+\n+            double[] BQDCallResults = null;\n+            double[] FRDCallResults = null;\n+            if (computeBQD) {\n+                BQDCallResults = likelihoodsCalculator.calculateBQDLikelihoods(sampleLikelihoods, strandForward, strandReverse, paddedReference, offsetForRefIntoEvent, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"BQD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(BQDCallResults));\n+                }\n+            }\n+            if (computeFRD) {\n+                FRDCallResults = likelihoodsCalculator.calculateFRDLikelihoods(sampleLikelihoods, ploidyModelGenotypeLikelihoods,\n+                        Stream.of(strandForward, strandReverse).flatMap(Collection::stream).collect(Collectors.toList()), // We filter out the HMM filtered reads as they do not apply to FRD\n+                        FLAT_SNP_HET_PRIOR, api, maxEffectiveDepthAdjustment, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"FRD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(FRDCallResults));\n+                }\n+            }\n+\n+            //make synthesized likelihoods object (NOTE that we can do this since for invalid model GT fields we simply infinity out the result in the array)\n+            for (int gt = 0; gt < ploidyModelGenotypeLikelihoods.length; gt++) {\n+                if (computeBQD) {\n+                    ploidyModelGenotypeLikelihoods[gt] = Math.max(ploidyModelGenotypeLikelihoods[gt], BQDCallResults[gt]);\n+                }\n+                if (computeFRD) {\n+                    ploidyModelGenotypeLikelihoods[gt] = Math.max(ploidyModelGenotypeLikelihoods[gt], FRDCallResults[gt]);\n+                }\n+            }\n+\n+            // this is what the work actually is, after we have computed a few things\n+            genotypeLikelihoods.add(GenotypeLikelihoods.fromLog10Likelihoods(ploidyModelGenotypeLikelihoods));\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"merged matrix:\");\n+                genotyperDebugStream.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+        }\n+        return new GenotypingLikelihoods<>(genotypingAlleles, ploidyModel, genotypeLikelihoods);\n+    }\n+\n+    // Adding the debug stream managed by the haplotype caller engine\n+    @Override\n+    public void addDebugOutStream(final PrintStream debugStream) {\n+        this.genotyperDebugStream = debugStream;\n+    }\n+\n+    private GenotypeLikelihoodCalculatorDRAGEN getLikelihoodsCalculator(final int samplePloidy, final int alleleCount) {\n+        if (samplePloidy >= cachePloidyCapacity || alleleCount >= cacheAlleleCountCapacity) {\n+            return calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+        }\n+        final GenotypeLikelihoodCalculatorDRAGEN result = likelihoodCalculators[samplePloidy][alleleCount];\n+        if (result != null) {\n+            return result;\n+        } else {\n+            final GenotypeLikelihoodCalculatorDRAGEN newOne = calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+            likelihoodCalculators[samplePloidy][alleleCount] = newOne;\n+            return newOne;\n+        }\n+    }\n+\n+    /**\n+     * This helper class is used to store the necessary data in order to sort a read based on its BQD \"feather end\" as\n+     * well as information relevant to re-associate the read with its position in the AlleleLikelihoods object arrays.\n+     */\n+    static class DragenReadContainer {\n+        final GATKRead underlyingRead;\n+        final int offsetIntoReadForBaseQuality;\n+        final int unclippedEnd;\n+        final int indexInLikelihoodsObject;\n+\n+        // Transient value used to store thresholds for FRD\n+       double phredPFValue = 0;\n+\n+\n+        public DragenReadContainer(final GATKRead underlyingRead, final int offsetIntoReadForBaseQuality, final int unclippedEnd, final int indexInLikelihoodsObject) {\n+            this.underlyingRead = underlyingRead;\n+            this.offsetIntoReadForBaseQuality = offsetIntoReadForBaseQuality;\n+            this.unclippedEnd = unclippedEnd;\n+            this.indexInLikelihoodsObject = indexInLikelihoodsObject;\n+        }\n+\n+        public int getUnclippedPosition() {\n+            return unclippedEnd;\n+        }\n+\n+        public int getIndexInLikelihoodsObject() {\n+            return indexInLikelihoodsObject;\n+        }\n+\n+        public boolean wasFilteredByHMM() {\n+            return this.getIndexInLikelihoodsObject() == -1;\n+        }\n+\n+        public boolean hasValidBaseQuality() {\n+            return offsetIntoReadForBaseQuality != -1;\n+        }\n+\n+        public int getBaseQuality() {\n+            return underlyingRead.getBaseQuality(offsetIntoReadForBaseQuality);\n+        }\n+\n+        public int getForwardsFeatherEnd() {\n+            return (underlyingRead.getSoftStart() - underlyingRead.getUnclippedStart()) + offsetIntoReadForBaseQuality;\n+        }\n+\n+        public int getReverseFeatherEnd() {\n+            return (underlyingRead.getUnclippedEnd() - underlyingRead.getSoftEnd()) + (underlyingRead.getLength() - offsetIntoReadForBaseQuality);\n+        }\n+\n+        public double getPhredScaledMappingQuality() {\n+            return DRAGENMappingQualityReadTransformer.mapMappingQualityToPhredLikelihoodScore(underlyingRead.getMappingQuality());\n+        }\n+\n+        public double getPhredPFValue() {\n+            return phredPFValue;\n+        }\n+\n+        public void setPhredPFValue(double phredPFValue) {\n+            this.phredPFValue = phredPFValue;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return String.format(\"Read: %s index: %d at unclipped end: %d with base quality %d\", underlyingRead.toString(), indexInLikelihoodsObject, unclippedEnd, (hasValidBaseQuality() ? getBaseQuality() : -1));\n+        }\n+\n+        public boolean isReverseStrand() {\n+            return underlyingRead.isReverseStrand();\n+        }\n+    }\n+\n+    //MAJOR TODO THIS IS CURRENTLY BASED OFF OF THE REFERENCE UNCLIPPED START AND NOT THE BASES IN THE READ CONSEQUENTLY AT SITES WITH\n+    //      TODO INDELS PRESENT WE ARE GOING TO BE LOOKING AT THE WRONG OFFSETS FOR THIS SORT... a minor issue but still...\n+    //UPDATE:APPARENTLY DRAGEN ONLY USES THE ORIGINAL BASE ALIGNMENT OFFSETS HERE (NO HARDCLIPPED BASES OR INDEL SITE HANDLING)\n+\n+    // Orders the reads based on the number of bases there are to the left of the fatherEndComparisonLocation as aligned according to the cigar\n+    // NOTE: here we compare the un-hardclipped edges for these reads as the model itself cares about the cycle count of the sequencer, and\n+    //       importantly this saves us having the thread the original alignment of these reads to this level, since by this point we have trimmed\n+    //       the reads twice, once to the active region with padding and again to the callable region within the active window and in both of these\n+    //       cases we have deleted bases with hardclips.\n+    public class ReadFeatherEndForwardComparator implements Comparator<DragenReadContainer>, Serializable {\n+        private static final long serialVersionUID = 1L;\n+        /**\n+         * Evaluate first the number of bases to the end of the read and follow that by the base quality\n+         */\n+        @Override\n+        public int compare(final DragenReadContainer read1, final DragenReadContainer read2) {\n+            //NOTE: here we want the reads to wind up in ascending order by unclipped position because the unclipped position should be on the left\n+\n+            int diffVal =  read2.getForwardsFeatherEnd() - read1.getForwardsFeatherEnd();\n+//\n+//            int diffVal = read1.getUnclippedPosition() - read2.getUnclippedPosition();\n+            if (diffVal == 0) {\n+                diffVal = (read1.hasValidBaseQuality() ? read1.getBaseQuality() : 0)\n+                        - (read2.hasValidBaseQuality() ? read2.getBaseQuality() : 0);\n+            }\n+            return diffVal;\n+        }\n+    }\n+\n+    // Orders the reads based on the number of bases in the read that occur before the fatherEndComparisonLocation as aligned according to the cigar\n+    // NOTE: here we compare the un-hardclipped edges for these reads as the model itself cares about the cycle count of the sequencer, and\n+    //       importantly this saves us having the thread the original alignment of these reads to this level, since by this point we have trimmed\n+    //       the reads twice, once to the active region with padding and again to the callable region within the active window and in both of these\n+    //       cases we have deleted bases with hardclips.\n+    public class ReadFeatherEndReverseComparator implements Comparator<DragenReadContainer>, Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 318}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTM1NzI5Ng==", "bodyText": "I see that there are many changes in protections. I guess this is due to the introduction of GenotypeLikekihoodsCalculatorDRAGEN. this one is the default package one some other are protected.... try to be consistent.... if this is needed I'm fine with it, just try to be consistent and the more restricted the better.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451357296", "createdAt": "2020-07-08T08:02:43Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculator.java", "diffHunk": "@@ -252,17 +229,26 @@ public GenotypeAlleleCounts genotypeAlleleCountsAt(final int index) {\n      * @return never {@code null}.\n      */\n     public <EVIDENCE, A extends Allele> GenotypeLikelihoods genotypeLikelihoods(final LikelihoodMatrix<EVIDENCE, A> likelihoods) {\n+        final double[] readLikelihoodsByGenotypeIndex = getReadRawReadLikelihoodsByGenotypeIndex(likelihoods);\n+        return GenotypeLikelihoods.fromLog10Likelihoods(readLikelihoodsByGenotypeIndex);\n+    }\n+\n+    /**\n+     * A helper method that actually does the matrix operations but returns the raw values.\n+     *\n+     * @return the raw array (in log10 likelihoods space) of the GL for each genotype\n+     */\n+    <EVIDENCE, A extends Allele> double[] getReadRawReadLikelihoodsByGenotypeIndex(final LikelihoodMatrix<EVIDENCE, A> likelihoods) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTM1ODgzMQ==", "bodyText": "Are we now storing log of log? ..... that does not work in general as LnLk are likely to be negative and log (x <0) is not defined.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451358831", "createdAt": "2020-07-08T08:05:21Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculator.java", "diffHunk": "@@ -252,17 +229,26 @@ public GenotypeAlleleCounts genotypeAlleleCountsAt(final int index) {\n      * @return never {@code null}.\n      */\n     public <EVIDENCE, A extends Allele> GenotypeLikelihoods genotypeLikelihoods(final LikelihoodMatrix<EVIDENCE, A> likelihoods) {\n+        final double[] readLikelihoodsByGenotypeIndex = getReadRawReadLikelihoodsByGenotypeIndex(likelihoods);\n+        return GenotypeLikelihoods.fromLog10Likelihoods(readLikelihoodsByGenotypeIndex);\n+    }\n+\n+    /**\n+     * A helper method that actually does the matrix operations but returns the raw values.\n+     *\n+     * @return the raw array (in log10 likelihoods space) of the GL for each genotype\n+     */\n+    <EVIDENCE, A extends Allele> double[] getReadRawReadLikelihoodsByGenotypeIndex(final LikelihoodMatrix<EVIDENCE, A> likelihoods) {\n         Utils.nonNull(likelihoods);\n         Utils.validateArg(likelihoods.numberOfAlleles() == alleleCount, \"mismatch between allele list and alleleCount\");\n         final int readCount = likelihoods.evidenceCount();\n         ensureReadCapacity(readCount);\n \n-        /// [x][y][z] = z * LnLk(Read_x | Allele_y)\n+        /// [x][y][z] = log(z * LnLk(Read_x | Allele_y))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTM2NDE2MQ==", "bodyText": "Space between 'for' and '('", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451364161", "createdAt": "2020-07-08T08:14:39Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,495 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    // Debug output stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTM2NTczMQ==", "bodyText": "Many of the local variables in this method can. be 'final', please add that modifier whenever possible.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451365731", "createdAt": "2020-07-08T08:17:29Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,495 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    // Debug output stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTM4MDk1Mg==", "bodyText": "is always better to multiply by -.1 than to divide by -10.0 (div is \"much\" slower than mult)", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451380952", "createdAt": "2020-07-08T08:42:22Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,495 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    // Debug output stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTM4MTU4Mg==", "bodyText": "Resolve the TODO please.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451381582", "createdAt": "2020-07-08T08:43:25Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,495 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    // Debug output stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTM4MzQzMA==", "bodyText": "cache the index in a local variable since you use the same expression so many times, The compiler cannot make assumptions as to how .size() will behave but you can.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451383430", "createdAt": "2020-07-08T08:46:17Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,495 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    // Debug output stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (genotyperDebugStream != null) {\n+            genotyperDebugStream.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTM4NTU4MQ==", "bodyText": "perhaps   + MathUtils.LOG10_ONE_HALF instead of  - MathUtils.log10(2);", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451385581", "createdAt": "2020-07-08T08:49:52Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,495 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    // Debug output stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (genotyperDebugStream != null) {\n+            genotyperDebugStream.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTM4ODAwMQ==", "bodyText": "Spaces around \"=\"", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r451388001", "createdAt": "2020-07-08T08:53:37Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,495 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    // Debug output stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (genotyperDebugStream != null) {\n+            genotyperDebugStream.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 204}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU3MDgzNQ==", "bodyText": "You have to test that the params are correct (e.g. non-null underlyingRead) since it is a public constructor.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456570835", "createdAt": "2020-07-17T17:15:47Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,336 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypingModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+    public static final double BQD_HOMOPOLYMER_PHRED_ADJUSTMENT_FACTOR = 5.0;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    //Debug stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream = null;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior to use for the alternate allele if it is an indel\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"No API from DRAGStrs found, falling back on snp het prior for indels\");\n+            }\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            final List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            final List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            final List<DragenReadContainer> strandForward = new ArrayList<>();\n+            final List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                (readForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                (filteredReadForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparator());\n+            strandReverse.sort(new ReadFeatherEndReverseComparator());\n+\n+            // Compute default likelihoods as normal (before we go ahead and alter the likelihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+            if (genotyperDebugStream != null) {\n+                likelihoodsCalculator.addGenotyperDebugOutputStream(genotyperDebugStream);\n+            }\n+\n+            // this is the data array for the read likelihoods without any trouble\n+            final LikelihoodMatrix<GATKRead, A> sampleLikelihoods = alleleLikelihoodMatrixMapper.mapAlleles(data.readLikelihoods().sampleMatrix(sampleIndex));\n+            final double[] ploidyModelGenotypeLikelihoods = likelihoodsCalculator.rawGenotypeLikelihoods(sampleLikelihoods);\n+\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"\\n Standard Genotyping Resutls:\");\n+                genotyperDebugStream.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+\n+            double[] BQDCallResults = null;\n+            double[] FRDCallResults = null;\n+            if (computeBQD) {\n+                BQDCallResults = likelihoodsCalculator.calculateBQDLikelihoods(sampleLikelihoods, strandForward, strandReverse, paddedReference, offsetForRefIntoEvent, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"BQD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(BQDCallResults));\n+                }\n+            }\n+            if (computeFRD) {\n+                FRDCallResults = likelihoodsCalculator.calculateFRDLikelihoods(sampleLikelihoods, ploidyModelGenotypeLikelihoods,\n+                        Stream.of(strandForward, strandReverse).flatMap(Collection::stream).collect(Collectors.toList()), // We filter out the HMM filtered reads as they do not apply to FRD\n+                        FLAT_SNP_HET_PRIOR, api, maxEffectiveDepthAdjustment, calculators);\n+                if (genotyperDebugStream != null) {\n+                    genotyperDebugStream.println(\"FRD results:\");\n+                    genotyperDebugStream.println(Arrays.toString(FRDCallResults));\n+                }\n+            }\n+\n+            //make synthesized likelihoods object (NOTE that we can do this since for invalid model GT fields we simply infinity out the result in the array)\n+            for (int gt = 0; gt < ploidyModelGenotypeLikelihoods.length; gt++) {\n+                if (computeBQD) {\n+                    ploidyModelGenotypeLikelihoods[gt] = Math.max(ploidyModelGenotypeLikelihoods[gt], BQDCallResults[gt]);\n+                }\n+                if (computeFRD) {\n+                    ploidyModelGenotypeLikelihoods[gt] = Math.max(ploidyModelGenotypeLikelihoods[gt], FRDCallResults[gt]);\n+                }\n+            }\n+\n+            // this is what the work actually is, after we have computed a few things\n+            genotypeLikelihoods.add(GenotypeLikelihoods.fromLog10Likelihoods(ploidyModelGenotypeLikelihoods));\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(\"merged matrix:\");\n+                genotyperDebugStream.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+        }\n+        return new GenotypingLikelihoods<>(genotypingAlleles, ploidyModel, genotypeLikelihoods);\n+    }\n+\n+    // Adding the debug stream managed by the haplotype caller engine\n+    @Override\n+    public void addDebugOutStream(final PrintStream debugStream) {\n+        this.genotyperDebugStream = debugStream;\n+    }\n+\n+    private GenotypeLikelihoodCalculatorDRAGEN getLikelihoodsCalculator(final int samplePloidy, final int alleleCount) {\n+        if (samplePloidy >= cachePloidyCapacity || alleleCount >= cacheAlleleCountCapacity) {\n+            return calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+        }\n+        final GenotypeLikelihoodCalculatorDRAGEN result = likelihoodCalculators[samplePloidy][alleleCount];\n+        if (result != null) {\n+            return result;\n+        } else {\n+            final GenotypeLikelihoodCalculatorDRAGEN newOne = calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+            likelihoodCalculators[samplePloidy][alleleCount] = newOne;\n+            return newOne;\n+        }\n+    }\n+\n+    /**\n+     * This helper class is used to store the necessary data in order to sort a read based on its BQD \"feather end\" as\n+     * well as information relevant to re-associate the read with its position in the AlleleLikelihoods object arrays.\n+     */\n+    static class DragenReadContainer {\n+        final GATKRead underlyingRead;\n+        final int offsetIntoReadForBaseQuality;\n+        final int unclippedEnd;\n+        final int indexInLikelihoodsObject;\n+\n+        // Transient value used to store thresholds for FRD\n+       double phredPFValue = 0;\n+\n+\n+        public DragenReadContainer(final GATKRead underlyingRead, final int offsetIntoReadForBaseQuality, final int unclippedEnd, final int indexInLikelihoodsObject) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5MjU0NA=="}, "originalCommit": {"oid": "0dc280500c06a7e156331618df782e2d0295feb9"}, "originalPosition": 227}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU3MTMwMQ==", "bodyText": "Remember to create an issue for this since is a mayor thing.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456571301", "createdAt": "2020-07-17T17:16:40Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/DRAGENGenotypesModel.java", "diffHunk": "@@ -0,0 +1,322 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.transformers.DRAGENMappingQualityReadTransformer;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleListPermutation;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrReferenceSTRs;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.read.ReadUtils;\n+\n+import java.io.PrintStream;\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * This is the DRAGEN-GATK genotyper model. This class manages the logic for likelihoods calculation between the IndependentSamplesGenotyperModel\n+ * and the two DRAGEN genotypes models (FRD and BQD).\n+ *\n+ * In order to access the FRD and BQD genotypes model simply initialize this class with the settings enabled, then call\n+ * {@link #calculateLikelihoods} to return a the modified likelihoods array with the relevant models applied. FRD and BQD\n+ * both work by computing alternate likelihoods scores for the array array corresponding to homozygous allele combinations\n+ * and choosing the best score for each element of the likelihoods array from among the different models. This has the net\n+ * effect of penalizing heterozygous allele combinations since they do not have their likelihoods modified.\n+ *\n+ */\n+public class DRAGENGenotypesModel implements GenotypingModel {\n+    private static final int DEFAULT_CACHE_PLOIDY_CAPACITY = 10;\n+    private static final int DEFAULT_CACHE_ALLELE_CAPACITY = 50;\n+    // Flat SNP het prior to use for genotyping\n+    public static final double FLAT_SNP_HET_PRIOR = 34.77;\n+    public static final double BQD_HOMOPOLYMER_PHRED_ADJUSTMENT_FACTOR = 5.0;\n+\n+    private final int cacheAlleleCountCapacity;\n+    private final int cachePloidyCapacity;\n+    private GenotypeLikelihoodCalculatorDRAGEN[][] likelihoodCalculators;\n+    private final GenotypeLikelihoodCalculators calculators;\n+    private final boolean computeBQD;\n+    private final boolean computeFRD;\n+    private final int allelePadding;\n+    private final int maxEffectiveDepthAdjustment;\n+    private final DragstrParams dragstrParams;\n+\n+    public DRAGENGenotypesModel(final boolean useBQDModel, final boolean useFRDModel, final int allelePadding, final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) { this(DEFAULT_CACHE_PLOIDY_CAPACITY, DEFAULT_CACHE_ALLELE_CAPACITY, useBQDModel, useFRDModel, allelePadding, maxEffectiveDepthAdjustment,  dragstrParams); }\n+\n+    /*\n+     *  Initialize model with given maximum allele count and ploidy for caching\n+     */\n+    public DRAGENGenotypesModel(final int calculatorCachePloidyCapacity, final int calculatorCacheAlleleCapacity,\n+                                final boolean useBQDModel, final boolean useFRDModel, final int allelePadding,\n+                                final int maxEffectiveDepthAdjustment, final DragstrParams dragstrParams) {\n+        cachePloidyCapacity = calculatorCachePloidyCapacity;\n+        cacheAlleleCountCapacity = calculatorCacheAlleleCapacity;\n+        likelihoodCalculators = new GenotypeLikelihoodCalculatorDRAGEN[calculatorCachePloidyCapacity][calculatorCacheAlleleCapacity];\n+        calculators = new GenotypeLikelihoodCalculators();\n+        this.computeBQD = useBQDModel;\n+        this.computeFRD = useFRDModel;\n+        this.allelePadding = allelePadding;\n+        this.maxEffectiveDepthAdjustment = maxEffectiveDepthAdjustment;\n+        this.dragstrParams = dragstrParams;\n+    }\n+\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+    public <A extends Allele> GenotypingLikelihoods<A> calculateLikelihoods(final AlleleList<A> genotypingAlleles, final GenotypingData<A> data, byte[] paddedReference, int offsetForRefIntoEvent, final DragstrReferenceSTRs dragstrs) {\n+        Utils.nonNull(genotypingAlleles, \"the allele cannot be null\");\n+        Utils.nonNull(data, \"the genotyping data cannot be null\");\n+\n+        //Get the prior to use for the alternate allele if it is an indel\n+        double api;\n+        if (dragstrs !=  null) {\n+            final int period = dragstrs.period(offsetForRefIntoEvent + 1 );\n+            final int repeats = dragstrs.repeatLength(offsetForRefIntoEvent + 1);\n+            api = dragstrParams.api(period, repeats);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"API found: \" + api + \" with period used: \" + period + \"  and repeats: \" + repeats);\n+            }\n+        } else {\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"No API from DRAGStrs found, falling back on snp het prior for indels\");\n+            }\n+            api = FLAT_SNP_HET_PRIOR;\n+        }\n+\n+\n+        final AlleleListPermutation<A> permutation = data.permutation(genotypingAlleles);\n+        final AlleleLikelihoodMatrixMapper<A> alleleLikelihoodMatrixMapper = new AlleleLikelihoodMatrixMapper(permutation);\n+\n+        final int sampleCount = data.numberOfSamples();\n+        final PloidyModel ploidyModel = data.ploidyModel();\n+        final List<GenotypeLikelihoods> genotypeLikelihoods = new ArrayList<>(sampleCount);\n+        final int alleleCount = genotypingAlleles.numberOfAlleles();\n+        final int variantOffset = data.readLikelihoods().getVariantCallingSubsetApplied().getStart() + allelePadding;\n+\n+        GenotypeLikelihoodCalculatorDRAGEN likelihoodsCalculator = getLikelihoodsCalculator(ploidyModel.samplePloidy(0), alleleCount); //TODO this needs to change\n+        for (int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++) {\n+\n+            ///////////////////////////////////////////////////////////////////////////\n+            ///// PREPROCESSING FOR BQD\n+            ///////////////////////////////////////////////////////////////////////////\n+\n+            // Separating the reads by their strand and sorting them appropriately.\n+            final List<GATKRead> readsForSample = data.readLikelihoods().sampleEvidence(sampleIndex);\n+            final List<GATKRead> hmmFilteredReadsForSample = data.readLikelihoods().filteredSampleEvidence(sampleIndex);\n+            // These objects are intended to store 3 things, the read, the inner (middle) int stores the offset into the read of the base in question, and the outer int stores the index of the read per sample\n+            final List<DragenReadContainer> strandForward = new ArrayList<>();\n+            final List<DragenReadContainer>  strandReverse = new ArrayList<>();\n+\n+            ////TODO reads with indels preceding the variant in question might have their cycle counts mismatched, its unclear whether dragen handles this case based on debug outputs\n+            for (int j = 0; j < readsForSample.size(); j++) {\n+                final GATKRead readForSample = readsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(readForSample, variantOffset).getLeft();\n+\n+                (readForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(readForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(readForSample), j));\n+            }\n+            for (int j = 0; j < hmmFilteredReadsForSample.size(); j++) {\n+                final GATKRead filteredReadForSample = hmmFilteredReadsForSample.get(j);\n+                final int indexForSnp = ReadUtils.getReadIndexForReferenceCoordinate(filteredReadForSample, variantOffset).getLeft();\n+\n+                (filteredReadForSample.isReverseStrand() ? strandReverse : strandForward)\n+                        .add(new DragenReadContainer(filteredReadForSample, indexForSnp, ReadUtils.getStrandedUnclippedStart(filteredReadForSample), -1));\n+            }\n+            strandForward.sort(new ReadFeatherEndForwardComparator());\n+            strandReverse.sort(new ReadFeatherEndReverseComparator());\n+\n+            // Compute default likelihoods as normal (before we go ahead and alter the likelihoods for the call)\n+            final int samplePloidy = ploidyModel.samplePloidy(sampleIndex);\n+\n+            // get a new likelihoodsCalculator if this sample's ploidy differs from the previous sample's\n+            if (samplePloidy != likelihoodsCalculator.ploidy()) {\n+                likelihoodsCalculator = getLikelihoodsCalculator(samplePloidy, alleleCount);\n+            }\n+\n+            // this is the data array for the read likelihoods without any trouble\n+            final LikelihoodMatrix<GATKRead, A> sampleLikelihoods = alleleLikelihoodMatrixMapper.mapAlleles(data.readLikelihoods().sampleMatrix(sampleIndex));\n+            final double[] ploidyModelGenotypeLikelihoods = likelihoodsCalculator.rawGenotypeLikelihoods(sampleLikelihoods);\n+\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"\\n Standard Genotyping Resutls:\");\n+                HaplotypeCallerGenotypingDebugger.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+\n+            double[] BQDCallResults = null;\n+            double[] FRDCallResults = null;\n+            if (computeBQD) {\n+                BQDCallResults = likelihoodsCalculator.calculateBQDLikelihoods(sampleLikelihoods, strandForward, strandReverse, paddedReference, offsetForRefIntoEvent, calculators);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"BQD results:\");\n+                    HaplotypeCallerGenotypingDebugger.println(Arrays.toString(BQDCallResults));\n+                }\n+            }\n+            if (computeFRD) {\n+                FRDCallResults = likelihoodsCalculator.calculateFRDLikelihoods(sampleLikelihoods, ploidyModelGenotypeLikelihoods,\n+                        Stream.of(strandForward, strandReverse).flatMap(Collection::stream).collect(Collectors.toList()), // We filter out the HMM filtered reads as they do not apply to FRD\n+                        FLAT_SNP_HET_PRIOR, api, maxEffectiveDepthAdjustment, calculators);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"FRD results:\");\n+                    HaplotypeCallerGenotypingDebugger.println(Arrays.toString(FRDCallResults));\n+                }\n+            }\n+\n+            //make synthesized likelihoods object (NOTE that we can do this since for invalid model GT fields we simply infinity out the result in the array)\n+            for (int gt = 0; gt < ploidyModelGenotypeLikelihoods.length; gt++) {\n+                if (computeBQD) {\n+                    ploidyModelGenotypeLikelihoods[gt] = Math.max(ploidyModelGenotypeLikelihoods[gt], BQDCallResults[gt]);\n+                }\n+                if (computeFRD) {\n+                    ploidyModelGenotypeLikelihoods[gt] = Math.max(ploidyModelGenotypeLikelihoods[gt], FRDCallResults[gt]);\n+                }\n+            }\n+\n+            // this is what the work actually is, after we have computed a few things\n+            genotypeLikelihoods.add(GenotypeLikelihoods.fromLog10Likelihoods(ploidyModelGenotypeLikelihoods));\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"merged matrix:\");\n+                HaplotypeCallerGenotypingDebugger.println(Arrays.toString(ploidyModelGenotypeLikelihoods));\n+            }\n+        }\n+        return new GenotypingLikelihoods<>(genotypingAlleles, ploidyModel, genotypeLikelihoods);\n+    }\n+\n+    private GenotypeLikelihoodCalculatorDRAGEN getLikelihoodsCalculator(final int samplePloidy, final int alleleCount) {\n+        if (samplePloidy >= cachePloidyCapacity || alleleCount >= cacheAlleleCountCapacity) {\n+            return calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+        }\n+        final GenotypeLikelihoodCalculatorDRAGEN result = likelihoodCalculators[samplePloidy][alleleCount];\n+        if (result != null) {\n+            return result;\n+        } else {\n+            final GenotypeLikelihoodCalculatorDRAGEN newOne = calculators.getInstanceDRAGEN(samplePloidy, alleleCount);\n+            likelihoodCalculators[samplePloidy][alleleCount] = newOne;\n+            return newOne;\n+        }\n+    }\n+\n+    /**\n+     * This helper class is used to store the necessary data in order to sort a read based on its BQD \"feather end\" as\n+     * well as information relevant to re-associate the read with its position in the AlleleLikelihoods object arrays.\n+     */\n+    static class DragenReadContainer {\n+        final GATKRead underlyingRead;\n+        final int offsetIntoReadForBaseQuality;\n+        final int unclippedEnd;\n+        final int indexInLikelihoodsObject;\n+\n+        // Transient value used to store thresholds for FRD\n+       double phredPFValue = 0;\n+\n+\n+        public DragenReadContainer(final GATKRead underlyingRead, final int offsetIntoReadForBaseQuality, final int unclippedEnd, final int indexInLikelihoodsObject) {\n+            this.underlyingRead = underlyingRead;\n+            this.offsetIntoReadForBaseQuality = offsetIntoReadForBaseQuality;\n+            this.unclippedEnd = unclippedEnd;\n+            this.indexInLikelihoodsObject = indexInLikelihoodsObject;\n+        }\n+\n+        public int getUnclippedPosition() {\n+            return unclippedEnd;\n+        }\n+\n+        public int getIndexInLikelihoodsObject() {\n+            return indexInLikelihoodsObject;\n+        }\n+\n+        public boolean wasFilteredByHMM() {\n+            return this.getIndexInLikelihoodsObject() == -1;\n+        }\n+\n+        public boolean hasValidBaseQuality() {\n+            return offsetIntoReadForBaseQuality != -1;\n+        }\n+\n+        public int getBaseQuality() {\n+            return underlyingRead.getBaseQuality(offsetIntoReadForBaseQuality);\n+        }\n+\n+        public int getForwardsFeatherEnd() {\n+            return (underlyingRead.getSoftStart() - underlyingRead.getUnclippedStart()) + offsetIntoReadForBaseQuality;\n+        }\n+\n+        public int getReverseFeatherEnd() {\n+            return (underlyingRead.getUnclippedEnd() - underlyingRead.getSoftEnd()) + (underlyingRead.getLength() - offsetIntoReadForBaseQuality);\n+        }\n+\n+        public double getPhredScaledMappingQuality() {\n+            return DRAGENMappingQualityReadTransformer.mapMappingQualityToPhredLikelihoodScore(underlyingRead.getMappingQuality());\n+        }\n+\n+        public double getPhredPFValue() {\n+            return phredPFValue;\n+        }\n+\n+        public void setPhredPFValue(double phredPFValue) {\n+            this.phredPFValue = phredPFValue;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return String.format(\"Read: %s index: %d at unclipped end: %d with base quality %d\", underlyingRead.toString(), indexInLikelihoodsObject, unclippedEnd, (hasValidBaseQuality() ? getBaseQuality() : -1));\n+        }\n+\n+        public boolean isReverseStrand() {\n+            return underlyingRead.isReverseStrand();\n+        }\n+    }\n+\n+    //MAJOR TODO THIS IS CURRENTLY BASED OFF OF THE REFERENCE UNCLIPPED START AND NOT THE BASES IN THE READ CONSEQUENTLY AT SITES WITH", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 273}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU3NDU0NQ==", "bodyText": "Hap.... should hap. (camelCaps for variables)", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456574545", "createdAt": "2020-07-17T17:23:08Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU3NTkxMg==", "bodyText": "I second @davidbenjamin comment I think we need to rethink this.... perhaps G..Calculators needs some refactoring or otherwise FRD and BQD Lk calculation should be done separately... the fact that this method consumes the \"raw\" lks that were calculated by another method in the same class is smelly.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456575912", "createdAt": "2020-07-17T17:26:01Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,495 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_DEFAULT_ALPHA = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoodsObject = null;\n+\n+    private final double cachedLog10Alpha;\n+    private final double cachedLog10AlphaInverse;\n+\n+    // Debug output stream to be managed by the HaplotypeCallerEngine\n+    private PrintStream genotyperDebugStream;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10Alpha = Math.log10(BQD_FIXED_DEFAULT_ALPHA);\n+        cachedLog10AlphaInverse = Math.log10(1 - BQD_FIXED_DEFAULT_ALPHA);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele liklihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        //TODO put a very stringent check that we have not invalidated the cache because we will be relying on it to get home in the storm\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoodsObject, \"There was a mismatch between the sample stored by the genotyper and the one requesed for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale liklihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (genotyperDebugStream != null) {\n+            genotyperDebugStream.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulative_p_R_for_E = new double[positionSortedReads.size() + 1];\n+        final double[] cumulative_mean_base_quality_phred_adjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulative_P_GT = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this seperately because not every read overlaps the SNP in quesiton due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulative_p_R_for_E.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10Alpha,\n+                                                       homozygousGenotypeContribution + cachedLog10AlphaInverse);\n+            cumulative_p_R_for_E[i] = cumulative_p_R_for_E[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulative_P_GT[i] = cumulative_P_GT[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulative_mean_base_quality_phred_adjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulative_mean_base_quality_phred_adjusted.length; n++) {\n+            final double bqdScore = cumulative_mean_base_quality_phred_adjusted[n] + cumulative_p_R_for_E[n] + (cumulative_P_GT[cumulative_P_GT.length-1] - cumulative_P_GT[n]);\n+            if (genotyperDebugStream != null) {\n+                genotyperDebugStream.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulative_mean_base_quality_phred_adjusted[n], cumulative_p_R_for_E[n],\n+                        (cumulative_P_GT[cumulative_P_GT.length - 1] - cumulative_P_GT[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (genotyperDebugStream != null) {\n+            genotyperDebugStream.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulative_mean_base_quality_phred_adjusted[nIndexUsed],\n+                    BQD_FIXED_DEFAULT_ALPHA, cumulative_p_R_for_E[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele liklihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA5NTIzOA=="}, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 254}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU3NjY1NA==", "bodyText": "Are we sure this will work? what is the lenght of '*' or ''?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456576654", "createdAt": "2020-07-17T17:27:40Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulativeMeanBaseQualityPhredAdjusted[nIndexUsed],\n+                    BQD_FIXED_ERROR_RATE, cumulativeProbReadForErrorAllele[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele likelihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> readContainers,\n+                                                               final double snipAprioriHet, final double indelAprioriHet, final int maxEffectiveDepthForHetAdjustment,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int fAlleleIndex = 0; fAlleleIndex < sampleLikelihoods.numberOfAlleles(); fAlleleIndex++) {\n+            // ignore symbolic alleles\n+            final boolean isIndel = sampleLikelihoods.getAllele(fAlleleIndex).length() != refAllele.length();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 268}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU3NzU1Nw==", "bodyText": "space around '('", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456577557", "createdAt": "2020-07-17T17:29:35Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulativeMeanBaseQualityPhredAdjusted[nIndexUsed],\n+                    BQD_FIXED_ERROR_RATE, cumulativeProbReadForErrorAllele[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele likelihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> readContainers,\n+                                                               final double snipAprioriHet, final double indelAprioriHet, final int maxEffectiveDepthForHetAdjustment,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int fAlleleIndex = 0; fAlleleIndex < sampleLikelihoods.numberOfAlleles(); fAlleleIndex++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 266}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU3Nzc1Ng==", "bodyText": "This offset could be managed in the for header avoiding multiplications.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456577756", "createdAt": "2020-07-17T17:29:59Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulativeMeanBaseQualityPhredAdjusted[nIndexUsed],\n+                    BQD_FIXED_ERROR_RATE, cumulativeProbReadForErrorAllele[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele likelihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> readContainers,\n+                                                               final double snipAprioriHet, final double indelAprioriHet, final int maxEffectiveDepthForHetAdjustment,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int fAlleleIndex = 0; fAlleleIndex < sampleLikelihoods.numberOfAlleles(); fAlleleIndex++) {\n+            // ignore symbolic alleles\n+            final boolean isIndel = sampleLikelihoods.getAllele(fAlleleIndex).length() != refAllele.length();\n+            final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * fAlleleIndex + readCount;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 269}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU3ODM1Ng==", "bodyText": "multiply by '-0.1' rather than divide by -10.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456578356", "createdAt": "2020-07-17T17:31:14Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulativeMeanBaseQualityPhredAdjusted[nIndexUsed],\n+                    BQD_FIXED_ERROR_RATE, cumulativeProbReadForErrorAllele[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele likelihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> readContainers,\n+                                                               final double snipAprioriHet, final double indelAprioriHet, final int maxEffectiveDepthForHetAdjustment,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int fAlleleIndex = 0; fAlleleIndex < sampleLikelihoods.numberOfAlleles(); fAlleleIndex++) {\n+            // ignore symbolic alleles\n+            final boolean isIndel = sampleLikelihoods.getAllele(fAlleleIndex).length() != refAllele.length();\n+            final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * fAlleleIndex + readCount;\n+\n+            // Here we generate a set of the critical log10(P(F)) values that we will iterate over\n+            final Set<Double> criticalThresholdsForward = new HashSet<>();\n+            final Set<Double> criticalThresholdsReverse = new HashSet<>();\n+            final Set<Double> criticalThresholds = new HashSet<>();\n+            computeCriticalValues(criticalThresholdsForward, criticalThresholdsReverse, criticalThresholds, readContainers, fAlleleIndex == 0 ? 0 : (isIndel? indelAprioriHet : snipAprioriHet)/-10); // simplified in line with DRAGEN, uses 1 alleledist for both snp and indels", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 275}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU3OTUxMQ==", "bodyText": "can this be final?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456579511", "createdAt": "2020-07-17T17:33:36Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulativeMeanBaseQualityPhredAdjusted[nIndexUsed],\n+                    BQD_FIXED_ERROR_RATE, cumulativeProbReadForErrorAllele[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele likelihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> readContainers,\n+                                                               final double snipAprioriHet, final double indelAprioriHet, final int maxEffectiveDepthForHetAdjustment,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int fAlleleIndex = 0; fAlleleIndex < sampleLikelihoods.numberOfAlleles(); fAlleleIndex++) {\n+            // ignore symbolic alleles\n+            final boolean isIndel = sampleLikelihoods.getAllele(fAlleleIndex).length() != refAllele.length();\n+            final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * fAlleleIndex + readCount;\n+\n+            // Here we generate a set of the critical log10(P(F)) values that we will iterate over\n+            final Set<Double> criticalThresholdsForward = new HashSet<>();\n+            final Set<Double> criticalThresholdsReverse = new HashSet<>();\n+            final Set<Double> criticalThresholds = new HashSet<>();\n+            computeCriticalValues(criticalThresholdsForward, criticalThresholdsReverse, criticalThresholds, readContainers, fAlleleIndex == 0 ? 0 : (isIndel? indelAprioriHet : snipAprioriHet)/-10); // simplified in line with DRAGEN, uses 1 alleledist for both snp and indels\n+            final List<Double> criticalThresholdsSortedForward = criticalThresholdsForward.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSortedReverse = criticalThresholdsReverse.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSorted = criticalThresholds.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"fIndex: \" + fAlleleIndex + \" criticalValues: \\n\" + criticalThresholds.stream().map(d -> Double.toString(d)).collect(Collectors.joining(\"\\n\")));\n+            }\n+            // iterate over all of the homozygous genotypes for the given allele\n+            for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+                // Skip over the allele corresponding to the \"foreign\" allele\n+                if (gtAlleleIndex == fAlleleIndex) {\n+                    continue;\n+                }\n+                // For right now we allow symbolic alleles, but this might be subject to change\n+//                if (sampleLikelihoods.getAllele(fAlleleIndex).isSymbolic() ) {\n+//                    continue;\n+//                }\n+\n+                //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+                //This should be pulled off as a calculator in some genotyping class.\n+                final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+                double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 296}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4MDcyNw==", "bodyText": "What is the intended effect of multiplying by \"1.0\" i.e. \"1\"?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456580727", "createdAt": "2020-07-17T17:36:00Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulativeMeanBaseQualityPhredAdjusted[nIndexUsed],\n+                    BQD_FIXED_ERROR_RATE, cumulativeProbReadForErrorAllele[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele likelihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> readContainers,\n+                                                               final double snipAprioriHet, final double indelAprioriHet, final int maxEffectiveDepthForHetAdjustment,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int fAlleleIndex = 0; fAlleleIndex < sampleLikelihoods.numberOfAlleles(); fAlleleIndex++) {\n+            // ignore symbolic alleles\n+            final boolean isIndel = sampleLikelihoods.getAllele(fAlleleIndex).length() != refAllele.length();\n+            final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * fAlleleIndex + readCount;\n+\n+            // Here we generate a set of the critical log10(P(F)) values that we will iterate over\n+            final Set<Double> criticalThresholdsForward = new HashSet<>();\n+            final Set<Double> criticalThresholdsReverse = new HashSet<>();\n+            final Set<Double> criticalThresholds = new HashSet<>();\n+            computeCriticalValues(criticalThresholdsForward, criticalThresholdsReverse, criticalThresholds, readContainers, fAlleleIndex == 0 ? 0 : (isIndel? indelAprioriHet : snipAprioriHet)/-10); // simplified in line with DRAGEN, uses 1 alleledist for both snp and indels\n+            final List<Double> criticalThresholdsSortedForward = criticalThresholdsForward.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSortedReverse = criticalThresholdsReverse.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSorted = criticalThresholds.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"fIndex: \" + fAlleleIndex + \" criticalValues: \\n\" + criticalThresholds.stream().map(d -> Double.toString(d)).collect(Collectors.joining(\"\\n\")));\n+            }\n+            // iterate over all of the homozygous genotypes for the given allele\n+            for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+                // Skip over the allele corresponding to the \"foreign\" allele\n+                if (gtAlleleIndex == fAlleleIndex) {\n+                    continue;\n+                }\n+                // For right now we allow symbolic alleles, but this might be subject to change\n+//                if (sampleLikelihoods.getAllele(fAlleleIndex).isSymbolic() ) {\n+//                    continue;\n+//                }\n+\n+                //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+                //This should be pulled off as a calculator in some genotyping class.\n+                final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+                double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+                // TODO restore the critical thresholds\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"indexForGT \"+indexForGT+ \" ooffsetForReadLikelihoodGivenAlleleIndex =\"+offsetForReadLikelihoodGivenAlleleIndex);\n+                    HaplotypeCallerGenotypingDebugger.println(\"\\nForwards Strands: \");\n+                }\n+                final double[] maxLog10FForwardsStrand = computeFRDModelForStrandData(readContainers, c -> !c.isReverseStrand() , offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nReverse Strands: \");}\n+                final double[] maxLog10FReverseStrand = computeFRDModelForStrandData(readContainers, c -> c.isReverseStrand(), offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nBoth Strands: \");}\n+                final double[] maxLog10FBothStrands = computeFRDModelForStrandData(readContainers, c -> true, offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"gtAlleleIndex : \"+gtAlleleIndex+ \" fAlleleIndex: \"+fAlleleIndex +\" forwards: \"+maxLog10FForwardsStrand+\" reverse: \"+maxLog10FReverseStrand+\" both: \"+maxLog10FBothStrands);\n+                }\n+                double[] localBestModel = maxLog10FForwardsStrand;\n+                if (localBestModel[0] < maxLog10FReverseStrand[0]) {\n+                    localBestModel = maxLog10FReverseStrand;\n+                }\n+                if (localBestModel[0] < maxLog10FBothStrands[0]) {\n+                    localBestModel = maxLog10FBothStrands;\n+                }\n+\n+                // Handle max effective depth adjustment if specified\n+                if (maxEffectiveDepthForHetAdjustment > 0) {\n+                    // Use the index corresponding the mixture of F and\n+                    double localBestModelScore = localBestModel[0] - localBestModel[1];\n+                    int closestGTAlleleIndex = allelesToIndex(gtAlleleIndex, fAlleleIndex);\n+                    double log10LikelihoodsForPloyidyModel = ploidyModelLikelihoods[closestGTAlleleIndex] - MathUtils.log10(2);\n+                    int depthForGenotyping = sampleLikelihoods.evidenceCount();\n+                    double adjustedBestModel = log10LikelihoodsForPloyidyModel + ((localBestModelScore - log10LikelihoodsForPloyidyModel)\n+                            * ((Math.min(depthForGenotyping, maxEffectiveDepthForHetAdjustment) * 1.0) / depthForGenotyping));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 328}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4MjM2Mw==", "bodyText": "Use a constant better.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456582363", "createdAt": "2020-07-17T17:39:23Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulativeMeanBaseQualityPhredAdjusted[nIndexUsed],\n+                    BQD_FIXED_ERROR_RATE, cumulativeProbReadForErrorAllele[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele likelihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> readContainers,\n+                                                               final double snipAprioriHet, final double indelAprioriHet, final int maxEffectiveDepthForHetAdjustment,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int fAlleleIndex = 0; fAlleleIndex < sampleLikelihoods.numberOfAlleles(); fAlleleIndex++) {\n+            // ignore symbolic alleles\n+            final boolean isIndel = sampleLikelihoods.getAllele(fAlleleIndex).length() != refAllele.length();\n+            final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * fAlleleIndex + readCount;\n+\n+            // Here we generate a set of the critical log10(P(F)) values that we will iterate over\n+            final Set<Double> criticalThresholdsForward = new HashSet<>();\n+            final Set<Double> criticalThresholdsReverse = new HashSet<>();\n+            final Set<Double> criticalThresholds = new HashSet<>();\n+            computeCriticalValues(criticalThresholdsForward, criticalThresholdsReverse, criticalThresholds, readContainers, fAlleleIndex == 0 ? 0 : (isIndel? indelAprioriHet : snipAprioriHet)/-10); // simplified in line with DRAGEN, uses 1 alleledist for both snp and indels\n+            final List<Double> criticalThresholdsSortedForward = criticalThresholdsForward.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSortedReverse = criticalThresholdsReverse.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSorted = criticalThresholds.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"fIndex: \" + fAlleleIndex + \" criticalValues: \\n\" + criticalThresholds.stream().map(d -> Double.toString(d)).collect(Collectors.joining(\"\\n\")));\n+            }\n+            // iterate over all of the homozygous genotypes for the given allele\n+            for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+                // Skip over the allele corresponding to the \"foreign\" allele\n+                if (gtAlleleIndex == fAlleleIndex) {\n+                    continue;\n+                }\n+                // For right now we allow symbolic alleles, but this might be subject to change\n+//                if (sampleLikelihoods.getAllele(fAlleleIndex).isSymbolic() ) {\n+//                    continue;\n+//                }\n+\n+                //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+                //This should be pulled off as a calculator in some genotyping class.\n+                final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+                double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+                // TODO restore the critical thresholds\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"indexForGT \"+indexForGT+ \" ooffsetForReadLikelihoodGivenAlleleIndex =\"+offsetForReadLikelihoodGivenAlleleIndex);\n+                    HaplotypeCallerGenotypingDebugger.println(\"\\nForwards Strands: \");\n+                }\n+                final double[] maxLog10FForwardsStrand = computeFRDModelForStrandData(readContainers, c -> !c.isReverseStrand() , offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nReverse Strands: \");}\n+                final double[] maxLog10FReverseStrand = computeFRDModelForStrandData(readContainers, c -> c.isReverseStrand(), offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nBoth Strands: \");}\n+                final double[] maxLog10FBothStrands = computeFRDModelForStrandData(readContainers, c -> true, offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"gtAlleleIndex : \"+gtAlleleIndex+ \" fAlleleIndex: \"+fAlleleIndex +\" forwards: \"+maxLog10FForwardsStrand+\" reverse: \"+maxLog10FReverseStrand+\" both: \"+maxLog10FBothStrands);\n+                }\n+                double[] localBestModel = maxLog10FForwardsStrand;\n+                if (localBestModel[0] < maxLog10FReverseStrand[0]) {\n+                    localBestModel = maxLog10FReverseStrand;\n+                }\n+                if (localBestModel[0] < maxLog10FBothStrands[0]) {\n+                    localBestModel = maxLog10FBothStrands;\n+                }\n+\n+                // Handle max effective depth adjustment if specified\n+                if (maxEffectiveDepthForHetAdjustment > 0) {\n+                    // Use the index corresponding the mixture of F and\n+                    double localBestModelScore = localBestModel[0] - localBestModel[1];\n+                    int closestGTAlleleIndex = allelesToIndex(gtAlleleIndex, fAlleleIndex);\n+                    double log10LikelihoodsForPloyidyModel = ploidyModelLikelihoods[closestGTAlleleIndex] - MathUtils.log10(2);\n+                    int depthForGenotyping = sampleLikelihoods.evidenceCount();\n+                    double adjustedBestModel = log10LikelihoodsForPloyidyModel + ((localBestModelScore - log10LikelihoodsForPloyidyModel)\n+                            * ((Math.min(depthForGenotyping, maxEffectiveDepthForHetAdjustment) * 1.0) / depthForGenotyping));\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], adjustedBestModel + localBestModel[1]);\n+\n+                    if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                        HaplotypeCallerGenotypingDebugger.println(\"best FRD likelihoods: \"+localBestModelScore+\" P(F) score used: \"+localBestModel[1]+\"  use MaxEffectiveDepth: \"+maxEffectiveDepthForHetAdjustment);\n+                        HaplotypeCallerGenotypingDebugger.println(\"Using array index \"+closestGTAlleleIndex+\" for mixture gt with likelihood of \"+log10LikelihoodsForPloyidyModel+\" adjusted based on depth: \"+depthForGenotyping);\n+                        HaplotypeCallerGenotypingDebugger.println(\"p_rG_adj : \"+adjustedBestModel);\n+                    }\n+                } else {\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], localBestModel[0]);\n+                }\n+\n+            }\n+\n+        }\n+\n+\n+        return outputArray;\n+    }\n+\n+\n+    /**\n+     * @param positionSortedReads read containers to use for genotyping\n+     * @param predicate predicate used to select the correct orientation combination for reads when genotyping\n+     * @param offsetForReadLikelihoodGivenAlleleIndex offset corresponding to the Error Allele in the reads likelihoods object array\n+     * @param readLikelihoodsForGT reads likelihoods for Genotype array table\n+     * @param criticalThresholdsSorted critical thresholds to use for this orientation combination\n+     * @return two doubles, index 0 is the frd score and the second is log p(F()) score used to adjust the score\n+     */\n+    private double[] computeFRDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads, final Predicate<DRAGENGenotypesModel.DragenReadContainer> predicate,\n+                                                  final int offsetForReadLikelihoodGivenAlleleIndex, final double[] readLikelihoodsForGT, final List<Double> criticalThresholdsSorted) {\n+        if (positionSortedReads.isEmpty()) {\n+            return new double[]{Double.NEGATIVE_INFINITY, 0};\n+        }\n+\n+        int counter = 0;\n+        double maxLpspi = Double.NEGATIVE_INFINITY;\n+        double lpfApplied = 0;\n+\n+        for (final Double logProbFAllele : criticalThresholdsSorted) {\n+            double fAlleleProbRatio = 0.0;\n+            double fAlleleProbDenom = 0.0;\n+            double localMaxLpspi = Double.NEGATIVE_INFINITY;\n+\n+            // iterate over the reads to compute the foreign allele alpha to use for genotyping with FRD\n+            for (final DRAGENGenotypesModel.DragenReadContainer container : positionSortedReads) {\n+                // Ignore reads that were disqualified by the HMM\n+                if (container.wasFilteredByHMM()) {\n+                    continue;\n+                }\n+\n+                final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+                // Keep track of the aggregate support for the foreign allele\n+                if (predicate.test(container)) {\n+                    // Only include reads with mapping quality adjustment < the critical threshold being used (i.e. exclude reads with MQ > than the threshold)\n+                    double LPd_r_F = container.getPhredPFValue() + 0.0000001 <= logProbFAllele ?\n+                            Double.NEGATIVE_INFINITY :\n+                            readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+                    double lp_r_GT = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 387}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4MzQ2Nw==", "bodyText": "perhaps the filtering should be done before calling this method?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456583467", "createdAt": "2020-07-17T17:41:37Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulativeMeanBaseQualityPhredAdjusted[nIndexUsed],\n+                    BQD_FIXED_ERROR_RATE, cumulativeProbReadForErrorAllele[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele likelihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> readContainers,\n+                                                               final double snipAprioriHet, final double indelAprioriHet, final int maxEffectiveDepthForHetAdjustment,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int fAlleleIndex = 0; fAlleleIndex < sampleLikelihoods.numberOfAlleles(); fAlleleIndex++) {\n+            // ignore symbolic alleles\n+            final boolean isIndel = sampleLikelihoods.getAllele(fAlleleIndex).length() != refAllele.length();\n+            final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * fAlleleIndex + readCount;\n+\n+            // Here we generate a set of the critical log10(P(F)) values that we will iterate over\n+            final Set<Double> criticalThresholdsForward = new HashSet<>();\n+            final Set<Double> criticalThresholdsReverse = new HashSet<>();\n+            final Set<Double> criticalThresholds = new HashSet<>();\n+            computeCriticalValues(criticalThresholdsForward, criticalThresholdsReverse, criticalThresholds, readContainers, fAlleleIndex == 0 ? 0 : (isIndel? indelAprioriHet : snipAprioriHet)/-10); // simplified in line with DRAGEN, uses 1 alleledist for both snp and indels\n+            final List<Double> criticalThresholdsSortedForward = criticalThresholdsForward.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSortedReverse = criticalThresholdsReverse.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSorted = criticalThresholds.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"fIndex: \" + fAlleleIndex + \" criticalValues: \\n\" + criticalThresholds.stream().map(d -> Double.toString(d)).collect(Collectors.joining(\"\\n\")));\n+            }\n+            // iterate over all of the homozygous genotypes for the given allele\n+            for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+                // Skip over the allele corresponding to the \"foreign\" allele\n+                if (gtAlleleIndex == fAlleleIndex) {\n+                    continue;\n+                }\n+                // For right now we allow symbolic alleles, but this might be subject to change\n+//                if (sampleLikelihoods.getAllele(fAlleleIndex).isSymbolic() ) {\n+//                    continue;\n+//                }\n+\n+                //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+                //This should be pulled off as a calculator in some genotyping class.\n+                final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+                double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+                // TODO restore the critical thresholds\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"indexForGT \"+indexForGT+ \" ooffsetForReadLikelihoodGivenAlleleIndex =\"+offsetForReadLikelihoodGivenAlleleIndex);\n+                    HaplotypeCallerGenotypingDebugger.println(\"\\nForwards Strands: \");\n+                }\n+                final double[] maxLog10FForwardsStrand = computeFRDModelForStrandData(readContainers, c -> !c.isReverseStrand() , offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nReverse Strands: \");}\n+                final double[] maxLog10FReverseStrand = computeFRDModelForStrandData(readContainers, c -> c.isReverseStrand(), offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nBoth Strands: \");}\n+                final double[] maxLog10FBothStrands = computeFRDModelForStrandData(readContainers, c -> true, offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"gtAlleleIndex : \"+gtAlleleIndex+ \" fAlleleIndex: \"+fAlleleIndex +\" forwards: \"+maxLog10FForwardsStrand+\" reverse: \"+maxLog10FReverseStrand+\" both: \"+maxLog10FBothStrands);\n+                }\n+                double[] localBestModel = maxLog10FForwardsStrand;\n+                if (localBestModel[0] < maxLog10FReverseStrand[0]) {\n+                    localBestModel = maxLog10FReverseStrand;\n+                }\n+                if (localBestModel[0] < maxLog10FBothStrands[0]) {\n+                    localBestModel = maxLog10FBothStrands;\n+                }\n+\n+                // Handle max effective depth adjustment if specified\n+                if (maxEffectiveDepthForHetAdjustment > 0) {\n+                    // Use the index corresponding the mixture of F and\n+                    double localBestModelScore = localBestModel[0] - localBestModel[1];\n+                    int closestGTAlleleIndex = allelesToIndex(gtAlleleIndex, fAlleleIndex);\n+                    double log10LikelihoodsForPloyidyModel = ploidyModelLikelihoods[closestGTAlleleIndex] - MathUtils.log10(2);\n+                    int depthForGenotyping = sampleLikelihoods.evidenceCount();\n+                    double adjustedBestModel = log10LikelihoodsForPloyidyModel + ((localBestModelScore - log10LikelihoodsForPloyidyModel)\n+                            * ((Math.min(depthForGenotyping, maxEffectiveDepthForHetAdjustment) * 1.0) / depthForGenotyping));\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], adjustedBestModel + localBestModel[1]);\n+\n+                    if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                        HaplotypeCallerGenotypingDebugger.println(\"best FRD likelihoods: \"+localBestModelScore+\" P(F) score used: \"+localBestModel[1]+\"  use MaxEffectiveDepth: \"+maxEffectiveDepthForHetAdjustment);\n+                        HaplotypeCallerGenotypingDebugger.println(\"Using array index \"+closestGTAlleleIndex+\" for mixture gt with likelihood of \"+log10LikelihoodsForPloyidyModel+\" adjusted based on depth: \"+depthForGenotyping);\n+                        HaplotypeCallerGenotypingDebugger.println(\"p_rG_adj : \"+adjustedBestModel);\n+                    }\n+                } else {\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], localBestModel[0]);\n+                }\n+\n+            }\n+\n+        }\n+\n+\n+        return outputArray;\n+    }\n+\n+\n+    /**\n+     * @param positionSortedReads read containers to use for genotyping\n+     * @param predicate predicate used to select the correct orientation combination for reads when genotyping\n+     * @param offsetForReadLikelihoodGivenAlleleIndex offset corresponding to the Error Allele in the reads likelihoods object array\n+     * @param readLikelihoodsForGT reads likelihoods for Genotype array table\n+     * @param criticalThresholdsSorted critical thresholds to use for this orientation combination\n+     * @return two doubles, index 0 is the frd score and the second is log p(F()) score used to adjust the score\n+     */\n+    private double[] computeFRDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads, final Predicate<DRAGENGenotypesModel.DragenReadContainer> predicate,\n+                                                  final int offsetForReadLikelihoodGivenAlleleIndex, final double[] readLikelihoodsForGT, final List<Double> criticalThresholdsSorted) {\n+        if (positionSortedReads.isEmpty()) {\n+            return new double[]{Double.NEGATIVE_INFINITY, 0};\n+        }\n+\n+        int counter = 0;\n+        double maxLpspi = Double.NEGATIVE_INFINITY;\n+        double lpfApplied = 0;\n+\n+        for (final Double logProbFAllele : criticalThresholdsSorted) {\n+            double fAlleleProbRatio = 0.0;\n+            double fAlleleProbDenom = 0.0;\n+            double localMaxLpspi = Double.NEGATIVE_INFINITY;\n+\n+            // iterate over the reads to compute the foreign allele alpha to use for genotyping with FRD\n+            for (final DRAGENGenotypesModel.DragenReadContainer container : positionSortedReads) {\n+                // Ignore reads that were disqualified by the HMM\n+                if (container.wasFilteredByHMM()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 375}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NTI1OA==", "bodyText": "Same, I guess yould could combine these two filters in one if and perhaps filter things outside this method.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456585258", "createdAt": "2020-07-17T17:45:13Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulativeMeanBaseQualityPhredAdjusted[nIndexUsed],\n+                    BQD_FIXED_ERROR_RATE, cumulativeProbReadForErrorAllele[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele likelihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> readContainers,\n+                                                               final double snipAprioriHet, final double indelAprioriHet, final int maxEffectiveDepthForHetAdjustment,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int fAlleleIndex = 0; fAlleleIndex < sampleLikelihoods.numberOfAlleles(); fAlleleIndex++) {\n+            // ignore symbolic alleles\n+            final boolean isIndel = sampleLikelihoods.getAllele(fAlleleIndex).length() != refAllele.length();\n+            final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * fAlleleIndex + readCount;\n+\n+            // Here we generate a set of the critical log10(P(F)) values that we will iterate over\n+            final Set<Double> criticalThresholdsForward = new HashSet<>();\n+            final Set<Double> criticalThresholdsReverse = new HashSet<>();\n+            final Set<Double> criticalThresholds = new HashSet<>();\n+            computeCriticalValues(criticalThresholdsForward, criticalThresholdsReverse, criticalThresholds, readContainers, fAlleleIndex == 0 ? 0 : (isIndel? indelAprioriHet : snipAprioriHet)/-10); // simplified in line with DRAGEN, uses 1 alleledist for both snp and indels\n+            final List<Double> criticalThresholdsSortedForward = criticalThresholdsForward.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSortedReverse = criticalThresholdsReverse.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSorted = criticalThresholds.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"fIndex: \" + fAlleleIndex + \" criticalValues: \\n\" + criticalThresholds.stream().map(d -> Double.toString(d)).collect(Collectors.joining(\"\\n\")));\n+            }\n+            // iterate over all of the homozygous genotypes for the given allele\n+            for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+                // Skip over the allele corresponding to the \"foreign\" allele\n+                if (gtAlleleIndex == fAlleleIndex) {\n+                    continue;\n+                }\n+                // For right now we allow symbolic alleles, but this might be subject to change\n+//                if (sampleLikelihoods.getAllele(fAlleleIndex).isSymbolic() ) {\n+//                    continue;\n+//                }\n+\n+                //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+                //This should be pulled off as a calculator in some genotyping class.\n+                final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+                double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+                // TODO restore the critical thresholds\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"indexForGT \"+indexForGT+ \" ooffsetForReadLikelihoodGivenAlleleIndex =\"+offsetForReadLikelihoodGivenAlleleIndex);\n+                    HaplotypeCallerGenotypingDebugger.println(\"\\nForwards Strands: \");\n+                }\n+                final double[] maxLog10FForwardsStrand = computeFRDModelForStrandData(readContainers, c -> !c.isReverseStrand() , offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nReverse Strands: \");}\n+                final double[] maxLog10FReverseStrand = computeFRDModelForStrandData(readContainers, c -> c.isReverseStrand(), offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nBoth Strands: \");}\n+                final double[] maxLog10FBothStrands = computeFRDModelForStrandData(readContainers, c -> true, offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"gtAlleleIndex : \"+gtAlleleIndex+ \" fAlleleIndex: \"+fAlleleIndex +\" forwards: \"+maxLog10FForwardsStrand+\" reverse: \"+maxLog10FReverseStrand+\" both: \"+maxLog10FBothStrands);\n+                }\n+                double[] localBestModel = maxLog10FForwardsStrand;\n+                if (localBestModel[0] < maxLog10FReverseStrand[0]) {\n+                    localBestModel = maxLog10FReverseStrand;\n+                }\n+                if (localBestModel[0] < maxLog10FBothStrands[0]) {\n+                    localBestModel = maxLog10FBothStrands;\n+                }\n+\n+                // Handle max effective depth adjustment if specified\n+                if (maxEffectiveDepthForHetAdjustment > 0) {\n+                    // Use the index corresponding the mixture of F and\n+                    double localBestModelScore = localBestModel[0] - localBestModel[1];\n+                    int closestGTAlleleIndex = allelesToIndex(gtAlleleIndex, fAlleleIndex);\n+                    double log10LikelihoodsForPloyidyModel = ploidyModelLikelihoods[closestGTAlleleIndex] - MathUtils.log10(2);\n+                    int depthForGenotyping = sampleLikelihoods.evidenceCount();\n+                    double adjustedBestModel = log10LikelihoodsForPloyidyModel + ((localBestModelScore - log10LikelihoodsForPloyidyModel)\n+                            * ((Math.min(depthForGenotyping, maxEffectiveDepthForHetAdjustment) * 1.0) / depthForGenotyping));\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], adjustedBestModel + localBestModel[1]);\n+\n+                    if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                        HaplotypeCallerGenotypingDebugger.println(\"best FRD likelihoods: \"+localBestModelScore+\" P(F) score used: \"+localBestModel[1]+\"  use MaxEffectiveDepth: \"+maxEffectiveDepthForHetAdjustment);\n+                        HaplotypeCallerGenotypingDebugger.println(\"Using array index \"+closestGTAlleleIndex+\" for mixture gt with likelihood of \"+log10LikelihoodsForPloyidyModel+\" adjusted based on depth: \"+depthForGenotyping);\n+                        HaplotypeCallerGenotypingDebugger.println(\"p_rG_adj : \"+adjustedBestModel);\n+                    }\n+                } else {\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], localBestModel[0]);\n+                }\n+\n+            }\n+\n+        }\n+\n+\n+        return outputArray;\n+    }\n+\n+\n+    /**\n+     * @param positionSortedReads read containers to use for genotyping\n+     * @param predicate predicate used to select the correct orientation combination for reads when genotyping\n+     * @param offsetForReadLikelihoodGivenAlleleIndex offset corresponding to the Error Allele in the reads likelihoods object array\n+     * @param readLikelihoodsForGT reads likelihoods for Genotype array table\n+     * @param criticalThresholdsSorted critical thresholds to use for this orientation combination\n+     * @return two doubles, index 0 is the frd score and the second is log p(F()) score used to adjust the score\n+     */\n+    private double[] computeFRDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads, final Predicate<DRAGENGenotypesModel.DragenReadContainer> predicate,\n+                                                  final int offsetForReadLikelihoodGivenAlleleIndex, final double[] readLikelihoodsForGT, final List<Double> criticalThresholdsSorted) {\n+        if (positionSortedReads.isEmpty()) {\n+            return new double[]{Double.NEGATIVE_INFINITY, 0};\n+        }\n+\n+        int counter = 0;\n+        double maxLpspi = Double.NEGATIVE_INFINITY;\n+        double lpfApplied = 0;\n+\n+        for (final Double logProbFAllele : criticalThresholdsSorted) {\n+            double fAlleleProbRatio = 0.0;\n+            double fAlleleProbDenom = 0.0;\n+            double localMaxLpspi = Double.NEGATIVE_INFINITY;\n+\n+            // iterate over the reads to compute the foreign allele alpha to use for genotyping with FRD\n+            for (final DRAGENGenotypesModel.DragenReadContainer container : positionSortedReads) {\n+                // Ignore reads that were disqualified by the HMM\n+                if (container.wasFilteredByHMM()) {\n+                    continue;\n+                }\n+\n+                final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+                // Keep track of the aggregate support for the foreign allele\n+                if (predicate.test(container)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 382}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NjAzNw==", "bodyText": "multiply by '-0.1'", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456586037", "createdAt": "2020-07-17T17:46:42Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulativeMeanBaseQualityPhredAdjusted[nIndexUsed],\n+                    BQD_FIXED_ERROR_RATE, cumulativeProbReadForErrorAllele[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele likelihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> readContainers,\n+                                                               final double snipAprioriHet, final double indelAprioriHet, final int maxEffectiveDepthForHetAdjustment,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int fAlleleIndex = 0; fAlleleIndex < sampleLikelihoods.numberOfAlleles(); fAlleleIndex++) {\n+            // ignore symbolic alleles\n+            final boolean isIndel = sampleLikelihoods.getAllele(fAlleleIndex).length() != refAllele.length();\n+            final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * fAlleleIndex + readCount;\n+\n+            // Here we generate a set of the critical log10(P(F)) values that we will iterate over\n+            final Set<Double> criticalThresholdsForward = new HashSet<>();\n+            final Set<Double> criticalThresholdsReverse = new HashSet<>();\n+            final Set<Double> criticalThresholds = new HashSet<>();\n+            computeCriticalValues(criticalThresholdsForward, criticalThresholdsReverse, criticalThresholds, readContainers, fAlleleIndex == 0 ? 0 : (isIndel? indelAprioriHet : snipAprioriHet)/-10); // simplified in line with DRAGEN, uses 1 alleledist for both snp and indels\n+            final List<Double> criticalThresholdsSortedForward = criticalThresholdsForward.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSortedReverse = criticalThresholdsReverse.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSorted = criticalThresholds.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"fIndex: \" + fAlleleIndex + \" criticalValues: \\n\" + criticalThresholds.stream().map(d -> Double.toString(d)).collect(Collectors.joining(\"\\n\")));\n+            }\n+            // iterate over all of the homozygous genotypes for the given allele\n+            for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+                // Skip over the allele corresponding to the \"foreign\" allele\n+                if (gtAlleleIndex == fAlleleIndex) {\n+                    continue;\n+                }\n+                // For right now we allow symbolic alleles, but this might be subject to change\n+//                if (sampleLikelihoods.getAllele(fAlleleIndex).isSymbolic() ) {\n+//                    continue;\n+//                }\n+\n+                //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+                //This should be pulled off as a calculator in some genotyping class.\n+                final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+                double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+                // TODO restore the critical thresholds\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"indexForGT \"+indexForGT+ \" ooffsetForReadLikelihoodGivenAlleleIndex =\"+offsetForReadLikelihoodGivenAlleleIndex);\n+                    HaplotypeCallerGenotypingDebugger.println(\"\\nForwards Strands: \");\n+                }\n+                final double[] maxLog10FForwardsStrand = computeFRDModelForStrandData(readContainers, c -> !c.isReverseStrand() , offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nReverse Strands: \");}\n+                final double[] maxLog10FReverseStrand = computeFRDModelForStrandData(readContainers, c -> c.isReverseStrand(), offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nBoth Strands: \");}\n+                final double[] maxLog10FBothStrands = computeFRDModelForStrandData(readContainers, c -> true, offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"gtAlleleIndex : \"+gtAlleleIndex+ \" fAlleleIndex: \"+fAlleleIndex +\" forwards: \"+maxLog10FForwardsStrand+\" reverse: \"+maxLog10FReverseStrand+\" both: \"+maxLog10FBothStrands);\n+                }\n+                double[] localBestModel = maxLog10FForwardsStrand;\n+                if (localBestModel[0] < maxLog10FReverseStrand[0]) {\n+                    localBestModel = maxLog10FReverseStrand;\n+                }\n+                if (localBestModel[0] < maxLog10FBothStrands[0]) {\n+                    localBestModel = maxLog10FBothStrands;\n+                }\n+\n+                // Handle max effective depth adjustment if specified\n+                if (maxEffectiveDepthForHetAdjustment > 0) {\n+                    // Use the index corresponding the mixture of F and\n+                    double localBestModelScore = localBestModel[0] - localBestModel[1];\n+                    int closestGTAlleleIndex = allelesToIndex(gtAlleleIndex, fAlleleIndex);\n+                    double log10LikelihoodsForPloyidyModel = ploidyModelLikelihoods[closestGTAlleleIndex] - MathUtils.log10(2);\n+                    int depthForGenotyping = sampleLikelihoods.evidenceCount();\n+                    double adjustedBestModel = log10LikelihoodsForPloyidyModel + ((localBestModelScore - log10LikelihoodsForPloyidyModel)\n+                            * ((Math.min(depthForGenotyping, maxEffectiveDepthForHetAdjustment) * 1.0) / depthForGenotyping));\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], adjustedBestModel + localBestModel[1]);\n+\n+                    if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                        HaplotypeCallerGenotypingDebugger.println(\"best FRD likelihoods: \"+localBestModelScore+\" P(F) score used: \"+localBestModel[1]+\"  use MaxEffectiveDepth: \"+maxEffectiveDepthForHetAdjustment);\n+                        HaplotypeCallerGenotypingDebugger.println(\"Using array index \"+closestGTAlleleIndex+\" for mixture gt with likelihood of \"+log10LikelihoodsForPloyidyModel+\" adjusted based on depth: \"+depthForGenotyping);\n+                        HaplotypeCallerGenotypingDebugger.println(\"p_rG_adj : \"+adjustedBestModel);\n+                    }\n+                } else {\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], localBestModel[0]);\n+                }\n+\n+            }\n+\n+        }\n+\n+\n+        return outputArray;\n+    }\n+\n+\n+    /**\n+     * @param positionSortedReads read containers to use for genotyping\n+     * @param predicate predicate used to select the correct orientation combination for reads when genotyping\n+     * @param offsetForReadLikelihoodGivenAlleleIndex offset corresponding to the Error Allele in the reads likelihoods object array\n+     * @param readLikelihoodsForGT reads likelihoods for Genotype array table\n+     * @param criticalThresholdsSorted critical thresholds to use for this orientation combination\n+     * @return two doubles, index 0 is the frd score and the second is log p(F()) score used to adjust the score\n+     */\n+    private double[] computeFRDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads, final Predicate<DRAGENGenotypesModel.DragenReadContainer> predicate,\n+                                                  final int offsetForReadLikelihoodGivenAlleleIndex, final double[] readLikelihoodsForGT, final List<Double> criticalThresholdsSorted) {\n+        if (positionSortedReads.isEmpty()) {\n+            return new double[]{Double.NEGATIVE_INFINITY, 0};\n+        }\n+\n+        int counter = 0;\n+        double maxLpspi = Double.NEGATIVE_INFINITY;\n+        double lpfApplied = 0;\n+\n+        for (final Double logProbFAllele : criticalThresholdsSorted) {\n+            double fAlleleProbRatio = 0.0;\n+            double fAlleleProbDenom = 0.0;\n+            double localMaxLpspi = Double.NEGATIVE_INFINITY;\n+\n+            // iterate over the reads to compute the foreign allele alpha to use for genotyping with FRD\n+            for (final DRAGENGenotypesModel.DragenReadContainer container : positionSortedReads) {\n+                // Ignore reads that were disqualified by the HMM\n+                if (container.wasFilteredByHMM()) {\n+                    continue;\n+                }\n+\n+                final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+                // Keep track of the aggregate support for the foreign allele\n+                if (predicate.test(container)) {\n+                    // Only include reads with mapping quality adjustment < the critical threshold being used (i.e. exclude reads with MQ > than the threshold)\n+                    double LPd_r_F = container.getPhredPFValue() + 0.0000001 <= logProbFAllele ?\n+                            Double.NEGATIVE_INFINITY :\n+                            readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+                    double lp_r_GT = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+\n+                    fAlleleProbRatio += Math.pow(10, LPd_r_F - MathUtils.approximateLog10SumLog10(LPd_r_F, lp_r_GT));\n+                    fAlleleProbDenom++;\n+                }\n+            }\n+\n+            // Don't learn the beta but approximate it based on the read support for the alt\n+            double foreignAlleleLikelihood = Math.min(fAlleleProbRatio / fAlleleProbDenom, 0.5);\n+            double log10ForeignAlleleLikelihood = Math.log10(foreignAlleleLikelihood);\n+            double log10NotForeignAlleleLikelihood = Math.log10(1.0 - foreignAlleleLikelihood);\n+            double cumulativeLog10LikelihoodOfForeignReadHypothesis = 0.0; // LP_R_GF\n+\n+            // iterate over the containers again using the approximated beta constraint\n+            for (final DRAGENGenotypesModel.DragenReadContainer container : positionSortedReads) {\n+                // Ignore reads that were disqualified by the HMM\n+                if (container.wasFilteredByHMM()) {\n+                    continue;\n+                }\n+                final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+                double log10LikelihoodReadForGenotype = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+\n+                // COMPUTE THE MODEL FOR THE STRAND IN QUESTION\n+                if (predicate.test(container)) {\n+                    double log10LikelihoodOfForeignAlleleGivenLPFCutoff = container.getPhredPFValue() + 0.0000001 <= logProbFAllele ?\n+                            Double.NEGATIVE_INFINITY :\n+                            readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+\n+                    cumulativeLog10LikelihoodOfForeignReadHypothesis += MathUtils.approximateLog10SumLog10(log10ForeignAlleleLikelihood + log10LikelihoodOfForeignAlleleGivenLPFCutoff, log10NotForeignAlleleLikelihood + log10LikelihoodReadForGenotype);\n+                } else {\n+                    cumulativeLog10LikelihoodOfForeignReadHypothesis += log10LikelihoodReadForGenotype;\n+                }\n+            }\n+            // Allele prior for error allele, plus posterior for foreign event, plus model posterior\n+            double LPsi = logProbFAllele + cumulativeLog10LikelihoodOfForeignReadHypothesis; // NOTE unlike DRAGEN we apply the prior to the combined likelihoods array after the fact so gtAllelePrior is not included at this stage\n+            localMaxLpspi = Math.max(localMaxLpspi, LPsi);\n+\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"beta: \"+foreignAlleleLikelihood+\" localMaxLpspi: \" + localMaxLpspi + \" for lpf: \"+logProbFAllele+\" with LP_R_GF: \"+cumulativeLog10LikelihoodOfForeignReadHypothesis+\" index: \"+counter++);\n+            }\n+            if (localMaxLpspi > maxLpspi) {\n+                maxLpspi = Math.max(maxLpspi, localMaxLpspi);\n+                lpfApplied = logProbFAllele;\n+            }\n+        }\n+\n+        //TODO javaize this\n+        // TODO soon should not need to use the LPF applied here...\n+        return new double[]{maxLpspi, lpfApplied};\n+    }\n+\n+\n+    // TODO: for reviewer... this code is meant to handle the performance regression brought about by FRD. Essentially in DRAGEN for every\n+    // TODO  combination of strandedness and mapping quality a computation is done, this is vaguely unnecessary. Because this leads to\n+    // TODO  a bias towards strandedness/allele combinations that have very few reads supporting them by virtue of the fact that a different\n+    // TODO  strand/allele combination had at least one low mapping quality read. I have reverted the change right now and consequently this genotyping\n+    // TODO  is taking somewhere in the order of ~5-6% runtime on the profiler whereas otherwise it could correspond to much less at the expense\n+    // TODO  of not matching DRAGEN properly.\n+    // helper method to populate the reads containers properly with their critical values and store them in the provided set\n+    // NOTE: this has the side effect of setting the DragenReadContainer setPhredPFValue() values for the reads for the given set of alleles\n+    private void computeCriticalValues(final Set<Double> criticalThresholdsForwards, final Set<Double> criticalThresholdsReverse, final Set<Double> criticalThresholdsTotal, final List<DRAGENGenotypesModel.DragenReadContainer> container, final double log10MapqPriorAdjustment) {\n+        for (int i = 0; i < container.size(); i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer readContainer = container.get(i);\n+            final double log10CriticalValue = readContainer.getPhredScaledMappingQuality() / -10.0 + log10MapqPriorAdjustment;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 451}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4ODQzNA==", "bodyText": "It seems that right after calling this method you go on to sort the resulting set into list.... I think you could do the sorting work in here instead.\nAlso I suggest that instead of creating the set/list outside the method and populate them here... that you define a \"struct\" class (inner private static) to return them all as a return value. Is more neat that way.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456588434", "createdAt": "2020-07-17T17:51:19Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulativeMeanBaseQualityPhredAdjusted[nIndexUsed],\n+                    BQD_FIXED_ERROR_RATE, cumulativeProbReadForErrorAllele[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele likelihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> readContainers,\n+                                                               final double snipAprioriHet, final double indelAprioriHet, final int maxEffectiveDepthForHetAdjustment,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int fAlleleIndex = 0; fAlleleIndex < sampleLikelihoods.numberOfAlleles(); fAlleleIndex++) {\n+            // ignore symbolic alleles\n+            final boolean isIndel = sampleLikelihoods.getAllele(fAlleleIndex).length() != refAllele.length();\n+            final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * fAlleleIndex + readCount;\n+\n+            // Here we generate a set of the critical log10(P(F)) values that we will iterate over\n+            final Set<Double> criticalThresholdsForward = new HashSet<>();\n+            final Set<Double> criticalThresholdsReverse = new HashSet<>();\n+            final Set<Double> criticalThresholds = new HashSet<>();\n+            computeCriticalValues(criticalThresholdsForward, criticalThresholdsReverse, criticalThresholds, readContainers, fAlleleIndex == 0 ? 0 : (isIndel? indelAprioriHet : snipAprioriHet)/-10); // simplified in line with DRAGEN, uses 1 alleledist for both snp and indels\n+            final List<Double> criticalThresholdsSortedForward = criticalThresholdsForward.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSortedReverse = criticalThresholdsReverse.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSorted = criticalThresholds.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"fIndex: \" + fAlleleIndex + \" criticalValues: \\n\" + criticalThresholds.stream().map(d -> Double.toString(d)).collect(Collectors.joining(\"\\n\")));\n+            }\n+            // iterate over all of the homozygous genotypes for the given allele\n+            for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+                // Skip over the allele corresponding to the \"foreign\" allele\n+                if (gtAlleleIndex == fAlleleIndex) {\n+                    continue;\n+                }\n+                // For right now we allow symbolic alleles, but this might be subject to change\n+//                if (sampleLikelihoods.getAllele(fAlleleIndex).isSymbolic() ) {\n+//                    continue;\n+//                }\n+\n+                //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+                //This should be pulled off as a calculator in some genotyping class.\n+                final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+                double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+                // TODO restore the critical thresholds\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"indexForGT \"+indexForGT+ \" ooffsetForReadLikelihoodGivenAlleleIndex =\"+offsetForReadLikelihoodGivenAlleleIndex);\n+                    HaplotypeCallerGenotypingDebugger.println(\"\\nForwards Strands: \");\n+                }\n+                final double[] maxLog10FForwardsStrand = computeFRDModelForStrandData(readContainers, c -> !c.isReverseStrand() , offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nReverse Strands: \");}\n+                final double[] maxLog10FReverseStrand = computeFRDModelForStrandData(readContainers, c -> c.isReverseStrand(), offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nBoth Strands: \");}\n+                final double[] maxLog10FBothStrands = computeFRDModelForStrandData(readContainers, c -> true, offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"gtAlleleIndex : \"+gtAlleleIndex+ \" fAlleleIndex: \"+fAlleleIndex +\" forwards: \"+maxLog10FForwardsStrand+\" reverse: \"+maxLog10FReverseStrand+\" both: \"+maxLog10FBothStrands);\n+                }\n+                double[] localBestModel = maxLog10FForwardsStrand;\n+                if (localBestModel[0] < maxLog10FReverseStrand[0]) {\n+                    localBestModel = maxLog10FReverseStrand;\n+                }\n+                if (localBestModel[0] < maxLog10FBothStrands[0]) {\n+                    localBestModel = maxLog10FBothStrands;\n+                }\n+\n+                // Handle max effective depth adjustment if specified\n+                if (maxEffectiveDepthForHetAdjustment > 0) {\n+                    // Use the index corresponding the mixture of F and\n+                    double localBestModelScore = localBestModel[0] - localBestModel[1];\n+                    int closestGTAlleleIndex = allelesToIndex(gtAlleleIndex, fAlleleIndex);\n+                    double log10LikelihoodsForPloyidyModel = ploidyModelLikelihoods[closestGTAlleleIndex] - MathUtils.log10(2);\n+                    int depthForGenotyping = sampleLikelihoods.evidenceCount();\n+                    double adjustedBestModel = log10LikelihoodsForPloyidyModel + ((localBestModelScore - log10LikelihoodsForPloyidyModel)\n+                            * ((Math.min(depthForGenotyping, maxEffectiveDepthForHetAdjustment) * 1.0) / depthForGenotyping));\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], adjustedBestModel + localBestModel[1]);\n+\n+                    if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                        HaplotypeCallerGenotypingDebugger.println(\"best FRD likelihoods: \"+localBestModelScore+\" P(F) score used: \"+localBestModel[1]+\"  use MaxEffectiveDepth: \"+maxEffectiveDepthForHetAdjustment);\n+                        HaplotypeCallerGenotypingDebugger.println(\"Using array index \"+closestGTAlleleIndex+\" for mixture gt with likelihood of \"+log10LikelihoodsForPloyidyModel+\" adjusted based on depth: \"+depthForGenotyping);\n+                        HaplotypeCallerGenotypingDebugger.println(\"p_rG_adj : \"+adjustedBestModel);\n+                    }\n+                } else {\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], localBestModel[0]);\n+                }\n+\n+            }\n+\n+        }\n+\n+\n+        return outputArray;\n+    }\n+\n+\n+    /**\n+     * @param positionSortedReads read containers to use for genotyping\n+     * @param predicate predicate used to select the correct orientation combination for reads when genotyping\n+     * @param offsetForReadLikelihoodGivenAlleleIndex offset corresponding to the Error Allele in the reads likelihoods object array\n+     * @param readLikelihoodsForGT reads likelihoods for Genotype array table\n+     * @param criticalThresholdsSorted critical thresholds to use for this orientation combination\n+     * @return two doubles, index 0 is the frd score and the second is log p(F()) score used to adjust the score\n+     */\n+    private double[] computeFRDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads, final Predicate<DRAGENGenotypesModel.DragenReadContainer> predicate,\n+                                                  final int offsetForReadLikelihoodGivenAlleleIndex, final double[] readLikelihoodsForGT, final List<Double> criticalThresholdsSorted) {\n+        if (positionSortedReads.isEmpty()) {\n+            return new double[]{Double.NEGATIVE_INFINITY, 0};\n+        }\n+\n+        int counter = 0;\n+        double maxLpspi = Double.NEGATIVE_INFINITY;\n+        double lpfApplied = 0;\n+\n+        for (final Double logProbFAllele : criticalThresholdsSorted) {\n+            double fAlleleProbRatio = 0.0;\n+            double fAlleleProbDenom = 0.0;\n+            double localMaxLpspi = Double.NEGATIVE_INFINITY;\n+\n+            // iterate over the reads to compute the foreign allele alpha to use for genotyping with FRD\n+            for (final DRAGENGenotypesModel.DragenReadContainer container : positionSortedReads) {\n+                // Ignore reads that were disqualified by the HMM\n+                if (container.wasFilteredByHMM()) {\n+                    continue;\n+                }\n+\n+                final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+                // Keep track of the aggregate support for the foreign allele\n+                if (predicate.test(container)) {\n+                    // Only include reads with mapping quality adjustment < the critical threshold being used (i.e. exclude reads with MQ > than the threshold)\n+                    double LPd_r_F = container.getPhredPFValue() + 0.0000001 <= logProbFAllele ?\n+                            Double.NEGATIVE_INFINITY :\n+                            readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+                    double lp_r_GT = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+\n+                    fAlleleProbRatio += Math.pow(10, LPd_r_F - MathUtils.approximateLog10SumLog10(LPd_r_F, lp_r_GT));\n+                    fAlleleProbDenom++;\n+                }\n+            }\n+\n+            // Don't learn the beta but approximate it based on the read support for the alt\n+            double foreignAlleleLikelihood = Math.min(fAlleleProbRatio / fAlleleProbDenom, 0.5);\n+            double log10ForeignAlleleLikelihood = Math.log10(foreignAlleleLikelihood);\n+            double log10NotForeignAlleleLikelihood = Math.log10(1.0 - foreignAlleleLikelihood);\n+            double cumulativeLog10LikelihoodOfForeignReadHypothesis = 0.0; // LP_R_GF\n+\n+            // iterate over the containers again using the approximated beta constraint\n+            for (final DRAGENGenotypesModel.DragenReadContainer container : positionSortedReads) {\n+                // Ignore reads that were disqualified by the HMM\n+                if (container.wasFilteredByHMM()) {\n+                    continue;\n+                }\n+                final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+                double log10LikelihoodReadForGenotype = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+\n+                // COMPUTE THE MODEL FOR THE STRAND IN QUESTION\n+                if (predicate.test(container)) {\n+                    double log10LikelihoodOfForeignAlleleGivenLPFCutoff = container.getPhredPFValue() + 0.0000001 <= logProbFAllele ?\n+                            Double.NEGATIVE_INFINITY :\n+                            readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+\n+                    cumulativeLog10LikelihoodOfForeignReadHypothesis += MathUtils.approximateLog10SumLog10(log10ForeignAlleleLikelihood + log10LikelihoodOfForeignAlleleGivenLPFCutoff, log10NotForeignAlleleLikelihood + log10LikelihoodReadForGenotype);\n+                } else {\n+                    cumulativeLog10LikelihoodOfForeignReadHypothesis += log10LikelihoodReadForGenotype;\n+                }\n+            }\n+            // Allele prior for error allele, plus posterior for foreign event, plus model posterior\n+            double LPsi = logProbFAllele + cumulativeLog10LikelihoodOfForeignReadHypothesis; // NOTE unlike DRAGEN we apply the prior to the combined likelihoods array after the fact so gtAllelePrior is not included at this stage\n+            localMaxLpspi = Math.max(localMaxLpspi, LPsi);\n+\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"beta: \"+foreignAlleleLikelihood+\" localMaxLpspi: \" + localMaxLpspi + \" for lpf: \"+logProbFAllele+\" with LP_R_GF: \"+cumulativeLog10LikelihoodOfForeignReadHypothesis+\" index: \"+counter++);\n+            }\n+            if (localMaxLpspi > maxLpspi) {\n+                maxLpspi = Math.max(maxLpspi, localMaxLpspi);\n+                lpfApplied = logProbFAllele;\n+            }\n+        }\n+\n+        //TODO javaize this\n+        // TODO soon should not need to use the LPF applied here...\n+        return new double[]{maxLpspi, lpfApplied};\n+    }\n+\n+\n+    // TODO: for reviewer... this code is meant to handle the performance regression brought about by FRD. Essentially in DRAGEN for every\n+    // TODO  combination of strandedness and mapping quality a computation is done, this is vaguely unnecessary. Because this leads to\n+    // TODO  a bias towards strandedness/allele combinations that have very few reads supporting them by virtue of the fact that a different\n+    // TODO  strand/allele combination had at least one low mapping quality read. I have reverted the change right now and consequently this genotyping\n+    // TODO  is taking somewhere in the order of ~5-6% runtime on the profiler whereas otherwise it could correspond to much less at the expense\n+    // TODO  of not matching DRAGEN properly.\n+    // helper method to populate the reads containers properly with their critical values and store them in the provided set\n+    // NOTE: this has the side effect of setting the DragenReadContainer setPhredPFValue() values for the reads for the given set of alleles\n+    private void computeCriticalValues(final Set<Double> criticalThresholdsForwards, final Set<Double> criticalThresholdsReverse, final Set<Double> criticalThresholdsTotal, final List<DRAGENGenotypesModel.DragenReadContainer> container, final double log10MapqPriorAdjustment) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 448}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4ODg2Mg==", "bodyText": "If you the sorting in here, the all-thresholds list would simply bet a merge of the other two lists.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456588862", "createdAt": "2020-07-17T17:52:13Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulativeMeanBaseQualityPhredAdjusted[nIndexUsed],\n+                    BQD_FIXED_ERROR_RATE, cumulativeProbReadForErrorAllele[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele likelihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> readContainers,\n+                                                               final double snipAprioriHet, final double indelAprioriHet, final int maxEffectiveDepthForHetAdjustment,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int fAlleleIndex = 0; fAlleleIndex < sampleLikelihoods.numberOfAlleles(); fAlleleIndex++) {\n+            // ignore symbolic alleles\n+            final boolean isIndel = sampleLikelihoods.getAllele(fAlleleIndex).length() != refAllele.length();\n+            final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * fAlleleIndex + readCount;\n+\n+            // Here we generate a set of the critical log10(P(F)) values that we will iterate over\n+            final Set<Double> criticalThresholdsForward = new HashSet<>();\n+            final Set<Double> criticalThresholdsReverse = new HashSet<>();\n+            final Set<Double> criticalThresholds = new HashSet<>();\n+            computeCriticalValues(criticalThresholdsForward, criticalThresholdsReverse, criticalThresholds, readContainers, fAlleleIndex == 0 ? 0 : (isIndel? indelAprioriHet : snipAprioriHet)/-10); // simplified in line with DRAGEN, uses 1 alleledist for both snp and indels\n+            final List<Double> criticalThresholdsSortedForward = criticalThresholdsForward.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSortedReverse = criticalThresholdsReverse.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSorted = criticalThresholds.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"fIndex: \" + fAlleleIndex + \" criticalValues: \\n\" + criticalThresholds.stream().map(d -> Double.toString(d)).collect(Collectors.joining(\"\\n\")));\n+            }\n+            // iterate over all of the homozygous genotypes for the given allele\n+            for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+                // Skip over the allele corresponding to the \"foreign\" allele\n+                if (gtAlleleIndex == fAlleleIndex) {\n+                    continue;\n+                }\n+                // For right now we allow symbolic alleles, but this might be subject to change\n+//                if (sampleLikelihoods.getAllele(fAlleleIndex).isSymbolic() ) {\n+//                    continue;\n+//                }\n+\n+                //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+                //This should be pulled off as a calculator in some genotyping class.\n+                final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+                double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+                // TODO restore the critical thresholds\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"indexForGT \"+indexForGT+ \" ooffsetForReadLikelihoodGivenAlleleIndex =\"+offsetForReadLikelihoodGivenAlleleIndex);\n+                    HaplotypeCallerGenotypingDebugger.println(\"\\nForwards Strands: \");\n+                }\n+                final double[] maxLog10FForwardsStrand = computeFRDModelForStrandData(readContainers, c -> !c.isReverseStrand() , offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nReverse Strands: \");}\n+                final double[] maxLog10FReverseStrand = computeFRDModelForStrandData(readContainers, c -> c.isReverseStrand(), offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nBoth Strands: \");}\n+                final double[] maxLog10FBothStrands = computeFRDModelForStrandData(readContainers, c -> true, offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"gtAlleleIndex : \"+gtAlleleIndex+ \" fAlleleIndex: \"+fAlleleIndex +\" forwards: \"+maxLog10FForwardsStrand+\" reverse: \"+maxLog10FReverseStrand+\" both: \"+maxLog10FBothStrands);\n+                }\n+                double[] localBestModel = maxLog10FForwardsStrand;\n+                if (localBestModel[0] < maxLog10FReverseStrand[0]) {\n+                    localBestModel = maxLog10FReverseStrand;\n+                }\n+                if (localBestModel[0] < maxLog10FBothStrands[0]) {\n+                    localBestModel = maxLog10FBothStrands;\n+                }\n+\n+                // Handle max effective depth adjustment if specified\n+                if (maxEffectiveDepthForHetAdjustment > 0) {\n+                    // Use the index corresponding the mixture of F and\n+                    double localBestModelScore = localBestModel[0] - localBestModel[1];\n+                    int closestGTAlleleIndex = allelesToIndex(gtAlleleIndex, fAlleleIndex);\n+                    double log10LikelihoodsForPloyidyModel = ploidyModelLikelihoods[closestGTAlleleIndex] - MathUtils.log10(2);\n+                    int depthForGenotyping = sampleLikelihoods.evidenceCount();\n+                    double adjustedBestModel = log10LikelihoodsForPloyidyModel + ((localBestModelScore - log10LikelihoodsForPloyidyModel)\n+                            * ((Math.min(depthForGenotyping, maxEffectiveDepthForHetAdjustment) * 1.0) / depthForGenotyping));\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], adjustedBestModel + localBestModel[1]);\n+\n+                    if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                        HaplotypeCallerGenotypingDebugger.println(\"best FRD likelihoods: \"+localBestModelScore+\" P(F) score used: \"+localBestModel[1]+\"  use MaxEffectiveDepth: \"+maxEffectiveDepthForHetAdjustment);\n+                        HaplotypeCallerGenotypingDebugger.println(\"Using array index \"+closestGTAlleleIndex+\" for mixture gt with likelihood of \"+log10LikelihoodsForPloyidyModel+\" adjusted based on depth: \"+depthForGenotyping);\n+                        HaplotypeCallerGenotypingDebugger.println(\"p_rG_adj : \"+adjustedBestModel);\n+                    }\n+                } else {\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], localBestModel[0]);\n+                }\n+\n+            }\n+\n+        }\n+\n+\n+        return outputArray;\n+    }\n+\n+\n+    /**\n+     * @param positionSortedReads read containers to use for genotyping\n+     * @param predicate predicate used to select the correct orientation combination for reads when genotyping\n+     * @param offsetForReadLikelihoodGivenAlleleIndex offset corresponding to the Error Allele in the reads likelihoods object array\n+     * @param readLikelihoodsForGT reads likelihoods for Genotype array table\n+     * @param criticalThresholdsSorted critical thresholds to use for this orientation combination\n+     * @return two doubles, index 0 is the frd score and the second is log p(F()) score used to adjust the score\n+     */\n+    private double[] computeFRDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads, final Predicate<DRAGENGenotypesModel.DragenReadContainer> predicate,\n+                                                  final int offsetForReadLikelihoodGivenAlleleIndex, final double[] readLikelihoodsForGT, final List<Double> criticalThresholdsSorted) {\n+        if (positionSortedReads.isEmpty()) {\n+            return new double[]{Double.NEGATIVE_INFINITY, 0};\n+        }\n+\n+        int counter = 0;\n+        double maxLpspi = Double.NEGATIVE_INFINITY;\n+        double lpfApplied = 0;\n+\n+        for (final Double logProbFAllele : criticalThresholdsSorted) {\n+            double fAlleleProbRatio = 0.0;\n+            double fAlleleProbDenom = 0.0;\n+            double localMaxLpspi = Double.NEGATIVE_INFINITY;\n+\n+            // iterate over the reads to compute the foreign allele alpha to use for genotyping with FRD\n+            for (final DRAGENGenotypesModel.DragenReadContainer container : positionSortedReads) {\n+                // Ignore reads that were disqualified by the HMM\n+                if (container.wasFilteredByHMM()) {\n+                    continue;\n+                }\n+\n+                final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+                // Keep track of the aggregate support for the foreign allele\n+                if (predicate.test(container)) {\n+                    // Only include reads with mapping quality adjustment < the critical threshold being used (i.e. exclude reads with MQ > than the threshold)\n+                    double LPd_r_F = container.getPhredPFValue() + 0.0000001 <= logProbFAllele ?\n+                            Double.NEGATIVE_INFINITY :\n+                            readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+                    double lp_r_GT = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+\n+                    fAlleleProbRatio += Math.pow(10, LPd_r_F - MathUtils.approximateLog10SumLog10(LPd_r_F, lp_r_GT));\n+                    fAlleleProbDenom++;\n+                }\n+            }\n+\n+            // Don't learn the beta but approximate it based on the read support for the alt\n+            double foreignAlleleLikelihood = Math.min(fAlleleProbRatio / fAlleleProbDenom, 0.5);\n+            double log10ForeignAlleleLikelihood = Math.log10(foreignAlleleLikelihood);\n+            double log10NotForeignAlleleLikelihood = Math.log10(1.0 - foreignAlleleLikelihood);\n+            double cumulativeLog10LikelihoodOfForeignReadHypothesis = 0.0; // LP_R_GF\n+\n+            // iterate over the containers again using the approximated beta constraint\n+            for (final DRAGENGenotypesModel.DragenReadContainer container : positionSortedReads) {\n+                // Ignore reads that were disqualified by the HMM\n+                if (container.wasFilteredByHMM()) {\n+                    continue;\n+                }\n+                final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+                double log10LikelihoodReadForGenotype = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+\n+                // COMPUTE THE MODEL FOR THE STRAND IN QUESTION\n+                if (predicate.test(container)) {\n+                    double log10LikelihoodOfForeignAlleleGivenLPFCutoff = container.getPhredPFValue() + 0.0000001 <= logProbFAllele ?\n+                            Double.NEGATIVE_INFINITY :\n+                            readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+\n+                    cumulativeLog10LikelihoodOfForeignReadHypothesis += MathUtils.approximateLog10SumLog10(log10ForeignAlleleLikelihood + log10LikelihoodOfForeignAlleleGivenLPFCutoff, log10NotForeignAlleleLikelihood + log10LikelihoodReadForGenotype);\n+                } else {\n+                    cumulativeLog10LikelihoodOfForeignReadHypothesis += log10LikelihoodReadForGenotype;\n+                }\n+            }\n+            // Allele prior for error allele, plus posterior for foreign event, plus model posterior\n+            double LPsi = logProbFAllele + cumulativeLog10LikelihoodOfForeignReadHypothesis; // NOTE unlike DRAGEN we apply the prior to the combined likelihoods array after the fact so gtAllelePrior is not included at this stage\n+            localMaxLpspi = Math.max(localMaxLpspi, LPsi);\n+\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"beta: \"+foreignAlleleLikelihood+\" localMaxLpspi: \" + localMaxLpspi + \" for lpf: \"+logProbFAllele+\" with LP_R_GF: \"+cumulativeLog10LikelihoodOfForeignReadHypothesis+\" index: \"+counter++);\n+            }\n+            if (localMaxLpspi > maxLpspi) {\n+                maxLpspi = Math.max(maxLpspi, localMaxLpspi);\n+                lpfApplied = logProbFAllele;\n+            }\n+        }\n+\n+        //TODO javaize this\n+        // TODO soon should not need to use the LPF applied here...\n+        return new double[]{maxLpspi, lpfApplied};\n+    }\n+\n+\n+    // TODO: for reviewer... this code is meant to handle the performance regression brought about by FRD. Essentially in DRAGEN for every\n+    // TODO  combination of strandedness and mapping quality a computation is done, this is vaguely unnecessary. Because this leads to\n+    // TODO  a bias towards strandedness/allele combinations that have very few reads supporting them by virtue of the fact that a different\n+    // TODO  strand/allele combination had at least one low mapping quality read. I have reverted the change right now and consequently this genotyping\n+    // TODO  is taking somewhere in the order of ~5-6% runtime on the profiler whereas otherwise it could correspond to much less at the expense\n+    // TODO  of not matching DRAGEN properly.\n+    // helper method to populate the reads containers properly with their critical values and store them in the provided set\n+    // NOTE: this has the side effect of setting the DragenReadContainer setPhredPFValue() values for the reads for the given set of alleles\n+    private void computeCriticalValues(final Set<Double> criticalThresholdsForwards, final Set<Double> criticalThresholdsReverse, final Set<Double> criticalThresholdsTotal, final List<DRAGENGenotypesModel.DragenReadContainer> container, final double log10MapqPriorAdjustment) {\n+        for (int i = 0; i < container.size(); i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer readContainer = container.get(i);\n+            final double log10CriticalValue = readContainer.getPhredScaledMappingQuality() / -10.0 + log10MapqPriorAdjustment;\n+            readContainer.setPhredPFValue(log10CriticalValue);\n+            // Split the critical thresholds up by their applicable strands in order to avoid repeated work\n+            if (readContainer.isReverseStrand()) {\n+                criticalThresholdsReverse.add(log10CriticalValue);\n+            } else {\n+                criticalThresholdsForwards.add(log10CriticalValue);\n+            }\n+            criticalThresholdsTotal.add(log10CriticalValue);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 459}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MDM5NA==", "bodyText": "Since cachedLikelihoods is a private field in this class, its value should never affect the super.genotypeLikehoods invoke and so this set to null is not necessary, right?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456590394", "createdAt": "2020-07-17T17:54:58Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulativeMeanBaseQualityPhredAdjusted[nIndexUsed],\n+                    BQD_FIXED_ERROR_RATE, cumulativeProbReadForErrorAllele[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele likelihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> readContainers,\n+                                                               final double snipAprioriHet, final double indelAprioriHet, final int maxEffectiveDepthForHetAdjustment,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int fAlleleIndex = 0; fAlleleIndex < sampleLikelihoods.numberOfAlleles(); fAlleleIndex++) {\n+            // ignore symbolic alleles\n+            final boolean isIndel = sampleLikelihoods.getAllele(fAlleleIndex).length() != refAllele.length();\n+            final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * fAlleleIndex + readCount;\n+\n+            // Here we generate a set of the critical log10(P(F)) values that we will iterate over\n+            final Set<Double> criticalThresholdsForward = new HashSet<>();\n+            final Set<Double> criticalThresholdsReverse = new HashSet<>();\n+            final Set<Double> criticalThresholds = new HashSet<>();\n+            computeCriticalValues(criticalThresholdsForward, criticalThresholdsReverse, criticalThresholds, readContainers, fAlleleIndex == 0 ? 0 : (isIndel? indelAprioriHet : snipAprioriHet)/-10); // simplified in line with DRAGEN, uses 1 alleledist for both snp and indels\n+            final List<Double> criticalThresholdsSortedForward = criticalThresholdsForward.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSortedReverse = criticalThresholdsReverse.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSorted = criticalThresholds.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"fIndex: \" + fAlleleIndex + \" criticalValues: \\n\" + criticalThresholds.stream().map(d -> Double.toString(d)).collect(Collectors.joining(\"\\n\")));\n+            }\n+            // iterate over all of the homozygous genotypes for the given allele\n+            for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+                // Skip over the allele corresponding to the \"foreign\" allele\n+                if (gtAlleleIndex == fAlleleIndex) {\n+                    continue;\n+                }\n+                // For right now we allow symbolic alleles, but this might be subject to change\n+//                if (sampleLikelihoods.getAllele(fAlleleIndex).isSymbolic() ) {\n+//                    continue;\n+//                }\n+\n+                //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+                //This should be pulled off as a calculator in some genotyping class.\n+                final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+                double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+                // TODO restore the critical thresholds\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"indexForGT \"+indexForGT+ \" ooffsetForReadLikelihoodGivenAlleleIndex =\"+offsetForReadLikelihoodGivenAlleleIndex);\n+                    HaplotypeCallerGenotypingDebugger.println(\"\\nForwards Strands: \");\n+                }\n+                final double[] maxLog10FForwardsStrand = computeFRDModelForStrandData(readContainers, c -> !c.isReverseStrand() , offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nReverse Strands: \");}\n+                final double[] maxLog10FReverseStrand = computeFRDModelForStrandData(readContainers, c -> c.isReverseStrand(), offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nBoth Strands: \");}\n+                final double[] maxLog10FBothStrands = computeFRDModelForStrandData(readContainers, c -> true, offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"gtAlleleIndex : \"+gtAlleleIndex+ \" fAlleleIndex: \"+fAlleleIndex +\" forwards: \"+maxLog10FForwardsStrand+\" reverse: \"+maxLog10FReverseStrand+\" both: \"+maxLog10FBothStrands);\n+                }\n+                double[] localBestModel = maxLog10FForwardsStrand;\n+                if (localBestModel[0] < maxLog10FReverseStrand[0]) {\n+                    localBestModel = maxLog10FReverseStrand;\n+                }\n+                if (localBestModel[0] < maxLog10FBothStrands[0]) {\n+                    localBestModel = maxLog10FBothStrands;\n+                }\n+\n+                // Handle max effective depth adjustment if specified\n+                if (maxEffectiveDepthForHetAdjustment > 0) {\n+                    // Use the index corresponding the mixture of F and\n+                    double localBestModelScore = localBestModel[0] - localBestModel[1];\n+                    int closestGTAlleleIndex = allelesToIndex(gtAlleleIndex, fAlleleIndex);\n+                    double log10LikelihoodsForPloyidyModel = ploidyModelLikelihoods[closestGTAlleleIndex] - MathUtils.log10(2);\n+                    int depthForGenotyping = sampleLikelihoods.evidenceCount();\n+                    double adjustedBestModel = log10LikelihoodsForPloyidyModel + ((localBestModelScore - log10LikelihoodsForPloyidyModel)\n+                            * ((Math.min(depthForGenotyping, maxEffectiveDepthForHetAdjustment) * 1.0) / depthForGenotyping));\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], adjustedBestModel + localBestModel[1]);\n+\n+                    if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                        HaplotypeCallerGenotypingDebugger.println(\"best FRD likelihoods: \"+localBestModelScore+\" P(F) score used: \"+localBestModel[1]+\"  use MaxEffectiveDepth: \"+maxEffectiveDepthForHetAdjustment);\n+                        HaplotypeCallerGenotypingDebugger.println(\"Using array index \"+closestGTAlleleIndex+\" for mixture gt with likelihood of \"+log10LikelihoodsForPloyidyModel+\" adjusted based on depth: \"+depthForGenotyping);\n+                        HaplotypeCallerGenotypingDebugger.println(\"p_rG_adj : \"+adjustedBestModel);\n+                    }\n+                } else {\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], localBestModel[0]);\n+                }\n+\n+            }\n+\n+        }\n+\n+\n+        return outputArray;\n+    }\n+\n+\n+    /**\n+     * @param positionSortedReads read containers to use for genotyping\n+     * @param predicate predicate used to select the correct orientation combination for reads when genotyping\n+     * @param offsetForReadLikelihoodGivenAlleleIndex offset corresponding to the Error Allele in the reads likelihoods object array\n+     * @param readLikelihoodsForGT reads likelihoods for Genotype array table\n+     * @param criticalThresholdsSorted critical thresholds to use for this orientation combination\n+     * @return two doubles, index 0 is the frd score and the second is log p(F()) score used to adjust the score\n+     */\n+    private double[] computeFRDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads, final Predicate<DRAGENGenotypesModel.DragenReadContainer> predicate,\n+                                                  final int offsetForReadLikelihoodGivenAlleleIndex, final double[] readLikelihoodsForGT, final List<Double> criticalThresholdsSorted) {\n+        if (positionSortedReads.isEmpty()) {\n+            return new double[]{Double.NEGATIVE_INFINITY, 0};\n+        }\n+\n+        int counter = 0;\n+        double maxLpspi = Double.NEGATIVE_INFINITY;\n+        double lpfApplied = 0;\n+\n+        for (final Double logProbFAllele : criticalThresholdsSorted) {\n+            double fAlleleProbRatio = 0.0;\n+            double fAlleleProbDenom = 0.0;\n+            double localMaxLpspi = Double.NEGATIVE_INFINITY;\n+\n+            // iterate over the reads to compute the foreign allele alpha to use for genotyping with FRD\n+            for (final DRAGENGenotypesModel.DragenReadContainer container : positionSortedReads) {\n+                // Ignore reads that were disqualified by the HMM\n+                if (container.wasFilteredByHMM()) {\n+                    continue;\n+                }\n+\n+                final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+                // Keep track of the aggregate support for the foreign allele\n+                if (predicate.test(container)) {\n+                    // Only include reads with mapping quality adjustment < the critical threshold being used (i.e. exclude reads with MQ > than the threshold)\n+                    double LPd_r_F = container.getPhredPFValue() + 0.0000001 <= logProbFAllele ?\n+                            Double.NEGATIVE_INFINITY :\n+                            readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+                    double lp_r_GT = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+\n+                    fAlleleProbRatio += Math.pow(10, LPd_r_F - MathUtils.approximateLog10SumLog10(LPd_r_F, lp_r_GT));\n+                    fAlleleProbDenom++;\n+                }\n+            }\n+\n+            // Don't learn the beta but approximate it based on the read support for the alt\n+            double foreignAlleleLikelihood = Math.min(fAlleleProbRatio / fAlleleProbDenom, 0.5);\n+            double log10ForeignAlleleLikelihood = Math.log10(foreignAlleleLikelihood);\n+            double log10NotForeignAlleleLikelihood = Math.log10(1.0 - foreignAlleleLikelihood);\n+            double cumulativeLog10LikelihoodOfForeignReadHypothesis = 0.0; // LP_R_GF\n+\n+            // iterate over the containers again using the approximated beta constraint\n+            for (final DRAGENGenotypesModel.DragenReadContainer container : positionSortedReads) {\n+                // Ignore reads that were disqualified by the HMM\n+                if (container.wasFilteredByHMM()) {\n+                    continue;\n+                }\n+                final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+                double log10LikelihoodReadForGenotype = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+\n+                // COMPUTE THE MODEL FOR THE STRAND IN QUESTION\n+                if (predicate.test(container)) {\n+                    double log10LikelihoodOfForeignAlleleGivenLPFCutoff = container.getPhredPFValue() + 0.0000001 <= logProbFAllele ?\n+                            Double.NEGATIVE_INFINITY :\n+                            readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+\n+                    cumulativeLog10LikelihoodOfForeignReadHypothesis += MathUtils.approximateLog10SumLog10(log10ForeignAlleleLikelihood + log10LikelihoodOfForeignAlleleGivenLPFCutoff, log10NotForeignAlleleLikelihood + log10LikelihoodReadForGenotype);\n+                } else {\n+                    cumulativeLog10LikelihoodOfForeignReadHypothesis += log10LikelihoodReadForGenotype;\n+                }\n+            }\n+            // Allele prior for error allele, plus posterior for foreign event, plus model posterior\n+            double LPsi = logProbFAllele + cumulativeLog10LikelihoodOfForeignReadHypothesis; // NOTE unlike DRAGEN we apply the prior to the combined likelihoods array after the fact so gtAllelePrior is not included at this stage\n+            localMaxLpspi = Math.max(localMaxLpspi, LPsi);\n+\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"beta: \"+foreignAlleleLikelihood+\" localMaxLpspi: \" + localMaxLpspi + \" for lpf: \"+logProbFAllele+\" with LP_R_GF: \"+cumulativeLog10LikelihoodOfForeignReadHypothesis+\" index: \"+counter++);\n+            }\n+            if (localMaxLpspi > maxLpspi) {\n+                maxLpspi = Math.max(maxLpspi, localMaxLpspi);\n+                lpfApplied = logProbFAllele;\n+            }\n+        }\n+\n+        //TODO javaize this\n+        // TODO soon should not need to use the LPF applied here...\n+        return new double[]{maxLpspi, lpfApplied};\n+    }\n+\n+\n+    // TODO: for reviewer... this code is meant to handle the performance regression brought about by FRD. Essentially in DRAGEN for every\n+    // TODO  combination of strandedness and mapping quality a computation is done, this is vaguely unnecessary. Because this leads to\n+    // TODO  a bias towards strandedness/allele combinations that have very few reads supporting them by virtue of the fact that a different\n+    // TODO  strand/allele combination had at least one low mapping quality read. I have reverted the change right now and consequently this genotyping\n+    // TODO  is taking somewhere in the order of ~5-6% runtime on the profiler whereas otherwise it could correspond to much less at the expense\n+    // TODO  of not matching DRAGEN properly.\n+    // helper method to populate the reads containers properly with their critical values and store them in the provided set\n+    // NOTE: this has the side effect of setting the DragenReadContainer setPhredPFValue() values for the reads for the given set of alleles\n+    private void computeCriticalValues(final Set<Double> criticalThresholdsForwards, final Set<Double> criticalThresholdsReverse, final Set<Double> criticalThresholdsTotal, final List<DRAGENGenotypesModel.DragenReadContainer> container, final double log10MapqPriorAdjustment) {\n+        for (int i = 0; i < container.size(); i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer readContainer = container.get(i);\n+            final double log10CriticalValue = readContainer.getPhredScaledMappingQuality() / -10.0 + log10MapqPriorAdjustment;\n+            readContainer.setPhredPFValue(log10CriticalValue);\n+            // Split the critical thresholds up by their applicable strands in order to avoid repeated work\n+            if (readContainer.isReverseStrand()) {\n+                criticalThresholdsReverse.add(log10CriticalValue);\n+            } else {\n+                criticalThresholdsForwards.add(log10CriticalValue);\n+            }\n+            criticalThresholdsTotal.add(log10CriticalValue);\n+        }\n+    }\n+\n+\n+    /**\n+     * See {@link GenotypeLikelihoodCalculator#genotypeLikelihoods}. This wrapper just enforces that the likelihoods object is recorded in the cache.\n+     *\n+     * @return never {@code null}.\n+     */\n+    public <EVIDENCE, A extends Allele> GenotypeLikelihoods genotypeLikelihoods(final LikelihoodMatrix<EVIDENCE, A> likelihoods) {\n+        cachedLikelihoods = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 470}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MDQ5Nw==", "bodyText": "Same here.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456590497", "createdAt": "2020-07-17T17:55:08Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypeLikelihoodCalculatorDRAGEN.java", "diffHunk": "@@ -0,0 +1,487 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.GenotypeLikelihoods;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingDebugger;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.QualityUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.io.PrintStream;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.function.ToDoubleFunction;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper to calculate genotype likelihoods for DRAGEN advanced genotyping models (BQD - Base Quality Dropout, and FRD - Foreign Reads Detection).\n+ *\n+ * This object is simply a thin wrapper on top of a regular GenotypeLikelihoods object with some extra logic for handling new inputs to the genotyper:\n+ *  - both BQD and FRD rely on per-read per-genotype scores as would be computed for the standard genotyper, rather than pay the cost of recomputing these\n+ *    for each of the 3 independent models this GenotypeLikelihoodCalculator simply makes the computation once and relies on the fact that the underlying\n+ *    readLikelihoodsByGenotypeIndex is still populated from the previous call. To this end strict object equality tests have been implemented to ensure\n+ *    that the cache is populated with the correct likelihoods before running either of the advanced models.\n+ */\n+public final class GenotypeLikelihoodCalculatorDRAGEN extends GenotypeLikelihoodCalculator {\n+    // measure of the likelihood of an error base occurring if we have triggered a base quality dropout\n+    static final double BQD_FIXED_ERROR_RATE = 0.5;\n+\n+    // PhredScaled adjustment applied to the BQD score (this controls the weight of the base quality prior term in the BQD calculation)\n+    static final double PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE = 2.5;\n+\n+    // Cache for enforcing the strictness of using the filled array with the correct likelihoods object\n+    private LikelihoodMatrix<?, ?> cachedLikelihoods = null;\n+\n+    private final double cachedLog10ErrorRate;\n+    private final double cachedLog10NonErrorRate;\n+\n+    /**\n+     * Creates a new calculator providing its ploidy and number of genotyping alleles.\n+     */\n+    protected GenotypeLikelihoodCalculatorDRAGEN(final int ploidy, final int alleleCount,\n+                                                 final int[][] alleleFirstGenotypeOffsetByPloidy,\n+                                                 final GenotypeAlleleCounts[][] genotypeTableByPloidy) {\n+        super(ploidy, alleleCount, alleleFirstGenotypeOffsetByPloidy, genotypeTableByPloidy);\n+        Utils.validateArg(ploidy > 0, () -> \"ploidy must be at least 1 but was \" + ploidy);\n+        // The number of possible components is limited by distinct allele count and ploidy.\n+        cachedLog10ErrorRate = Math.log10(BQD_FIXED_ERROR_RATE);\n+        cachedLog10NonErrorRate = Math.log10(1 - BQD_FIXED_ERROR_RATE);\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     * This method handles splitting the model by strand and selecting the best scoring parameters across the two for return in the likelihoods array.\n+     *\n+     * BQD needs to see reads that have been disqualified in {@link org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods#filterPoorlyModeledEvidence(ToDoubleFunction)} as\n+     * well as reads that only overlap the variant in question in their low quality ends. Reads in the former category do not have their hmm scores\n+     * accounted for in the genotyping model, whereas reads in the later category do. All reads, (disqualified, low quality ends, and all others)\n+     * are sorted by the cycle-count of the SNP being genotyped, and the average base qualities for the SNP base are computed across partitions\n+     * of the reads in aggregate.\n+     *\n+     * NOTES:\n+     * - The model will not handle indel alleles\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     *\n+     * @param sampleLikelihoods allele likelihoods containing data for reads\n+     * @param strandForward list of reads in the forwards orientation overlapping the site\n+     * @param strandReverse list of reads in the reverse orientation overlapping the site\n+     * @param paddedReference reference bases (with padding) used for calculating homopolymer adjustemnt\n+     * @param offsetForRefIntoEvent offset of the variant into the reference event\n+     * @param calculators likelihoods calculators object pre-filled with scores\n+     * @return An array corresponding to the likelihoods array score for BQD, with Double.NEGATIVE_INFINITY filling all mixed allele/indel allelse\n+     */\n+    public <A extends Allele> double[] calculateBQDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandForward,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> strandReverse,\n+                                                               final byte[] paddedReference,\n+                                                               final int offsetForRefIntoEvent,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        // First we invalidate the cache\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotype calling\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+            //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+            //This should be pulled off as a calculator in some genotyping class.\n+            final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+            double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+            for(int errorAlleleIndex = 0; errorAlleleIndex < sampleLikelihoods.numberOfAlleles(); errorAlleleIndex++) {\n+                // We only want to make calls on SNPs for now\n+                if (sampleLikelihoods.getAllele(gtAlleleIndex) == sampleLikelihoods.getAllele(errorAlleleIndex) ||\n+                        sampleLikelihoods.getAllele(gtAlleleIndex).length() != refAllele.length() ||\n+                        sampleLikelihoods.getAllele(errorAlleleIndex).length() != refAllele.length()) {\n+                    continue;\n+                }\n+                // TODO super validate this\n+                byte baseOfErrorAllele = sampleLikelihoods.getAllele(errorAlleleIndex).getBases()[0];\n+\n+                double forwardHomopolymerAdjustment = FRDBQDUtils.computeForwardHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+                double reverseHomopolymerAdjustment = FRDBQDUtils.computeReverseHomopolymerAdjustment(paddedReference, offsetForRefIntoEvent, baseOfErrorAllele);\n+\n+                // This selects the index for the reads page in the table corresponding to the likelihoods of the read given allele frequency of 1\n+                final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * errorAlleleIndex + readCount;\n+\n+                // BQD scores by strand\n+                double minScoreFoundForwardsStrand = computeBQDModelForStrandData(strandForward, forwardHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, true, errorAlleleIndex);\n+                double minScoreFoundReverseStrand = computeBQDModelForStrandData(strandReverse, reverseHomopolymerAdjustment, readLikelihoodsForGT, offsetForReadLikelihoodGivenAlleleIndex, false, errorAlleleIndex);\n+\n+                double modelScoreInLog10 = (minScoreFoundForwardsStrand + minScoreFoundReverseStrand)/-10.0;\n+                //////\n+                // NOTE we have not applied the prior here, this is because that gets applied downstream to each genotype in the array.\n+                // since the prior is applied evenly to the defualt and error models this should not change this selection here.\n+                outputArray[indexForGT] = Math.max(outputArray[indexForGT], modelScoreInLog10);\n+            }\n+        }\n+        return outputArray;\n+    }\n+\n+    /**\n+     * Helper function that actually manages the math for BQD;\n+     *\n+     * This method works by combining the computed genotype scores for reads with the raw allele likelihoods scores for each evidence\n+     *\n+     * @param positionSortedReads  Reads pairs objects (Pair<Pair<read,readBaseOffset>, sampleReadIndex>) objects sorted in the correct order for partitioning.\n+     *                             This means that the \"error\" reads in the partition are sorted by read cycle first in the provided list\n+     * @param homopolymerAdjustment  Penalty to be applied to reads based on the homopolymer run (this should be precomputed for the ref site in quesiton)\n+     * @param readLikelihoodsForGT  The array corresponding to the log_10 genotype scores for the genotype in question\n+     * @param offsetForReadLikelihoodGivenAlleleIndex\n+     * @return phred scale likelihood for a BQD error mode for reads in the given direction according to the offsets requested\n+     */\n+    private double computeBQDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads,\n+                                                final double homopolymerAdjustment, final double[] readLikelihoodsForGT,\n+                                                final int offsetForReadLikelihoodGivenAlleleIndex, final boolean forwards, final int errorAlleleIndex) {\n+        if (positionSortedReads.isEmpty()) {\n+            return 0.0; // TODO check up on this\n+        }\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"errorAllele index: \" + errorAlleleIndex + \" theta: \" + (forwards ? \"1\" : \"2\") + \" homopolymerAdjustment: \" + homopolymerAdjustment);\n+        }\n+\n+        // Forwards strand tables (all in phred space for the sake of conveient debugging with provided scripts)\n+        final double[] cumulativeProbReadForErrorAllele = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeMeanBaseQualityPhredAdjusted = new double[positionSortedReads.size() + 1];\n+        final double[] cumulativeProbGenotype = new double[positionSortedReads.size() + 1];\n+\n+        double totalBaseQuality = 0;\n+        int baseQualityDenominator = 0; // We track this separately because not every read overlaps the SNP in question due to padding.\n+        // Iterate over the reads and populate the cumulative arrays\n+        for (int i = 1; i < cumulativeProbReadForErrorAllele.length; i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer container = positionSortedReads.get(i - 1);\n+            final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+            // Retrieve the homozygous genotype score and the error allele scores for the read in question\n+            final double homozygousGenotypeContribution;\n+            final double errorAlleleContribution;\n+            if (readIndex != -1) {\n+                homozygousGenotypeContribution = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+                errorAlleleContribution = readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+            } else {\n+                // If read index == -1 then we are evaluating a read that was rejected by the HMM and therefore doesn't have genotype scores\n+                homozygousGenotypeContribution = 0;\n+                errorAlleleContribution = 0;\n+            }\n+\n+//            genotyperDebugStream.println(\"read index:\"+readIndex+\" read: \"+container.underlyingRead+\" \"+container.underlyingRead.getCigar()+\" pos: \"+container.getUnclippedPosition()+\"  snpOffset: \"+container.offsetIntoReadForBaseQuality+\"  forward feather end: \"+container.getForwardsFeatherEnd()+\" reverse: \"+container.getReverseFeatherEnd()+\" ihomGT:\"+homozygousGenotypeContribution+\"  errorAlleleContribution:\"+errorAlleleContribution + \" difference in log10: \"+ (errorAlleleContribution - homozygousGenotypeContribution));\n+\n+            // Populate the error probability array in phred space\n+            // Calculation: Alpha * P(r|E_allele) + (1 - Alpha) * P(r | G_homozygousGT))\n+            double phredContributionForRead = (homozygousGenotypeContribution==0 && errorAlleleContribution==0) ? 0 : -10 *\n+                    MathUtils.approximateLog10SumLog10(errorAlleleContribution + cachedLog10ErrorRate,\n+                                                       homozygousGenotypeContribution + cachedLog10NonErrorRate);\n+            cumulativeProbReadForErrorAllele[i] = cumulativeProbReadForErrorAllele[i-1] + phredContributionForRead;\n+\n+            // Populate the cumulative genotype contribution array with the score for this read\n+            // Calculation: (P(r | G_A1) + P(r | G_A2)) / 2\n+            cumulativeProbGenotype[i] = cumulativeProbGenotype[i - 1] + -10 * homozygousGenotypeContribution;\n+\n+            // Populate the mean base quality array\n+            if (container.hasValidBaseQuality()) {\n+                totalBaseQuality += container.getBaseQuality();\n+                baseQualityDenominator++;\n+            }\n+            cumulativeMeanBaseQualityPhredAdjusted[i] = Math.max(0,\n+                    ((totalBaseQuality / (baseQualityDenominator == 0 ? 1 : baseQualityDenominator)) * PHRED_SCALED_ADJUSTMENT_FOR_BQ_SCORE) - homopolymerAdjustment);\n+        }\n+\n+        // Now we find the best partitioning N for the forwards evaluation of the data\n+        double minScoreFound = Double.POSITIVE_INFINITY;\n+        int nIndexUsed = 0;\n+        double lastProbE=0;\n+        double lastGQQual=0;\n+        for (int n = 0; n < cumulativeMeanBaseQualityPhredAdjusted.length; n++) {\n+            final double bqdScore = cumulativeMeanBaseQualityPhredAdjusted[n] + cumulativeProbReadForErrorAllele[n] + (cumulativeProbGenotype[cumulativeProbGenotype.length-1] - cumulativeProbGenotype[n]);\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(String.format(\"n=%d: %.2f, cum_phred_bq=%.2f, cum_prob_r_Error=%.2f, prob_G_remaining=%.2f\",\n+                        n, bqdScore, cumulativeMeanBaseQualityPhredAdjusted[n], cumulativeProbReadForErrorAllele[n],\n+                        (cumulativeProbGenotype[cumulativeProbGenotype.length - 1] - cumulativeProbGenotype[n])));\n+            }\n+            if (minScoreFound > bqdScore) {\n+                minScoreFound = bqdScore;\n+                nIndexUsed = n;\n+            }\n+        }\n+\n+        // Debug output for the genotyper to see into the calculation itself\n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(String.format(\"theta=%d n%d=%2d, best_phred_score =%5.2f q_mean=%5.2f, alpha=%4.2f, Ph(E)=%4.2f;  \", forwards ? 1 : 0,\n+                    (forwards ? 1 : 2), nIndexUsed, minScoreFound, cumulativeMeanBaseQualityPhredAdjusted[nIndexUsed],\n+                    BQD_FIXED_ERROR_RATE, cumulativeProbReadForErrorAllele[nIndexUsed]));\n+        }\n+\n+        //TODO this will be where i put my debuglogging (if i had any)\n+        return minScoreFound;\n+    }\n+\n+    /**\n+     * Calculate the BQD model outputs to the likelihoods array.\n+     *\n+     * This method is responsible for computing critical phred-mapping quality adjustments for the entire pool of reads (Disqualified reads,\n+     * reads only overlapping in low quality ends, and otherwise) and selecting true-allele/error-allele combinations as well as strand model\n+     * combinations (all forward reads/ all reverse reads/ all reads) and calling {@link #computeFRDModelForStrandData} for each of these\n+     * combinations selecting the best scoring columns in the final likelihoods array output.\n+     *\n+     * Like BQD this model genotypes with all reads that overlap the site in either their accepted bases or low quality ends, but it does\n+     * not include disqualified reads for genotyping. All reads are used for computing the critical values for the mapping quality cutoffs.\n+     *\n+     * NOTES:\n+     * - The model currently does not support mixed-allele mode (modifying 0/1 GTs in addition to 0/0 GTs)\n+     * - The model will not treat symbolic alleles specially, always treating them as indels. This might or might not be the best way to handle them.\n+     *\n+     * @param sampleLikelihoods the likelihoods object with allele likelihoods for the reads to be genotyped\n+     * @param ploidyModelLikelihoods standard genotyping model allele likelihoods (to be used for maxEffectiveDepthAdjustment)\n+     * @param readContainers reads (both forwards and reverse orientation as well as disqualified reads) overlapping the site in question\n+     * @param snipAprioriHet prior for heterozygus SNP allele\n+     * @param indelAprioriHet prior for heterozygus indel alleles based on the STRE tables if present\n+     * @param maxEffectiveDepthForHetAdjustment maxEffectiveDepthAdjustment used to reduce the effect of FRD at high depth sites (0 means no adjustment)\n+     * @param calculators DRAGENlikelihoodsCalculator object to manage GT array math\n+     * @return a likelihoods array corrsponding to the log10 likelihoods scores for the best combination of model parameters for each Genotype (Double.NEGATIVE_INFINITY for Genotypes not considered)\n+     */\n+    public <A extends Allele> double[] calculateFRDLikelihoods(final LikelihoodMatrix<GATKRead, A> sampleLikelihoods, final double[] ploidyModelLikelihoods,\n+                                                               final List<DRAGENGenotypesModel.DragenReadContainer> readContainers,\n+                                                               final double snipAprioriHet, final double indelAprioriHet, final int maxEffectiveDepthForHetAdjustment,\n+                                                               final GenotypeLikelihoodCalculators calculators) {\n+        Utils.validate(sampleLikelihoods == cachedLikelihoods, \"There was a mismatch between the sample stored by the genotyper and the one requested for BQD, this will result in invalid genotyping\");\n+        final double[] outputArray = new double[genotypeCount];\n+        Arrays.fill(outputArray, Double.NEGATIVE_INFINITY);\n+\n+        final Allele refAllele = sampleLikelihoods.getAllele(0);\n+\n+        //Determine the size of an allele page for the readsLikelihoodsByAlleleFrequency table\n+        final int readCount = sampleLikelihoods.evidenceCount();\n+        final int alleleDataSize = readCount * (ploidy + 1);\n+\n+        for(int fAlleleIndex = 0; fAlleleIndex < sampleLikelihoods.numberOfAlleles(); fAlleleIndex++) {\n+            // ignore symbolic alleles\n+            final boolean isIndel = sampleLikelihoods.getAllele(fAlleleIndex).length() != refAllele.length();\n+            final int offsetForReadLikelihoodGivenAlleleIndex = alleleDataSize * fAlleleIndex + readCount;\n+\n+            // Here we generate a set of the critical log10(P(F)) values that we will iterate over\n+            final Set<Double> criticalThresholdsForward = new HashSet<>();\n+            final Set<Double> criticalThresholdsReverse = new HashSet<>();\n+            final Set<Double> criticalThresholds = new HashSet<>();\n+            computeCriticalValues(criticalThresholdsForward, criticalThresholdsReverse, criticalThresholds, readContainers, fAlleleIndex == 0 ? 0 : (isIndel? indelAprioriHet : snipAprioriHet)/-10); // simplified in line with DRAGEN, uses 1 alleledist for both snp and indels\n+            final List<Double> criticalThresholdsSortedForward = criticalThresholdsForward.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSortedReverse = criticalThresholdsReverse.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            final List<Double> criticalThresholdsSorted = criticalThresholds.stream().sorted(Double::compareTo).collect(Collectors.toList());\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"fIndex: \" + fAlleleIndex + \" criticalValues: \\n\" + criticalThresholds.stream().map(d -> Double.toString(d)).collect(Collectors.joining(\"\\n\")));\n+            }\n+            // iterate over all of the homozygous genotypes for the given allele\n+            for(int gtAlleleIndex = 0; gtAlleleIndex < sampleLikelihoods.numberOfAlleles(); gtAlleleIndex++) {\n+                // Skip over the allele corresponding to the \"foreign\" allele\n+                if (gtAlleleIndex == fAlleleIndex) {\n+                    continue;\n+                }\n+                // For right now we allow symbolic alleles, but this might be subject to change\n+//                if (sampleLikelihoods.getAllele(fAlleleIndex).isSymbolic() ) {\n+//                    continue;\n+//                }\n+\n+                //This is crufty, it just so happens that the index of the homozygous genotype corresponds to the maximum genotype count per field.\n+                //This should be pulled off as a calculator in some genotyping class.\n+                final int indexForGT = calculators.genotypeCount(ploidy, gtAlleleIndex + 1) - 1;\n+                double[] readLikelihoodsForGT = readLikelihoodsByGenotypeIndex[indexForGT];\n+\n+                // TODO restore the critical thresholds\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"indexForGT \"+indexForGT+ \" ooffsetForReadLikelihoodGivenAlleleIndex =\"+offsetForReadLikelihoodGivenAlleleIndex);\n+                    HaplotypeCallerGenotypingDebugger.println(\"\\nForwards Strands: \");\n+                }\n+                final double[] maxLog10FForwardsStrand = computeFRDModelForStrandData(readContainers, c -> !c.isReverseStrand() , offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nReverse Strands: \");}\n+                final double[] maxLog10FReverseStrand = computeFRDModelForStrandData(readContainers, c -> c.isReverseStrand(), offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {  HaplotypeCallerGenotypingDebugger.println(\"\\nBoth Strands: \");}\n+                final double[] maxLog10FBothStrands = computeFRDModelForStrandData(readContainers, c -> true, offsetForReadLikelihoodGivenAlleleIndex, readLikelihoodsForGT, criticalThresholdsSorted);\n+\n+                if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                    HaplotypeCallerGenotypingDebugger.println(\"gtAlleleIndex : \"+gtAlleleIndex+ \" fAlleleIndex: \"+fAlleleIndex +\" forwards: \"+maxLog10FForwardsStrand+\" reverse: \"+maxLog10FReverseStrand+\" both: \"+maxLog10FBothStrands);\n+                }\n+                double[] localBestModel = maxLog10FForwardsStrand;\n+                if (localBestModel[0] < maxLog10FReverseStrand[0]) {\n+                    localBestModel = maxLog10FReverseStrand;\n+                }\n+                if (localBestModel[0] < maxLog10FBothStrands[0]) {\n+                    localBestModel = maxLog10FBothStrands;\n+                }\n+\n+                // Handle max effective depth adjustment if specified\n+                if (maxEffectiveDepthForHetAdjustment > 0) {\n+                    // Use the index corresponding the mixture of F and\n+                    double localBestModelScore = localBestModel[0] - localBestModel[1];\n+                    int closestGTAlleleIndex = allelesToIndex(gtAlleleIndex, fAlleleIndex);\n+                    double log10LikelihoodsForPloyidyModel = ploidyModelLikelihoods[closestGTAlleleIndex] - MathUtils.log10(2);\n+                    int depthForGenotyping = sampleLikelihoods.evidenceCount();\n+                    double adjustedBestModel = log10LikelihoodsForPloyidyModel + ((localBestModelScore - log10LikelihoodsForPloyidyModel)\n+                            * ((Math.min(depthForGenotyping, maxEffectiveDepthForHetAdjustment) * 1.0) / depthForGenotyping));\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], adjustedBestModel + localBestModel[1]);\n+\n+                    if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                        HaplotypeCallerGenotypingDebugger.println(\"best FRD likelihoods: \"+localBestModelScore+\" P(F) score used: \"+localBestModel[1]+\"  use MaxEffectiveDepth: \"+maxEffectiveDepthForHetAdjustment);\n+                        HaplotypeCallerGenotypingDebugger.println(\"Using array index \"+closestGTAlleleIndex+\" for mixture gt with likelihood of \"+log10LikelihoodsForPloyidyModel+\" adjusted based on depth: \"+depthForGenotyping);\n+                        HaplotypeCallerGenotypingDebugger.println(\"p_rG_adj : \"+adjustedBestModel);\n+                    }\n+                } else {\n+                    outputArray[indexForGT] = Math.max(outputArray[indexForGT], localBestModel[0]);\n+                }\n+\n+            }\n+\n+        }\n+\n+\n+        return outputArray;\n+    }\n+\n+\n+    /**\n+     * @param positionSortedReads read containers to use for genotyping\n+     * @param predicate predicate used to select the correct orientation combination for reads when genotyping\n+     * @param offsetForReadLikelihoodGivenAlleleIndex offset corresponding to the Error Allele in the reads likelihoods object array\n+     * @param readLikelihoodsForGT reads likelihoods for Genotype array table\n+     * @param criticalThresholdsSorted critical thresholds to use for this orientation combination\n+     * @return two doubles, index 0 is the frd score and the second is log p(F()) score used to adjust the score\n+     */\n+    private double[] computeFRDModelForStrandData(final List<DRAGENGenotypesModel.DragenReadContainer> positionSortedReads, final Predicate<DRAGENGenotypesModel.DragenReadContainer> predicate,\n+                                                  final int offsetForReadLikelihoodGivenAlleleIndex, final double[] readLikelihoodsForGT, final List<Double> criticalThresholdsSorted) {\n+        if (positionSortedReads.isEmpty()) {\n+            return new double[]{Double.NEGATIVE_INFINITY, 0};\n+        }\n+\n+        int counter = 0;\n+        double maxLpspi = Double.NEGATIVE_INFINITY;\n+        double lpfApplied = 0;\n+\n+        for (final Double logProbFAllele : criticalThresholdsSorted) {\n+            double fAlleleProbRatio = 0.0;\n+            double fAlleleProbDenom = 0.0;\n+            double localMaxLpspi = Double.NEGATIVE_INFINITY;\n+\n+            // iterate over the reads to compute the foreign allele alpha to use for genotyping with FRD\n+            for (final DRAGENGenotypesModel.DragenReadContainer container : positionSortedReads) {\n+                // Ignore reads that were disqualified by the HMM\n+                if (container.wasFilteredByHMM()) {\n+                    continue;\n+                }\n+\n+                final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+                // Keep track of the aggregate support for the foreign allele\n+                if (predicate.test(container)) {\n+                    // Only include reads with mapping quality adjustment < the critical threshold being used (i.e. exclude reads with MQ > than the threshold)\n+                    double LPd_r_F = container.getPhredPFValue() + 0.0000001 <= logProbFAllele ?\n+                            Double.NEGATIVE_INFINITY :\n+                            readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+                    double lp_r_GT = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+\n+                    fAlleleProbRatio += Math.pow(10, LPd_r_F - MathUtils.approximateLog10SumLog10(LPd_r_F, lp_r_GT));\n+                    fAlleleProbDenom++;\n+                }\n+            }\n+\n+            // Don't learn the beta but approximate it based on the read support for the alt\n+            double foreignAlleleLikelihood = Math.min(fAlleleProbRatio / fAlleleProbDenom, 0.5);\n+            double log10ForeignAlleleLikelihood = Math.log10(foreignAlleleLikelihood);\n+            double log10NotForeignAlleleLikelihood = Math.log10(1.0 - foreignAlleleLikelihood);\n+            double cumulativeLog10LikelihoodOfForeignReadHypothesis = 0.0; // LP_R_GF\n+\n+            // iterate over the containers again using the approximated beta constraint\n+            for (final DRAGENGenotypesModel.DragenReadContainer container : positionSortedReads) {\n+                // Ignore reads that were disqualified by the HMM\n+                if (container.wasFilteredByHMM()) {\n+                    continue;\n+                }\n+                final int readIndex = container.getIndexInLikelihoodsObject();\n+\n+                double log10LikelihoodReadForGenotype = readLikelihoodsForGT[readIndex] - MathUtils.log10(2);\n+\n+                // COMPUTE THE MODEL FOR THE STRAND IN QUESTION\n+                if (predicate.test(container)) {\n+                    double log10LikelihoodOfForeignAlleleGivenLPFCutoff = container.getPhredPFValue() + 0.0000001 <= logProbFAllele ?\n+                            Double.NEGATIVE_INFINITY :\n+                            readAlleleLikelihoodByAlleleCount[offsetForReadLikelihoodGivenAlleleIndex + readIndex];\n+\n+                    cumulativeLog10LikelihoodOfForeignReadHypothesis += MathUtils.approximateLog10SumLog10(log10ForeignAlleleLikelihood + log10LikelihoodOfForeignAlleleGivenLPFCutoff, log10NotForeignAlleleLikelihood + log10LikelihoodReadForGenotype);\n+                } else {\n+                    cumulativeLog10LikelihoodOfForeignReadHypothesis += log10LikelihoodReadForGenotype;\n+                }\n+            }\n+            // Allele prior for error allele, plus posterior for foreign event, plus model posterior\n+            double LPsi = logProbFAllele + cumulativeLog10LikelihoodOfForeignReadHypothesis; // NOTE unlike DRAGEN we apply the prior to the combined likelihoods array after the fact so gtAllelePrior is not included at this stage\n+            localMaxLpspi = Math.max(localMaxLpspi, LPsi);\n+\n+            if (HaplotypeCallerGenotypingDebugger.exists()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"beta: \"+foreignAlleleLikelihood+\" localMaxLpspi: \" + localMaxLpspi + \" for lpf: \"+logProbFAllele+\" with LP_R_GF: \"+cumulativeLog10LikelihoodOfForeignReadHypothesis+\" index: \"+counter++);\n+            }\n+            if (localMaxLpspi > maxLpspi) {\n+                maxLpspi = Math.max(maxLpspi, localMaxLpspi);\n+                lpfApplied = logProbFAllele;\n+            }\n+        }\n+\n+        //TODO javaize this\n+        // TODO soon should not need to use the LPF applied here...\n+        return new double[]{maxLpspi, lpfApplied};\n+    }\n+\n+\n+    // TODO: for reviewer... this code is meant to handle the performance regression brought about by FRD. Essentially in DRAGEN for every\n+    // TODO  combination of strandedness and mapping quality a computation is done, this is vaguely unnecessary. Because this leads to\n+    // TODO  a bias towards strandedness/allele combinations that have very few reads supporting them by virtue of the fact that a different\n+    // TODO  strand/allele combination had at least one low mapping quality read. I have reverted the change right now and consequently this genotyping\n+    // TODO  is taking somewhere in the order of ~5-6% runtime on the profiler whereas otherwise it could correspond to much less at the expense\n+    // TODO  of not matching DRAGEN properly.\n+    // helper method to populate the reads containers properly with their critical values and store them in the provided set\n+    // NOTE: this has the side effect of setting the DragenReadContainer setPhredPFValue() values for the reads for the given set of alleles\n+    private void computeCriticalValues(final Set<Double> criticalThresholdsForwards, final Set<Double> criticalThresholdsReverse, final Set<Double> criticalThresholdsTotal, final List<DRAGENGenotypesModel.DragenReadContainer> container, final double log10MapqPriorAdjustment) {\n+        for (int i = 0; i < container.size(); i++) {\n+            final DRAGENGenotypesModel.DragenReadContainer readContainer = container.get(i);\n+            final double log10CriticalValue = readContainer.getPhredScaledMappingQuality() / -10.0 + log10MapqPriorAdjustment;\n+            readContainer.setPhredPFValue(log10CriticalValue);\n+            // Split the critical thresholds up by their applicable strands in order to avoid repeated work\n+            if (readContainer.isReverseStrand()) {\n+                criticalThresholdsReverse.add(log10CriticalValue);\n+            } else {\n+                criticalThresholdsForwards.add(log10CriticalValue);\n+            }\n+            criticalThresholdsTotal.add(log10CriticalValue);\n+        }\n+    }\n+\n+\n+    /**\n+     * See {@link GenotypeLikelihoodCalculator#genotypeLikelihoods}. This wrapper just enforces that the likelihoods object is recorded in the cache.\n+     *\n+     * @return never {@code null}.\n+     */\n+    public <EVIDENCE, A extends Allele> GenotypeLikelihoods genotypeLikelihoods(final LikelihoodMatrix<EVIDENCE, A> likelihoods) {\n+        cachedLikelihoods = null;\n+        GenotypeLikelihoods output = super.genotypeLikelihoods(likelihoods);\n+        cachedLikelihoods = likelihoods;\n+        return output;\n+    }\n+\n+    /**\n+     * See {@link GenotypeLikelihoodCalculator#getReadRawReadLikelihoodsByGenotypeIndex}. This wrapper just enforces that the likelihoods object is recorded in the cache.\n+     *\n+     * @return never {@code null}.\n+     */\n+    public <EVIDENCE, A extends Allele> double[] rawGenotypeLikelihoods(final LikelihoodMatrix<EVIDENCE, A> likelihoods) {\n+        cachedLikelihoods = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 482}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0NzM0Nw==", "bodyText": "I guess intended to convert phred to log10 but need to check it is correct as the name of the method right for it suggest it is allready log10.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456647347", "createdAt": "2020-07-17T19:59:03Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java", "diffHunk": "@@ -168,7 +171,16 @@ public VariantContext calculateGenotypes(final VariantContext vc, final List<Var\n         // create the genotypes\n         //TODO: omit subsetting if output alleles is not a proper subset of vc.getAlleles\n         final GenotypesContext genotypes = outputAlleles.size() == 1 ? GATKVariantContextUtils.subsetToRefOnly(vc, defaultPloidy) :\n-                AlleleSubsettingUtils.subsetAlleles(vc.getGenotypes(), defaultPloidy, vc.getAlleles(), outputAlleles, GenotypeAssignmentMethod.USE_PLS_TO_ASSIGN, vc.getAttributeAsInt(VCFConstants.DEPTH_KEY, 0));\n+                AlleleSubsettingUtils.subsetAlleles(vc.getGenotypes(), defaultPloidy, vc.getAlleles(), outputAlleles, gpc, configuration.genotypeArgs.genotypeAssignmentMethod, vc.getAttributeAsInt(VCFConstants.DEPTH_KEY, 0));\n+\n+        if (configuration.genotypeArgs.usePosteriorProbabilitiesToCalculateQual && hasPosteriors(genotypes)) {\n+            final double log10NoVariantPosterior = nonVariantPresentLog10PosteriorProbability(genotypes) * -.1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0MDQzMg=="}, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1MDM0MA==", "bodyText": "well spotted, since is it phred scale it should be the max not the min.... but I guess is even better to do a sum. will do.\nSince we have always run it only on single sample runs didn't make a difference.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456650340", "createdAt": "2020-07-17T20:06:16Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java", "diffHunk": "@@ -177,8 +189,66 @@ public VariantContext calculateGenotypes(final VariantContext vc, final List<Var\n         return builder.genotypes(genotypes).attributes(attributes).make();\n     }\n \n+    protected double nonVariantPresentLog10PosteriorProbability(final GenotypesContext gc) {\n+        return gc.stream()\n+                .map(gt -> gt.getExtendedAttribute(VCFConstants.GENOTYPE_POSTERIORS_KEY))\n+                .mapToDouble(v -> coherceToDouble(v, Double.NaN, true))\n+                .filter(v -> !Double.isNaN(v))\n+                .min().orElse(Double.NaN);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1MDk5MA==", "bodyText": "Will check... if there is no gain will remove, if that method is in GATK and not in htjsdk I will instead move the improvements there, in neither of those is true, will leave it as it is.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456650990", "createdAt": "2020-07-17T20:07:44Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java", "diffHunk": "@@ -177,8 +189,66 @@ public VariantContext calculateGenotypes(final VariantContext vc, final List<Var\n         return builder.genotypes(genotypes).attributes(attributes).make();\n     }\n \n+    protected double nonVariantPresentLog10PosteriorProbability(final GenotypesContext gc) {\n+        return gc.stream()\n+                .map(gt -> gt.getExtendedAttribute(VCFConstants.GENOTYPE_POSTERIORS_KEY))\n+                .mapToDouble(v -> coherceToDouble(v, Double.NaN, true))\n+                .filter(v -> !Double.isNaN(v))\n+                .min().orElse(Double.NaN);\n+    }\n+\n+    private double coherceToDouble(final Object obj, final double defaultValue, final boolean takeFirstElement) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0MTM5Ng=="}, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1MTE5Mw==", "bodyText": "Ok.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456651193", "createdAt": "2020-07-17T20:08:14Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java", "diffHunk": "@@ -177,8 +189,66 @@ public VariantContext calculateGenotypes(final VariantContext vc, final List<Var\n         return builder.genotypes(genotypes).attributes(attributes).make();\n     }\n \n+    protected double nonVariantPresentLog10PosteriorProbability(final GenotypesContext gc) {\n+        return gc.stream()\n+                .map(gt -> gt.getExtendedAttribute(VCFConstants.GENOTYPE_POSTERIORS_KEY))\n+                .mapToDouble(v -> coherceToDouble(v, Double.NaN, true))\n+                .filter(v -> !Double.isNaN(v))\n+                .min().orElse(Double.NaN);\n+    }\n+\n+    private double coherceToDouble(final Object obj, final double defaultValue, final boolean takeFirstElement) {\n+        if (obj == null) {\n+            return defaultValue;\n+        } else if (obj instanceof CharSequence) {\n+            try {\n+                return Double.parseDouble(obj.toString());\n+            } catch (final NumberFormatException ex) {\n+                return defaultValue;\n+            }\n+        } else if (obj instanceof Number) {\n+            return ((Number) obj).doubleValue();\n+        } else if (takeFirstElement) {\n+            if (obj instanceof Collection) {\n+                if( ((Collection)obj).isEmpty()) {\n+                    return defaultValue;\n+                } else if (obj instanceof List) {\n+                    final List<?> asList = (List<?>) obj;\n+                    return coherceToDouble(asList.get(0), defaultValue, false);\n+                } else {\n+                    final Collection<?> collection = (Collection<?>) obj;\n+                    return coherceToDouble(collection.iterator().next(), defaultValue, false);\n+                }\n+            } else if (obj.getClass().isArray()) {\n+                if (obj.getClass().getComponentType().isPrimitive()) {\n+                    if (Array.getLength(obj) == 0) {\n+                        return defaultValue;\n+                    } else {\n+                        return coherceToDouble(Array.get(obj, 1), defaultValue, false);\n+                    }\n+                } else {\n+                    final Object[] array = (Object[]) obj;\n+                    return array.length != 0 ? coherceToDouble(array[0], defaultValue, false) : defaultValue;\n+                }\n+            } else {\n+                return defaultValue;\n+            }\n+        } else {\n+            return defaultValue;\n+        }\n+    }\n+\n+    private boolean hasPosteriors(final GenotypesContext gc) {\n+        for (final Genotype genotype : gc) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0MTczNw=="}, "originalCommit": {"oid": "14007d11529c2c3c85951793ea283a2a542d6f8c"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1MjUwMw==", "bodyText": "oh yeah I guess I left in the IntelliJ's generated code", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456652503", "createdAt": "2020-07-17T20:11:16Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/afcalc/DragstrAlleleFrequencyCalculator.java", "diffHunk": "@@ -0,0 +1,51 @@\n+package org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleList;\n+import org.broadinstitute.hellbender.utils.pairhmm.DragstrParams;\n+\n+public class DragstrAlleleFrequencyCalculator implements AlleleFrequencyCalculator {\n+\n+    private static final double HOM_REF_PRIOR = 0;\n+    private static final double SNP_SIMPLE_HET_PRIOR = 34.77;\n+    private static final double SNP_COMPOSITE_HET_PRIOR = 69.54;\n+    private static final double SNP_HOMVAR_PRIOR = 37.77;\n+\n+    private final double api;\n+    private final int defaultPloidy;\n+\n+    private DragstrAlleleFrequencyCalculator(final double api, final int defaultPloidy) {\n+        this.api = api;\n+        this.defaultPloidy = defaultPloidy;\n+    }\n+\n+    public static DragstrAlleleFrequencyCalculator makeCalculator(final DragstrParams params, final int period, final int repeats, final int defaultPloidy) {\n+        return new DragstrAlleleFrequencyCalculator(params.api(period, repeats), defaultPloidy);\n+    }\n+\n+    @Override\n+    public int getPloidy() {\n+        return defaultPloidy;\n+    }\n+\n+    @Override\n+    public double[] getPriorFrequencies(AlleleList<Allele> alleleList) {\n+        return new double[0];\n+    }\n+\n+    @Override\n+    public AFCalculationResult calculate(VariantContext vc) {\n+        return null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3NjY1Mw=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1Mzg3OQ==", "bodyText": "Wow! need to explain the loops right below... they are quite enignmtic.... so readsToUse is kinda sorted the same way as regions.getReads() .....", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456653879", "createdAt": "2020-07-17T20:14:40Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyBasedCallerUtils.java", "diffHunk": "@@ -102,17 +104,33 @@ public static void finalizeRegion(final AssemblyRegion region,\n         final List<GATKRead> readsToUse = region.getReads().stream()\n                 // TODO unclipping soft clips may introduce bases that aren't in the extended region if the unclipped bases\n                 // TODO include a deletion w.r.t. the reference.  We must remove kmers that occur before the reference haplotype start\n-                .map(read -> skipSoftClips || ! ReadUtils.hasWellDefinedFragmentSize(read) ?\n+                .map(read -> dontUseSoftClippedBases || ! ReadUtils.hasWellDefinedFragmentSize(read) ?\n                     ReadClipper.hardClipSoftClippedBases(read) : ReadClipper.revertSoftClippedBases(read))\n-                .map(read -> ReadClipper.hardClipLowQualEnds(read, minTailQualityToUse))\n+                .map(read -> softClipLowQualityEnds ? ReadClipper.softClipLowQualEnds(read, minTailQualityToUse) :\n+                        ReadClipper.hardClipLowQualEnds(read, minTailQualityToUse))\n                 .filter(read -> read.getStart() <= read.getEnd())\n                 .map(read -> read.isUnmapped() ? read : ReadClipper.hardClipAdaptorSequence(read))\n                 .filter(read ->  !read.isEmpty() && read.getCigar().getReadLength() > 0)\n                 .map(read -> ReadClipper.hardClipToRegion(read, region.getPaddedSpan().getStart(), region.getPaddedSpan().getEnd() ))\n                 .filter(read -> read.getStart() <= read.getEnd() && read.getLength() > 0 && read.overlaps(region.getPaddedSpan()))\n-                .sorted(new ReadCoordinateComparator(readsHeader)) // TODO: sort may be unnecessary here\n                 .collect(Collectors.toList());\n \n+        // This will probably have to change...", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1Njc2MA==", "bodyText": "Ok so it seems that the loops position need to be swap... for each in readsToUse find it in original reads and if the flags are the same we set the attribute's value. ... this is very cumbersome and seems inefficient.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456656760", "createdAt": "2020-07-17T20:22:12Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyBasedCallerUtils.java", "diffHunk": "@@ -102,17 +104,33 @@ public static void finalizeRegion(final AssemblyRegion region,\n         final List<GATKRead> readsToUse = region.getReads().stream()\n                 // TODO unclipping soft clips may introduce bases that aren't in the extended region if the unclipped bases\n                 // TODO include a deletion w.r.t. the reference.  We must remove kmers that occur before the reference haplotype start\n-                .map(read -> skipSoftClips || ! ReadUtils.hasWellDefinedFragmentSize(read) ?\n+                .map(read -> dontUseSoftClippedBases || ! ReadUtils.hasWellDefinedFragmentSize(read) ?\n                     ReadClipper.hardClipSoftClippedBases(read) : ReadClipper.revertSoftClippedBases(read))\n-                .map(read -> ReadClipper.hardClipLowQualEnds(read, minTailQualityToUse))\n+                .map(read -> softClipLowQualityEnds ? ReadClipper.softClipLowQualEnds(read, minTailQualityToUse) :\n+                        ReadClipper.hardClipLowQualEnds(read, minTailQualityToUse))\n                 .filter(read -> read.getStart() <= read.getEnd())\n                 .map(read -> read.isUnmapped() ? read : ReadClipper.hardClipAdaptorSequence(read))\n                 .filter(read ->  !read.isEmpty() && read.getCigar().getReadLength() > 0)\n                 .map(read -> ReadClipper.hardClipToRegion(read, region.getPaddedSpan().getStart(), region.getPaddedSpan().getEnd() ))\n                 .filter(read -> read.getStart() <= read.getEnd() && read.getLength() > 0 && read.overlaps(region.getPaddedSpan()))\n-                .sorted(new ReadCoordinateComparator(readsHeader)) // TODO: sort may be unnecessary here\n                 .collect(Collectors.toList());\n \n+        // This will probably have to change...", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1Mzg3OQ=="}, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1NzE4OQ==", "bodyText": "Why not simply set the variable for all in the stream above and add a last step to null the ones with different flags when compare to the original?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456657189", "createdAt": "2020-07-17T20:23:19Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyBasedCallerUtils.java", "diffHunk": "@@ -102,17 +104,33 @@ public static void finalizeRegion(final AssemblyRegion region,\n         final List<GATKRead> readsToUse = region.getReads().stream()\n                 // TODO unclipping soft clips may introduce bases that aren't in the extended region if the unclipped bases\n                 // TODO include a deletion w.r.t. the reference.  We must remove kmers that occur before the reference haplotype start\n-                .map(read -> skipSoftClips || ! ReadUtils.hasWellDefinedFragmentSize(read) ?\n+                .map(read -> dontUseSoftClippedBases || ! ReadUtils.hasWellDefinedFragmentSize(read) ?\n                     ReadClipper.hardClipSoftClippedBases(read) : ReadClipper.revertSoftClippedBases(read))\n-                .map(read -> ReadClipper.hardClipLowQualEnds(read, minTailQualityToUse))\n+                .map(read -> softClipLowQualityEnds ? ReadClipper.softClipLowQualEnds(read, minTailQualityToUse) :\n+                        ReadClipper.hardClipLowQualEnds(read, minTailQualityToUse))\n                 .filter(read -> read.getStart() <= read.getEnd())\n                 .map(read -> read.isUnmapped() ? read : ReadClipper.hardClipAdaptorSequence(read))\n                 .filter(read ->  !read.isEmpty() && read.getCigar().getReadLength() > 0)\n                 .map(read -> ReadClipper.hardClipToRegion(read, region.getPaddedSpan().getStart(), region.getPaddedSpan().getEnd() ))\n                 .filter(read -> read.getStart() <= read.getEnd() && read.getLength() > 0 && read.overlaps(region.getPaddedSpan()))\n-                .sorted(new ReadCoordinateComparator(readsHeader)) // TODO: sort may be unnecessary here\n                 .collect(Collectors.toList());\n \n+        // This will probably have to change...", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1Mzg3OQ=="}, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1NzkwNQ==", "bodyText": "I things is just better to create a single method that would do all the work for a given read in a single map operation.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456657905", "createdAt": "2020-07-17T20:25:07Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyBasedCallerUtils.java", "diffHunk": "@@ -102,17 +104,33 @@ public static void finalizeRegion(final AssemblyRegion region,\n         final List<GATKRead> readsToUse = region.getReads().stream()\n                 // TODO unclipping soft clips may introduce bases that aren't in the extended region if the unclipped bases\n                 // TODO include a deletion w.r.t. the reference.  We must remove kmers that occur before the reference haplotype start\n-                .map(read -> skipSoftClips || ! ReadUtils.hasWellDefinedFragmentSize(read) ?\n+                .map(read -> dontUseSoftClippedBases || ! ReadUtils.hasWellDefinedFragmentSize(read) ?\n                     ReadClipper.hardClipSoftClippedBases(read) : ReadClipper.revertSoftClippedBases(read))\n-                .map(read -> ReadClipper.hardClipLowQualEnds(read, minTailQualityToUse))\n+                .map(read -> softClipLowQualityEnds ? ReadClipper.softClipLowQualEnds(read, minTailQualityToUse) :\n+                        ReadClipper.hardClipLowQualEnds(read, minTailQualityToUse))\n                 .filter(read -> read.getStart() <= read.getEnd())\n                 .map(read -> read.isUnmapped() ? read : ReadClipper.hardClipAdaptorSequence(read))\n                 .filter(read ->  !read.isEmpty() && read.getCigar().getReadLength() > 0)\n                 .map(read -> ReadClipper.hardClipToRegion(read, region.getPaddedSpan().getStart(), region.getPaddedSpan().getEnd() ))\n                 .filter(read -> read.getStart() <= read.getEnd() && read.getLength() > 0 && read.overlaps(region.getPaddedSpan()))\n-                .sorted(new ReadCoordinateComparator(readsHeader)) // TODO: sort may be unnecessary here\n                 .collect(Collectors.toList());\n \n+        // This will probably have to change...", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1Mzg3OQ=="}, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1OTUxNg==", "bodyText": "I think you can use some like Map::computeIfAbsent... to do all this in a single operation.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456659516", "createdAt": "2020-07-17T20:28:57Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyBasedCallerUtils.java", "diffHunk": "@@ -584,12 +606,18 @@ public static void annotateReadLikelihoodsWithSupportedAlleles(final VariantCont\n                     }\n \n                 } else {\n-                    // the event starts prior to the current location, so it's a spanning deletion\n-                    if (! result.containsKey(Allele.SPAN_DEL)) {\n-                        result.put(Allele.SPAN_DEL, new ArrayList<>());\n+                    if (emitSpanningDels) {\n+                        // the event starts prior to the current location, so it's a spanning deletion\n+                        if (!result.containsKey(Allele.SPAN_DEL)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY2MDgwNw==", "bodyText": "can be final.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456660807", "createdAt": "2020-07-17T20:32:08Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyResultSet.java", "diffHunk": "@@ -519,13 +520,40 @@ public void regenerateVariationEvents(int maxMnpDistance) {\n     }\n \n     /**\n-     * Get all of the VariantContexts in the event maps for all haplotypes, sorted by their start position\n+     * Get all of the VariantContexts in the event maps for all haplotypes, sorted by their start position and then arbitrarily by indel length followed by bases\n      * @param haplotypes the set of haplotypes to grab the VCs from\n      * @return a sorted set of variant contexts\n      */\n     private static SortedSet<VariantContext> getAllVariantContexts( final List<Haplotype> haplotypes ) {\n         // Using the cigar from each called haplotype figure out what events need to be written out in a VCF file\n-        final TreeSet<VariantContext> vcs = new TreeSet<>(Comparator.comparingInt(VariantContext::getStart));\n+        final TreeSet<VariantContext> vcs = new TreeSet<>(\n+                Comparator.comparingInt(VariantContext::getStart)\n+                        // Decide arbitrarily so as not to accidentally throw away overlapping variants\n+                .thenComparing(new Comparator<VariantContext>() {\n+                    @Override\n+                    public int compare(VariantContext v1, VariantContext v2) {\n+                        List<Allele> v1Alleles = v1.getAlleles();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY2MTUzOQ==", "bodyText": "You could rewrithis comparator using more chained .thenComparing  since you started.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456661539", "createdAt": "2020-07-17T20:34:01Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyResultSet.java", "diffHunk": "@@ -519,13 +520,40 @@ public void regenerateVariationEvents(int maxMnpDistance) {\n     }\n \n     /**\n-     * Get all of the VariantContexts in the event maps for all haplotypes, sorted by their start position\n+     * Get all of the VariantContexts in the event maps for all haplotypes, sorted by their start position and then arbitrarily by indel length followed by bases\n      * @param haplotypes the set of haplotypes to grab the VCs from\n      * @return a sorted set of variant contexts\n      */\n     private static SortedSet<VariantContext> getAllVariantContexts( final List<Haplotype> haplotypes ) {\n         // Using the cigar from each called haplotype figure out what events need to be written out in a VCF file\n-        final TreeSet<VariantContext> vcs = new TreeSet<>(Comparator.comparingInt(VariantContext::getStart));\n+        final TreeSet<VariantContext> vcs = new TreeSet<>(\n+                Comparator.comparingInt(VariantContext::getStart)\n+                        // Decide arbitrarily so as not to accidentally throw away overlapping variants\n+                .thenComparing(new Comparator<VariantContext>() {\n+                    @Override\n+                    public int compare(VariantContext v1, VariantContext v2) {\n+                        List<Allele> v1Alleles = v1.getAlleles();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY2MDgwNw=="}, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY2MjcxOA==", "bodyText": "If you want to keep it like this is a bit more readable using '!=' rather than '==':\nif ((diff = expr1) != 0) {\n   return diff;\n} else if ((diff = expr2) != 0) {\n   return diff;\n} else if ... {\n   ...\n} else {\n   return exprN; // for the last one we don't need to check the result.\n}", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456662718", "createdAt": "2020-07-17T20:36:47Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyResultSet.java", "diffHunk": "@@ -519,13 +520,40 @@ public void regenerateVariationEvents(int maxMnpDistance) {\n     }\n \n     /**\n-     * Get all of the VariantContexts in the event maps for all haplotypes, sorted by their start position\n+     * Get all of the VariantContexts in the event maps for all haplotypes, sorted by their start position and then arbitrarily by indel length followed by bases\n      * @param haplotypes the set of haplotypes to grab the VCs from\n      * @return a sorted set of variant contexts\n      */\n     private static SortedSet<VariantContext> getAllVariantContexts( final List<Haplotype> haplotypes ) {\n         // Using the cigar from each called haplotype figure out what events need to be written out in a VCF file\n-        final TreeSet<VariantContext> vcs = new TreeSet<>(Comparator.comparingInt(VariantContext::getStart));\n+        final TreeSet<VariantContext> vcs = new TreeSet<>(\n+                Comparator.comparingInt(VariantContext::getStart)\n+                        // Decide arbitrarily so as not to accidentally throw away overlapping variants\n+                .thenComparing(new Comparator<VariantContext>() {\n+                    @Override\n+                    public int compare(VariantContext v1, VariantContext v2) {\n+                        List<Allele> v1Alleles = v1.getAlleles();\n+                        List<Allele> v2Alleles = v2.getAlleles();\n+                        Utils.validate(v1Alleles.size() == 2, () -> \"Error Haplotype event map Variant Context has too many alleles: \"+v1);\n+                        Utils.validate(v2Alleles.size() == 2, () -> \"Error Haplotype event map Variant Context has too many alleles: \"+v2);\n+\n+                        int diff = v1.getReference().length() - v2.getReference().length();\n+                        if (diff == 0) {\n+                            byte[] v1allele = v1.getAlternateAllele(0).getBases();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY2MzY0MQ==", "bodyText": "Perhaps override Region's toString with something meaninful.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456663641", "createdAt": "2020-07-17T20:39:14Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerEngine.java", "diffHunk": "@@ -587,24 +604,59 @@ public ActivityProfileState isActive( final AlignmentContext context, final Refe\n             return referenceModelForNoVariation(region, false, VCpriors);\n         }\n \n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"\\n=================================================\");\n+            HaplotypeCallerGenotypingDebugger.println(\"assemblyRegion: \"+new SimpleInterval(region));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY2NzkwMA==", "bodyText": "isEnabled() would be better?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r456667900", "createdAt": "2020-07-17T20:49:52Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerGenotypingDebugger.java", "diffHunk": "@@ -0,0 +1,45 @@\n+package org.broadinstitute.hellbender.tools.walkers.haplotypecaller;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+\n+import java.io.IOException;\n+import java.io.PrintStream;\n+import java.nio.file.Files;\n+\n+/**\n+ * A short helper class that manages a singleton debug stream for HaplotypeCaller genotyping information that is useful for debugging.\n+ *\n+ * In order to use simply call initialize() providing a location for an output path, then call exists() to evaluate if the processing\n+ * for printing debug statements should be performed, and call println() to output to the file in a synchronized fashion.\n+ */\n+public class HaplotypeCallerGenotypingDebugger{\n+    private static PrintStream genotyperDebugOutStream;\n+\n+    public static void initialize(final String debugLocation) {\n+        try {\n+            genotyperDebugOutStream = new PrintStream(Files.newOutputStream(IOUtils.getPath(debugLocation)));\n+        } catch (IOException e) {\n+            throw new UserException.CouldNotCreateOutputFile(debugLocation, \"Provided argument for genotyper debug location could not be created\");\n+        }\n+    }\n+\n+    // Is the debugger enabled\n+    public static boolean exists() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzE4NzYxNQ==", "bodyText": "I would packet debug block like this ones in private methods to make the code more readable.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457187615", "createdAt": "2020-07-20T08:48:06Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerEngine.java", "diffHunk": "@@ -587,24 +604,59 @@ public ActivityProfileState isActive( final AlignmentContext context, final Refe\n             return referenceModelForNoVariation(region, false, VCpriors);\n         }\n \n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"\\n=================================================\");\n+            HaplotypeCallerGenotypingDebugger.println(\"assemblyRegion: \"+new SimpleInterval(region));\n+            HaplotypeCallerGenotypingDebugger.println(\"=================================================\");\n+        }\n+\n         // evaluate each sample's reads against all haplotypes\n         final List<Haplotype> haplotypes = assemblyResult.getHaplotypeList();\n         final Map<String,List<GATKRead>> reads = AssemblyBasedCallerUtils.splitReadsBySample(samplesList, readsHeader, regionForGenotyping.getReads());\n \n+        if (HaplotypeCallerGenotypingDebugger.exists()) {\n+            HaplotypeCallerGenotypingDebugger.println(\"\\nUnclipped Haplotypes(\"+haplotypes.size()+\"):\");\n+            for (Haplotype haplotype : untrimmedAssemblyResult.getHaplotypeList()) {\n+                HaplotypeCallerGenotypingDebugger.println(\"[\"+haplotype.getStartPosition()+\"-\"+haplotype.getStopPosition()+\"] k=\"+haplotype.getKmerSize()+\" len: \"+haplotype.length()+\" \"+haplotype.getCigar()+(haplotype.isReference()?\"ref\":\"\"));\n+                HaplotypeCallerGenotypingDebugger.println(haplotype.toString());\n+            }\n+\n+            HaplotypeCallerGenotypingDebugger.println(\"\\nClipped Haplotyes(\"+haplotypes.size()+\"):\");\n+            for (Haplotype haplotype : haplotypes) {\n+                HaplotypeCallerGenotypingDebugger.println(\"[\"+haplotype.getStartPosition()+\"-\"+haplotype.getStopPosition()+\"] k=\"+haplotype.getKmerSize()+\" len: \"+haplotype.length()+\" \"+haplotype.getCigar()+(haplotype.isReference()?\"ref\":\"\"));\n+                HaplotypeCallerGenotypingDebugger.println(haplotype.toString());\n+            }\n+            HaplotypeCallerGenotypingDebugger.println(\"\");\n+        }\n+\n         // Calculate the likelihoods: CPU intensive part.\n         final AlleleLikelihoods<GATKRead, Haplotype> readLikelihoods =\n                 likelihoodCalculationEngine.computeReadLikelihoods(assemblyResult, samplesList, reads);\n \n         // Realign reads to their best haplotype.\n-        final Map<GATKRead, GATKRead> readRealignments = AssemblyBasedCallerUtils.realignReadsToTheirBestHaplotype(readLikelihoods, assemblyResult.getReferenceHaplotype(), assemblyResult.getPaddedReferenceLoc(), aligner);\n-        readLikelihoods.changeEvidence(readRealignments);\n+        if (!hcArgs.retainBasedOnOriginalAlignment) {\n+            final Map<GATKRead, GATKRead> readRealignments = AssemblyBasedCallerUtils.realignReadsToTheirBestHaplotype(readLikelihoods, assemblyResult.getReferenceHaplotype(), assemblyResult.getPaddedReferenceLoc(), aligner);\n+            readLikelihoods.changeEvidence(readRealignments);\n+        }\n \n         // Note: we used to subset down at this point to only the \"best\" haplotypes in all samples for genotyping, but there\n         //  was a bad interaction between that selection and the marginalization that happens over each event when computing\n         //  GLs.  In particular, for samples that are heterozygous non-reference (B/C) the marginalization for B treats the\n         //  haplotype containing C as reference (and vice versa).  Now this is fine if all possible haplotypes are included\n         //  in the genotyping, but we lose information if we select down to a few haplotypes.  [EB]\n \n+        if (HaplotypeCallerGenotypingDebugger.exists()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzE4ODg1Nw==", "bodyText": "Break the line in severlap, too long.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457188857", "createdAt": "2020-07-20T08:49:46Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerEngine.java", "diffHunk": "@@ -726,7 +779,10 @@ public void shutdown() {\n \n         final Set<GATKRead> readsToRemove = new LinkedHashSet<>();\n         for( final GATKRead rec : activeRegion.getReads() ) {\n-            if( rec.getLength() < READ_LENGTH_FILTER_THRESHOLD || rec.getMappingQuality() < READ_QUALITY_FILTER_THRESHOLD || ! ReadFilterLibrary.MATE_ON_SAME_CONTIG_OR_NO_MAPPED_MATE.test(rec) || (hcArgs.keepRG != null && !rec.getReadGroup().equals(hcArgs.keepRG)) ) {\n+            if( AlignmentUtils.unclippedReadLength(rec) < READ_LENGTH_FILTER_THRESHOLD || rec.getMappingQuality() < hcArgs.mappingQualityThreshold || ! ReadFilterLibrary.MATE_ON_SAME_CONTIG_OR_NO_MAPPED_MATE.test(rec) || (hcArgs.keepRG != null && !rec.getReadGroup().equals(hcArgs.keepRG)) ) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 195}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzE4OTIxNg==", "bodyText": "perhaps a private test method to encapsulate it?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457189216", "createdAt": "2020-07-20T08:50:16Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/HaplotypeCallerEngine.java", "diffHunk": "@@ -726,7 +779,10 @@ public void shutdown() {\n \n         final Set<GATKRead> readsToRemove = new LinkedHashSet<>();\n         for( final GATKRead rec : activeRegion.getReads() ) {\n-            if( rec.getLength() < READ_LENGTH_FILTER_THRESHOLD || rec.getMappingQuality() < READ_QUALITY_FILTER_THRESHOLD || ! ReadFilterLibrary.MATE_ON_SAME_CONTIG_OR_NO_MAPPED_MATE.test(rec) || (hcArgs.keepRG != null && !rec.getReadGroup().equals(hcArgs.keepRG)) ) {\n+            if( AlignmentUtils.unclippedReadLength(rec) < READ_LENGTH_FILTER_THRESHOLD || rec.getMappingQuality() < hcArgs.mappingQualityThreshold || ! ReadFilterLibrary.MATE_ON_SAME_CONTIG_OR_NO_MAPPED_MATE.test(rec) || (hcArgs.keepRG != null && !rec.getReadGroup().equals(hcArgs.keepRG)) ) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzE4ODg1Nw=="}, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 195}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzE5MDM4NQ==", "bodyText": "Not needed extra space.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457190385", "createdAt": "2020-07-20T08:51:47Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/PairHMMLikelihoodCalculationEngine.java", "diffHunk": "@@ -65,14 +74,14 @@\n \n     private final PCRErrorModel pcrErrorModel;\n     \n-    private final byte baseQualityScoreThreshold;\n+    private final byte  baseQualityScoreThreshold;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzE5MTYzNg==", "bodyText": "perhaps we owe to create a parameters class to group all this parameters.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457191636", "createdAt": "2020-07-20T08:53:25Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/PairHMMLikelihoodCalculationEngine.java", "diffHunk": "@@ -115,11 +125,17 @@ public PairHMMLikelihoodCalculationEngine(final byte constantGCP,\n      *                                  quality.\n      */\n     public PairHMMLikelihoodCalculationEngine(final byte constantGCP,\n+                                              final DragstrParams dragstrParams,\n                                               final PairHMMNativeArguments arguments,\n                                               final PairHMM.Implementation hmmType,\n                                               final double log10globalReadMismappingRate,\n                                               final PCRErrorModel pcrErrorModel,\n-                                              final byte baseQualityScoreThreshold) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzE5MzIzNw==", "bodyText": "same. also it can be final.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457193237", "createdAt": "2020-07-20T08:55:28Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/PairHMMLikelihoodCalculationEngine.java", "diffHunk": "@@ -164,14 +193,76 @@ public void close() {\n             computeReadLikelihoods(result.sampleMatrix(i));\n         }\n \n-        result.normalizeLikelihoods(log10globalReadMismappingRate);\n-        result.filterPoorlyModeledEvidence(log10MinTrueLikelihood(EXPECTED_ERROR_RATE_PER_BASE));\n+        result.normalizeLikelihoods(log10globalReadMismappingRate, symmetricallyNormalizeAllelesToReference);\n+\n+        if (dynamicDisqualification) {\n+            result.filterPoorlyModeledEvidence(daynamicLog10MinLiklihoodModel(readDisqualificationScale, log10MinTrueLikelihood(expectedErrorRatePerBase, false)), genotyperDebugOutStream);\n+        } else {\n+            result.filterPoorlyModeledEvidence(log10MinTrueLikelihood(expectedErrorRatePerBase, true), genotyperDebugOutStream);\n+        }\n         return result;\n     }\n \n-    private ToDoubleFunction<GATKRead> log10MinTrueLikelihood(final double maximumErrorPerBase) {\n+    private ToDoubleFunction<GATKRead> daynamicLog10MinLiklihoodModel(final double dynamicRadQualConstant, final ToDoubleFunction<GATKRead> log10MinTrueLikelihood) {\n+        return read -> {\n+            double dynamicThreshold = calculateDynamicThreshold(read, dynamicRadQualConstant);\n+            double log10MaxLikelihoodForTrueAllele = log10MinTrueLikelihood.applyAsDouble(read);\n+            if (dynamicThreshold < log10MaxLikelihoodForTrueAllele ) {\n+                if (genotyperDebugOutStream != null) {\n+                    genotyperDebugOutStream.println(\"For read \"+ read.getName() + \" replacing old threshold (\"+log10MaxLikelihoodForTrueAllele+\") with new threshold: \"+dynamicThreshold);\n+                }\n+                return dynamicThreshold;\n+            } else {\n+                return log10MaxLikelihoodForTrueAllele;\n+            }\n+        };\n+    }\n+\n+    static double calculateDynamicThreshold(final GATKRead read, final double dynamicRadQualConstant) {\n+        double sumMean = 0;\n+        double sumVariance = 0;\n+        byte[] baseQualities = read.getTransientAttribute(\"HMMQuals\") != null ?\n+                (byte[]) read.getTransientAttribute(\"HMMQuals\") : read.getBaseQualities();\n+\n+        for( int i = 0; i < baseQualities.length; i++) {\n+            int bq = baseQualities[i];\n+            int boundedBq = bq < 1 ? 1 : bq > 40 ? 40 : bq;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA5ODc3Nw=="}, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 172}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzIxOTM5NA==", "bodyText": "the top bit of the lowest byte is only on for ascii 128 and above which are not valid. Despite the bug is equivalent as baseToValue[x] for x >= 128 is set to invalid anyway; I guess I cut that array down to 128 positions rather than 256.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457219394", "createdAt": "2020-07-20T09:30:18Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/Nucleotide.java", "diffHunk": "@@ -284,10 +284,11 @@ public static Nucleotide decode(final byte base) {\n      * to a valid nucleotide specification.\n      */\n     public static Nucleotide decode(final char ch) {\n-        if ((ch & 0xFF00) != 0) {\n-            return INVALID;\n+        if ((ch & 0xFFFFFF80) == 0) { // all valid codes have ascii lower than 127 so we may just treat all", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI5MDM5NQ=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzQ2MDIwOA==", "bodyText": "we could defined concat(byte[], byte[]) using concat(a,b, Byte[]::new) to avoid repetition.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457460208", "createdAt": "2020-07-20T14:52:05Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/Utils.java", "diffHunk": "@@ -266,17 +267,74 @@ public static String join(final String separator, final double[] doubles) {\n      * @return a concat of all bytes in allBytes in order\n      */\n     public static byte[] concat(final byte[] ... allBytes) {\n-        int size = 0;\n-        for ( final byte[] bytes : allBytes ) size += bytes.length;\n+        if (allBytes.length == 0) {\n+            return ArrayUtils.EMPTY_BYTE_ARRAY;\n+        } else if (allBytes.length == 1) {\n+            return allBytes[0].length == 0 ? allBytes[0] : allBytes[0].clone();\n+        } else {\n+            int size = 0;\n+            for (final byte[] bytes : allBytes) size += bytes.length;\n+            if (size == 0) {\n+                return ArrayUtils.EMPTY_BYTE_ARRAY;\n+            } else {\n+                final byte[] c = new byte[size];\n+                int offset = 0;\n+                for (final byte[] bytes : allBytes) {\n+                    System.arraycopy(bytes, 0, c, offset, bytes.length);\n+                    offset += bytes.length;\n+                }\n+                return c;\n+            }\n+        }\n+    }\n \n-        final byte[] c = new byte[size];\n-        int offset = 0;\n-        for ( final byte[] bytes : allBytes ) {\n-            System.arraycopy(bytes, 0, c, offset, bytes.length);\n-            offset += bytes.length;\n+    public static <T> T[] concat(final T[] a, final T[] b, final IntFunction<T[]> constructor) {\n+        Utils.nonNull(a);\n+        Utils.nonNull(b);\n+        if (a.length != 0) {\n+            if (b.length != 0) {\n+                final T[] c = constructor.apply(a.length + b.length);\n+                System.arraycopy(a, 0, c, 0, a.length);\n+                System.arraycopy(b, 0, c, a.length, b.length);\n+                return c;\n+            } else {\n+                return a.clone();\n+            }\n+        } else if (b.length != 0) {\n+            return b.clone();\n+        } else {\n+            return a.clone();\n         }\n+    }\n \n-        return c;\n+    /**\n+     * Concats two byte arrays.\n+     * <p>\n+     *     A bit more efficient than calling the more general {@link #concat(byte[]...)}.\n+     * </p>\n+     * @param a left array to concat.\n+     * @param b right array to concat.\n+     * @return never {@code null};\n+     */\n+    public static byte[] concat(final byte[] a, final byte[] b) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzQ2NjU5OQ==", "bodyText": "That means that BucketUtils should be marked as deprecated?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457466599", "createdAt": "2020-07-20T14:58:55Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/gcs/BucketUtils.java", "diffHunk": "@@ -203,6 +206,58 @@ public static void copyFile(String sourcePath, String destPath) throws IOExcepti\n         }\n     }\n \n+    /**\n+     * If a provided path makes reference to a remote resource, it copies it over to a temporary local file that\n+     * is returned.\n+     * <p>\n+     *     The returned temporary file will be marked for deletion on exit if so requested.\n+     * </p>\n+     * <p>\n+     *     If the source is actually a local file it is considered to be already staged, unless forceStaggingOfLocalFiles is true,\n+     *     and it simply returns the corresponding File object, neither the deletion\n+     *     on exit request nor the tempDir input have any effect.\n+     * </p>\n+     * <p>\n+     *     In order to distinguish between these two scenarios you must call {@link BucketUtils#isCloudStorageUrl(String)}\n+     *     independently.\n+     * </p>\n+     * @param sourcePath\n+     * @param tempDir if provided the temporary file is created under this directory, if needed. If {@code null} the system\n+     *                default temporary file location is used instead.\n+     * @return never {@code null}.\n+     * @throws IOException if such an exeception occurs.\n+     */\n+    public static File stageFile(String sourcePath, final File tempDir, final boolean markForDeletionOnExit, final boolean forceStaggingOfLocalFiles) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyNTg0OA=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzQ3MDA2Mg==", "bodyText": "!isEmpty", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457470062", "createdAt": "2020-07-20T15:02:35Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/haplotype/EventMap.java", "diffHunk": "@@ -319,7 +319,26 @@ public String toString() {\n      * Returns any events in the map that overlap loc, including spanning deletions and events that start at loc.\n      */\n     public List<VariantContext> getOverlappingEvents(final int loc) {\n-        return headMap(loc, true).values().stream().filter(v -> v.getEnd() >= loc).collect(Collectors.toList());\n+        final List<VariantContext> overlappingEvents = headMap(loc, true).values().stream().filter(v -> v.getEnd() >= loc).collect(Collectors.toList());\n+        final List<VariantContext> deletionEvents = overlappingEvents.stream().filter(v -> v.isSimpleDeletion()).collect(Collectors.toList());\n+        final boolean containsDeletion = deletionEvents.size() > 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzQ5ODU1Mw==", "bodyText": "AbstractFeatureCodec work with non-text?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457498553", "createdAt": "2020-07-20T15:32:47Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/BinaryTableWriter.java", "diffHunk": "@@ -0,0 +1,55 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.io.*;\n+\n+public abstract class BinaryTableWriter<R> implements AutoCloseable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMyOTUxMg=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUwNzY4OA==", "bodyText": "That is what they do... this code does what the matlab script does but it turned out that they changed it for DRAGEN although they didn't tell us/me that until quite late in the development. yet still is a subset.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457507688", "createdAt": "2020-07-20T15:43:36Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSampler.java", "diffHunk": "@@ -0,0 +1,152 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.IntervalPileup;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+\n+public class DragstrCasesSampler {\n+\n+    private SAMSequenceDictionary dictionary;\n+    private ReadsDataSource readsSource;\n+    private ReferenceDataSource referenceSource;\n+    private static final Logger logger = LogManager.getLogger(EstimateDragstrModelParameters.class);\n+    private DragstrCasesSamplerArgumentCollection config;\n+\n+    public DragstrCasesSampler(final DragstrCasesSamplerArgumentCollection dragstrCasesSamplerArgumentCollection,\n+                               final ReferenceDataSource referenceSource,\n+                               final ReadsDataSource readsSource) {\n+        this.config = dragstrCasesSamplerArgumentCollection;\n+        this.readsSource = readsSource;\n+        this.referenceSource = referenceSource;\n+        this.dictionary = referenceSource.getSequenceDictionary();\n+    }\n+\n+    void sample(final DragstrModelEstimator.RepeatCases dest, final List<DragstrLocus> loci) {\n+        final int period = dest.getPeriod();\n+        final int repeats = dest.getRepeats();\n+        logger.info(\"Sampling period = \" + period + \" and repeat count = \" + repeats);\n+        final Random rdn = new Random(((config.randomSeed * 31) + period * 31) + repeats * 31);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzMzOTM0OA=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxMjczNA==", "bodyText": "No that I know of and I don't dear to ask... I don't see ad obvious function here.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457512734", "createdAt": "2020-07-20T15:49:53Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrModelEstimator.java", "diffHunk": "@@ -0,0 +1,416 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.apache.commons.lang.math.IntRange;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+\n+import java.util.ArrayDeque;\n+import java.util.Arrays;\n+import java.util.Deque;\n+\n+\n+public class DragstrModelEstimator {\n+\n+    private static double[][] DEFAULT_GOP = {\n+            {45.00, 45.00, 45.00, 45.00, 45.00, 45.00, 40.50, 33.50, 28.00, 24.00, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75},", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MzI1MQ=="}, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxMzgwNA==", "bodyText": "ah yeah, since I was initially trascribing their matlab code I though was a good idea to keep the same names for traceability but eventually I moved on from that leaving this behind, will change.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457513804", "createdAt": "2020-07-20T15:51:08Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrModelEstimator.java", "diffHunk": "@@ -0,0 +1,416 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.apache.commons.lang.math.IntRange;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+\n+import java.util.ArrayDeque;\n+import java.util.Arrays;\n+import java.util.Deque;\n+\n+\n+public class DragstrModelEstimator {\n+\n+    private static double[][] DEFAULT_GOP = {\n+            {45.00, 45.00, 45.00, 45.00, 45.00, 45.00, 40.50, 33.50, 28.00, 24.00, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75},\n+            {39.50, 39.50, 39.50, 39.50, 36.00, 30.00, 27.25, 25.00, 24.25, 24.75, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.75},\n+            {38.50, 41.00, 41.00, 41.00, 41.00, 37.50, 35.25, 34.75, 34.75, 33.25, 33.25, 33.25, 32.50, 30.75, 28.50, 29.00, 29.00, 29.00, 29.00, 29.00},\n+            {37.50, 39.00, 39.00, 37.75, 34.00, 34.00, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 31.75, 31.75, 31.75, 31.75, 31.75},\n+            {37.00, 40.00, 40.00, 40.00, 36.00, 35.00, 24.50, 24.50, 24.50, 24.50, 22.50, 22.50, 22.50, 23.50, 23.50, 23.50, 23.50, 23.50, 23.50, 23.50},\n+            {36.25, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00},\n+            {36.00, 40.50, 40.50, 40.50, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75},\n+            {36.25, 39.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75}};\n+\n+    private static double[][] DEFAULT_API = {\n+            {39.00, 39.00, 37.00, 35.00, 32.00, 26.00, 20.00, 16.00, 12.00, 10.00, 8.00, 7.00, 7.00, 6.00, 6.00, 5.00, 5.00, 4.00, 4.00, 4.00},\n+            {30.00, 30.00, 29.00, 22.00, 17.00, 14.00, 11.00, 8.00, 6.00, 5.00, 4.00, 4.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00},\n+            {27.00, 27.00, 25.00, 18.00, 14.00, 12.00, 9.00, 7.00, 5.00, 4.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {27.00, 27.00, 18.00, 9.00, 9.00, 9.00, 9.00, 3.00, 3.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {29.00, 29.00, 18.00, 8.00, 8.00, 8.00, 4.00, 3.00, 3.00, 3.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00},\n+            {25.00, 25.00, 10.00, 10.00, 10.00, 4.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00},\n+            {21.00, 21.00, 11.00, 11.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00, 5.00},\n+            {18.00, 18.00, 10.00, 6.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00}};\n+\n+    private static final Logger logger = LogManager.getLogger(DragstrModelEstimator.class);\n+\n+    private final double[] phred_gp_range;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1MzE1Mw=="}, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxNDkzMw==", "bodyText": "exactly!!!!", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457514933", "createdAt": "2020-07-20T15:52:29Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrModelEstimatorArgumentCollection.java", "diffHunk": "@@ -0,0 +1,92 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineException;\n+\n+public class DragstrModelEstimatorArgumentCollection {\n+\n+    public static final String GP_VALUES_ARGUMENT_FULL_NAME = \"gp-values\";\n+    public static final String API_VALUES_ARGUMENT_FULL_NAME = \"api-values\";\n+    public static final String HET_TO_HOM_RATIO_FULL_NAME = \"het-to-hom-ratio\";\n+    public static final String MIN_LOCI_COUNT_FULL_NAME = \"min-loci-count\";\n+    public static final String API_MONO_THRESHOD_FULL_NAME = \"api-mono-threshold\";\n+    public static final String MIN_GOP_FULL_NAME = \"min-gop\";\n+    public static final String MAX_GOP_FULL_NAME = \"max-gop\";\n+    public static final DoubleSequence DEFAULT_PHRED_GP_VALUES = new DoubleSequence(\"10:1.0:50\");\n+    public static final DoubleSequence DEFAULT_PHRED_API_VALUES = new DoubleSequence(\"0:1.0:40\");\n+    public static final double DEFAULT_HET_TO_HOM_RATIO = 2.0;\n+    public static final int DEFAULT_MIN_LOCI_COUNT = 50;\n+    public static final int DEFAULT_API_MONO_THRESHOLD = 3;\n+    public static final double DEFAULT_MIN_GOP = 10;\n+    public static final double DEFAULT_MAX_GOP = 50;\n+\n+    @Argument(doc = \"Possible Gap-Penalty values for the DRAGstr model parameter esimation. \" +\n+            \"These are expressed in Phred scaled values with the following format: start:step:end. For example the default '10:1.0:50' indicate the sequence starting at 10 finishing at 50 sampled at 1.0 intervals.\",\n+             optional = true,\n+             fullName = GP_VALUES_ARGUMENT_FULL_NAME)\n+    public DoubleSequence phredGpValues = DEFAULT_PHRED_GP_VALUES;\n+\n+    @Argument(doc = \"Possible a-priori probabilities for the heterozygous indel call for the DRAGstr model parameter esimation. \" +\n+            \"These are expressed in Phred scaled values with the following format: start:step:end. For example the default '10:1.0:50' indicate the sequence starting at 10 finishing at 50 sampled at 1.0 intervals.\",\n+            optional = true,\n+            fullName = API_VALUES_ARGUMENT_FULL_NAME)\n+    public DoubleSequence phredApiValues = DEFAULT_PHRED_API_VALUES;\n+\n+    @Argument(doc = \"Possible a-priori probabilities for the heterozygous indel call for the DRAGstr model parameter esimation. \" +\n+            \"These are expressed in Phred scaled values with the following format: start:step:end. For example the default '10:1.0:50' indicate the sequence starting at 10 finishing at 50 sampled at 1.0 intervals.\",\n+            optional = true,\n+            fullName = HET_TO_HOM_RATIO_FULL_NAME,\n+            minValue = 0.0,\n+            maxValue = Double.MAX_VALUE)\n+    public double hetToHomRatio = DEFAULT_HET_TO_HOM_RATIO;\n+\n+    @Argument(doc = \"Minimum number of sites for a repeat count and period length pair. We will combine pairs that have a smaller number of such cases (same period but +/- 1 repeat count)\",\n+              optional = true,\n+              fullName = MIN_LOCI_COUNT_FULL_NAME,\n+              minValue = 1.0)\n+    public int minLociCount = DEFAULT_MIN_LOCI_COUNT;\n+\n+    @Argument(doc = \"<Not quite understand this one yet>\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1NDYwOQ=="}, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxOTYxNA==", "bodyText": "What is the alternative? for a quicker '\"%5s\"'", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457519614", "createdAt": "2020-07-20T15:58:07Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrParams.java", "diffHunk": "@@ -0,0 +1,245 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.VariationalAlleleFrequencyCalculator;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+public class DragstrParams {\n+\n+    private final int maxPeriod;\n+    private final int maxRepeats;\n+    private final double[][] gop;\n+    private final double[][] gcp;\n+    private final double[][] api;\n+    private final Map<Object, VariationalAlleleFrequencyCalculator> afcs;\n+\n+    public DragstrParams(final String path) {\n+        this(openBufferedReader(path), path);\n+    }\n+\n+    private static BufferedReader openBufferedReader(String path) {\n+        try {\n+            return Files.newBufferedReader(Paths.get(path));\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotReadInputFile(path, ex);\n+        }\n+    }\n+\n+    private BufferedWriter openBufferedWriter(final String path) {\n+        try {\n+            return Files.newBufferedWriter(Paths.get(path));\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(path, ex);\n+        }\n+    }\n+\n+    public static DragstrParams fromEstimate(final DragstrModelEstimator.Estimate estimate) {\n+        Utils.nonNull(estimate);\n+        final int maxPeriod = estimate.gp.length;\n+        final int maxRepeats = estimate.gp[0].length;\n+        final double[][] gop = new double[maxPeriod][maxRepeats];\n+        final double[][] gcp = new double[maxPeriod][maxRepeats];\n+        final double[][] api = new double[maxPeriod][maxRepeats];\n+        for (int i = 0; i < maxPeriod; i++) {\n+            for (int j = 0; j < maxRepeats; j++) {\n+                gop[i][j] = estimate.gop(i +1, j + 1);\n+                gcp[i][j] = estimate.gcp(i + 1, j+ 1);\n+                api[i][j] = estimate.api( i + 1, j + 1);\n+            }\n+        }\n+        return new DragstrParams(maxPeriod, maxRepeats, gop, gcp, api);\n+    }\n+\n+    private DragstrParams(final BufferedReader reader, final String path) {\n+        try {\n+            final String header = reader.readLine();\n+            final String[] headerParts = header.split(\"\\\\s+\");\n+            final int[] repeats = Arrays.stream(headerParts)\n+                    .filter(str -> !str.isEmpty())\n+                    .mapToInt(str -> {\n+                        try {\n+                            return Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw new UserException.BadInput(\"bad format for an integer\", ex);\n+                        }\n+                    })\n+                    .toArray();\n+            final int maxRepeats = repeats.length;\n+            for (int i = 0; i < repeats.length; i++) {\n+                if (repeats[i] != i + 1) {\n+                    throw new UserException.BadInput(\"the DRAGstr parameter file header line must contain integers starting at 1 \" + Arrays.toString(repeats));\n+                }\n+            }\n+            final Map<String, double[][]> tables = new HashMap<>();\n+            String line = reader.readLine();\n+            if (line == null) {\n+                throw new UserException.BadInput(\"end of table list before expected\");\n+            }\n+            String tableName = line.replaceAll(\":$\", \"\");\n+            List<String> tableLines = new ArrayList<>();\n+            while ((line = reader.readLine()) != null) {\n+                if (line.charAt(line.length() - 1) == ':') {\n+                    tables.put(tableName, linesToMatrix(tableLines, repeats.length));\n+                    tableName = line.replaceAll(\":$\", \"\");\n+                    tableLines.clear();\n+                } else {\n+                    tableLines.add(line);\n+                }\n+            }\n+            if (tableName == null) {\n+                throw new UserException.BadInput(\"table with no name\");\n+            }\n+            tables.put(tableName, linesToMatrix(tableLines, repeats.length));\n+            final double[][] gopMatrix = mandatoryMatrix(tables, \"GOP\");\n+            final double[][] gcpMatrix = mandatoryMatrix(tables, \"GCP\");\n+            final double[][] apiMatrix = mandatoryMatrix(tables, \"API\");\n+            final int maxPeriod = gopMatrix.length;\n+            checkMatricesAreValid(maxPeriod, maxRepeats, gopMatrix, gcpMatrix, apiMatrix);\n+\n+            this.maxPeriod = maxPeriod;\n+            this.maxRepeats = maxRepeats;\n+            this.gop = gopMatrix;\n+            this.gcp = gcpMatrix;\n+            this.api = apiMatrix;\n+            this.afcs = new HashMap<>(maxPeriod * maxRepeats);\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotReadInputFile(path, ex);\n+        }\n+    }\n+\n+    public void print(final String path) {\n+        try (final BufferedWriter writer = openBufferedWriter(path);\n+             final PrintWriter printWriter = new PrintWriter(writer)) {\n+            final StringBuilder lineBuilder = new StringBuilder(1024);\n+            lineBuilder.append(String.format(\"%5s\", \"1\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc0OTY0MA=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUzNDIwMA==", "bodyText": "The existing machinery is gear towards analyzing one position at a time. I needed something that takes care of a whole interval... up to 8 x 20 + 10 bases.\nFor example PileupElements are created for each base-pair seemed overkill to me.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457534200", "createdAt": "2020-07-20T16:20:04Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrCasesSampler.java", "diffHunk": "@@ -0,0 +1,152 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.engine.ReadsDataSource;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.utils.IntervalPileup;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceBases;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+\n+public class DragstrCasesSampler {\n+\n+    private SAMSequenceDictionary dictionary;\n+    private ReadsDataSource readsSource;\n+    private ReferenceDataSource referenceSource;\n+    private static final Logger logger = LogManager.getLogger(EstimateDragstrModelParameters.class);\n+    private DragstrCasesSamplerArgumentCollection config;\n+\n+    public DragstrCasesSampler(final DragstrCasesSamplerArgumentCollection dragstrCasesSamplerArgumentCollection,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MTE2MQ=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUzNzIxMQ==", "bodyText": "want/wanted to store several files and access them without having to decompress the whole archive.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457537211", "createdAt": "2020-07-20T16:24:52Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private DecimationTable decimationTable = DecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(fullName=\"max-period\", doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = 8;\n+\n+    @Argument(fullName=\"max-repeats\", doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = 20;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private File tempDir;\n+\n+    private long[][] totalCounts;\n+\n+    private long[][] emittedCounts;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+        totalCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+        emittedCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+    }\n+\n+    public void onShutdown() {\n+        Utils.deleteFileTree(tempDir);\n+        super.onShutdown();\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        final SAMSequenceDictionary dictionary = Utils.nonNull(getReferenceDictionary());\n+        final File tempDir = this.tempDir;\n+        try (final AutoCloseableList<BinaryTableWriter<DragstrLocus>> allSitesWriter\n+                     = AutoCloseableList.of(maxPeriod, i -> createDragstrLocusWriter(tempDir, \"period_\" + (i + 1) + \".bin\"))) {\n+            if (!intervalArgumentCollection.intervalsSpecified()) {\n+                for (final SAMSequenceRecord sequence : dictionary.getSequences()) {\n+                    traverse(sequence.getSequenceIndex(), sequence, 1, sequence.getSequenceLength(), allSitesWriter, decimationTable);\n+                }\n+            } else {\n+                for (final SimpleInterval interval : intervalArgumentCollection.getIntervals(dictionary)) {\n+                    final SAMSequenceRecord sequence = dictionary.getSequence(interval.getContig());\n+                    traverse(sequence.getSequenceIndex(), sequence, interval.getStart(), interval.getEnd(), allSitesWriter, decimationTable);\n+                }\n+            }\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(tempDir, ex);\n+        }\n+        progressMeter.stop();\n+        logger.info(\"Finishing reference sampling. Proceeding to splitting each period case by repeat count\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            progressMeter = new ProgressMeter(secondsBetweenProgressUpdates);\n+            progressMeter.setRecordLabel(\"Splitting cases with period \" + i + \" by repeat count\");\n+            progressMeter.start();\n+            splitPeriodLociByRepeat(i);\n+            progressMeter.stop();\n+            logger.info(\"Done with period \" + i);\n+        }\n+        logger.info(\"Composing output zip\");\n+        composeOutputZip();\n+    }\n+\n+    private void splitPeriodLociByRepeat(final int period) {\n+        final File lociFile = new File(tempDir, \"period_\" + period + \".bin\");\n+        final File periodDir = new File(tempDir, \"\" + period);\n+        if (!periodDir.mkdir()) {\n+            throw new UserException.CouldNotCreateOutputFile(periodDir, \"create period loci split directory\");\n+        }\n+        try (final BinaryTableReader<DragstrLocus> reader = DragstrLocus.binaryReader(new FileInputStream(lociFile));\n+             final AutoCloseableList<BinaryTableWriter<DragstrLocus>> writers =\n+                     AutoCloseableList.of(maxRepeat, i -> createDragstrLocusWriter(periodDir, \"\" + (i+1) + \".bin\"))) {\n+            while (reader.hasNext()) {\n+                final DragstrLocus next = reader.next();\n+                final int effectiveRepeats = next.getRepeats() > maxRepeat ? maxRepeat : next.getRepeats();\n+                progressMeter.update(next.getStartInterval(getReferenceDictionary(), 0));\n+                final BinaryTableWriter<DragstrLocus> writer = writers.get(effectiveRepeats - 1);\n+                writer.write(next);\n+\n+            }\n+        } catch (final Exception ex) {\n+            throw new UserException.CouldNotCreateOutputFile(lociFile, \"temporary period loci splitting failed\");\n+        }\n+        if (!lociFile.delete()) {\n+            throw new GATKException(\"could not delete temporary file \" + lociFile);\n+        }\n+    }\n+\n+    private void composeOutputZip() {\n+        final byte[] buffer = new byte[1 << 16];\n+        try (final ZipArchiveOutputStream output = new JarArchiveOutputStream(BucketUtils.createFile(outputPath))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MjU1MA=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 312}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzU0MjgwMw==", "bodyText": "Not sure several million perhaps around ~20M... would use a 1G roughly for the actual + overhead. I have to say new code has simplified some of this although the zip is maintained", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r457542803", "createdAt": "2020-07-20T16:33:34Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/SampleSitesForDRAGstrModel.java", "diffHunk": "@@ -0,0 +1,437 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.reference.FastaReferenceWriter;\n+import org.apache.commons.compress.archivers.jar.JarArchiveOutputStream;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\n+import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.engine.ProgressMeter;\n+import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.fasta.FastaReferenceMaker;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.collections.AutoCloseableList;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class SampleSitesForDRAGstrModel extends GATKTool {\n+\n+    private static final Logger logger = LogManager.getLogger(SampleSitesForDRAGstrModel.class);\n+\n+    public static class DecimationTable {\n+\n+        private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+                {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+                {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+                {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+                {0, 0, 8, 4, 1, 0},\n+                {0, 0, 6, 0},\n+                {0, 0, 5, 0},\n+                {0, 0, 4, 0},\n+                {0}};\n+\n+        public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+        public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+        public static final DecimationTable DEFAULT = new DecimationTable(DEFAULT_DECIMATION_STR);\n+\n+        public static final DecimationTable NONE = new DecimationTable(NO_DECIMATION_STR);\n+\n+        private final long[][] decimationMask;\n+\n+        private final long[][] counts;\n+\n+        public DecimationTable(final String spec) {\n+            Utils.nonNull(spec);\n+            final int[][] decimation;\n+            if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+                decimation = new int[][] {{0}};\n+            } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+                decimation = DEFAULT_DECIMATION_MATRIX;\n+            } else {\n+                decimation = parseDecimationMatrixFromPath(spec);\n+            }\n+            decimationMask = calculateDecimationMask(decimation);\n+            counts = composeDecimationCounts(decimationMask);\n+        }\n+\n+        public long decimationMask(final int period, final int repeats) {\n+            if (decimationMask.length <= period) {\n+                return -1;\n+            } else if (decimationMask[period].length <= repeats) {\n+                return -1;\n+            } else {\n+                return decimationMask[period][repeats];\n+            }\n+        }\n+\n+        private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+            final long[][] result = new long[decimationMask.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                result[i] = new long[decimationMask[i].length];\n+            }\n+            return result;\n+        }\n+\n+        private static int[][] parseDecimationMatrixFromPath(String spec) {\n+            try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+                final String[][] values = reader.lines()\n+                        .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                        .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                                   .mapToDouble(Double::parseDouble)\n+                                   .toArray())\n+                        .toArray(String[][]::new);\n+                return parseDecimationMatrixValues(values, spec);\n+            } catch (final IOException ex) {\n+                throw new UserException.CouldNotReadInputFile(spec, ex);\n+            } catch (final NumberFormatException ex){\n+                throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+            }\n+        }\n+\n+        private static int[][] parseDecimationMatrixValues(final String[][] values, final String path) {\n+            Utils.nonNull(values);\n+            if (values.length == 0) {\n+                logger.warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+                return new int[0][];\n+            } else {\n+                int totalValues = 0;\n+                final int[][] result = new int[values.length][];\n+                for (int i = 0; i < values.length; i++) {\n+                    final String[] row = values[i];\n+                    final int[] rowValues = new int[values.length];\n+                    for (int j = 0; j <  row.length; j++) {\n+                        final String str = row[j];\n+                        final int value;\n+                        try {\n+                            value = Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                        }\n+                        if (value < 0) {\n+                            throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                        } else if (Double.isNaN(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                        } else if (!Double.isFinite(value)) {\n+                            throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                        }\n+                        rowValues[j] = value;\n+                        totalValues++;\n+                    }\n+                    result[i] = rowValues;\n+                }\n+                if (totalValues == 0) {\n+                    throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+                }\n+                return result;\n+            }\n+        }\n+\n+        private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                    final String details) {\n+            throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                    path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+        }\n+\n+        public static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+            Utils.nonNull(decimationMatrix);\n+            final long[][] result = new long[decimationMatrix.length][];\n+            for (int i = 0; i < result.length; i++) {\n+                final int[] row = decimationMatrix[i];\n+                result[i] = new long[row.length];\n+                for (int j = 0; j < row.length; j++) {\n+                    result[i][j] = (1 << row[j]) - 1;\n+                }\n+            }\n+            return result;\n+        }\n+\n+        public long mask(final int period, final int repeats) {\n+            final int p = period >= decimationMask.length ? decimationMask.length - 1 : period;\n+            final long[] masks = decimationMask[p];\n+            if (masks.length == 0) {\n+                return 0;\n+            } else if (repeats >= masks.length) {\n+                return masks[masks.length - 1];\n+            } else {\n+                return masks[repeats];\n+            }\n+        }\n+\n+        public boolean decimate(final int seqNumber, final int bestPeriod, final long bestPeriodRepeats) {\n+            if (counts.length <= bestPeriod) {\n+                return false;\n+            } else {\n+                final long[] periodCounts = counts[bestPeriod];\n+                if (periodCounts.length == 0) {\n+                    return false;\n+                } else {\n+                    final int effectiveRepeatCount\n+                            = (int) (bestPeriodRepeats < periodCounts.length ? bestPeriodRepeats : periodCounts.length - 1);\n+                    final long count = periodCounts[effectiveRepeatCount]++;\n+                    final long left = count + seqNumber;\n+                    final long right = decimationMask[bestPeriod][effectiveRepeatCount];\n+                    return ((int) left & (int) right) != 0 || ((left >> 32) & (right >> 32)) != 0;\n+                }\n+            }\n+        }\n+    }\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private DecimationTable decimationTable = DecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(fullName=\"max-period\", doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 10)\n+    private int maxPeriod = 8;\n+\n+    @Argument(fullName=\"max-repeats\", doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxRepeat = 20;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private File tempDir;\n+\n+    private long[][] totalCounts;\n+\n+    private long[][] emittedCounts;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+        totalCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+        emittedCounts = new long[maxPeriod + 1][maxRepeat + 1];\n+    }\n+\n+    public void onShutdown() {\n+        Utils.deleteFileTree(tempDir);\n+        super.onShutdown();\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        final SAMSequenceDictionary dictionary = Utils.nonNull(getReferenceDictionary());\n+        final File tempDir = this.tempDir;\n+        try (final AutoCloseableList<BinaryTableWriter<DragstrLocus>> allSitesWriter\n+                     = AutoCloseableList.of(maxPeriod, i -> createDragstrLocusWriter(tempDir, \"period_\" + (i + 1) + \".bin\"))) {\n+            if (!intervalArgumentCollection.intervalsSpecified()) {\n+                for (final SAMSequenceRecord sequence : dictionary.getSequences()) {\n+                    traverse(sequence.getSequenceIndex(), sequence, 1, sequence.getSequenceLength(), allSitesWriter, decimationTable);\n+                }\n+            } else {\n+                for (final SimpleInterval interval : intervalArgumentCollection.getIntervals(dictionary)) {\n+                    final SAMSequenceRecord sequence = dictionary.getSequence(interval.getContig());\n+                    traverse(sequence.getSequenceIndex(), sequence, interval.getStart(), interval.getEnd(), allSitesWriter, decimationTable);\n+                }\n+            }\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(tempDir, ex);\n+        }\n+        progressMeter.stop();\n+        logger.info(\"Finishing reference sampling. Proceeding to splitting each period case by repeat count\");\n+        for (int i = 1; i <= maxPeriod; i++) {\n+            progressMeter = new ProgressMeter(secondsBetweenProgressUpdates);\n+            progressMeter.setRecordLabel(\"Splitting cases with period \" + i + \" by repeat count\");\n+            progressMeter.start();\n+            splitPeriodLociByRepeat(i);\n+            progressMeter.stop();\n+            logger.info(\"Done with period \" + i);\n+        }\n+        logger.info(\"Composing output zip\");\n+        composeOutputZip();\n+    }\n+\n+    private void splitPeriodLociByRepeat(final int period) {\n+        final File lociFile = new File(tempDir, \"period_\" + period + \".bin\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MjQyOQ=="}, "originalCommit": {"oid": "2ebd6cda9b9288ed1a673ec1f873da6a43691ac5"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTEyMzY1OA==", "bodyText": "newCigar can be final, right?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r461123658", "createdAt": "2020-07-27T19:38:36Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/read/AlignmentUtils.java", "diffHunk": "@@ -101,33 +103,56 @@ public static GATKRead createReadAlignedToRef(final GATKRead originalRead,\n         final Cigar haplotypeToRef = trimCigarByBases(rightPaddedHaplotypeVsRefCigar, readToHaplotypeSWAlignment.getAlignmentOffset(), rightPaddedHaplotypeVsRefCigar.getReadLength() - 1).getCigar();\n \n         final Cigar readToRefCigar = applyCigarToCigar(swCigar, haplotypeToRef);\n-        final CigarBuilder.Result leftAlignedReadToRefCigarResult = leftAlignIndels(readToRefCigar, refHaplotype.getBases(), originalRead.getBases(), readStartOnReferenceHaplotype);\n+        final CigarBuilder.Result leftAlignedReadToRefCigarResult = leftAlignIndels(readToRefCigar, refHaplotype.getBases(), readMinusSoftClips.getBases(), readStartOnReferenceHaplotype);\n         final Cigar leftAlignedReadToRefCigar = leftAlignedReadToRefCigarResult.getCigar();\n         // it's possible that left-alignment shifted a deletion to the beginning of a read and removed it, shifting the first aligned base to the right\n-        read.setPosition(read.getContig(), readStartOnReference + leftAlignedReadToRefCigarResult.getLeadingDeletionBasesRemoved());\n+        copiedRead.setPosition(copiedRead.getContig(), readStartOnReference + leftAlignedReadToRefCigarResult.getLeadingDeletionBasesRemoved());\n \n         // the SW Cigar does not contain the hard clips of the original read\n+        // Here we reconcile the aligned read (that has had any softclips removed) with its softclipped bases\n         final Cigar originalCigar = originalRead.getCigar();\n-        final CigarElement firstElement = originalCigar.getFirstCigarElement();\n-        final CigarElement lastElement = originalCigar.getLastCigarElement();\n+        Cigar newCigar = appendClippedElementsFromCigarToCigar(leftAlignedReadToRefCigar, originalCigar);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ecf7202de4ead39ec335f7cfdd68967f7a3b046"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTEyNDU0Nw==", "bodyText": "Avandonware.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r461124547", "createdAt": "2020-07-27T19:40:27Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/pairhmm/DragstrSTRTable.java", "diffHunk": "@@ -0,0 +1,4 @@\n+package org.broadinstitute.hellbender.utils.pairhmm;\n+\n+public class DragstrSTRTable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU1NTYyNw=="}, "originalCommit": {"oid": "bcd2b11b6517a7db68ca30324368fd1d1188f8b4"}, "originalPosition": 3}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg4MzUxODU2", "url": "https://github.com/broadinstitute/gatk/pull/6634#pullrequestreview-488351856", "createdAt": "2020-09-15T06:10:08Z", "commit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwNjoxMDowOFrOHRyIgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwNjoxNDo0OFrOHRyPhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODQwOTIxNg==", "bodyText": "Forgot to add some javadoc here....\n/** \n * Collection of {@link DragstrLocusCase} stratified by period and repeat length.\n * <p>\n *  Cases belonging to every possible combination can be added individually, and \n *   added or retrieved as a group using a {@link DragstrLocusCases} instance. \n * </p>\n */", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r488409216", "createdAt": "2020-09-15T06:10:08Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/StratifiedDragstrLocusCases.java", "diffHunk": "@@ -0,0 +1,95 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODQxMTAxNQ==", "bodyText": "/**\n * Returns the subset of cases that pass a set of standard filters \n * based on depth, minimum MQ and maximum number of \n * supplementary alignments.\n *\n * <p>\n *  Changes on the returned collection won't have any effect on this one and <i>vice-versa</i>.\n * </p>\n */", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r488411015", "createdAt": "2020-09-15T06:14:48Z", "author": {"login": "vruano"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/StratifiedDragstrLocusCases.java", "diffHunk": "@@ -0,0 +1,95 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+class StratifiedDragstrLocusCases {\n+\n+    private int size;\n+    DragstrLocusCases[][] perPeriodAndRepeat;\n+\n+    StratifiedDragstrLocusCases(final int maxPeriod, final int maxRepeats) {\n+        perPeriodAndRepeat = new DragstrLocusCases[maxPeriod][maxRepeats];\n+        for (int i = 0; i < maxPeriod; i++) {\n+            for (int j = 0; j < maxRepeats; j++) {\n+                perPeriodAndRepeat[i][j] = new DragstrLocusCases(i + 1, j + 1);\n+            }\n+        }\n+    }\n+\n+    public static StratifiedDragstrLocusCases make(final int maxPeriod, final int maxRepeats) {\n+        ParamUtils.isPositive(maxPeriod, \"max-period must be greater than 0\");\n+        ParamUtils.isPositive(maxRepeats, \"max-repeats must be greater than 0\");\n+        return new StratifiedDragstrLocusCases(maxPeriod, maxRepeats);\n+    }\n+\n+    public StratifiedDragstrLocusCases addAll(final StratifiedDragstrLocusCases other) {\n+        Utils.validate(perPeriodAndRepeat.length == other.perPeriodAndRepeat.length, \"incompatible dimensions\");\n+        for (int i = 0; i < perPeriodAndRepeat.length; i++) {\n+            Utils.validate(perPeriodAndRepeat[i].length == other.perPeriodAndRepeat[i].length, \"invalid dimensions\");\n+            for (int j = 0; j < perPeriodAndRepeat[i].length; j++) {\n+                perPeriodAndRepeat[i][j].addAll(other.perPeriodAndRepeat[i][j]);\n+            }\n+        }\n+        size += other.size;\n+        return this;\n+    }\n+\n+    public int size() {\n+        return size;\n+    }\n+\n+    public static StratifiedDragstrLocusCases merge(final StratifiedDragstrLocusCases... cazes) {\n+        if (cazes.length == 0) {\n+            return new StratifiedDragstrLocusCases(DragstrHyperParameters.DEFAULT_MAX_PERIOD, DragstrHyperParameters.DEFAULT_MAX_REPEAT_LENGTH);\n+        } else {\n+            final StratifiedDragstrLocusCases result = new StratifiedDragstrLocusCases(cazes[0].perPeriodAndRepeat.length, cazes[0].perPeriodAndRepeat[0].length);\n+            for (final StratifiedDragstrLocusCases col : cazes) {\n+                result.addAll(col);\n+            }\n+            return result;\n+        }\n+    }\n+\n+\n+    public DragstrLocusCases get(final int period, final int repeats) {\n+        final int periodIndex = Utils.validIndex(period - 1, perPeriodAndRepeat.length, \"period is out of range\");\n+        final int repeatIndex = Math.min(ParamUtils.isPositive(repeats, \"repeats must be greater than 0 \") - 1,\n+                         perPeriodAndRepeat[periodIndex].length - 1);\n+        return perPeriodAndRepeat[periodIndex][repeatIndex];\n+    }\n+\n+    /**\n+     * Adds all the cases in the input collection into this one segratting them\n+     * based on period and repeat length.\n+     * @param in the collection of cases to add in.\n+     */\n+    public void addAll(final DragstrLocusCases in) {\n+        final int periodIndex = in.getPeriod() - 1;\n+        final DragstrLocusCases cases = perPeriodAndRepeat[periodIndex]\n+                [Math.min(perPeriodAndRepeat[periodIndex].length - 1, in.getRepeatLength() - 1)];\n+        cases.addAll(in);\n+        size += in.size();\n+    }\n+\n+    /**\n+     * Adds a single case.\n+     * @param caze the case to add.\n+     */\n+    public void add(final DragstrLocusCase caze) {\n+        final DragstrLocusCases cases = perPeriodAndRepeat[caze.getPeriod() - 1]\n+                [Math.min(perPeriodAndRepeat[0].length - 1, caze.getRepeatLength() - 1)];\n+        cases.add(caze);\n+        size++;\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 85}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk2NTE2MDU3", "url": "https://github.com/broadinstitute/gatk/pull/6634#pullrequestreview-496516057", "createdAt": "2020-09-25T15:15:33Z", "commit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "state": "COMMENTED", "comments": {"totalCount": 38, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNToxNTozM1rOHYH5tw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQyMToxOTo1OVrOHYTA3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA1NzMzNQ==", "bodyText": "So I think this change + the thread pool you added are not exactly in line with how we have handled paralellism in gatk4. Generally we have pushed this sort of threading/sharding behavior off onto spark tools that exist in addition to the single threaded tool implementations. I Don't think you have to change it in order to get this branch in, especially if there is a significant performance difference but you should probably spin this off into a \"EstimatDragstrParametersSpark\" that handles the paralellization.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495057335", "createdAt": "2020-09-25T15:15:33Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/EstimateDragstrParameters.java", "diffHunk": "@@ -0,0 +1,935 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.*;\n+import htsjdk.samtools.util.IntervalTree;\n+import it.unimi.dsi.fastutil.ints.IntArrayList;\n+import it.unimi.dsi.fastutil.ints.IntList;\n+import org.apache.commons.io.output.NullOutputStream;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.ExampleProgramGroup;\n+import org.broadinstitute.hellbender.engine.*;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrParams;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrUtils;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFile;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.AbsoluteCoordinates;\n+\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.*;\n+import java.util.stream.*;\n+\n+/**\n+ * Estimates the parameters for the DRAGstr model for an input sample.\n+ * <p>\n+ *     This tools takes in the sampling sites generated by {@link ComposeSTRTableFile} on the same reference\n+ *     as the input sample.\n+ * </p>\n+ * <p>\n+ *     The end result is a text file containing three parameter tables (GOP, GCP, API) that can be fed\n+ *     directly to {@link HaplotypeCaller} --dragstr-params-path.\n+ * </p>\n+ */\n+@CommandLineProgramProperties(\n+        summary = \"estimates the parameters for the DRAGstr model for the input sample\",\n+        oneLineSummary = \"summary\",\n+        programGroup = ExampleProgramGroup.class\n+)\n+public class EstimateDragstrParameters extends GATKTool {\n+\n+    public static final String STR_TABLE_PATH_SHORT_NAME = \"str\";\n+    public static final String STR_TABLE_PATH_FULL_NAME = \"str-table-path\";\n+    public static final String PARALLEL_FULL_NAME = \"parallel\";\n+    public static final String THREADS_FULL_NAME = \"threads\";\n+    public static final String SHARD_SIZE_FULL_NAME = \"shard-size\";\n+    public static final String DOWN_SAMPLE_SIZE_FULL_NAME = \"down-sample-size\";\n+    public static final String DEBUG_SITES_OUTPUT_FULL_NAME = \"debug-sites-output\";\n+\n+    public static final int DEFAULT_SHARD_SIZE = 1_000_000;\n+    public static final int DEFAULT_DOWN_SAMPLE_SIZE = 4096;\n+    public static final int SYSTEM_SUGGESTED_THREAD_NUMBER = 0;\n+    public static final int MINIMUM_SHARD_SIZE = 100;\n+    public static final int MINIMUM_DOWN_SAMPLE_SIZE = 512;\n+\n+    @ArgumentCollection\n+    private DragstrHyperParameters hyperParameters = new DragstrHyperParameters();\n+\n+    @Argument(shortName=STR_TABLE_PATH_SHORT_NAME, fullName=STR_TABLE_PATH_FULL_NAME, doc=\"location of the zip that contains the sampling sites for the reference\")\n+    private String strTablePath = null;\n+\n+    @Argument(fullName=PARALLEL_FULL_NAME, doc=\"run alignment data collection and  estimation in parallel\")\n+    private boolean runInParallel = false;\n+\n+    @Argument(fullName=THREADS_FULL_NAME, minValue = SYSTEM_SUGGESTED_THREAD_NUMBER, doc=\"suggested number of parallel threads to perform the estimation, \"\n+            + \"the default 0 leave it up to the VM to decide. When set to more than 1, this will activate parallel in the absence of --parallel\")\n+    private int threads = SYSTEM_SUGGESTED_THREAD_NUMBER;\n+\n+    @Argument(fullName=SHARD_SIZE_FULL_NAME, doc=\"when running in parallel this is the suggested shard size in base pairs. \" +\n+            \"The actual shard-size may vary to adapt to small contigs and the requested number of threads\",\n+              minValue = MINIMUM_SHARD_SIZE)\n+    private int shardSize = DEFAULT_SHARD_SIZE;\n+\n+    @Argument(fullName=DOWN_SAMPLE_SIZE_FULL_NAME, doc=\"Targeted maximum number of cases per combination period repeat count, \" +\n+            \"the larger the more precise but also the slower estimation.\",\n+              minValue = MINIMUM_DOWN_SAMPLE_SIZE)\n+    private int downsampleSize = DEFAULT_DOWN_SAMPLE_SIZE;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME, shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME, doc = \"where to write the parameter output file.\")\n+    private String output = null;\n+\n+    @Argument(fullName= DEBUG_SITES_OUTPUT_FULL_NAME, doc = \"table with information gather on the samples sites. Includes what sites were downsampled, disqualified or accepted for parameter estimation\", optional = true)\n+    private String sitesOutput = null;\n+\n+    private SAMSequenceDictionary dictionary;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean requiresReads() {\n+        return true;\n+    }\n+\n+    @Override\n+    protected void onStartup() {\n+        super.onStartup();\n+        hyperParameters.validate();\n+        dictionary = getBestAvailableSequenceDictionary();\n+        if (runInParallel) {\n+            if (threads == 1) {\n+                logger.warn(\"parallel processing was requested but the number of threads was set to 1\");\n+            }\n+        } else if (threads > 1) {\n+            runInParallel = true;\n+        }\n+        if (runInParallel) {\n+            if (threads == 0) {\n+                logger.info(\"Running in parallel using the system suggested default thread count: \" + Runtime.getRuntime().availableProcessors());\n+            } else {\n+                logger.info(\"Running in parallel using the requested number of threads: \" + threads);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        hyperParameters.validate();\n+        dictionary = getBestAvailableSequenceDictionary();\n+        final List<SAMReadGroupRecord> readGroups = hasReads() ? getHeaderForReads().getReadGroups() : Collections.emptyList();\n+        final List<String> readGroupIds = readGroups.stream()\n+                .map(SAMReadGroupRecord::getId)\n+                .collect(Collectors.toList());\n+        final List<String> sampleNames = readGroups.stream()\n+                .map(SAMReadGroupRecord::getSample)\n+                .distinct().collect(Collectors.toList());\n+        final Optional<String> sampleName = resolveSampleName(sampleNames);\n+\n+        try (final PrintWriter sitesOutputWriter = openSitesOutputWriter(sitesOutput);\n+             final STRTableFile strTable = STRTableFile.open(strTablePath)) {\n+\n+            checkSequenceDictionaryCompatibility(dictionary, strTable.dictionary());\n+            final StratifiedDragstrLocusCases allSites;\n+            final List<SimpleInterval> intervals = getTraversalIntervals();\n+\n+            runInParallel |= threads > 1;\n+            if (runInParallel) {\n+                if (threads == 1) {\n+                    logger.warn(\"parallel processing was requested but the number of threads was set to 1\");\n+                }\n+                allSites = collectCaseStatsParallel(intervals, shardSize, strTable);\n+            } else {\n+                allSites = collectCaseStatsSequencial(intervals, strTable);\n+            }\n+            logSiteCounts(allSites, \"all loci/cases\");\n+            final StratifiedDragstrLocusCases downSampledSites = downSample(allSites, strTable, sitesOutputWriter);\n+            logSiteCounts(downSampledSites, \"all downsampled (kept) loci/cases\");\n+            final StratifiedDragstrLocusCases finalSites = downSampledSites.qualifyingOnly(hyperParameters.minDepth, hyperParameters.minMQ, 0);\n+            logSiteCounts(finalSites, \"all qualifying loci/cases\");\n+            outputDownSampledSiteDetails(downSampledSites, sitesOutputWriter, hyperParameters.minDepth, hyperParameters.minMQ, 0);\n+            printOutput(finalSites, sampleName.orElse(null), readGroupIds);\n+        }\n+    }\n+\n+    private void printOutput(final StratifiedDragstrLocusCases finalSites, final String sampleName, final List<String> readGroups) {\n+        try (final PrintWriter writer = new PrintWriter(openBufferedWriter(output))) {\n+            final boolean usingDefaults = !isThereEnoughCases(finalSites);\n+            writer.println(\"############################################################################################\");\n+            writer.println(\"# DragstrParams\");\n+            writer.println(\"# -------------------------\");\n+            writer.println(\"# sample = \" + (sampleName == null ? \"<unspecified>\" : sampleName));\n+            writer.println(\"# readGroups = \" + (readGroups.isEmpty()? \"<unspecified>\" : Utils.join(\", \", readGroups)));\n+            writer.println(\"# estimatedOrDefaults = \" + (usingDefaults ? \"defaults\" : \"estimated\"));\n+            writer.println(\"# commandLine = \" + getCommandLine());\n+            writer.println(\"############################################################################################\");\n+            if (!usingDefaults) {\n+                logger.info(\"Estimating parameters used sampled down cases\");\n+                final DragstrParams estimate = estimateParams(finalSites);\n+                logger.info(\"Done with estimation, printing output\");\n+                estimate.print(writer);\n+            } else {\n+                logger.warn(\"Not enough cases to estimate parameters, using defaults\");\n+                DragstrParams.DEFAULT.print(writer);\n+            }\n+        }\n+    }\n+\n+    private BufferedWriter openBufferedWriter(final String path) {\n+        try {\n+            return Files.newBufferedWriter(Paths.get(path));\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(path, ex);\n+        }\n+    }\n+\n+    private Optional<String> resolveSampleName(List<String> sampleNames) {\n+        if (sampleNames.size() > 1) {\n+            throw new GATKException(\"the input alignment(s) have more than one sample: \" + String.join(\", \", sampleNames));\n+        } else if (sampleNames.isEmpty() || sampleNames.get(0) == null) {\n+            logger.warn(\"there is no sample id in the alignment header, assuming that all reads and read/groups make reference to the same anonymous sample\");\n+            return Optional.empty();\n+        } else {\n+            return Optional.of(sampleNames.get(0));\n+        }\n+    }\n+\n+    private void checkSequenceDictionaryCompatibility(final SAMSequenceDictionary reference, final SAMSequenceDictionary strTable) {\n+        final SequenceDictionaryUtils.SequenceDictionaryCompatibility compatibility = SequenceDictionaryUtils.compareDictionaries(reference, strTable, false);\n+        switch (compatibility) {\n+            case IDENTICAL: return;\n+            case SUPERSET: return;\n+            // probably these two below aren't ever be returned since we ask for no check on order but\n+            // adding them it just in case\n+            case NON_CANONICAL_HUMAN_ORDER: return; // we don't care about the order.\n+            case OUT_OF_ORDER: return; // we don't care about the order.\n+            default:\n+                throw new GATKException(\"the reference and str-table sequence dictionary are incompatible: \" + compatibility);\n+        }\n+    }\n+\n+    private PrintWriter openSitesOutputWriter(final String sitesOutput) {\n+        return sitesOutput == null ? new PrintWriter(new NullOutputStream())\n+                : new PrintWriter(BucketUtils.createFile(sitesOutput));\n+    }\n+\n+    private void outputDownSampledSiteDetails(final StratifiedDragstrLocusCases finalSites,\n+                                              final PrintWriter writer,\n+                                              final int minDepth,\n+                                              final int samplingMinMQ,\n+                                              final int maxSup) {\n+        if (sitesOutput != null) {\n+            for (final DragstrLocusCases[] periodCases : finalSites.perPeriodAndRepeat) {\n+                for (final DragstrLocusCases repeatCases : periodCases) {\n+                    for (final DragstrLocusCase caze : repeatCases) {\n+                        outputSiteDetails(writer, caze, caze.qualifies(minDepth, samplingMinMQ, maxSup) ? \"used\" : \"skipped\");\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Holds the minimum counts for each period, repeat-length combo.\n+     * If there is lack of data for any of these we use the default param\n+     * tables. Missing values, row (periods) or columns (repeat-length) are\n+     * interpreted as 0.\n+     */\n+    private static final int[][] MINIMUM_CASES_BY_PERIOD_AND_LENGTH =\n+            // @formatter:off ; prevents code reformatting by IntelliJ\n+            //                  if enabled:\n+            //                    Preferences > Editor > Code Style > Formatter Control\n+            // run-length:\n+            //  0,   1,   2,   3,   4,   5,   6,   7,   8,   9, 10+   // period\n+            {  {},\n+               {0, 200, 200, 200, 200, 200, 200, 200, 200, 200,   0}, // 1\n+               {0,   0, 200, 200, 200, 200,   0,   0,   0,   0,   0}, // 2\n+               {0,   0, 200, 200, 200,   0,   0,   0,   0,   0,   0}, // 3\n+               {0,   0, 200, 200,   0,   0,   0,   0,   0,   0,   0}, // 4\n+               {0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0}, // 5\n+               {0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0}, // 6\n+               {0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0}, // 7\n+               {0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0}, // 8\n+            };\n+            // zeros to the right are actually not necessary, but add them to make it look more like a matrix.\n+            // @formatter:on\n+\n+    /**\n+     * Check that a minimum number of cases are available in key bins (combo period, repeat).\n+     */\n+    private boolean isThereEnoughCases(final StratifiedDragstrLocusCases allSites) {\n+        // period 1, repeat length 1 to 9 (inclusive)\n+        final int[][] MCBL = MINIMUM_CASES_BY_PERIOD_AND_LENGTH;\n+        final int maxP = Math.min(hyperParameters.maxPeriod, MCBL.length - 1);\n+        for (int i = 1; i <= maxP; i++) {\n+            final int maxL = Math.min(hyperParameters.maxRepeatLength, MCBL[i].length - 1);\n+            for (int j = 1; j <= maxL; j++) {\n+                if (allSites.get(i, j).size() < MCBL[i][j]) {\n+                    return false;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Performs the final estimation step.\n+     * @param finalSites the site to use for the estimation.\n+     * @return {@code never null}.\n+     */\n+    private DragstrParams estimateParams(final StratifiedDragstrLocusCases finalSites) {\n+        final DragstrParametersEstimator estimator = new DragstrParametersEstimator(hyperParameters);\n+        return runInParallel ? Utils.runInParallel(threads, () -> estimator.estimate(finalSites)) : estimator.estimate(finalSites);\n+    }\n+\n+    /**\n+     * Downsample sites so that at most as many as {@link #downsampleSize} cases remain for each period and repeat-length combination.\n+     * @param allSites the sites to downsample.\n+     * @param strTable that contains the decimation table used to generate those sites.\n+     * @param sitesOutputWriter an optional per site informattion output argument for debugging purposes.\n+     * @return never {@code null}.\n+     */\n+    private StratifiedDragstrLocusCases downSample(final StratifiedDragstrLocusCases allSites, final STRTableFile strTable,\n+                                                   final PrintWriter sitesOutputWriter) {\n+        final STRDecimationTable decimationTable = strTable.decimationTable();\n+        final List<PeriodAndRepeatLength> prCombos = new ArrayList<>(hyperParameters.maxPeriod * hyperParameters.maxRepeatLength);\n+        for (int i = 1; i <= hyperParameters.maxPeriod; i++) {\n+            for (int j = 1; j <= hyperParameters.maxRepeatLength; j++) {\n+                prCombos.add(PeriodAndRepeatLength.of(i, j));\n+            }\n+        }\n+\n+        final Stream<PeriodAndRepeatLength> prCombosStream = runInParallel ? prCombos.parallelStream() : prCombos.stream();\n+        final Stream<DragstrLocusCase> downsampledStream = prCombosStream\n+                .flatMap(combo -> {\n+                    final DragstrLocusCases all = allSites.perPeriodAndRepeat[combo.period - 1][combo.repeatLength - 1];\n+                    final int decimationBit = decimationTable.decimationBit(combo.period, combo.repeatLength);\n+                    return downSample(all, decimationBit, downsampleSize, sitesOutputWriter).stream();\n+                });\n+\n+        if (runInParallel) {\n+            return Utils.runInParallel(threads,\n+                    () -> downsampledStream.collect(DragstrLocusCaseStratificator.make(hyperParameters.maxPeriod, hyperParameters.maxRepeatLength)));\n+        } else {\n+            return downsampledStream.collect(DragstrLocusCaseStratificator.make(hyperParameters.maxPeriod, hyperParameters.maxRepeatLength));\n+        }\n+    }\n+\n+    /**\n+     * Pre-calculated decimation masks used depending on the final decimation bit/level.\n+     */\n+    private static final long[] DECIMATION_MASKS_BY_BIT = new long[Long.SIZE];\n+\n+    // Code to populate DECIMATION_MASKS_BY_BIT.\n+    static {\n+        DECIMATION_MASKS_BY_BIT[0] = 1;\n+        for (int i = 1, j = 0; i < Long.SIZE; i++, j++) {\n+            DECIMATION_MASKS_BY_BIT[i] = DECIMATION_MASKS_BY_BIT[j] << 1;\n+            DECIMATION_MASKS_BY_BIT[j] = ~DECIMATION_MASKS_BY_BIT[j];\n+        }\n+        DECIMATION_MASKS_BY_BIT[Long.SIZE -1] = ~DECIMATION_MASKS_BY_BIT[Long.SIZE - 1];\n+    }\n+\n+    /**\n+     * Decimates the collection of locus/cases to the downsample size provided or smaller.\n+     * <p>\n+     *     Notice that if we need to downsample (the input size is larger than the downsample size provided)\n+     *     we take care of not counting those cases that have zero-length toward that limit.\n+     *     This is due to the apparent behaviour in DRAGEN where those are \"sort-of\" filtered before\n+     *     decimation as far as meeting the final downsample size limit is concerned.\n+     * </p>\n+     * <p>\n+     *     They usually would be skipped eventually in post-downsampling filtering but we don't consider\n+     *     their number here we end up downsampling some period, repeat-length combintions too much\n+     *     as compare to DRAGEN.\n+     * </p>\n+     * <p>\n+     *     This behavior in DRAGEN may well change in future releases.\n+     * </p>\n+     * @param in input collection of cases to downsample.\n+     * @param minDecimationBit The start decimation bit. Usually the input cases collection won't contain any\n+     *                         cases with lower bit set (already decimated).\n+     * @param downsampleSize the target size.\n+     * @return never {@code null}. At most the return would contain {@code downsampleSize} cases discounting cases with zero depth. It could be empty.\n+     */\n+    private DragstrLocusCases downSample(final DragstrLocusCases in, final int minDecimationBit, final int downsampleSize, final PrintWriter sitesOutputWriter) {\n+        final int inSize = in.size();\n+        if (inSize <= downsampleSize) { // we already satisfy the imposed size limit so we do nothing.\n+            return in;\n+        } else {\n+            int zeroDepth = 0;\n+            final int[] countByFirstDecimatingBit = new int[Long.SIZE - minDecimationBit];\n+            for (final DragstrLocusCase caze: in) {\n+                final DragstrLocus locus = caze.getLocus();\n+                final int depth = caze.getN();\n+                if (depth <= 0) { // we discount cases with zero depth as these are going to be skipped eventually.\n+                    zeroDepth++;\n+                    continue;\n+                }\n+                long mask = locus.getMask();\n+                for (int j = minDecimationBit; mask != 0 && j < Long.SIZE;  j++) {\n+                    final long newMask = mask & DECIMATION_MASKS_BY_BIT[j];\n+                    if (newMask != mask) {\n+                         countByFirstDecimatingBit[j]++;\n+                         break;\n+                    }\n+                }\n+            }\n+\n+            final IntList progressiveSizes = new IntArrayList(Long.SIZE + 1);\n+            progressiveSizes.add(inSize);\n+            int finalSize = inSize - zeroDepth;\n+            progressiveSizes.add(finalSize);\n+            long filterMask = 0;\n+            for (int j = minDecimationBit; finalSize > downsampleSize && j < Long.SIZE; j++) {\n+                finalSize -= countByFirstDecimatingBit[j];\n+                filterMask |= ~DECIMATION_MASKS_BY_BIT[j];\n+                progressiveSizes.add(finalSize);\n+            }\n+            final DragstrLocusCases discarded = new DragstrLocusCases(finalSize, in.getPeriod(), in.getRepeatLength());\n+            final DragstrLocusCases result = new DragstrLocusCases(in.size() - finalSize, in.getPeriod(), in.getRepeatLength());\n+            for (final DragstrLocusCase caze: in) {\n+                final long mask = caze.getLocus().getMask();\n+                if ((mask & filterMask) == 0 & caze.getN() > 0) {\n+                    discarded.add(caze);\n+                } else {\n+                    result.add(caze);\n+                }\n+            }\n+\n+            // Debug-log message format explained:\n+            // period repeat-length [x0, x00, x1, x2, x3 ... xN]\n+            // where x0 is the input size.\n+            //       x00 = x0 - #zero depth cases\n+            //       x1 = x00 - #first round of decimation\n+            //       x2 = x1  - #second round of decimation.\n+            //       ...\n+            //       xN = final size <= downsampleSize\n+            logger.debug(() -> \"\" + in.getPeriod() + \" \"  + in.getRepeatLength() + \" \"\n+                    + Arrays.toString(progressiveSizes.toArray()));\n+\n+            // we output info about the sites that are discarded:\n+            if (sitesOutput != null && result.size() > 0) {\n+                synchronized (sitesOutputWriter) {\n+                    for (final DragstrLocusCase caze : result) {\n+                        outputSiteDetails(sitesOutputWriter, caze, \"downsampled-out\");\n+                    }\n+                }\n+            }\n+            return discarded;\n+        }\n+    }\n+\n+    /**\n+     * Logs cases counts in a matrix where columns are periods and rows are\n+     *  repeat length in repeat units.\n+     * @param cases the cases whose counts are to be logged.\n+     * @param title the title of the debug message.\n+     */\n+    private void logSiteCounts(final StratifiedDragstrLocusCases cases, final String title) {\n+        if (logger.isDebugEnabled()) { // here it seems pertinent to check to save time if DEBUG is off since\n+                                       // this method is all about debug logging.\n+            logger.debug(title);\n+            final int[] columnWidths = IntStream.range(1, hyperParameters.maxPeriod + 1).map(period -> {\n+                final int max = IntStream.range(1, hyperParameters.maxRepeatLength + 1).map(repeat -> cases.get(period,repeat).size())\n+                        .max().orElse(0);\n+                return (int) Math.max(7, Math.ceil(Math.log10(max)) + 1); }).toArray();\n+            logger.debug(\"      \" + IntStream.range(0, hyperParameters.maxPeriod).mapToObj(i -> String.format(\"%-\" + columnWidths[i] + \"s\", (i + 1))).collect(Collectors.joining()));\n+            for (int i = 1; i <= hyperParameters.maxRepeatLength; i++) {\n+                final int repeat = i;\n+                logger.debug(String.format(\"%-4s\", repeat) + \"  \" + IntStream.range(1, hyperParameters.maxPeriod + 1)\n+                        .mapToObj(period -> String.format(\"%-\" + columnWidths[period - 1] + \"s\",\n+                                cases.get(period, repeat).size())).collect(Collectors.joining(\"\")));\n+            }\n+        }\n+    }\n+\n+    private StratifiedDragstrLocusCases collectCaseStatsSequencial(final List<SimpleInterval> intervals, final STRTableFile strTable) {\n+        return intervals.stream()\n+                .flatMap(interval -> streamShardCasesStats(interval, directlyAccessEngineReadsDataSource(), strTable))\n+                .peek(caze -> progressMeter.update(caze.getLocation(dictionary)))\n+                .collect(DragstrLocusCaseStratificator.make(hyperParameters.maxPeriod, hyperParameters.maxRepeatLength));\n+    }\n+\n+    private StratifiedDragstrLocusCases collectCaseStatsParallel(final List<SimpleInterval> intervals, final int shardSize, final STRTableFile strTable) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 467}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA1OTMxMQ==", "bodyText": "Please make a ticket to sparkify this in the future.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495059311", "createdAt": "2020-09-25T15:18:36Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/EstimateDragstrParameters.java", "diffHunk": "@@ -0,0 +1,935 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.*;\n+import htsjdk.samtools.util.IntervalTree;\n+import it.unimi.dsi.fastutil.ints.IntArrayList;\n+import it.unimi.dsi.fastutil.ints.IntList;\n+import org.apache.commons.io.output.NullOutputStream;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.ExampleProgramGroup;\n+import org.broadinstitute.hellbender.engine.*;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrParams;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrUtils;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFile;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.AbsoluteCoordinates;\n+\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.*;\n+import java.util.stream.*;\n+\n+/**\n+ * Estimates the parameters for the DRAGstr model for an input sample.\n+ * <p>\n+ *     This tools takes in the sampling sites generated by {@link ComposeSTRTableFile} on the same reference\n+ *     as the input sample.\n+ * </p>\n+ * <p>\n+ *     The end result is a text file containing three parameter tables (GOP, GCP, API) that can be fed\n+ *     directly to {@link HaplotypeCaller} --dragstr-params-path.\n+ * </p>\n+ */\n+@CommandLineProgramProperties(\n+        summary = \"estimates the parameters for the DRAGstr model for the input sample\",\n+        oneLineSummary = \"summary\",\n+        programGroup = ExampleProgramGroup.class\n+)\n+public class EstimateDragstrParameters extends GATKTool {\n+\n+    public static final String STR_TABLE_PATH_SHORT_NAME = \"str\";\n+    public static final String STR_TABLE_PATH_FULL_NAME = \"str-table-path\";\n+    public static final String PARALLEL_FULL_NAME = \"parallel\";\n+    public static final String THREADS_FULL_NAME = \"threads\";\n+    public static final String SHARD_SIZE_FULL_NAME = \"shard-size\";\n+    public static final String DOWN_SAMPLE_SIZE_FULL_NAME = \"down-sample-size\";\n+    public static final String DEBUG_SITES_OUTPUT_FULL_NAME = \"debug-sites-output\";\n+\n+    public static final int DEFAULT_SHARD_SIZE = 1_000_000;\n+    public static final int DEFAULT_DOWN_SAMPLE_SIZE = 4096;\n+    public static final int SYSTEM_SUGGESTED_THREAD_NUMBER = 0;\n+    public static final int MINIMUM_SHARD_SIZE = 100;\n+    public static final int MINIMUM_DOWN_SAMPLE_SIZE = 512;\n+\n+    @ArgumentCollection\n+    private DragstrHyperParameters hyperParameters = new DragstrHyperParameters();\n+\n+    @Argument(shortName=STR_TABLE_PATH_SHORT_NAME, fullName=STR_TABLE_PATH_FULL_NAME, doc=\"location of the zip that contains the sampling sites for the reference\")\n+    private String strTablePath = null;\n+\n+    @Argument(fullName=PARALLEL_FULL_NAME, doc=\"run alignment data collection and  estimation in parallel\")\n+    private boolean runInParallel = false;\n+\n+    @Argument(fullName=THREADS_FULL_NAME, minValue = SYSTEM_SUGGESTED_THREAD_NUMBER, doc=\"suggested number of parallel threads to perform the estimation, \"\n+            + \"the default 0 leave it up to the VM to decide. When set to more than 1, this will activate parallel in the absence of --parallel\")\n+    private int threads = SYSTEM_SUGGESTED_THREAD_NUMBER;\n+\n+    @Argument(fullName=SHARD_SIZE_FULL_NAME, doc=\"when running in parallel this is the suggested shard size in base pairs. \" +\n+            \"The actual shard-size may vary to adapt to small contigs and the requested number of threads\",\n+              minValue = MINIMUM_SHARD_SIZE)\n+    private int shardSize = DEFAULT_SHARD_SIZE;\n+\n+    @Argument(fullName=DOWN_SAMPLE_SIZE_FULL_NAME, doc=\"Targeted maximum number of cases per combination period repeat count, \" +\n+            \"the larger the more precise but also the slower estimation.\",\n+              minValue = MINIMUM_DOWN_SAMPLE_SIZE)\n+    private int downsampleSize = DEFAULT_DOWN_SAMPLE_SIZE;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME, shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME, doc = \"where to write the parameter output file.\")\n+    private String output = null;\n+\n+    @Argument(fullName= DEBUG_SITES_OUTPUT_FULL_NAME, doc = \"table with information gather on the samples sites. Includes what sites were downsampled, disqualified or accepted for parameter estimation\", optional = true)\n+    private String sitesOutput = null;\n+\n+    private SAMSequenceDictionary dictionary;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean requiresReads() {\n+        return true;\n+    }\n+\n+    @Override\n+    protected void onStartup() {\n+        super.onStartup();\n+        hyperParameters.validate();\n+        dictionary = getBestAvailableSequenceDictionary();\n+        if (runInParallel) {\n+            if (threads == 1) {\n+                logger.warn(\"parallel processing was requested but the number of threads was set to 1\");\n+            }\n+        } else if (threads > 1) {\n+            runInParallel = true;\n+        }\n+        if (runInParallel) {\n+            if (threads == 0) {\n+                logger.info(\"Running in parallel using the system suggested default thread count: \" + Runtime.getRuntime().availableProcessors());\n+            } else {\n+                logger.info(\"Running in parallel using the requested number of threads: \" + threads);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        hyperParameters.validate();\n+        dictionary = getBestAvailableSequenceDictionary();\n+        final List<SAMReadGroupRecord> readGroups = hasReads() ? getHeaderForReads().getReadGroups() : Collections.emptyList();\n+        final List<String> readGroupIds = readGroups.stream()\n+                .map(SAMReadGroupRecord::getId)\n+                .collect(Collectors.toList());\n+        final List<String> sampleNames = readGroups.stream()\n+                .map(SAMReadGroupRecord::getSample)\n+                .distinct().collect(Collectors.toList());\n+        final Optional<String> sampleName = resolveSampleName(sampleNames);\n+\n+        try (final PrintWriter sitesOutputWriter = openSitesOutputWriter(sitesOutput);\n+             final STRTableFile strTable = STRTableFile.open(strTablePath)) {\n+\n+            checkSequenceDictionaryCompatibility(dictionary, strTable.dictionary());\n+            final StratifiedDragstrLocusCases allSites;\n+            final List<SimpleInterval> intervals = getTraversalIntervals();\n+\n+            runInParallel |= threads > 1;\n+            if (runInParallel) {\n+                if (threads == 1) {\n+                    logger.warn(\"parallel processing was requested but the number of threads was set to 1\");\n+                }\n+                allSites = collectCaseStatsParallel(intervals, shardSize, strTable);\n+            } else {\n+                allSites = collectCaseStatsSequencial(intervals, strTable);\n+            }\n+            logSiteCounts(allSites, \"all loci/cases\");\n+            final StratifiedDragstrLocusCases downSampledSites = downSample(allSites, strTable, sitesOutputWriter);\n+            logSiteCounts(downSampledSites, \"all downsampled (kept) loci/cases\");\n+            final StratifiedDragstrLocusCases finalSites = downSampledSites.qualifyingOnly(hyperParameters.minDepth, hyperParameters.minMQ, 0);\n+            logSiteCounts(finalSites, \"all qualifying loci/cases\");\n+            outputDownSampledSiteDetails(downSampledSites, sitesOutputWriter, hyperParameters.minDepth, hyperParameters.minMQ, 0);\n+            printOutput(finalSites, sampleName.orElse(null), readGroupIds);\n+        }\n+    }\n+\n+    private void printOutput(final StratifiedDragstrLocusCases finalSites, final String sampleName, final List<String> readGroups) {\n+        try (final PrintWriter writer = new PrintWriter(openBufferedWriter(output))) {\n+            final boolean usingDefaults = !isThereEnoughCases(finalSites);\n+            writer.println(\"############################################################################################\");\n+            writer.println(\"# DragstrParams\");\n+            writer.println(\"# -------------------------\");\n+            writer.println(\"# sample = \" + (sampleName == null ? \"<unspecified>\" : sampleName));\n+            writer.println(\"# readGroups = \" + (readGroups.isEmpty()? \"<unspecified>\" : Utils.join(\", \", readGroups)));\n+            writer.println(\"# estimatedOrDefaults = \" + (usingDefaults ? \"defaults\" : \"estimated\"));\n+            writer.println(\"# commandLine = \" + getCommandLine());\n+            writer.println(\"############################################################################################\");\n+            if (!usingDefaults) {\n+                logger.info(\"Estimating parameters used sampled down cases\");\n+                final DragstrParams estimate = estimateParams(finalSites);\n+                logger.info(\"Done with estimation, printing output\");\n+                estimate.print(writer);\n+            } else {\n+                logger.warn(\"Not enough cases to estimate parameters, using defaults\");\n+                DragstrParams.DEFAULT.print(writer);\n+            }\n+        }\n+    }\n+\n+    private BufferedWriter openBufferedWriter(final String path) {\n+        try {\n+            return Files.newBufferedWriter(Paths.get(path));\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(path, ex);\n+        }\n+    }\n+\n+    private Optional<String> resolveSampleName(List<String> sampleNames) {\n+        if (sampleNames.size() > 1) {\n+            throw new GATKException(\"the input alignment(s) have more than one sample: \" + String.join(\", \", sampleNames));\n+        } else if (sampleNames.isEmpty() || sampleNames.get(0) == null) {\n+            logger.warn(\"there is no sample id in the alignment header, assuming that all reads and read/groups make reference to the same anonymous sample\");\n+            return Optional.empty();\n+        } else {\n+            return Optional.of(sampleNames.get(0));\n+        }\n+    }\n+\n+    private void checkSequenceDictionaryCompatibility(final SAMSequenceDictionary reference, final SAMSequenceDictionary strTable) {\n+        final SequenceDictionaryUtils.SequenceDictionaryCompatibility compatibility = SequenceDictionaryUtils.compareDictionaries(reference, strTable, false);\n+        switch (compatibility) {\n+            case IDENTICAL: return;\n+            case SUPERSET: return;\n+            // probably these two below aren't ever be returned since we ask for no check on order but\n+            // adding them it just in case\n+            case NON_CANONICAL_HUMAN_ORDER: return; // we don't care about the order.\n+            case OUT_OF_ORDER: return; // we don't care about the order.\n+            default:\n+                throw new GATKException(\"the reference and str-table sequence dictionary are incompatible: \" + compatibility);\n+        }\n+    }\n+\n+    private PrintWriter openSitesOutputWriter(final String sitesOutput) {\n+        return sitesOutput == null ? new PrintWriter(new NullOutputStream())\n+                : new PrintWriter(BucketUtils.createFile(sitesOutput));\n+    }\n+\n+    private void outputDownSampledSiteDetails(final StratifiedDragstrLocusCases finalSites,\n+                                              final PrintWriter writer,\n+                                              final int minDepth,\n+                                              final int samplingMinMQ,\n+                                              final int maxSup) {\n+        if (sitesOutput != null) {\n+            for (final DragstrLocusCases[] periodCases : finalSites.perPeriodAndRepeat) {\n+                for (final DragstrLocusCases repeatCases : periodCases) {\n+                    for (final DragstrLocusCase caze : repeatCases) {\n+                        outputSiteDetails(writer, caze, caze.qualifies(minDepth, samplingMinMQ, maxSup) ? \"used\" : \"skipped\");\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Holds the minimum counts for each period, repeat-length combo.\n+     * If there is lack of data for any of these we use the default param\n+     * tables. Missing values, row (periods) or columns (repeat-length) are\n+     * interpreted as 0.\n+     */\n+    private static final int[][] MINIMUM_CASES_BY_PERIOD_AND_LENGTH =\n+            // @formatter:off ; prevents code reformatting by IntelliJ\n+            //                  if enabled:\n+            //                    Preferences > Editor > Code Style > Formatter Control\n+            // run-length:\n+            //  0,   1,   2,   3,   4,   5,   6,   7,   8,   9, 10+   // period\n+            {  {},\n+               {0, 200, 200, 200, 200, 200, 200, 200, 200, 200,   0}, // 1\n+               {0,   0, 200, 200, 200, 200,   0,   0,   0,   0,   0}, // 2\n+               {0,   0, 200, 200, 200,   0,   0,   0,   0,   0,   0}, // 3\n+               {0,   0, 200, 200,   0,   0,   0,   0,   0,   0,   0}, // 4\n+               {0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0}, // 5\n+               {0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0}, // 6\n+               {0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0}, // 7\n+               {0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0}, // 8\n+            };\n+            // zeros to the right are actually not necessary, but add them to make it look more like a matrix.\n+            // @formatter:on\n+\n+    /**\n+     * Check that a minimum number of cases are available in key bins (combo period, repeat).\n+     */\n+    private boolean isThereEnoughCases(final StratifiedDragstrLocusCases allSites) {\n+        // period 1, repeat length 1 to 9 (inclusive)\n+        final int[][] MCBL = MINIMUM_CASES_BY_PERIOD_AND_LENGTH;\n+        final int maxP = Math.min(hyperParameters.maxPeriod, MCBL.length - 1);\n+        for (int i = 1; i <= maxP; i++) {\n+            final int maxL = Math.min(hyperParameters.maxRepeatLength, MCBL[i].length - 1);\n+            for (int j = 1; j <= maxL; j++) {\n+                if (allSites.get(i, j).size() < MCBL[i][j]) {\n+                    return false;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Performs the final estimation step.\n+     * @param finalSites the site to use for the estimation.\n+     * @return {@code never null}.\n+     */\n+    private DragstrParams estimateParams(final StratifiedDragstrLocusCases finalSites) {\n+        final DragstrParametersEstimator estimator = new DragstrParametersEstimator(hyperParameters);\n+        return runInParallel ? Utils.runInParallel(threads, () -> estimator.estimate(finalSites)) : estimator.estimate(finalSites);\n+    }\n+\n+    /**\n+     * Downsample sites so that at most as many as {@link #downsampleSize} cases remain for each period and repeat-length combination.\n+     * @param allSites the sites to downsample.\n+     * @param strTable that contains the decimation table used to generate those sites.\n+     * @param sitesOutputWriter an optional per site informattion output argument for debugging purposes.\n+     * @return never {@code null}.\n+     */\n+    private StratifiedDragstrLocusCases downSample(final StratifiedDragstrLocusCases allSites, final STRTableFile strTable,\n+                                                   final PrintWriter sitesOutputWriter) {\n+        final STRDecimationTable decimationTable = strTable.decimationTable();\n+        final List<PeriodAndRepeatLength> prCombos = new ArrayList<>(hyperParameters.maxPeriod * hyperParameters.maxRepeatLength);\n+        for (int i = 1; i <= hyperParameters.maxPeriod; i++) {\n+            for (int j = 1; j <= hyperParameters.maxRepeatLength; j++) {\n+                prCombos.add(PeriodAndRepeatLength.of(i, j));\n+            }\n+        }\n+\n+        final Stream<PeriodAndRepeatLength> prCombosStream = runInParallel ? prCombos.parallelStream() : prCombos.stream();\n+        final Stream<DragstrLocusCase> downsampledStream = prCombosStream\n+                .flatMap(combo -> {\n+                    final DragstrLocusCases all = allSites.perPeriodAndRepeat[combo.period - 1][combo.repeatLength - 1];\n+                    final int decimationBit = decimationTable.decimationBit(combo.period, combo.repeatLength);\n+                    return downSample(all, decimationBit, downsampleSize, sitesOutputWriter).stream();\n+                });\n+\n+        if (runInParallel) {\n+            return Utils.runInParallel(threads,\n+                    () -> downsampledStream.collect(DragstrLocusCaseStratificator.make(hyperParameters.maxPeriod, hyperParameters.maxRepeatLength)));\n+        } else {\n+            return downsampledStream.collect(DragstrLocusCaseStratificator.make(hyperParameters.maxPeriod, hyperParameters.maxRepeatLength));\n+        }\n+    }\n+\n+    /**\n+     * Pre-calculated decimation masks used depending on the final decimation bit/level.\n+     */\n+    private static final long[] DECIMATION_MASKS_BY_BIT = new long[Long.SIZE];\n+\n+    // Code to populate DECIMATION_MASKS_BY_BIT.\n+    static {\n+        DECIMATION_MASKS_BY_BIT[0] = 1;\n+        for (int i = 1, j = 0; i < Long.SIZE; i++, j++) {\n+            DECIMATION_MASKS_BY_BIT[i] = DECIMATION_MASKS_BY_BIT[j] << 1;\n+            DECIMATION_MASKS_BY_BIT[j] = ~DECIMATION_MASKS_BY_BIT[j];\n+        }\n+        DECIMATION_MASKS_BY_BIT[Long.SIZE -1] = ~DECIMATION_MASKS_BY_BIT[Long.SIZE - 1];\n+    }\n+\n+    /**\n+     * Decimates the collection of locus/cases to the downsample size provided or smaller.\n+     * <p>\n+     *     Notice that if we need to downsample (the input size is larger than the downsample size provided)\n+     *     we take care of not counting those cases that have zero-length toward that limit.\n+     *     This is due to the apparent behaviour in DRAGEN where those are \"sort-of\" filtered before\n+     *     decimation as far as meeting the final downsample size limit is concerned.\n+     * </p>\n+     * <p>\n+     *     They usually would be skipped eventually in post-downsampling filtering but we don't consider\n+     *     their number here we end up downsampling some period, repeat-length combintions too much\n+     *     as compare to DRAGEN.\n+     * </p>\n+     * <p>\n+     *     This behavior in DRAGEN may well change in future releases.\n+     * </p>\n+     * @param in input collection of cases to downsample.\n+     * @param minDecimationBit The start decimation bit. Usually the input cases collection won't contain any\n+     *                         cases with lower bit set (already decimated).\n+     * @param downsampleSize the target size.\n+     * @return never {@code null}. At most the return would contain {@code downsampleSize} cases discounting cases with zero depth. It could be empty.\n+     */\n+    private DragstrLocusCases downSample(final DragstrLocusCases in, final int minDecimationBit, final int downsampleSize, final PrintWriter sitesOutputWriter) {\n+        final int inSize = in.size();\n+        if (inSize <= downsampleSize) { // we already satisfy the imposed size limit so we do nothing.\n+            return in;\n+        } else {\n+            int zeroDepth = 0;\n+            final int[] countByFirstDecimatingBit = new int[Long.SIZE - minDecimationBit];\n+            for (final DragstrLocusCase caze: in) {\n+                final DragstrLocus locus = caze.getLocus();\n+                final int depth = caze.getN();\n+                if (depth <= 0) { // we discount cases with zero depth as these are going to be skipped eventually.\n+                    zeroDepth++;\n+                    continue;\n+                }\n+                long mask = locus.getMask();\n+                for (int j = minDecimationBit; mask != 0 && j < Long.SIZE;  j++) {\n+                    final long newMask = mask & DECIMATION_MASKS_BY_BIT[j];\n+                    if (newMask != mask) {\n+                         countByFirstDecimatingBit[j]++;\n+                         break;\n+                    }\n+                }\n+            }\n+\n+            final IntList progressiveSizes = new IntArrayList(Long.SIZE + 1);\n+            progressiveSizes.add(inSize);\n+            int finalSize = inSize - zeroDepth;\n+            progressiveSizes.add(finalSize);\n+            long filterMask = 0;\n+            for (int j = minDecimationBit; finalSize > downsampleSize && j < Long.SIZE; j++) {\n+                finalSize -= countByFirstDecimatingBit[j];\n+                filterMask |= ~DECIMATION_MASKS_BY_BIT[j];\n+                progressiveSizes.add(finalSize);\n+            }\n+            final DragstrLocusCases discarded = new DragstrLocusCases(finalSize, in.getPeriod(), in.getRepeatLength());\n+            final DragstrLocusCases result = new DragstrLocusCases(in.size() - finalSize, in.getPeriod(), in.getRepeatLength());\n+            for (final DragstrLocusCase caze: in) {\n+                final long mask = caze.getLocus().getMask();\n+                if ((mask & filterMask) == 0 & caze.getN() > 0) {\n+                    discarded.add(caze);\n+                } else {\n+                    result.add(caze);\n+                }\n+            }\n+\n+            // Debug-log message format explained:\n+            // period repeat-length [x0, x00, x1, x2, x3 ... xN]\n+            // where x0 is the input size.\n+            //       x00 = x0 - #zero depth cases\n+            //       x1 = x00 - #first round of decimation\n+            //       x2 = x1  - #second round of decimation.\n+            //       ...\n+            //       xN = final size <= downsampleSize\n+            logger.debug(() -> \"\" + in.getPeriod() + \" \"  + in.getRepeatLength() + \" \"\n+                    + Arrays.toString(progressiveSizes.toArray()));\n+\n+            // we output info about the sites that are discarded:\n+            if (sitesOutput != null && result.size() > 0) {\n+                synchronized (sitesOutputWriter) {\n+                    for (final DragstrLocusCase caze : result) {\n+                        outputSiteDetails(sitesOutputWriter, caze, \"downsampled-out\");\n+                    }\n+                }\n+            }\n+            return discarded;\n+        }\n+    }\n+\n+    /**\n+     * Logs cases counts in a matrix where columns are periods and rows are\n+     *  repeat length in repeat units.\n+     * @param cases the cases whose counts are to be logged.\n+     * @param title the title of the debug message.\n+     */\n+    private void logSiteCounts(final StratifiedDragstrLocusCases cases, final String title) {\n+        if (logger.isDebugEnabled()) { // here it seems pertinent to check to save time if DEBUG is off since\n+                                       // this method is all about debug logging.\n+            logger.debug(title);\n+            final int[] columnWidths = IntStream.range(1, hyperParameters.maxPeriod + 1).map(period -> {\n+                final int max = IntStream.range(1, hyperParameters.maxRepeatLength + 1).map(repeat -> cases.get(period,repeat).size())\n+                        .max().orElse(0);\n+                return (int) Math.max(7, Math.ceil(Math.log10(max)) + 1); }).toArray();\n+            logger.debug(\"      \" + IntStream.range(0, hyperParameters.maxPeriod).mapToObj(i -> String.format(\"%-\" + columnWidths[i] + \"s\", (i + 1))).collect(Collectors.joining()));\n+            for (int i = 1; i <= hyperParameters.maxRepeatLength; i++) {\n+                final int repeat = i;\n+                logger.debug(String.format(\"%-4s\", repeat) + \"  \" + IntStream.range(1, hyperParameters.maxPeriod + 1)\n+                        .mapToObj(period -> String.format(\"%-\" + columnWidths[period - 1] + \"s\",\n+                                cases.get(period, repeat).size())).collect(Collectors.joining(\"\")));\n+            }\n+        }\n+    }\n+\n+    private StratifiedDragstrLocusCases collectCaseStatsSequencial(final List<SimpleInterval> intervals, final STRTableFile strTable) {\n+        return intervals.stream()\n+                .flatMap(interval -> streamShardCasesStats(interval, directlyAccessEngineReadsDataSource(), strTable))\n+                .peek(caze -> progressMeter.update(caze.getLocation(dictionary)))\n+                .collect(DragstrLocusCaseStratificator.make(hyperParameters.maxPeriod, hyperParameters.maxRepeatLength));\n+    }\n+\n+    private StratifiedDragstrLocusCases collectCaseStatsParallel(final List<SimpleInterval> intervals, final int shardSize, final STRTableFile strTable) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA1NzMzNQ=="}, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 467}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEzMjQxMQ==", "bodyText": "Is the solution here really to have the GATK accept the zipped file as input here? Can we not expect the user to have unzipped the input files into a folder before running this tool?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495132411", "createdAt": "2020-09-25T17:29:08Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/ZipUtils.java", "diffHunk": "@@ -0,0 +1,128 @@\n+package org.broadinstitute.hellbender.utils;\n+\n+import org.apache.commons.io.IOUtils;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+\n+import java.io.*;\n+import java.util.ArrayDeque;\n+import java.util.Arrays;\n+import java.util.Deque;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.zip.*;\n+\n+/**\n+ * Utility class to zip and unzip files.\n+ */\n+public class ZipUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEzNDc3Nw==", "bodyText": "Update the javadoc to include the record count increase line. Otherwise probably good to include this.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495134777", "createdAt": "2020-09-25T17:33:46Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/engine/ProgressMeter.java", "diffHunk": "@@ -222,6 +222,21 @@ public void update( final Locatable currentLocus ) {\n         }\n     }\n \n+    public void update( final Locatable currentLocus, final long recordCountIncrease ) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE0MzU0NA==", "bodyText": "I reiterate that I probably think the solution here is to use spark but this seems like a sound implementation on top of the Apache library code.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495143544", "createdAt": "2020-09-25T17:51:51Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/engine/ReadsDataSourcePool.java", "diffHunk": "@@ -0,0 +1,80 @@\n+package org.broadinstitute.hellbender.engine;\n+\n+import org.apache.commons.pool.BasePoolableObjectFactory;\n+import org.apache.commons.pool.impl.GenericObjectPool;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.utils.AutoCloseableReference;\n+\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Pool of {@link ReadsDataSource} instances.\n+ */\n+public final class ReadsDataSourcePool extends GenericObjectPool<ReadsDataSource> implements AutoCloseable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE0NTY5Mg==", "bodyText": "Since the buffer size is ultimately coming from the command line I would actually throw a user exception here about the buffer size if its outside of the bounds.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495145692", "createdAt": "2020-09-25T17:56:06Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/BufferedReferenceBases.java", "diffHunk": "@@ -1,63 +1,87 @@\n-package org.broadinstitute.hellbender.utils.pairhmm;\n+package org.broadinstitute.hellbender.tools.dragstr;\n \n import htsjdk.samtools.SAMSequenceRecord;\n import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n import org.broadinstitute.hellbender.utils.Utils;\n import org.broadinstitute.hellbender.utils.param.ParamUtils;\n \n-public class LazyLoadingReferenceNucleotideSequence implements NucleotideSequence {\n+/**\n+ * Buffered access to long reference sequences.\n+ * <p>\n+ * Allows to access the whole contig sequence down to a single base at a time without worrying about loading the next frame.\n+ * </p>\n+ * Allows contiguous access of individual bases for down to single base retrieval loading more bases from the underlying sequence as needed.\n+ * <p>\n+ *     It keep a copy of the previous loaded section in case there some additional bases up-stream are requested.\n+ * </p>\n+ */\n+final class BufferedReferenceBases {\n     private final String id;\n     private final long length;\n     private int bufferSize;\n     private byte[] buffer;\n-    private byte[] upstreamBuffer;\n+    private byte[] previousBuffer;\n     private final ReferenceDataSource dataSource;\n     private long bufferStart;\n     private long bufferEnd;\n \n+    private static final int MIN_BUFFER_SIZE = 1024;\n \n-    private LazyLoadingReferenceNucleotideSequence(final ReferenceDataSource dataSource, final String id, final long length, final int bufferSize) {\n+    private BufferedReferenceBases(final ReferenceDataSource dataSource, final String id, final long length, final int bufferSize) {\n         this.dataSource = dataSource;\n         this.id = id;\n         this.length = length;\n-        this.bufferSize = bufferSize < 128 ? 128 : bufferSize;\n+        this.bufferSize = Math.max(MIN_BUFFER_SIZE, (int) Math.min(bufferSize , length));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE0NjM1Mw==", "bodyText": "The name needs to be either \"contigName\" or \"contigID\" id is too ambiguous", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495146353", "createdAt": "2020-09-25T17:57:25Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/BufferedReferenceBases.java", "diffHunk": "@@ -1,63 +1,87 @@\n-package org.broadinstitute.hellbender.utils.pairhmm;\n+package org.broadinstitute.hellbender.tools.dragstr;\n \n import htsjdk.samtools.SAMSequenceRecord;\n import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n import org.broadinstitute.hellbender.utils.Utils;\n import org.broadinstitute.hellbender.utils.param.ParamUtils;\n \n-public class LazyLoadingReferenceNucleotideSequence implements NucleotideSequence {\n+/**\n+ * Buffered access to long reference sequences.\n+ * <p>\n+ * Allows to access the whole contig sequence down to a single base at a time without worrying about loading the next frame.\n+ * </p>\n+ * Allows contiguous access of individual bases for down to single base retrieval loading more bases from the underlying sequence as needed.\n+ * <p>\n+ *     It keep a copy of the previous loaded section in case there some additional bases up-stream are requested.\n+ * </p>\n+ */\n+final class BufferedReferenceBases {\n     private final String id;\n     private final long length;\n     private int bufferSize;\n     private byte[] buffer;\n-    private byte[] upstreamBuffer;\n+    private byte[] previousBuffer;\n     private final ReferenceDataSource dataSource;\n     private long bufferStart;\n     private long bufferEnd;\n \n+    private static final int MIN_BUFFER_SIZE = 1024;\n \n-    private LazyLoadingReferenceNucleotideSequence(final ReferenceDataSource dataSource, final String id, final long length, final int bufferSize) {\n+    private BufferedReferenceBases(final ReferenceDataSource dataSource, final String id, final long length, final int bufferSize) {\n         this.dataSource = dataSource;\n         this.id = id;\n         this.length = length;\n-        this.bufferSize = bufferSize < 128 ? 128 : bufferSize;\n+        this.bufferSize = Math.max(MIN_BUFFER_SIZE, (int) Math.min(bufferSize , length));\n         this.buffer = null;\n-        this.upstreamBuffer = null;\n+        this.previousBuffer = null;\n         this.bufferStart = -1;\n         this.bufferEnd = -1;\n     }\n \n+    public String id() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE0OTQxNg==", "bodyText": "Mention that this is a wrapper on top of ReferenceDataSource and is intended to supplement the existing buffering that already exists there.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495149416", "createdAt": "2020-09-25T18:03:51Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/BufferedReferenceBases.java", "diffHunk": "@@ -1,63 +1,87 @@\n-package org.broadinstitute.hellbender.utils.pairhmm;\n+package org.broadinstitute.hellbender.tools.dragstr;\n \n import htsjdk.samtools.SAMSequenceRecord;\n import org.broadinstitute.hellbender.engine.ReferenceDataSource;\n import org.broadinstitute.hellbender.utils.Utils;\n import org.broadinstitute.hellbender.utils.param.ParamUtils;\n \n-public class LazyLoadingReferenceNucleotideSequence implements NucleotideSequence {\n+/**\n+ * Buffered access to long reference sequences.\n+ * <p>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1MjUwOQ==", "bodyText": "These warning suppression lines are are unnecessary.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495152509", "createdAt": "2020-09-25T18:10:14Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/ComposeSTRTableFile.java", "diffHunk": "@@ -0,0 +1,269 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import org.apache.commons.io.FileUtils;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.barclay.argparser.Hidden;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFile;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFileBuilder;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * This tools look for STR sequences in the reference that will used later to estimate the Dragstr parameters values\n+ * using {@link EstimateDragstrParameters}.\n+ * <h3>Inputs</h3>\n+ * <p>\n+ *     This command takes as input the reference (possibly traversal intervals) and a {@link STRDecimationTable decimation table} herein\n+ *     referred as DT.\n+ * </p>\n+ * <p>\n+ *     The DT modulates how often we sample a site for each possible period and repeat length. Since there is far more\n+ *     positions with short period and short repeat length sampling for those combinations should be less frequent.\n+ *     For further details about the format of this table and interpretation of its values please check the documentation\n+ *     in class {@link STRDecimationTable}.\n+ * </p>\n+ * <h3>Output</h3>\n+ * <p>\n+ *     The output of this command is a zip file that contain the collection of sampled sites in binary form (all.bin),\n+ *     and index for that file for quick access by location interval (all.idx), a copy of the reference sequence dictionary\n+ *     (reference.dict), a copy of the DT (decimation.txt) and additional information and stats (e.g. summary.txt)\n+ * </p>\n+ * <p>\n+ *     The reference dictionary file may be used by commands downstream that need to verify that\n+ *     the reference that wa use to generate the sample sites matches the one that is provided by the user to that command.\n+ * </p>\n+ * <p>\n+ *     The DT also provide downstream commands with the information as to how the resulting collection of sites was downSampled,\n+ *     in case further down-sampling is necessary.\n+ * </p>\n+ */\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class ComposeSTRTableFile extends GATKTool {\n+\n+    @SuppressWarnings(\"WeakerAccess\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1MjcxNQ==", "bodyText": "Add one usage example for the tool for documentation purposes.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495152715", "createdAt": "2020-09-25T18:10:34Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/ComposeSTRTableFile.java", "diffHunk": "@@ -0,0 +1,269 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import org.apache.commons.io.FileUtils;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.barclay.argparser.Hidden;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFile;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFileBuilder;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * This tools look for STR sequences in the reference that will used later to estimate the Dragstr parameters values\n+ * using {@link EstimateDragstrParameters}.\n+ * <h3>Inputs</h3>\n+ * <p>\n+ *     This command takes as input the reference (possibly traversal intervals) and a {@link STRDecimationTable decimation table} herein\n+ *     referred as DT.\n+ * </p>\n+ * <p>\n+ *     The DT modulates how often we sample a site for each possible period and repeat length. Since there is far more\n+ *     positions with short period and short repeat length sampling for those combinations should be less frequent.\n+ *     For further details about the format of this table and interpretation of its values please check the documentation\n+ *     in class {@link STRDecimationTable}.\n+ * </p>\n+ * <h3>Output</h3>\n+ * <p>\n+ *     The output of this command is a zip file that contain the collection of sampled sites in binary form (all.bin),\n+ *     and index for that file for quick access by location interval (all.idx), a copy of the reference sequence dictionary\n+ *     (reference.dict), a copy of the DT (decimation.txt) and additional information and stats (e.g. summary.txt)\n+ * </p>\n+ * <p>\n+ *     The reference dictionary file may be used by commands downstream that need to verify that\n+ *     the reference that wa use to generate the sample sites matches the one that is provided by the user to that command.\n+ * </p>\n+ * <p>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1OTA5Ng==", "bodyText": "A sentance explaining that the decimation table is downsampling.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495159096", "createdAt": "2020-09-25T18:23:38Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/ComposeSTRTableFile.java", "diffHunk": "@@ -0,0 +1,269 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import org.apache.commons.io.FileUtils;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.barclay.argparser.Hidden;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFile;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFileBuilder;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * This tools look for STR sequences in the reference that will used later to estimate the Dragstr parameters values\n+ * using {@link EstimateDragstrParameters}.\n+ * <h3>Inputs</h3>\n+ * <p>\n+ *     This command takes as input the reference (possibly traversal intervals) and a {@link STRDecimationTable decimation table} herein\n+ *     referred as DT.\n+ * </p>\n+ * <p>\n+ *     The DT modulates how often we sample a site for each possible period and repeat length. Since there is far more\n+ *     positions with short period and short repeat length sampling for those combinations should be less frequent.\n+ *     For further details about the format of this table and interpretation of its values please check the documentation\n+ *     in class {@link STRDecimationTable}.\n+ * </p>\n+ * <h3>Output</h3>\n+ * <p>\n+ *     The output of this command is a zip file that contain the collection of sampled sites in binary form (all.bin),\n+ *     and index for that file for quick access by location interval (all.idx), a copy of the reference sequence dictionary\n+ *     (reference.dict), a copy of the DT (decimation.txt) and additional information and stats (e.g. summary.txt)\n+ * </p>\n+ * <p>\n+ *     The reference dictionary file may be used by commands downstream that need to verify that\n+ *     the reference that wa use to generate the sample sites matches the one that is provided by the user to that command.\n+ * </p>\n+ * <p>\n+ *     The DT also provide downstream commands with the information as to how the resulting collection of sites was downSampled,\n+ *     in case further down-sampling is necessary.\n+ * </p>\n+ */\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class ComposeSTRTableFile extends GATKTool {\n+\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final String REFERENCE_SEQUENCE_BUFFER_SIZE_FULL_NAME = \"reference-sequence-buffer-size\";\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final String GENERATE_SITES_TEXT_OUTPUT_FULL_NAME = \"generate-sites-text-output\";\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final int DEFAULT_REFERENCE_SEQUENCE_BUFFER_SIZE = 100_000;\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NDM0MQ==", "bodyText": "Get rid of these code inspection stubs.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495164341", "createdAt": "2020-09-25T18:34:07Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/ComposeSTRTableFile.java", "diffHunk": "@@ -0,0 +1,269 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import org.apache.commons.io.FileUtils;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.barclay.argparser.Hidden;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFile;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFileBuilder;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * This tools look for STR sequences in the reference that will used later to estimate the Dragstr parameters values\n+ * using {@link EstimateDragstrParameters}.\n+ * <h3>Inputs</h3>\n+ * <p>\n+ *     This command takes as input the reference (possibly traversal intervals) and a {@link STRDecimationTable decimation table} herein\n+ *     referred as DT.\n+ * </p>\n+ * <p>\n+ *     The DT modulates how often we sample a site for each possible period and repeat length. Since there is far more\n+ *     positions with short period and short repeat length sampling for those combinations should be less frequent.\n+ *     For further details about the format of this table and interpretation of its values please check the documentation\n+ *     in class {@link STRDecimationTable}.\n+ * </p>\n+ * <h3>Output</h3>\n+ * <p>\n+ *     The output of this command is a zip file that contain the collection of sampled sites in binary form (all.bin),\n+ *     and index for that file for quick access by location interval (all.idx), a copy of the reference sequence dictionary\n+ *     (reference.dict), a copy of the DT (decimation.txt) and additional information and stats (e.g. summary.txt)\n+ * </p>\n+ * <p>\n+ *     The reference dictionary file may be used by commands downstream that need to verify that\n+ *     the reference that wa use to generate the sample sites matches the one that is provided by the user to that command.\n+ * </p>\n+ * <p>\n+ *     The DT also provide downstream commands with the information as to how the resulting collection of sites was downSampled,\n+ *     in case further down-sampling is necessary.\n+ * </p>\n+ */\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class ComposeSTRTableFile extends GATKTool {\n+\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final String REFERENCE_SEQUENCE_BUFFER_SIZE_FULL_NAME = \"reference-sequence-buffer-size\";\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final String GENERATE_SITES_TEXT_OUTPUT_FULL_NAME = \"generate-sites-text-output\";\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final int DEFAULT_REFERENCE_SEQUENCE_BUFFER_SIZE = 100_000;\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private STRDecimationTable decimationTable = STRDecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(doc = \"request to generate a text formatted version of the STR table in the output zip (\" + STRTableFile.SITES_TEXT_FILE_NAME + \")\",\n+              fullName = GENERATE_SITES_TEXT_OUTPUT_FULL_NAME, optional = true)\n+    @Hidden\n+    private boolean generateSitesTextOutput = false;\n+\n+    @Argument(fullName = DragstrHyperParameters.MAX_PERIOD_ARGUMENT_FULL_NAME, doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxPeriod = DragstrHyperParameters.DEFAULT_MAX_PERIOD;\n+\n+    @Argument(fullName = DragstrHyperParameters.MAX_REPEATS_ARGUMENT_FULL_NAME, doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 100)\n+    private int maxRepeat = DragstrHyperParameters.DEFAULT_MAX_REPEAT_LENGTH;\n+\n+    @Argument(fullName= REFERENCE_SEQUENCE_BUFFER_SIZE_FULL_NAME, doc=\"size of the look ahead reference sequence buffer\", optional = true, minValue = 100, maxValue = 1_000_000_000)\n+    @Hidden\n+    private int referenceSequenceBufferSize = DEFAULT_REFERENCE_SEQUENCE_BUFFER_SIZE;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private static final String COMMAND_LINE_ANNOTATION_NAME = \"commandLine\";\n+\n+    private File tempDir;\n+    private int[][][] nextMasks;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+    }\n+\n+    public void onShutdown() {\n+        try {\n+            if (tempDir != null) {\n+                FileUtils.deleteDirectory(tempDir);\n+            }\n+        } catch (final IOException e) {\n+            throw new GATKException(\"issues removing temporary directory: \" + tempDir, e);\n+        } finally {\n+            super.onShutdown();\n+        }\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        final SAMSequenceDictionary dictionary = getBestAvailableSequenceDictionary();\n+        initializeMasks(dictionary);\n+        try (final STRTableFileBuilder output = STRTableFileBuilder.newInstance(dictionary, decimationTable, generateSitesTextOutput, maxPeriod, maxRepeat)) {\n+            output.annotate(COMMAND_LINE_ANNOTATION_NAME, getCommandLine());\n+            final Map<String, List<SimpleInterval>> intervalsByContig = composeAndGroupTraversalIntervalsByContig(dictionary);\n+            for (final Map.Entry<String, List<SimpleInterval>> contigEntry : intervalsByContig.entrySet()) {\n+                final BufferedReferenceBases nucleotideSequence = BufferedReferenceBases.of(directlyAccessEngineReferenceDataSource(), contigEntry.getKey(), referenceSequenceBufferSize);\n+                final SAMSequenceRecord sequenceRecord = dictionary.getSequence(contigEntry.getKey());\n+                for (final SimpleInterval interval : contigEntry.getValue()) {\n+                    traverseInterval(sequenceRecord.getSequenceIndex(), nucleotideSequence, interval.getStart(), interval.getEnd(), decimationTable, output);\n+                }\n+            }\n+            output.store(outputPath);\n+        }\n+        progressMeter.stop();\n+    }\n+\n+    private Map<String, List<SimpleInterval>> composeAndGroupTraversalIntervalsByContig(final SAMSequenceDictionary dictionary) {\n+        if (!intervalArgumentCollection.intervalsSpecified()) {\n+            return dictionary.getSequences().stream()\n+                    .map(s -> new SimpleInterval(s.getSequenceName(), 1, s.getSequenceLength()))\n+                    .collect(Collectors.groupingBy(SimpleInterval::getContig, LinkedHashMap::new, Collectors.toList()));\n+        } else {\n+            final Map<String, List<SimpleInterval>> keyUnsorted = IntervalUtils.sortAndMergeOverlappingIntervals(intervalArgumentCollection.getIntervals(dictionary));\n+            final Map<String, List<SimpleInterval>> keySorted = new LinkedHashMap<>(keyUnsorted.size());\n+            keyUnsorted.entrySet().stream()\n+                    .sorted(Comparator.comparingInt(entry -> dictionary.getSequenceIndex(entry.getKey())))\n+                    .forEach(entry -> keySorted.put(entry.getKey(), entry.getValue()));\n+            return keySorted;\n+        }\n+    }\n+\n+    private void initializeMasks(final SAMSequenceDictionary dictionary) {\n+        nextMasks = new int[dictionary.getSequences().size()][maxPeriod + 1][maxRepeat + 1];\n+        for (int i = 0; i < nextMasks.length; i++) {\n+            for (final int[] masks : nextMasks[i]) {\n+                Arrays.fill(masks, i);\n+            }\n+        }\n+    }\n+\n+    private void traverseInterval(final int seqNumber, final BufferedReferenceBases sequence, final long seqStart, final long seqEnd,\n+                                  final STRDecimationTable decimationTable, final STRTableFileBuilder output)\n+    {\n+        final String id = sequence.id();\n+        final long length = sequence.length();\n+        final byte[] unitBuffer = new byte[this.maxPeriod];\n+        long pos = seqStart;\n+        while (pos <= seqEnd) {\n+           // usually maxPeriodAtPos == maxPeriod except when close to the end of the sequence\n+           // is the maximum period to be considered that cannot exceed min(maxPeriod, seq-length - pos + 1)\n+           // but is always 1 or greater.\n+           final int maxPeriodAtPos = sequence.copyBytesAt(pos, unitBuffer, 0, maxPeriod);\n+\n+           // Efficient code for period == 1:\n+           final byte firstUnitBase;\n+           if (!Nucleotide.decode(firstUnitBase = unitBuffer[0]).isStandard()) {\n+               pos++; continue;\n+           }\n+           long beg, end;\n+\n+           // we look upstream for same bases:\n+           //noinspection StatementWithEmptyBody\n+           for (beg = pos - 1; beg >= 1 && Nucleotide.same(sequence.byteAt(beg), firstUnitBase); beg--) { /* nothing to be done */ }\n+           beg++;\n+\n+           // we look downstream for same bases:\n+           //noinspection StatementWithEmptyBody", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NjQ0NA==", "bodyText": "Pull out a method in the vein of \"calculate best STR for this the results of which you feed to emitOrDecimateSTR() That will separate the STR logic from the \"traverse every base\" logic", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495166444", "createdAt": "2020-09-25T18:38:17Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/ComposeSTRTableFile.java", "diffHunk": "@@ -0,0 +1,269 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import org.apache.commons.io.FileUtils;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.barclay.argparser.Hidden;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFile;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFileBuilder;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * This tools look for STR sequences in the reference that will used later to estimate the Dragstr parameters values\n+ * using {@link EstimateDragstrParameters}.\n+ * <h3>Inputs</h3>\n+ * <p>\n+ *     This command takes as input the reference (possibly traversal intervals) and a {@link STRDecimationTable decimation table} herein\n+ *     referred as DT.\n+ * </p>\n+ * <p>\n+ *     The DT modulates how often we sample a site for each possible period and repeat length. Since there is far more\n+ *     positions with short period and short repeat length sampling for those combinations should be less frequent.\n+ *     For further details about the format of this table and interpretation of its values please check the documentation\n+ *     in class {@link STRDecimationTable}.\n+ * </p>\n+ * <h3>Output</h3>\n+ * <p>\n+ *     The output of this command is a zip file that contain the collection of sampled sites in binary form (all.bin),\n+ *     and index for that file for quick access by location interval (all.idx), a copy of the reference sequence dictionary\n+ *     (reference.dict), a copy of the DT (decimation.txt) and additional information and stats (e.g. summary.txt)\n+ * </p>\n+ * <p>\n+ *     The reference dictionary file may be used by commands downstream that need to verify that\n+ *     the reference that wa use to generate the sample sites matches the one that is provided by the user to that command.\n+ * </p>\n+ * <p>\n+ *     The DT also provide downstream commands with the information as to how the resulting collection of sites was downSampled,\n+ *     in case further down-sampling is necessary.\n+ * </p>\n+ */\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class ComposeSTRTableFile extends GATKTool {\n+\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final String REFERENCE_SEQUENCE_BUFFER_SIZE_FULL_NAME = \"reference-sequence-buffer-size\";\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final String GENERATE_SITES_TEXT_OUTPUT_FULL_NAME = \"generate-sites-text-output\";\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final int DEFAULT_REFERENCE_SEQUENCE_BUFFER_SIZE = 100_000;\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private STRDecimationTable decimationTable = STRDecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(doc = \"request to generate a text formatted version of the STR table in the output zip (\" + STRTableFile.SITES_TEXT_FILE_NAME + \")\",\n+              fullName = GENERATE_SITES_TEXT_OUTPUT_FULL_NAME, optional = true)\n+    @Hidden\n+    private boolean generateSitesTextOutput = false;\n+\n+    @Argument(fullName = DragstrHyperParameters.MAX_PERIOD_ARGUMENT_FULL_NAME, doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxPeriod = DragstrHyperParameters.DEFAULT_MAX_PERIOD;\n+\n+    @Argument(fullName = DragstrHyperParameters.MAX_REPEATS_ARGUMENT_FULL_NAME, doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 100)\n+    private int maxRepeat = DragstrHyperParameters.DEFAULT_MAX_REPEAT_LENGTH;\n+\n+    @Argument(fullName= REFERENCE_SEQUENCE_BUFFER_SIZE_FULL_NAME, doc=\"size of the look ahead reference sequence buffer\", optional = true, minValue = 100, maxValue = 1_000_000_000)\n+    @Hidden\n+    private int referenceSequenceBufferSize = DEFAULT_REFERENCE_SEQUENCE_BUFFER_SIZE;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private static final String COMMAND_LINE_ANNOTATION_NAME = \"commandLine\";\n+\n+    private File tempDir;\n+    private int[][][] nextMasks;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+    }\n+\n+    public void onShutdown() {\n+        try {\n+            if (tempDir != null) {\n+                FileUtils.deleteDirectory(tempDir);\n+            }\n+        } catch (final IOException e) {\n+            throw new GATKException(\"issues removing temporary directory: \" + tempDir, e);\n+        } finally {\n+            super.onShutdown();\n+        }\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        final SAMSequenceDictionary dictionary = getBestAvailableSequenceDictionary();\n+        initializeMasks(dictionary);\n+        try (final STRTableFileBuilder output = STRTableFileBuilder.newInstance(dictionary, decimationTable, generateSitesTextOutput, maxPeriod, maxRepeat)) {\n+            output.annotate(COMMAND_LINE_ANNOTATION_NAME, getCommandLine());\n+            final Map<String, List<SimpleInterval>> intervalsByContig = composeAndGroupTraversalIntervalsByContig(dictionary);\n+            for (final Map.Entry<String, List<SimpleInterval>> contigEntry : intervalsByContig.entrySet()) {\n+                final BufferedReferenceBases nucleotideSequence = BufferedReferenceBases.of(directlyAccessEngineReferenceDataSource(), contigEntry.getKey(), referenceSequenceBufferSize);\n+                final SAMSequenceRecord sequenceRecord = dictionary.getSequence(contigEntry.getKey());\n+                for (final SimpleInterval interval : contigEntry.getValue()) {\n+                    traverseInterval(sequenceRecord.getSequenceIndex(), nucleotideSequence, interval.getStart(), interval.getEnd(), decimationTable, output);\n+                }\n+            }\n+            output.store(outputPath);\n+        }\n+        progressMeter.stop();\n+    }\n+\n+    private Map<String, List<SimpleInterval>> composeAndGroupTraversalIntervalsByContig(final SAMSequenceDictionary dictionary) {\n+        if (!intervalArgumentCollection.intervalsSpecified()) {\n+            return dictionary.getSequences().stream()\n+                    .map(s -> new SimpleInterval(s.getSequenceName(), 1, s.getSequenceLength()))\n+                    .collect(Collectors.groupingBy(SimpleInterval::getContig, LinkedHashMap::new, Collectors.toList()));\n+        } else {\n+            final Map<String, List<SimpleInterval>> keyUnsorted = IntervalUtils.sortAndMergeOverlappingIntervals(intervalArgumentCollection.getIntervals(dictionary));\n+            final Map<String, List<SimpleInterval>> keySorted = new LinkedHashMap<>(keyUnsorted.size());\n+            keyUnsorted.entrySet().stream()\n+                    .sorted(Comparator.comparingInt(entry -> dictionary.getSequenceIndex(entry.getKey())))\n+                    .forEach(entry -> keySorted.put(entry.getKey(), entry.getValue()));\n+            return keySorted;\n+        }\n+    }\n+\n+    private void initializeMasks(final SAMSequenceDictionary dictionary) {\n+        nextMasks = new int[dictionary.getSequences().size()][maxPeriod + 1][maxRepeat + 1];\n+        for (int i = 0; i < nextMasks.length; i++) {\n+            for (final int[] masks : nextMasks[i]) {\n+                Arrays.fill(masks, i);\n+            }\n+        }\n+    }\n+\n+    private void traverseInterval(final int seqNumber, final BufferedReferenceBases sequence, final long seqStart, final long seqEnd,\n+                                  final STRDecimationTable decimationTable, final STRTableFileBuilder output)\n+    {\n+        final String id = sequence.id();\n+        final long length = sequence.length();\n+        final byte[] unitBuffer = new byte[this.maxPeriod];\n+        long pos = seqStart;\n+        while (pos <= seqEnd) {\n+           // usually maxPeriodAtPos == maxPeriod except when close to the end of the sequence\n+           // is the maximum period to be considered that cannot exceed min(maxPeriod, seq-length - pos + 1)\n+           // but is always 1 or greater.\n+           final int maxPeriodAtPos = sequence.copyBytesAt(pos, unitBuffer, 0, maxPeriod);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE3MDY2MA==", "bodyText": "Also possibly pull this out into some common utility since it seems the logic for \"what is the operative period/repeat combination at this position?\" is presumably the same as it is for reading the STR table...", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495170660", "createdAt": "2020-09-25T18:46:50Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/ComposeSTRTableFile.java", "diffHunk": "@@ -0,0 +1,269 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import org.apache.commons.io.FileUtils;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.barclay.argparser.Hidden;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.engine.GATKTool;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFile;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFileBuilder;\n+import picard.cmdline.programgroups.ReferenceProgramGroup;\n+\n+import java.io.*;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * This tools look for STR sequences in the reference that will used later to estimate the Dragstr parameters values\n+ * using {@link EstimateDragstrParameters}.\n+ * <h3>Inputs</h3>\n+ * <p>\n+ *     This command takes as input the reference (possibly traversal intervals) and a {@link STRDecimationTable decimation table} herein\n+ *     referred as DT.\n+ * </p>\n+ * <p>\n+ *     The DT modulates how often we sample a site for each possible period and repeat length. Since there is far more\n+ *     positions with short period and short repeat length sampling for those combinations should be less frequent.\n+ *     For further details about the format of this table and interpretation of its values please check the documentation\n+ *     in class {@link STRDecimationTable}.\n+ * </p>\n+ * <h3>Output</h3>\n+ * <p>\n+ *     The output of this command is a zip file that contain the collection of sampled sites in binary form (all.bin),\n+ *     and index for that file for quick access by location interval (all.idx), a copy of the reference sequence dictionary\n+ *     (reference.dict), a copy of the DT (decimation.txt) and additional information and stats (e.g. summary.txt)\n+ * </p>\n+ * <p>\n+ *     The reference dictionary file may be used by commands downstream that need to verify that\n+ *     the reference that wa use to generate the sample sites matches the one that is provided by the user to that command.\n+ * </p>\n+ * <p>\n+ *     The DT also provide downstream commands with the information as to how the resulting collection of sites was downSampled,\n+ *     in case further down-sampling is necessary.\n+ * </p>\n+ */\n+@CommandLineProgramProperties(\n+        programGroup = ReferenceProgramGroup.class,\n+        summary = \"Determine the presence of STR in a reference sequence\",\n+        oneLineSummary = \"Determines the presence of STR in a reference sequence\"\n+)\n+public class ComposeSTRTableFile extends GATKTool {\n+\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final String REFERENCE_SEQUENCE_BUFFER_SIZE_FULL_NAME = \"reference-sequence-buffer-size\";\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final String GENERATE_SITES_TEXT_OUTPUT_FULL_NAME = \"generate-sites-text-output\";\n+    @SuppressWarnings(\"WeakerAccess\")\n+    public static final int DEFAULT_REFERENCE_SEQUENCE_BUFFER_SIZE = 100_000;\n+\n+    @Argument(fullName=\"decimation\", doc=\"decimation per period and repeat. It can be \\\"DEFAULT\\\" to use the default values (default), \" +\n+            \" \\\"NONE\\\" to deactivate decimation (potentially resulting in a very large output file) or indicate the path to a file\" +\n+            \" that contains the decimation matrix.\", optional = true)\n+    private STRDecimationTable decimationTable = STRDecimationTable.DEFAULT;\n+\n+    @Argument(doc = \"name of the zip file where the sites sampled will be stored\",\n+              fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+              shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME)\n+    private String outputPath = null;\n+\n+    @Argument(doc = \"request to generate a text formatted version of the STR table in the output zip (\" + STRTableFile.SITES_TEXT_FILE_NAME + \")\",\n+              fullName = GENERATE_SITES_TEXT_OUTPUT_FULL_NAME, optional = true)\n+    @Hidden\n+    private boolean generateSitesTextOutput = false;\n+\n+    @Argument(fullName = DragstrHyperParameters.MAX_PERIOD_ARGUMENT_FULL_NAME, doc=\"maximum STR period sampled\", optional = true, minValue = 1, maxValue = 20)\n+    private int maxPeriod = DragstrHyperParameters.DEFAULT_MAX_PERIOD;\n+\n+    @Argument(fullName = DragstrHyperParameters.MAX_REPEATS_ARGUMENT_FULL_NAME, doc=\"maximum STR repeat sampled\", optional = true, minValue = 1, maxValue = 100)\n+    private int maxRepeat = DragstrHyperParameters.DEFAULT_MAX_REPEAT_LENGTH;\n+\n+    @Argument(fullName= REFERENCE_SEQUENCE_BUFFER_SIZE_FULL_NAME, doc=\"size of the look ahead reference sequence buffer\", optional = true, minValue = 100, maxValue = 1_000_000_000)\n+    @Hidden\n+    private int referenceSequenceBufferSize = DEFAULT_REFERENCE_SEQUENCE_BUFFER_SIZE;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    private static final String COMMAND_LINE_ANNOTATION_NAME = \"commandLine\";\n+\n+    private File tempDir;\n+    private int[][][] nextMasks;\n+\n+    public void onStartup() {\n+        super.onStartup();\n+        try {\n+            tempDir = File.createTempFile(\"gatk-sample-dragstr-sites\", \".tmp\");\n+        } catch (final IOException ex) {\n+            throw new GATKException(\"could not create temporary disk space\", ex);\n+        }\n+        if (!tempDir.delete()) {\n+            throw new GATKException(\"could not create temporary disk space: could not delete tempfile\");\n+        } else if (!tempDir.mkdir()) {\n+            throw new GATKException(\"could not create temporary disk space: could not create tempdir\");\n+        }\n+    }\n+\n+    public void onShutdown() {\n+        try {\n+            if (tempDir != null) {\n+                FileUtils.deleteDirectory(tempDir);\n+            }\n+        } catch (final IOException e) {\n+            throw new GATKException(\"issues removing temporary directory: \" + tempDir, e);\n+        } finally {\n+            super.onShutdown();\n+        }\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        final SAMSequenceDictionary dictionary = getBestAvailableSequenceDictionary();\n+        initializeMasks(dictionary);\n+        try (final STRTableFileBuilder output = STRTableFileBuilder.newInstance(dictionary, decimationTable, generateSitesTextOutput, maxPeriod, maxRepeat)) {\n+            output.annotate(COMMAND_LINE_ANNOTATION_NAME, getCommandLine());\n+            final Map<String, List<SimpleInterval>> intervalsByContig = composeAndGroupTraversalIntervalsByContig(dictionary);\n+            for (final Map.Entry<String, List<SimpleInterval>> contigEntry : intervalsByContig.entrySet()) {\n+                final BufferedReferenceBases nucleotideSequence = BufferedReferenceBases.of(directlyAccessEngineReferenceDataSource(), contigEntry.getKey(), referenceSequenceBufferSize);\n+                final SAMSequenceRecord sequenceRecord = dictionary.getSequence(contigEntry.getKey());\n+                for (final SimpleInterval interval : contigEntry.getValue()) {\n+                    traverseInterval(sequenceRecord.getSequenceIndex(), nucleotideSequence, interval.getStart(), interval.getEnd(), decimationTable, output);\n+                }\n+            }\n+            output.store(outputPath);\n+        }\n+        progressMeter.stop();\n+    }\n+\n+    private Map<String, List<SimpleInterval>> composeAndGroupTraversalIntervalsByContig(final SAMSequenceDictionary dictionary) {\n+        if (!intervalArgumentCollection.intervalsSpecified()) {\n+            return dictionary.getSequences().stream()\n+                    .map(s -> new SimpleInterval(s.getSequenceName(), 1, s.getSequenceLength()))\n+                    .collect(Collectors.groupingBy(SimpleInterval::getContig, LinkedHashMap::new, Collectors.toList()));\n+        } else {\n+            final Map<String, List<SimpleInterval>> keyUnsorted = IntervalUtils.sortAndMergeOverlappingIntervals(intervalArgumentCollection.getIntervals(dictionary));\n+            final Map<String, List<SimpleInterval>> keySorted = new LinkedHashMap<>(keyUnsorted.size());\n+            keyUnsorted.entrySet().stream()\n+                    .sorted(Comparator.comparingInt(entry -> dictionary.getSequenceIndex(entry.getKey())))\n+                    .forEach(entry -> keySorted.put(entry.getKey(), entry.getValue()));\n+            return keySorted;\n+        }\n+    }\n+\n+    private void initializeMasks(final SAMSequenceDictionary dictionary) {\n+        nextMasks = new int[dictionary.getSequences().size()][maxPeriod + 1][maxRepeat + 1];\n+        for (int i = 0; i < nextMasks.length; i++) {\n+            for (final int[] masks : nextMasks[i]) {\n+                Arrays.fill(masks, i);\n+            }\n+        }\n+    }\n+\n+    private void traverseInterval(final int seqNumber, final BufferedReferenceBases sequence, final long seqStart, final long seqEnd,\n+                                  final STRDecimationTable decimationTable, final STRTableFileBuilder output)\n+    {\n+        final String id = sequence.id();\n+        final long length = sequence.length();\n+        final byte[] unitBuffer = new byte[this.maxPeriod];\n+        long pos = seqStart;\n+        while (pos <= seqEnd) {\n+           // usually maxPeriodAtPos == maxPeriod except when close to the end of the sequence\n+           // is the maximum period to be considered that cannot exceed min(maxPeriod, seq-length - pos + 1)\n+           // but is always 1 or greater.\n+           final int maxPeriodAtPos = sequence.copyBytesAt(pos, unitBuffer, 0, maxPeriod);\n+\n+           // Efficient code for period == 1:\n+           final byte firstUnitBase;\n+           if (!Nucleotide.decode(firstUnitBase = unitBuffer[0]).isStandard()) {\n+               pos++; continue;\n+           }\n+           long beg, end;\n+\n+           // we look upstream for same bases:\n+           //noinspection StatementWithEmptyBody\n+           for (beg = pos - 1; beg >= 1 && Nucleotide.same(sequence.byteAt(beg), firstUnitBase); beg--) { /* nothing to be done */ }\n+           beg++;\n+\n+           // we look downstream for same bases:\n+           //noinspection StatementWithEmptyBody", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NDM0MQ=="}, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE3NTMzOA==", "bodyText": "Hmm... I don't like this class being a mishmash of static reader/writer classes. I think at the very least we should pull out a separate reader/writer for the binary locus object to clarify the scope of this code. After that if you are ambitious (if not make a ticket out of it) we can probably turn this into a feature and use the feature data source machinery to read/write these things since they have locations etc... but i'm not going to ask you to do that for this branch.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495175338", "createdAt": "2020-09-25T18:56:56Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/DragstrLocus.java", "diffHunk": "@@ -0,0 +1,370 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import it.unimi.dsi.fastutil.ints.*;\n+import org.apache.hadoop.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.BinaryTableReader;\n+import org.broadinstitute.hellbender.utils.BinaryTableWriter;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+import org.broadinstitute.hellbender.utils.tsv.DataLine;\n+import org.broadinstitute.hellbender.utils.tsv.TableColumnCollection;\n+import org.broadinstitute.hellbender.utils.tsv.TableReader;\n+import org.broadinstitute.hellbender.utils.tsv.TableWriter;\n+\n+import java.io.*;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Holds information about a locus on the reference that might be used to estimate the DRAGstr model parameters.\n+ */\n+public class DragstrLocus {\n+\n+    private final int chromosomeIndex;\n+    private final long start;\n+    private final byte period;\n+    private final short length;\n+    private final long mask;\n+\n+    private static final int INDEX_BYTE_INTERVAL = 1 << 16; // every 64KB\n+\n+    private DragstrLocus(final int chrIdx, final long start, final byte period, final short length, final long mask) {\n+        chromosomeIndex = chrIdx;\n+        this.start = start;\n+        this.period = period;\n+        this.length = length;\n+        this.mask = mask;\n+    }\n+\n+    public static DragstrLocus make(final int chrIdx, final long start, final byte period, final short length, final long mask) {\n+        ParamUtils.isPositiveOrZero(chrIdx, \"chromosome index\");\n+        ParamUtils.isPositive(start, \"start position\");\n+        ParamUtils.isPositive(period, \"period\");\n+        ParamUtils.isPositive(length, \"length\");\n+        return new DragstrLocus(chrIdx, start, period, length, mask);\n+    }\n+\n+    public int getChromosomeIndex() { return chromosomeIndex; }\n+\n+    public long getMask() { return mask; }\n+\n+    public long getStart() {\n+        return start;\n+    }\n+\n+    public long getEnd() {\n+        return start + length - 1;\n+    }\n+\n+    public int getPeriod() {\n+        return period;\n+    }\n+\n+    public int getRepeats() {\n+        return period == 0 ? 0 : length / period;\n+    }\n+\n+    @FunctionalInterface\n+    public interface WriteAction {\n+        void write(final DragstrLocus locus, final DataOutput dest) throws IOException;\n+    }\n+\n+\n+    private static BinaryTableWriter<DragstrLocus> binaryWriter(final OutputStream out, final OutputStream indexOut, final String path) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE3NzA0Ng==", "bodyText": "more informative names than \"n\" and \"k\"", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495177046", "createdAt": "2020-09-25T19:00:42Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/DragstrLocusCase.java", "diffHunk": "@@ -0,0 +1,80 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.util.Locatable;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+public final class DragstrLocusCase {\n+    public DragstrLocus getLocus() {\n+        return locus;\n+    }\n+\n+    public int getPeriod() {\n+        return locus.getPeriod();\n+    }\n+\n+    public int getRepeatLength() {\n+        return locus.getRepeats();\n+    }\n+\n+    public int getN() {\n+        return n;\n+    }\n+\n+    public int getK() {\n+        return k;\n+    }\n+\n+    public int getMinMQ() {\n+        return minMQ;\n+    }\n+\n+    public int getNSup() {\n+        return nSup;\n+    }\n+\n+    private final DragstrLocus locus;\n+    private final int n;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE3Nzg3MQ==", "bodyText": "Can be made Locatable (Which should be done as a first step to plugging this into our other reader code)", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495177871", "createdAt": "2020-09-25T19:02:28Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/DragstrLocus.java", "diffHunk": "@@ -0,0 +1,370 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import it.unimi.dsi.fastutil.ints.*;\n+import org.apache.hadoop.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.BinaryTableReader;\n+import org.broadinstitute.hellbender.utils.BinaryTableWriter;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+import org.broadinstitute.hellbender.utils.tsv.DataLine;\n+import org.broadinstitute.hellbender.utils.tsv.TableColumnCollection;\n+import org.broadinstitute.hellbender.utils.tsv.TableReader;\n+import org.broadinstitute.hellbender.utils.tsv.TableWriter;\n+\n+import java.io.*;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Holds information about a locus on the reference that might be used to estimate the DRAGstr model parameters.\n+ */\n+public class DragstrLocus {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NTc3Ng==", "bodyText": "It seems to me that functionally this could be folderd up as attributes into the above dragstLocus class that get populated in EstimateDragstrParameters reducing the current headache of DragstrLocus + DragstrLocusCase + DragstrLocusCases + StratifiedDragstrLocusCases since as far as i know only one of these gets generated for every site anyway then we create this summary object and pass it into the other classes.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495185776", "createdAt": "2020-09-25T19:19:58Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/DragstrLocusCase.java", "diffHunk": "@@ -0,0 +1,80 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.util.Locatable;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+public final class DragstrLocusCase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE5MTQ4Mg==", "bodyText": "I think it would be possible to switch these over to paths at this level (as opposed to calling bucket utils later) to support the GCS connector?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495191482", "createdAt": "2020-09-25T19:32:36Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/EstimateDragstrParameters.java", "diffHunk": "@@ -0,0 +1,935 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.*;\n+import htsjdk.samtools.util.IntervalTree;\n+import it.unimi.dsi.fastutil.ints.IntArrayList;\n+import it.unimi.dsi.fastutil.ints.IntList;\n+import org.apache.commons.io.output.NullOutputStream;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.ExampleProgramGroup;\n+import org.broadinstitute.hellbender.engine.*;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrParams;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrUtils;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFile;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.AbsoluteCoordinates;\n+\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.*;\n+import java.util.stream.*;\n+\n+/**\n+ * Estimates the parameters for the DRAGstr model for an input sample.\n+ * <p>\n+ *     This tools takes in the sampling sites generated by {@link ComposeSTRTableFile} on the same reference\n+ *     as the input sample.\n+ * </p>\n+ * <p>\n+ *     The end result is a text file containing three parameter tables (GOP, GCP, API) that can be fed\n+ *     directly to {@link HaplotypeCaller} --dragstr-params-path.\n+ * </p>\n+ */\n+@CommandLineProgramProperties(\n+        summary = \"estimates the parameters for the DRAGstr model for the input sample\",\n+        oneLineSummary = \"summary\",\n+        programGroup = ExampleProgramGroup.class\n+)\n+public class EstimateDragstrParameters extends GATKTool {\n+\n+    public static final String STR_TABLE_PATH_SHORT_NAME = \"str\";\n+    public static final String STR_TABLE_PATH_FULL_NAME = \"str-table-path\";\n+    public static final String PARALLEL_FULL_NAME = \"parallel\";\n+    public static final String THREADS_FULL_NAME = \"threads\";\n+    public static final String SHARD_SIZE_FULL_NAME = \"shard-size\";\n+    public static final String DOWN_SAMPLE_SIZE_FULL_NAME = \"down-sample-size\";\n+    public static final String DEBUG_SITES_OUTPUT_FULL_NAME = \"debug-sites-output\";\n+\n+    public static final int DEFAULT_SHARD_SIZE = 1_000_000;\n+    public static final int DEFAULT_DOWN_SAMPLE_SIZE = 4096;\n+    public static final int SYSTEM_SUGGESTED_THREAD_NUMBER = 0;\n+    public static final int MINIMUM_SHARD_SIZE = 100;\n+    public static final int MINIMUM_DOWN_SAMPLE_SIZE = 512;\n+\n+    @ArgumentCollection\n+    private DragstrHyperParameters hyperParameters = new DragstrHyperParameters();\n+\n+    @Argument(shortName=STR_TABLE_PATH_SHORT_NAME, fullName=STR_TABLE_PATH_FULL_NAME, doc=\"location of the zip that contains the sampling sites for the reference\")\n+    private String strTablePath = null;\n+\n+    @Argument(fullName=PARALLEL_FULL_NAME, doc=\"run alignment data collection and  estimation in parallel\")\n+    private boolean runInParallel = false;\n+\n+    @Argument(fullName=THREADS_FULL_NAME, minValue = SYSTEM_SUGGESTED_THREAD_NUMBER, doc=\"suggested number of parallel threads to perform the estimation, \"\n+            + \"the default 0 leave it up to the VM to decide. When set to more than 1, this will activate parallel in the absence of --parallel\")\n+    private int threads = SYSTEM_SUGGESTED_THREAD_NUMBER;\n+\n+    @Argument(fullName=SHARD_SIZE_FULL_NAME, doc=\"when running in parallel this is the suggested shard size in base pairs. \" +\n+            \"The actual shard-size may vary to adapt to small contigs and the requested number of threads\",\n+              minValue = MINIMUM_SHARD_SIZE)\n+    private int shardSize = DEFAULT_SHARD_SIZE;\n+\n+    @Argument(fullName=DOWN_SAMPLE_SIZE_FULL_NAME, doc=\"Targeted maximum number of cases per combination period repeat count, \" +\n+            \"the larger the more precise but also the slower estimation.\",\n+              minValue = MINIMUM_DOWN_SAMPLE_SIZE)\n+    private int downsampleSize = DEFAULT_DOWN_SAMPLE_SIZE;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME, shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME, doc = \"where to write the parameter output file.\")\n+    private String output = null;\n+\n+    @Argument(fullName= DEBUG_SITES_OUTPUT_FULL_NAME, doc = \"table with information gather on the samples sites. Includes what sites were downsampled, disqualified or accepted for parameter estimation\", optional = true)\n+    private String sitesOutput = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE5MzI3Mw==", "bodyText": "So we downsample the sites at this stage even though we already decimated the sites to be as few as 1/1000 of the actual occurances in the previous tool?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495193273", "createdAt": "2020-09-25T19:36:37Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/EstimateDragstrParameters.java", "diffHunk": "@@ -0,0 +1,935 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.*;\n+import htsjdk.samtools.util.IntervalTree;\n+import it.unimi.dsi.fastutil.ints.IntArrayList;\n+import it.unimi.dsi.fastutil.ints.IntList;\n+import org.apache.commons.io.output.NullOutputStream;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.ExampleProgramGroup;\n+import org.broadinstitute.hellbender.engine.*;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrParams;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrUtils;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFile;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.AbsoluteCoordinates;\n+\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.*;\n+import java.util.stream.*;\n+\n+/**\n+ * Estimates the parameters for the DRAGstr model for an input sample.\n+ * <p>\n+ *     This tools takes in the sampling sites generated by {@link ComposeSTRTableFile} on the same reference\n+ *     as the input sample.\n+ * </p>\n+ * <p>\n+ *     The end result is a text file containing three parameter tables (GOP, GCP, API) that can be fed\n+ *     directly to {@link HaplotypeCaller} --dragstr-params-path.\n+ * </p>\n+ */\n+@CommandLineProgramProperties(\n+        summary = \"estimates the parameters for the DRAGstr model for the input sample\",\n+        oneLineSummary = \"summary\",\n+        programGroup = ExampleProgramGroup.class\n+)\n+public class EstimateDragstrParameters extends GATKTool {\n+\n+    public static final String STR_TABLE_PATH_SHORT_NAME = \"str\";\n+    public static final String STR_TABLE_PATH_FULL_NAME = \"str-table-path\";\n+    public static final String PARALLEL_FULL_NAME = \"parallel\";\n+    public static final String THREADS_FULL_NAME = \"threads\";\n+    public static final String SHARD_SIZE_FULL_NAME = \"shard-size\";\n+    public static final String DOWN_SAMPLE_SIZE_FULL_NAME = \"down-sample-size\";\n+    public static final String DEBUG_SITES_OUTPUT_FULL_NAME = \"debug-sites-output\";\n+\n+    public static final int DEFAULT_SHARD_SIZE = 1_000_000;\n+    public static final int DEFAULT_DOWN_SAMPLE_SIZE = 4096;\n+    public static final int SYSTEM_SUGGESTED_THREAD_NUMBER = 0;\n+    public static final int MINIMUM_SHARD_SIZE = 100;\n+    public static final int MINIMUM_DOWN_SAMPLE_SIZE = 512;\n+\n+    @ArgumentCollection\n+    private DragstrHyperParameters hyperParameters = new DragstrHyperParameters();\n+\n+    @Argument(shortName=STR_TABLE_PATH_SHORT_NAME, fullName=STR_TABLE_PATH_FULL_NAME, doc=\"location of the zip that contains the sampling sites for the reference\")\n+    private String strTablePath = null;\n+\n+    @Argument(fullName=PARALLEL_FULL_NAME, doc=\"run alignment data collection and  estimation in parallel\")\n+    private boolean runInParallel = false;\n+\n+    @Argument(fullName=THREADS_FULL_NAME, minValue = SYSTEM_SUGGESTED_THREAD_NUMBER, doc=\"suggested number of parallel threads to perform the estimation, \"\n+            + \"the default 0 leave it up to the VM to decide. When set to more than 1, this will activate parallel in the absence of --parallel\")\n+    private int threads = SYSTEM_SUGGESTED_THREAD_NUMBER;\n+\n+    @Argument(fullName=SHARD_SIZE_FULL_NAME, doc=\"when running in parallel this is the suggested shard size in base pairs. \" +\n+            \"The actual shard-size may vary to adapt to small contigs and the requested number of threads\",\n+              minValue = MINIMUM_SHARD_SIZE)\n+    private int shardSize = DEFAULT_SHARD_SIZE;\n+\n+    @Argument(fullName=DOWN_SAMPLE_SIZE_FULL_NAME, doc=\"Targeted maximum number of cases per combination period repeat count, \" +\n+            \"the larger the more precise but also the slower estimation.\",\n+              minValue = MINIMUM_DOWN_SAMPLE_SIZE)\n+    private int downsampleSize = DEFAULT_DOWN_SAMPLE_SIZE;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME, shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME, doc = \"where to write the parameter output file.\")\n+    private String output = null;\n+\n+    @Argument(fullName= DEBUG_SITES_OUTPUT_FULL_NAME, doc = \"table with information gather on the samples sites. Includes what sites were downsampled, disqualified or accepted for parameter estimation\", optional = true)\n+    private String sitesOutput = null;\n+\n+    private SAMSequenceDictionary dictionary;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean requiresReads() {\n+        return true;\n+    }\n+\n+    @Override\n+    protected void onStartup() {\n+        super.onStartup();\n+        hyperParameters.validate();\n+        dictionary = getBestAvailableSequenceDictionary();\n+        if (runInParallel) {\n+            if (threads == 1) {\n+                logger.warn(\"parallel processing was requested but the number of threads was set to 1\");\n+            }\n+        } else if (threads > 1) {\n+            runInParallel = true;\n+        }\n+        if (runInParallel) {\n+            if (threads == 0) {\n+                logger.info(\"Running in parallel using the system suggested default thread count: \" + Runtime.getRuntime().availableProcessors());\n+            } else {\n+                logger.info(\"Running in parallel using the requested number of threads: \" + threads);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        hyperParameters.validate();\n+        dictionary = getBestAvailableSequenceDictionary();\n+        final List<SAMReadGroupRecord> readGroups = hasReads() ? getHeaderForReads().getReadGroups() : Collections.emptyList();\n+        final List<String> readGroupIds = readGroups.stream()\n+                .map(SAMReadGroupRecord::getId)\n+                .collect(Collectors.toList());\n+        final List<String> sampleNames = readGroups.stream()\n+                .map(SAMReadGroupRecord::getSample)\n+                .distinct().collect(Collectors.toList());\n+        final Optional<String> sampleName = resolveSampleName(sampleNames);\n+\n+        try (final PrintWriter sitesOutputWriter = openSitesOutputWriter(sitesOutput);\n+             final STRTableFile strTable = STRTableFile.open(strTablePath)) {\n+\n+            checkSequenceDictionaryCompatibility(dictionary, strTable.dictionary());\n+            final StratifiedDragstrLocusCases allSites;\n+            final List<SimpleInterval> intervals = getTraversalIntervals();\n+\n+            runInParallel |= threads > 1;\n+            if (runInParallel) {\n+                if (threads == 1) {\n+                    logger.warn(\"parallel processing was requested but the number of threads was set to 1\");\n+                }\n+                allSites = collectCaseStatsParallel(intervals, shardSize, strTable);\n+            } else {\n+                allSites = collectCaseStatsSequencial(intervals, strTable);\n+            }\n+            logSiteCounts(allSites, \"all loci/cases\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE5NTIyMA==", "bodyText": "Wait, this is reusing the decimation table, so we are reapplying the same table as before for downsampling here aswell? Does that make sense?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495195220", "createdAt": "2020-09-25T19:40:55Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/EstimateDragstrParameters.java", "diffHunk": "@@ -0,0 +1,935 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.*;\n+import htsjdk.samtools.util.IntervalTree;\n+import it.unimi.dsi.fastutil.ints.IntArrayList;\n+import it.unimi.dsi.fastutil.ints.IntList;\n+import org.apache.commons.io.output.NullOutputStream;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.ExampleProgramGroup;\n+import org.broadinstitute.hellbender.engine.*;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrParams;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrUtils;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFile;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.AbsoluteCoordinates;\n+\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.*;\n+import java.util.stream.*;\n+\n+/**\n+ * Estimates the parameters for the DRAGstr model for an input sample.\n+ * <p>\n+ *     This tools takes in the sampling sites generated by {@link ComposeSTRTableFile} on the same reference\n+ *     as the input sample.\n+ * </p>\n+ * <p>\n+ *     The end result is a text file containing three parameter tables (GOP, GCP, API) that can be fed\n+ *     directly to {@link HaplotypeCaller} --dragstr-params-path.\n+ * </p>\n+ */\n+@CommandLineProgramProperties(\n+        summary = \"estimates the parameters for the DRAGstr model for the input sample\",\n+        oneLineSummary = \"summary\",\n+        programGroup = ExampleProgramGroup.class\n+)\n+public class EstimateDragstrParameters extends GATKTool {\n+\n+    public static final String STR_TABLE_PATH_SHORT_NAME = \"str\";\n+    public static final String STR_TABLE_PATH_FULL_NAME = \"str-table-path\";\n+    public static final String PARALLEL_FULL_NAME = \"parallel\";\n+    public static final String THREADS_FULL_NAME = \"threads\";\n+    public static final String SHARD_SIZE_FULL_NAME = \"shard-size\";\n+    public static final String DOWN_SAMPLE_SIZE_FULL_NAME = \"down-sample-size\";\n+    public static final String DEBUG_SITES_OUTPUT_FULL_NAME = \"debug-sites-output\";\n+\n+    public static final int DEFAULT_SHARD_SIZE = 1_000_000;\n+    public static final int DEFAULT_DOWN_SAMPLE_SIZE = 4096;\n+    public static final int SYSTEM_SUGGESTED_THREAD_NUMBER = 0;\n+    public static final int MINIMUM_SHARD_SIZE = 100;\n+    public static final int MINIMUM_DOWN_SAMPLE_SIZE = 512;\n+\n+    @ArgumentCollection\n+    private DragstrHyperParameters hyperParameters = new DragstrHyperParameters();\n+\n+    @Argument(shortName=STR_TABLE_PATH_SHORT_NAME, fullName=STR_TABLE_PATH_FULL_NAME, doc=\"location of the zip that contains the sampling sites for the reference\")\n+    private String strTablePath = null;\n+\n+    @Argument(fullName=PARALLEL_FULL_NAME, doc=\"run alignment data collection and  estimation in parallel\")\n+    private boolean runInParallel = false;\n+\n+    @Argument(fullName=THREADS_FULL_NAME, minValue = SYSTEM_SUGGESTED_THREAD_NUMBER, doc=\"suggested number of parallel threads to perform the estimation, \"\n+            + \"the default 0 leave it up to the VM to decide. When set to more than 1, this will activate parallel in the absence of --parallel\")\n+    private int threads = SYSTEM_SUGGESTED_THREAD_NUMBER;\n+\n+    @Argument(fullName=SHARD_SIZE_FULL_NAME, doc=\"when running in parallel this is the suggested shard size in base pairs. \" +\n+            \"The actual shard-size may vary to adapt to small contigs and the requested number of threads\",\n+              minValue = MINIMUM_SHARD_SIZE)\n+    private int shardSize = DEFAULT_SHARD_SIZE;\n+\n+    @Argument(fullName=DOWN_SAMPLE_SIZE_FULL_NAME, doc=\"Targeted maximum number of cases per combination period repeat count, \" +\n+            \"the larger the more precise but also the slower estimation.\",\n+              minValue = MINIMUM_DOWN_SAMPLE_SIZE)\n+    private int downsampleSize = DEFAULT_DOWN_SAMPLE_SIZE;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME, shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME, doc = \"where to write the parameter output file.\")\n+    private String output = null;\n+\n+    @Argument(fullName= DEBUG_SITES_OUTPUT_FULL_NAME, doc = \"table with information gather on the samples sites. Includes what sites were downsampled, disqualified or accepted for parameter estimation\", optional = true)\n+    private String sitesOutput = null;\n+\n+    private SAMSequenceDictionary dictionary;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean requiresReads() {\n+        return true;\n+    }\n+\n+    @Override\n+    protected void onStartup() {\n+        super.onStartup();\n+        hyperParameters.validate();\n+        dictionary = getBestAvailableSequenceDictionary();\n+        if (runInParallel) {\n+            if (threads == 1) {\n+                logger.warn(\"parallel processing was requested but the number of threads was set to 1\");\n+            }\n+        } else if (threads > 1) {\n+            runInParallel = true;\n+        }\n+        if (runInParallel) {\n+            if (threads == 0) {\n+                logger.info(\"Running in parallel using the system suggested default thread count: \" + Runtime.getRuntime().availableProcessors());\n+            } else {\n+                logger.info(\"Running in parallel using the requested number of threads: \" + threads);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        hyperParameters.validate();\n+        dictionary = getBestAvailableSequenceDictionary();\n+        final List<SAMReadGroupRecord> readGroups = hasReads() ? getHeaderForReads().getReadGroups() : Collections.emptyList();\n+        final List<String> readGroupIds = readGroups.stream()\n+                .map(SAMReadGroupRecord::getId)\n+                .collect(Collectors.toList());\n+        final List<String> sampleNames = readGroups.stream()\n+                .map(SAMReadGroupRecord::getSample)\n+                .distinct().collect(Collectors.toList());\n+        final Optional<String> sampleName = resolveSampleName(sampleNames);\n+\n+        try (final PrintWriter sitesOutputWriter = openSitesOutputWriter(sitesOutput);\n+             final STRTableFile strTable = STRTableFile.open(strTablePath)) {\n+\n+            checkSequenceDictionaryCompatibility(dictionary, strTable.dictionary());\n+            final StratifiedDragstrLocusCases allSites;\n+            final List<SimpleInterval> intervals = getTraversalIntervals();\n+\n+            runInParallel |= threads > 1;\n+            if (runInParallel) {\n+                if (threads == 1) {\n+                    logger.warn(\"parallel processing was requested but the number of threads was set to 1\");\n+                }\n+                allSites = collectCaseStatsParallel(intervals, shardSize, strTable);\n+            } else {\n+                allSites = collectCaseStatsSequencial(intervals, strTable);\n+            }\n+            logSiteCounts(allSites, \"all loci/cases\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE5MzI3Mw=="}, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE5NTc4OA==", "bodyText": "What does \"zero length towards the limit\" mean here?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495195788", "createdAt": "2020-09-25T19:42:08Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/EstimateDragstrParameters.java", "diffHunk": "@@ -0,0 +1,935 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.*;\n+import htsjdk.samtools.util.IntervalTree;\n+import it.unimi.dsi.fastutil.ints.IntArrayList;\n+import it.unimi.dsi.fastutil.ints.IntList;\n+import org.apache.commons.io.output.NullOutputStream;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.ExampleProgramGroup;\n+import org.broadinstitute.hellbender.engine.*;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrParams;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrUtils;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFile;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.AbsoluteCoordinates;\n+\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.*;\n+import java.util.stream.*;\n+\n+/**\n+ * Estimates the parameters for the DRAGstr model for an input sample.\n+ * <p>\n+ *     This tools takes in the sampling sites generated by {@link ComposeSTRTableFile} on the same reference\n+ *     as the input sample.\n+ * </p>\n+ * <p>\n+ *     The end result is a text file containing three parameter tables (GOP, GCP, API) that can be fed\n+ *     directly to {@link HaplotypeCaller} --dragstr-params-path.\n+ * </p>\n+ */\n+@CommandLineProgramProperties(\n+        summary = \"estimates the parameters for the DRAGstr model for the input sample\",\n+        oneLineSummary = \"summary\",\n+        programGroup = ExampleProgramGroup.class\n+)\n+public class EstimateDragstrParameters extends GATKTool {\n+\n+    public static final String STR_TABLE_PATH_SHORT_NAME = \"str\";\n+    public static final String STR_TABLE_PATH_FULL_NAME = \"str-table-path\";\n+    public static final String PARALLEL_FULL_NAME = \"parallel\";\n+    public static final String THREADS_FULL_NAME = \"threads\";\n+    public static final String SHARD_SIZE_FULL_NAME = \"shard-size\";\n+    public static final String DOWN_SAMPLE_SIZE_FULL_NAME = \"down-sample-size\";\n+    public static final String DEBUG_SITES_OUTPUT_FULL_NAME = \"debug-sites-output\";\n+\n+    public static final int DEFAULT_SHARD_SIZE = 1_000_000;\n+    public static final int DEFAULT_DOWN_SAMPLE_SIZE = 4096;\n+    public static final int SYSTEM_SUGGESTED_THREAD_NUMBER = 0;\n+    public static final int MINIMUM_SHARD_SIZE = 100;\n+    public static final int MINIMUM_DOWN_SAMPLE_SIZE = 512;\n+\n+    @ArgumentCollection\n+    private DragstrHyperParameters hyperParameters = new DragstrHyperParameters();\n+\n+    @Argument(shortName=STR_TABLE_PATH_SHORT_NAME, fullName=STR_TABLE_PATH_FULL_NAME, doc=\"location of the zip that contains the sampling sites for the reference\")\n+    private String strTablePath = null;\n+\n+    @Argument(fullName=PARALLEL_FULL_NAME, doc=\"run alignment data collection and  estimation in parallel\")\n+    private boolean runInParallel = false;\n+\n+    @Argument(fullName=THREADS_FULL_NAME, minValue = SYSTEM_SUGGESTED_THREAD_NUMBER, doc=\"suggested number of parallel threads to perform the estimation, \"\n+            + \"the default 0 leave it up to the VM to decide. When set to more than 1, this will activate parallel in the absence of --parallel\")\n+    private int threads = SYSTEM_SUGGESTED_THREAD_NUMBER;\n+\n+    @Argument(fullName=SHARD_SIZE_FULL_NAME, doc=\"when running in parallel this is the suggested shard size in base pairs. \" +\n+            \"The actual shard-size may vary to adapt to small contigs and the requested number of threads\",\n+              minValue = MINIMUM_SHARD_SIZE)\n+    private int shardSize = DEFAULT_SHARD_SIZE;\n+\n+    @Argument(fullName=DOWN_SAMPLE_SIZE_FULL_NAME, doc=\"Targeted maximum number of cases per combination period repeat count, \" +\n+            \"the larger the more precise but also the slower estimation.\",\n+              minValue = MINIMUM_DOWN_SAMPLE_SIZE)\n+    private int downsampleSize = DEFAULT_DOWN_SAMPLE_SIZE;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME, shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME, doc = \"where to write the parameter output file.\")\n+    private String output = null;\n+\n+    @Argument(fullName= DEBUG_SITES_OUTPUT_FULL_NAME, doc = \"table with information gather on the samples sites. Includes what sites were downsampled, disqualified or accepted for parameter estimation\", optional = true)\n+    private String sitesOutput = null;\n+\n+    private SAMSequenceDictionary dictionary;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean requiresReads() {\n+        return true;\n+    }\n+\n+    @Override\n+    protected void onStartup() {\n+        super.onStartup();\n+        hyperParameters.validate();\n+        dictionary = getBestAvailableSequenceDictionary();\n+        if (runInParallel) {\n+            if (threads == 1) {\n+                logger.warn(\"parallel processing was requested but the number of threads was set to 1\");\n+            }\n+        } else if (threads > 1) {\n+            runInParallel = true;\n+        }\n+        if (runInParallel) {\n+            if (threads == 0) {\n+                logger.info(\"Running in parallel using the system suggested default thread count: \" + Runtime.getRuntime().availableProcessors());\n+            } else {\n+                logger.info(\"Running in parallel using the requested number of threads: \" + threads);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        hyperParameters.validate();\n+        dictionary = getBestAvailableSequenceDictionary();\n+        final List<SAMReadGroupRecord> readGroups = hasReads() ? getHeaderForReads().getReadGroups() : Collections.emptyList();\n+        final List<String> readGroupIds = readGroups.stream()\n+                .map(SAMReadGroupRecord::getId)\n+                .collect(Collectors.toList());\n+        final List<String> sampleNames = readGroups.stream()\n+                .map(SAMReadGroupRecord::getSample)\n+                .distinct().collect(Collectors.toList());\n+        final Optional<String> sampleName = resolveSampleName(sampleNames);\n+\n+        try (final PrintWriter sitesOutputWriter = openSitesOutputWriter(sitesOutput);\n+             final STRTableFile strTable = STRTableFile.open(strTablePath)) {\n+\n+            checkSequenceDictionaryCompatibility(dictionary, strTable.dictionary());\n+            final StratifiedDragstrLocusCases allSites;\n+            final List<SimpleInterval> intervals = getTraversalIntervals();\n+\n+            runInParallel |= threads > 1;\n+            if (runInParallel) {\n+                if (threads == 1) {\n+                    logger.warn(\"parallel processing was requested but the number of threads was set to 1\");\n+                }\n+                allSites = collectCaseStatsParallel(intervals, shardSize, strTable);\n+            } else {\n+                allSites = collectCaseStatsSequencial(intervals, strTable);\n+            }\n+            logSiteCounts(allSites, \"all loci/cases\");\n+            final StratifiedDragstrLocusCases downSampledSites = downSample(allSites, strTable, sitesOutputWriter);\n+            logSiteCounts(downSampledSites, \"all downsampled (kept) loci/cases\");\n+            final StratifiedDragstrLocusCases finalSites = downSampledSites.qualifyingOnly(hyperParameters.minDepth, hyperParameters.minMQ, 0);\n+            logSiteCounts(finalSites, \"all qualifying loci/cases\");\n+            outputDownSampledSiteDetails(downSampledSites, sitesOutputWriter, hyperParameters.minDepth, hyperParameters.minMQ, 0);\n+            printOutput(finalSites, sampleName.orElse(null), readGroupIds);\n+        }\n+    }\n+\n+    private void printOutput(final StratifiedDragstrLocusCases finalSites, final String sampleName, final List<String> readGroups) {\n+        try (final PrintWriter writer = new PrintWriter(openBufferedWriter(output))) {\n+            final boolean usingDefaults = !isThereEnoughCases(finalSites);\n+            writer.println(\"############################################################################################\");\n+            writer.println(\"# DragstrParams\");\n+            writer.println(\"# -------------------------\");\n+            writer.println(\"# sample = \" + (sampleName == null ? \"<unspecified>\" : sampleName));\n+            writer.println(\"# readGroups = \" + (readGroups.isEmpty()? \"<unspecified>\" : Utils.join(\", \", readGroups)));\n+            writer.println(\"# estimatedOrDefaults = \" + (usingDefaults ? \"defaults\" : \"estimated\"));\n+            writer.println(\"# commandLine = \" + getCommandLine());\n+            writer.println(\"############################################################################################\");\n+            if (!usingDefaults) {\n+                logger.info(\"Estimating parameters used sampled down cases\");\n+                final DragstrParams estimate = estimateParams(finalSites);\n+                logger.info(\"Done with estimation, printing output\");\n+                estimate.print(writer);\n+            } else {\n+                logger.warn(\"Not enough cases to estimate parameters, using defaults\");\n+                DragstrParams.DEFAULT.print(writer);\n+            }\n+        }\n+    }\n+\n+    private BufferedWriter openBufferedWriter(final String path) {\n+        try {\n+            return Files.newBufferedWriter(Paths.get(path));\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(path, ex);\n+        }\n+    }\n+\n+    private Optional<String> resolveSampleName(List<String> sampleNames) {\n+        if (sampleNames.size() > 1) {\n+            throw new GATKException(\"the input alignment(s) have more than one sample: \" + String.join(\", \", sampleNames));\n+        } else if (sampleNames.isEmpty() || sampleNames.get(0) == null) {\n+            logger.warn(\"there is no sample id in the alignment header, assuming that all reads and read/groups make reference to the same anonymous sample\");\n+            return Optional.empty();\n+        } else {\n+            return Optional.of(sampleNames.get(0));\n+        }\n+    }\n+\n+    private void checkSequenceDictionaryCompatibility(final SAMSequenceDictionary reference, final SAMSequenceDictionary strTable) {\n+        final SequenceDictionaryUtils.SequenceDictionaryCompatibility compatibility = SequenceDictionaryUtils.compareDictionaries(reference, strTable, false);\n+        switch (compatibility) {\n+            case IDENTICAL: return;\n+            case SUPERSET: return;\n+            // probably these two below aren't ever be returned since we ask for no check on order but\n+            // adding them it just in case\n+            case NON_CANONICAL_HUMAN_ORDER: return; // we don't care about the order.\n+            case OUT_OF_ORDER: return; // we don't care about the order.\n+            default:\n+                throw new GATKException(\"the reference and str-table sequence dictionary are incompatible: \" + compatibility);\n+        }\n+    }\n+\n+    private PrintWriter openSitesOutputWriter(final String sitesOutput) {\n+        return sitesOutput == null ? new PrintWriter(new NullOutputStream())\n+                : new PrintWriter(BucketUtils.createFile(sitesOutput));\n+    }\n+\n+    private void outputDownSampledSiteDetails(final StratifiedDragstrLocusCases finalSites,\n+                                              final PrintWriter writer,\n+                                              final int minDepth,\n+                                              final int samplingMinMQ,\n+                                              final int maxSup) {\n+        if (sitesOutput != null) {\n+            for (final DragstrLocusCases[] periodCases : finalSites.perPeriodAndRepeat) {\n+                for (final DragstrLocusCases repeatCases : periodCases) {\n+                    for (final DragstrLocusCase caze : repeatCases) {\n+                        outputSiteDetails(writer, caze, caze.qualifies(minDepth, samplingMinMQ, maxSup) ? \"used\" : \"skipped\");\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Holds the minimum counts for each period, repeat-length combo.\n+     * If there is lack of data for any of these we use the default param\n+     * tables. Missing values, row (periods) or columns (repeat-length) are\n+     * interpreted as 0.\n+     */\n+    private static final int[][] MINIMUM_CASES_BY_PERIOD_AND_LENGTH =\n+            // @formatter:off ; prevents code reformatting by IntelliJ\n+            //                  if enabled:\n+            //                    Preferences > Editor > Code Style > Formatter Control\n+            // run-length:\n+            //  0,   1,   2,   3,   4,   5,   6,   7,   8,   9, 10+   // period\n+            {  {},\n+               {0, 200, 200, 200, 200, 200, 200, 200, 200, 200,   0}, // 1\n+               {0,   0, 200, 200, 200, 200,   0,   0,   0,   0,   0}, // 2\n+               {0,   0, 200, 200, 200,   0,   0,   0,   0,   0,   0}, // 3\n+               {0,   0, 200, 200,   0,   0,   0,   0,   0,   0,   0}, // 4\n+               {0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0}, // 5\n+               {0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0}, // 6\n+               {0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0}, // 7\n+               {0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0}, // 8\n+            };\n+            // zeros to the right are actually not necessary, but add them to make it look more like a matrix.\n+            // @formatter:on\n+\n+    /**\n+     * Check that a minimum number of cases are available in key bins (combo period, repeat).\n+     */\n+    private boolean isThereEnoughCases(final StratifiedDragstrLocusCases allSites) {\n+        // period 1, repeat length 1 to 9 (inclusive)\n+        final int[][] MCBL = MINIMUM_CASES_BY_PERIOD_AND_LENGTH;\n+        final int maxP = Math.min(hyperParameters.maxPeriod, MCBL.length - 1);\n+        for (int i = 1; i <= maxP; i++) {\n+            final int maxL = Math.min(hyperParameters.maxRepeatLength, MCBL[i].length - 1);\n+            for (int j = 1; j <= maxL; j++) {\n+                if (allSites.get(i, j).size() < MCBL[i][j]) {\n+                    return false;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Performs the final estimation step.\n+     * @param finalSites the site to use for the estimation.\n+     * @return {@code never null}.\n+     */\n+    private DragstrParams estimateParams(final StratifiedDragstrLocusCases finalSites) {\n+        final DragstrParametersEstimator estimator = new DragstrParametersEstimator(hyperParameters);\n+        return runInParallel ? Utils.runInParallel(threads, () -> estimator.estimate(finalSites)) : estimator.estimate(finalSites);\n+    }\n+\n+    /**\n+     * Downsample sites so that at most as many as {@link #downsampleSize} cases remain for each period and repeat-length combination.\n+     * @param allSites the sites to downsample.\n+     * @param strTable that contains the decimation table used to generate those sites.\n+     * @param sitesOutputWriter an optional per site informattion output argument for debugging purposes.\n+     * @return never {@code null}.\n+     */\n+    private StratifiedDragstrLocusCases downSample(final StratifiedDragstrLocusCases allSites, final STRTableFile strTable,\n+                                                   final PrintWriter sitesOutputWriter) {\n+        final STRDecimationTable decimationTable = strTable.decimationTable();\n+        final List<PeriodAndRepeatLength> prCombos = new ArrayList<>(hyperParameters.maxPeriod * hyperParameters.maxRepeatLength);\n+        for (int i = 1; i <= hyperParameters.maxPeriod; i++) {\n+            for (int j = 1; j <= hyperParameters.maxRepeatLength; j++) {\n+                prCombos.add(PeriodAndRepeatLength.of(i, j));\n+            }\n+        }\n+\n+        final Stream<PeriodAndRepeatLength> prCombosStream = runInParallel ? prCombos.parallelStream() : prCombos.stream();\n+        final Stream<DragstrLocusCase> downsampledStream = prCombosStream\n+                .flatMap(combo -> {\n+                    final DragstrLocusCases all = allSites.perPeriodAndRepeat[combo.period - 1][combo.repeatLength - 1];\n+                    final int decimationBit = decimationTable.decimationBit(combo.period, combo.repeatLength);\n+                    return downSample(all, decimationBit, downsampleSize, sitesOutputWriter).stream();\n+                });\n+\n+        if (runInParallel) {\n+            return Utils.runInParallel(threads,\n+                    () -> downsampledStream.collect(DragstrLocusCaseStratificator.make(hyperParameters.maxPeriod, hyperParameters.maxRepeatLength)));\n+        } else {\n+            return downsampledStream.collect(DragstrLocusCaseStratificator.make(hyperParameters.maxPeriod, hyperParameters.maxRepeatLength));\n+        }\n+    }\n+\n+    /**\n+     * Pre-calculated decimation masks used depending on the final decimation bit/level.\n+     */\n+    private static final long[] DECIMATION_MASKS_BY_BIT = new long[Long.SIZE];\n+\n+    // Code to populate DECIMATION_MASKS_BY_BIT.\n+    static {\n+        DECIMATION_MASKS_BY_BIT[0] = 1;\n+        for (int i = 1, j = 0; i < Long.SIZE; i++, j++) {\n+            DECIMATION_MASKS_BY_BIT[i] = DECIMATION_MASKS_BY_BIT[j] << 1;\n+            DECIMATION_MASKS_BY_BIT[j] = ~DECIMATION_MASKS_BY_BIT[j];\n+        }\n+        DECIMATION_MASKS_BY_BIT[Long.SIZE -1] = ~DECIMATION_MASKS_BY_BIT[Long.SIZE - 1];\n+    }\n+\n+    /**\n+     * Decimates the collection of locus/cases to the downsample size provided or smaller.\n+     * <p>\n+     *     Notice that if we need to downsample (the input size is larger than the downsample size provided)\n+     *     we take care of not counting those cases that have zero-length toward that limit.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 350}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE5NjU5NQ==", "bodyText": "Get rid of code cleanup stubs", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495196595", "createdAt": "2020-09-25T19:44:10Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/EstimateDragstrParameters.java", "diffHunk": "@@ -0,0 +1,935 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.*;\n+import htsjdk.samtools.util.IntervalTree;\n+import it.unimi.dsi.fastutil.ints.IntArrayList;\n+import it.unimi.dsi.fastutil.ints.IntList;\n+import org.apache.commons.io.output.NullOutputStream;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.ExampleProgramGroup;\n+import org.broadinstitute.hellbender.engine.*;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrParams;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrUtils;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFile;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.AbsoluteCoordinates;\n+\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.*;\n+import java.util.stream.*;\n+\n+/**\n+ * Estimates the parameters for the DRAGstr model for an input sample.\n+ * <p>\n+ *     This tools takes in the sampling sites generated by {@link ComposeSTRTableFile} on the same reference\n+ *     as the input sample.\n+ * </p>\n+ * <p>\n+ *     The end result is a text file containing three parameter tables (GOP, GCP, API) that can be fed\n+ *     directly to {@link HaplotypeCaller} --dragstr-params-path.\n+ * </p>\n+ */\n+@CommandLineProgramProperties(\n+        summary = \"estimates the parameters for the DRAGstr model for the input sample\",\n+        oneLineSummary = \"summary\",\n+        programGroup = ExampleProgramGroup.class\n+)\n+public class EstimateDragstrParameters extends GATKTool {\n+\n+    public static final String STR_TABLE_PATH_SHORT_NAME = \"str\";\n+    public static final String STR_TABLE_PATH_FULL_NAME = \"str-table-path\";\n+    public static final String PARALLEL_FULL_NAME = \"parallel\";\n+    public static final String THREADS_FULL_NAME = \"threads\";\n+    public static final String SHARD_SIZE_FULL_NAME = \"shard-size\";\n+    public static final String DOWN_SAMPLE_SIZE_FULL_NAME = \"down-sample-size\";\n+    public static final String DEBUG_SITES_OUTPUT_FULL_NAME = \"debug-sites-output\";\n+\n+    public static final int DEFAULT_SHARD_SIZE = 1_000_000;\n+    public static final int DEFAULT_DOWN_SAMPLE_SIZE = 4096;\n+    public static final int SYSTEM_SUGGESTED_THREAD_NUMBER = 0;\n+    public static final int MINIMUM_SHARD_SIZE = 100;\n+    public static final int MINIMUM_DOWN_SAMPLE_SIZE = 512;\n+\n+    @ArgumentCollection\n+    private DragstrHyperParameters hyperParameters = new DragstrHyperParameters();\n+\n+    @Argument(shortName=STR_TABLE_PATH_SHORT_NAME, fullName=STR_TABLE_PATH_FULL_NAME, doc=\"location of the zip that contains the sampling sites for the reference\")\n+    private String strTablePath = null;\n+\n+    @Argument(fullName=PARALLEL_FULL_NAME, doc=\"run alignment data collection and  estimation in parallel\")\n+    private boolean runInParallel = false;\n+\n+    @Argument(fullName=THREADS_FULL_NAME, minValue = SYSTEM_SUGGESTED_THREAD_NUMBER, doc=\"suggested number of parallel threads to perform the estimation, \"\n+            + \"the default 0 leave it up to the VM to decide. When set to more than 1, this will activate parallel in the absence of --parallel\")\n+    private int threads = SYSTEM_SUGGESTED_THREAD_NUMBER;\n+\n+    @Argument(fullName=SHARD_SIZE_FULL_NAME, doc=\"when running in parallel this is the suggested shard size in base pairs. \" +\n+            \"The actual shard-size may vary to adapt to small contigs and the requested number of threads\",\n+              minValue = MINIMUM_SHARD_SIZE)\n+    private int shardSize = DEFAULT_SHARD_SIZE;\n+\n+    @Argument(fullName=DOWN_SAMPLE_SIZE_FULL_NAME, doc=\"Targeted maximum number of cases per combination period repeat count, \" +\n+            \"the larger the more precise but also the slower estimation.\",\n+              minValue = MINIMUM_DOWN_SAMPLE_SIZE)\n+    private int downsampleSize = DEFAULT_DOWN_SAMPLE_SIZE;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME, shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME, doc = \"where to write the parameter output file.\")\n+    private String output = null;\n+\n+    @Argument(fullName= DEBUG_SITES_OUTPUT_FULL_NAME, doc = \"table with information gather on the samples sites. Includes what sites were downsampled, disqualified or accepted for parameter estimation\", optional = true)\n+    private String sitesOutput = null;\n+\n+    private SAMSequenceDictionary dictionary;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean requiresReads() {\n+        return true;\n+    }\n+\n+    @Override\n+    protected void onStartup() {\n+        super.onStartup();\n+        hyperParameters.validate();\n+        dictionary = getBestAvailableSequenceDictionary();\n+        if (runInParallel) {\n+            if (threads == 1) {\n+                logger.warn(\"parallel processing was requested but the number of threads was set to 1\");\n+            }\n+        } else if (threads > 1) {\n+            runInParallel = true;\n+        }\n+        if (runInParallel) {\n+            if (threads == 0) {\n+                logger.info(\"Running in parallel using the system suggested default thread count: \" + Runtime.getRuntime().availableProcessors());\n+            } else {\n+                logger.info(\"Running in parallel using the requested number of threads: \" + threads);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        hyperParameters.validate();\n+        dictionary = getBestAvailableSequenceDictionary();\n+        final List<SAMReadGroupRecord> readGroups = hasReads() ? getHeaderForReads().getReadGroups() : Collections.emptyList();\n+        final List<String> readGroupIds = readGroups.stream()\n+                .map(SAMReadGroupRecord::getId)\n+                .collect(Collectors.toList());\n+        final List<String> sampleNames = readGroups.stream()\n+                .map(SAMReadGroupRecord::getSample)\n+                .distinct().collect(Collectors.toList());\n+        final Optional<String> sampleName = resolveSampleName(sampleNames);\n+\n+        try (final PrintWriter sitesOutputWriter = openSitesOutputWriter(sitesOutput);\n+             final STRTableFile strTable = STRTableFile.open(strTablePath)) {\n+\n+            checkSequenceDictionaryCompatibility(dictionary, strTable.dictionary());\n+            final StratifiedDragstrLocusCases allSites;\n+            final List<SimpleInterval> intervals = getTraversalIntervals();\n+\n+            runInParallel |= threads > 1;\n+            if (runInParallel) {\n+                if (threads == 1) {\n+                    logger.warn(\"parallel processing was requested but the number of threads was set to 1\");\n+                }\n+                allSites = collectCaseStatsParallel(intervals, shardSize, strTable);\n+            } else {\n+                allSites = collectCaseStatsSequencial(intervals, strTable);\n+            }\n+            logSiteCounts(allSites, \"all loci/cases\");\n+            final StratifiedDragstrLocusCases downSampledSites = downSample(allSites, strTable, sitesOutputWriter);\n+            logSiteCounts(downSampledSites, \"all downsampled (kept) loci/cases\");\n+            final StratifiedDragstrLocusCases finalSites = downSampledSites.qualifyingOnly(hyperParameters.minDepth, hyperParameters.minMQ, 0);\n+            logSiteCounts(finalSites, \"all qualifying loci/cases\");\n+            outputDownSampledSiteDetails(downSampledSites, sitesOutputWriter, hyperParameters.minDepth, hyperParameters.minMQ, 0);\n+            printOutput(finalSites, sampleName.orElse(null), readGroupIds);\n+        }\n+    }\n+\n+    private void printOutput(final StratifiedDragstrLocusCases finalSites, final String sampleName, final List<String> readGroups) {\n+        try (final PrintWriter writer = new PrintWriter(openBufferedWriter(output))) {\n+            final boolean usingDefaults = !isThereEnoughCases(finalSites);\n+            writer.println(\"############################################################################################\");\n+            writer.println(\"# DragstrParams\");\n+            writer.println(\"# -------------------------\");\n+            writer.println(\"# sample = \" + (sampleName == null ? \"<unspecified>\" : sampleName));\n+            writer.println(\"# readGroups = \" + (readGroups.isEmpty()? \"<unspecified>\" : Utils.join(\", \", readGroups)));\n+            writer.println(\"# estimatedOrDefaults = \" + (usingDefaults ? \"defaults\" : \"estimated\"));\n+            writer.println(\"# commandLine = \" + getCommandLine());\n+            writer.println(\"############################################################################################\");\n+            if (!usingDefaults) {\n+                logger.info(\"Estimating parameters used sampled down cases\");\n+                final DragstrParams estimate = estimateParams(finalSites);\n+                logger.info(\"Done with estimation, printing output\");\n+                estimate.print(writer);\n+            } else {\n+                logger.warn(\"Not enough cases to estimate parameters, using defaults\");\n+                DragstrParams.DEFAULT.print(writer);\n+            }\n+        }\n+    }\n+\n+    private BufferedWriter openBufferedWriter(final String path) {\n+        try {\n+            return Files.newBufferedWriter(Paths.get(path));\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(path, ex);\n+        }\n+    }\n+\n+    private Optional<String> resolveSampleName(List<String> sampleNames) {\n+        if (sampleNames.size() > 1) {\n+            throw new GATKException(\"the input alignment(s) have more than one sample: \" + String.join(\", \", sampleNames));\n+        } else if (sampleNames.isEmpty() || sampleNames.get(0) == null) {\n+            logger.warn(\"there is no sample id in the alignment header, assuming that all reads and read/groups make reference to the same anonymous sample\");\n+            return Optional.empty();\n+        } else {\n+            return Optional.of(sampleNames.get(0));\n+        }\n+    }\n+\n+    private void checkSequenceDictionaryCompatibility(final SAMSequenceDictionary reference, final SAMSequenceDictionary strTable) {\n+        final SequenceDictionaryUtils.SequenceDictionaryCompatibility compatibility = SequenceDictionaryUtils.compareDictionaries(reference, strTable, false);\n+        switch (compatibility) {\n+            case IDENTICAL: return;\n+            case SUPERSET: return;\n+            // probably these two below aren't ever be returned since we ask for no check on order but\n+            // adding them it just in case\n+            case NON_CANONICAL_HUMAN_ORDER: return; // we don't care about the order.\n+            case OUT_OF_ORDER: return; // we don't care about the order.\n+            default:\n+                throw new GATKException(\"the reference and str-table sequence dictionary are incompatible: \" + compatibility);\n+        }\n+    }\n+\n+    private PrintWriter openSitesOutputWriter(final String sitesOutput) {\n+        return sitesOutput == null ? new PrintWriter(new NullOutputStream())\n+                : new PrintWriter(BucketUtils.createFile(sitesOutput));\n+    }\n+\n+    private void outputDownSampledSiteDetails(final StratifiedDragstrLocusCases finalSites,\n+                                              final PrintWriter writer,\n+                                              final int minDepth,\n+                                              final int samplingMinMQ,\n+                                              final int maxSup) {\n+        if (sitesOutput != null) {\n+            for (final DragstrLocusCases[] periodCases : finalSites.perPeriodAndRepeat) {\n+                for (final DragstrLocusCases repeatCases : periodCases) {\n+                    for (final DragstrLocusCase caze : repeatCases) {\n+                        outputSiteDetails(writer, caze, caze.qualifies(minDepth, samplingMinMQ, maxSup) ? \"used\" : \"skipped\");\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Holds the minimum counts for each period, repeat-length combo.\n+     * If there is lack of data for any of these we use the default param\n+     * tables. Missing values, row (periods) or columns (repeat-length) are\n+     * interpreted as 0.\n+     */\n+    private static final int[][] MINIMUM_CASES_BY_PERIOD_AND_LENGTH =\n+            // @formatter:off ; prevents code reformatting by IntelliJ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 252}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIwMzA5Ng==", "bodyText": "I don't understand why you do this in this order. You downsample first then filter based on coverage. Doesn't it make more sense to do it in the opposite order since you might end up with a relatively small subset of sites due to coverage issues or is there some bias in doing that? It seems youve already paid the cost of sampling/reading the bam at every site so there is no cost to do it the other way. If thats not the case then why not do the downsampling before reading the sites off of the bam? The decimation table has you reduce some loci classes by 1024x  but you paid ingest and compute the reads for all of those reads ahead of time when you are ultimately going to use something like <4096 of them for estimation anyway...", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495203096", "createdAt": "2020-09-25T19:58:14Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/EstimateDragstrParameters.java", "diffHunk": "@@ -0,0 +1,935 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import htsjdk.samtools.*;\n+import htsjdk.samtools.util.IntervalTree;\n+import it.unimi.dsi.fastutil.ints.IntArrayList;\n+import it.unimi.dsi.fastutil.ints.IntList;\n+import org.apache.commons.io.output.NullOutputStream;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.ExampleProgramGroup;\n+import org.broadinstitute.hellbender.engine.*;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrParams;\n+import org.broadinstitute.hellbender.utils.dragstr.DragstrUtils;\n+import org.broadinstitute.hellbender.utils.dragstr.STRTableFile;\n+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.reference.AbsoluteCoordinates;\n+\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.*;\n+import java.util.stream.*;\n+\n+/**\n+ * Estimates the parameters for the DRAGstr model for an input sample.\n+ * <p>\n+ *     This tools takes in the sampling sites generated by {@link ComposeSTRTableFile} on the same reference\n+ *     as the input sample.\n+ * </p>\n+ * <p>\n+ *     The end result is a text file containing three parameter tables (GOP, GCP, API) that can be fed\n+ *     directly to {@link HaplotypeCaller} --dragstr-params-path.\n+ * </p>\n+ */\n+@CommandLineProgramProperties(\n+        summary = \"estimates the parameters for the DRAGstr model for the input sample\",\n+        oneLineSummary = \"summary\",\n+        programGroup = ExampleProgramGroup.class\n+)\n+public class EstimateDragstrParameters extends GATKTool {\n+\n+    public static final String STR_TABLE_PATH_SHORT_NAME = \"str\";\n+    public static final String STR_TABLE_PATH_FULL_NAME = \"str-table-path\";\n+    public static final String PARALLEL_FULL_NAME = \"parallel\";\n+    public static final String THREADS_FULL_NAME = \"threads\";\n+    public static final String SHARD_SIZE_FULL_NAME = \"shard-size\";\n+    public static final String DOWN_SAMPLE_SIZE_FULL_NAME = \"down-sample-size\";\n+    public static final String DEBUG_SITES_OUTPUT_FULL_NAME = \"debug-sites-output\";\n+\n+    public static final int DEFAULT_SHARD_SIZE = 1_000_000;\n+    public static final int DEFAULT_DOWN_SAMPLE_SIZE = 4096;\n+    public static final int SYSTEM_SUGGESTED_THREAD_NUMBER = 0;\n+    public static final int MINIMUM_SHARD_SIZE = 100;\n+    public static final int MINIMUM_DOWN_SAMPLE_SIZE = 512;\n+\n+    @ArgumentCollection\n+    private DragstrHyperParameters hyperParameters = new DragstrHyperParameters();\n+\n+    @Argument(shortName=STR_TABLE_PATH_SHORT_NAME, fullName=STR_TABLE_PATH_FULL_NAME, doc=\"location of the zip that contains the sampling sites for the reference\")\n+    private String strTablePath = null;\n+\n+    @Argument(fullName=PARALLEL_FULL_NAME, doc=\"run alignment data collection and  estimation in parallel\")\n+    private boolean runInParallel = false;\n+\n+    @Argument(fullName=THREADS_FULL_NAME, minValue = SYSTEM_SUGGESTED_THREAD_NUMBER, doc=\"suggested number of parallel threads to perform the estimation, \"\n+            + \"the default 0 leave it up to the VM to decide. When set to more than 1, this will activate parallel in the absence of --parallel\")\n+    private int threads = SYSTEM_SUGGESTED_THREAD_NUMBER;\n+\n+    @Argument(fullName=SHARD_SIZE_FULL_NAME, doc=\"when running in parallel this is the suggested shard size in base pairs. \" +\n+            \"The actual shard-size may vary to adapt to small contigs and the requested number of threads\",\n+              minValue = MINIMUM_SHARD_SIZE)\n+    private int shardSize = DEFAULT_SHARD_SIZE;\n+\n+    @Argument(fullName=DOWN_SAMPLE_SIZE_FULL_NAME, doc=\"Targeted maximum number of cases per combination period repeat count, \" +\n+            \"the larger the more precise but also the slower estimation.\",\n+              minValue = MINIMUM_DOWN_SAMPLE_SIZE)\n+    private int downsampleSize = DEFAULT_DOWN_SAMPLE_SIZE;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME, shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME, doc = \"where to write the parameter output file.\")\n+    private String output = null;\n+\n+    @Argument(fullName= DEBUG_SITES_OUTPUT_FULL_NAME, doc = \"table with information gather on the samples sites. Includes what sites were downsampled, disqualified or accepted for parameter estimation\", optional = true)\n+    private String sitesOutput = null;\n+\n+    private SAMSequenceDictionary dictionary;\n+\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean requiresReads() {\n+        return true;\n+    }\n+\n+    @Override\n+    protected void onStartup() {\n+        super.onStartup();\n+        hyperParameters.validate();\n+        dictionary = getBestAvailableSequenceDictionary();\n+        if (runInParallel) {\n+            if (threads == 1) {\n+                logger.warn(\"parallel processing was requested but the number of threads was set to 1\");\n+            }\n+        } else if (threads > 1) {\n+            runInParallel = true;\n+        }\n+        if (runInParallel) {\n+            if (threads == 0) {\n+                logger.info(\"Running in parallel using the system suggested default thread count: \" + Runtime.getRuntime().availableProcessors());\n+            } else {\n+                logger.info(\"Running in parallel using the requested number of threads: \" + threads);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public void traverse() {\n+        hyperParameters.validate();\n+        dictionary = getBestAvailableSequenceDictionary();\n+        final List<SAMReadGroupRecord> readGroups = hasReads() ? getHeaderForReads().getReadGroups() : Collections.emptyList();\n+        final List<String> readGroupIds = readGroups.stream()\n+                .map(SAMReadGroupRecord::getId)\n+                .collect(Collectors.toList());\n+        final List<String> sampleNames = readGroups.stream()\n+                .map(SAMReadGroupRecord::getSample)\n+                .distinct().collect(Collectors.toList());\n+        final Optional<String> sampleName = resolveSampleName(sampleNames);\n+\n+        try (final PrintWriter sitesOutputWriter = openSitesOutputWriter(sitesOutput);\n+             final STRTableFile strTable = STRTableFile.open(strTablePath)) {\n+\n+            checkSequenceDictionaryCompatibility(dictionary, strTable.dictionary());\n+            final StratifiedDragstrLocusCases allSites;\n+            final List<SimpleInterval> intervals = getTraversalIntervals();\n+\n+            runInParallel |= threads > 1;\n+            if (runInParallel) {\n+                if (threads == 1) {\n+                    logger.warn(\"parallel processing was requested but the number of threads was set to 1\");\n+                }\n+                allSites = collectCaseStatsParallel(intervals, shardSize, strTable);\n+            } else {\n+                allSites = collectCaseStatsSequencial(intervals, strTable);\n+            }\n+            logSiteCounts(allSites, \"all loci/cases\");\n+            final StratifiedDragstrLocusCases downSampledSites = downSample(allSites, strTable, sitesOutputWriter);\n+            logSiteCounts(downSampledSites, \"all downsampled (kept) loci/cases\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIwNDYxNg==", "bodyText": "Good javadoc comments here", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495204616", "createdAt": "2020-09-25T20:01:45Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/STRDecimationTable.java", "diffHunk": "@@ -0,0 +1,235 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+\n+/**\n+ * Represents a decimation table.\n+ * <p>\n+ * A decimation table (DT) has one entry for each possible period and repeat length.\n+ * Each entry's value indicates the lowest \"mask\" bit (0-based) that is allowed to be set\n+ * for sampled sites.\n+ * </p>\n+ * <p>\n+ *     The \"mask\" of a site is roughly the number of sites with same period and repeat-length that\n+ *     have been noted before this one; actually some number in the series are skipped at the beginning\n+ *     of each contig (as many as the contig 0-based index in the dictionary).\n+ * </p>\n+ * <p>\n+ *     Thus if the DT entry for a period and repeat-length combination is 0, all sites are sampled, if it is 1\n+ *     every second site is discarded, if it 10, one every 1024 sites is sampled and the rest discarded.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIwNjAyNw==", "bodyText": "Unused apparently? It seems like the decimation mask is in practiced applied to the sequential count of the occurance for a number not the actual locus itself correct?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495206027", "createdAt": "2020-09-25T20:05:11Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/STRDecimationTable.java", "diffHunk": "@@ -0,0 +1,235 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+\n+/**\n+ * Represents a decimation table.\n+ * <p>\n+ * A decimation table (DT) has one entry for each possible period and repeat length.\n+ * Each entry's value indicates the lowest \"mask\" bit (0-based) that is allowed to be set\n+ * for sampled sites.\n+ * </p>\n+ * <p>\n+ *     The \"mask\" of a site is roughly the number of sites with same period and repeat-length that\n+ *     have been noted before this one; actually some number in the series are skipped at the beginning\n+ *     of each contig (as many as the contig 0-based index in the dictionary).\n+ * </p>\n+ * <p>\n+ *     Thus if the DT entry for a period and repeat-length combination is 0, all sites are sampled, if it is 1\n+ *     every second site is discarded, if it 10, one every 1024 sites is sampled and the rest discarded.\n+ * </p>\n+ */\n+public class STRDecimationTable {\n+\n+    /**\n+     * Default decimation matrix. Missing entries are assumed to be 0.\n+     */\n+    private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+            {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+            {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+            {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+            {0, 0, 8, 4, 1, 0},\n+            {0, 0, 6, 0},\n+            {0, 0, 5, 0},\n+            {0, 0, 4, 0},\n+            {0, 0, 1, 0},\n+            {0}};\n+\n+    /**\n+     * String that represents the special DT with no decimation ... ie all entries set to 0 and every site is kept.\n+     */\n+    public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+    /**\n+     * Strings that represents the default decimation table.\n+     */\n+    public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+    /**\n+     * The default decimation table.\n+     */\n+    public static final STRDecimationTable DEFAULT = new STRDecimationTable(DEFAULT_DECIMATION_STR);\n+\n+    /**\n+     * The no-decimating table.\n+     */\n+    public static final STRDecimationTable NONE = new STRDecimationTable(NO_DECIMATION_STR);\n+\n+    private final int[][] decimationMatrix;\n+    private final long[][] decimationMask;\n+\n+    private final long[][] counts;\n+\n+    /**\n+     * Creates a decimation table from its string representation. It might be a special table (NONE, DEFAULT) or the\n+     * path to a file that contains the table.\n+     */\n+    public STRDecimationTable(final String spec) {\n+        Utils.nonNull(spec);\n+        if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+            decimationMatrix = new int[][] {{0}};\n+        } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+            decimationMatrix = DEFAULT_DECIMATION_MATRIX;\n+        } else {\n+            decimationMatrix = parseDecimationMatrixFromPath(spec);\n+        }\n+        decimationMask = calculateDecimationMask(decimationMatrix);\n+        counts = composeDecimationCounts(decimationMask);\n+    }\n+\n+    /**\n+     * Returns the decimation mask for a given period and repeat length.\n+     * <p>\n+     *     If the mask of a STR locus and this mask have any bit in common, the locus should be decimated/filtered out.\n+     * </p>\n+     * @param period\n+     * @param repeats\n+     * @return\n+     */\n+    public long decimationMask(final int period, final int repeats) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIwNjcxNg==", "bodyText": "Unused?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495206716", "createdAt": "2020-09-25T20:06:51Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/STRDecimationTable.java", "diffHunk": "@@ -0,0 +1,235 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+\n+/**\n+ * Represents a decimation table.\n+ * <p>\n+ * A decimation table (DT) has one entry for each possible period and repeat length.\n+ * Each entry's value indicates the lowest \"mask\" bit (0-based) that is allowed to be set\n+ * for sampled sites.\n+ * </p>\n+ * <p>\n+ *     The \"mask\" of a site is roughly the number of sites with same period and repeat-length that\n+ *     have been noted before this one; actually some number in the series are skipped at the beginning\n+ *     of each contig (as many as the contig 0-based index in the dictionary).\n+ * </p>\n+ * <p>\n+ *     Thus if the DT entry for a period and repeat-length combination is 0, all sites are sampled, if it is 1\n+ *     every second site is discarded, if it 10, one every 1024 sites is sampled and the rest discarded.\n+ * </p>\n+ */\n+public class STRDecimationTable {\n+\n+    /**\n+     * Default decimation matrix. Missing entries are assumed to be 0.\n+     */\n+    private static final int[][] DEFAULT_DECIMATION_MATRIX = new int[][] {\n+            {0}, // 0, 0, 0, 0, 0, 0, 0, 0 ...\n+            {0, 10, 10, 9, 8, 7, 5, 3, 1, 0},\n+            {0, 0, 9, 6, 3, 0}, // 0, 0, 0 ...\n+            {0, 0, 8, 4, 1, 0},\n+            {0, 0, 6, 0},\n+            {0, 0, 5, 0},\n+            {0, 0, 4, 0},\n+            {0, 0, 1, 0},\n+            {0}};\n+\n+    /**\n+     * String that represents the special DT with no decimation ... ie all entries set to 0 and every site is kept.\n+     */\n+    public static final String NO_DECIMATION_STR = \"NONE\";\n+\n+    /**\n+     * Strings that represents the default decimation table.\n+     */\n+    public static final String DEFAULT_DECIMATION_STR = \"DEFAULT\";\n+\n+    /**\n+     * The default decimation table.\n+     */\n+    public static final STRDecimationTable DEFAULT = new STRDecimationTable(DEFAULT_DECIMATION_STR);\n+\n+    /**\n+     * The no-decimating table.\n+     */\n+    public static final STRDecimationTable NONE = new STRDecimationTable(NO_DECIMATION_STR);\n+\n+    private final int[][] decimationMatrix;\n+    private final long[][] decimationMask;\n+\n+    private final long[][] counts;\n+\n+    /**\n+     * Creates a decimation table from its string representation. It might be a special table (NONE, DEFAULT) or the\n+     * path to a file that contains the table.\n+     */\n+    public STRDecimationTable(final String spec) {\n+        Utils.nonNull(spec);\n+        if (spec.equalsIgnoreCase(NO_DECIMATION_STR)) {\n+            decimationMatrix = new int[][] {{0}};\n+        } else if (spec.equalsIgnoreCase(DEFAULT_DECIMATION_STR)) {\n+            decimationMatrix = DEFAULT_DECIMATION_MATRIX;\n+        } else {\n+            decimationMatrix = parseDecimationMatrixFromPath(spec);\n+        }\n+        decimationMask = calculateDecimationMask(decimationMatrix);\n+        counts = composeDecimationCounts(decimationMask);\n+    }\n+\n+    /**\n+     * Returns the decimation mask for a given period and repeat length.\n+     * <p>\n+     *     If the mask of a STR locus and this mask have any bit in common, the locus should be decimated/filtered out.\n+     * </p>\n+     * @param period\n+     * @param repeats\n+     * @return\n+     */\n+    public long decimationMask(final int period, final int repeats) {\n+        ParamUtils.isPositive(period, \"period must be positive\");\n+        ParamUtils.isPositive(repeats, \"repeat length must be positive\");\n+\n+        return (period < decimationMask.length &&\n+                  repeats < decimationMatrix[period].length)\n+                ? decimationMatrix[period][repeats]\n+                : 0;\n+    }\n+\n+    private long[][] composeDecimationCounts(final long[][] decimationMask) {\n+        final long[][] result = new long[decimationMask.length][];\n+        for (int i = 0; i < result.length; i++) {\n+            result[i] = new long[decimationMask[i].length];\n+        }\n+        return result;\n+    }\n+\n+    private static int[][] parseDecimationMatrixFromPath(String spec) {\n+        try (final BufferedReader reader = new BufferedReader(IOUtils.makeReaderMaybeGzipped(Paths.get(spec)))) {\n+            final String[][] values = reader.lines()\n+                    .filter(str -> !str.startsWith(\"#\") && !str.trim().isEmpty())\n+                    .map(str -> Arrays.stream(str.split(\"\\\\s+\"))\n+                               //.mapToDouble(Double::parseDouble)\n+                               .toArray(String[]::new))\n+                    .toArray(String[][]::new);\n+            return coerceStringMatrix(values, spec);\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotReadInputFile(spec, ex);\n+        } catch (final NumberFormatException ex){\n+            throw new UserException.BadInput(String.format(\"input decimation file %s contains non-valid values: %s\", spec, ex.getMessage()));\n+        }\n+    }\n+\n+    public void print(final PrintWriter writer) {\n+        Utils.nonNull(writer);\n+        for (final int[] row : decimationMatrix) {\n+            writer.println(Utils.join(\"\\t\", row));\n+        }\n+        writer.flush();\n+    }\n+\n+    private static int[][] coerceStringMatrix(final String[][] values, final String path) {\n+        Utils.nonNull(values);\n+        if (values.length == 0) {\n+            LogManager.getLogger(STRDecimationTable.class)\n+                    .warn(\"Decimation matrix path provided does not seem to contain any values, we will proceed without any decimation\");\n+            return new int[0][];\n+        } else {\n+            int totalValues = 0;\n+            final int[][] result = new int[values.length][];\n+            for (int i = 0; i < values.length; i++) {\n+                final String[] row = values[i];\n+                final int[] rowValues = new int[values[i].length];\n+                for (int j = 0; j < row.length; j++) {\n+                    final String str = row[j];\n+                    final int value;\n+                    try {\n+                        value = Integer.parseInt(str);\n+                    } catch (final NumberFormatException ex) {\n+                        throw badDecimationValueException(str, path, i, j, \"not a valid double literal\");\n+                    }\n+                    if (value < 0) {\n+                        throw badDecimationValueException(str, path, i, j, \"negatives are not allowed\");\n+                    } else if (Double.isNaN(value)) {\n+                        throw badDecimationValueException(str, path, i, j, \"NaN are not allowed\");\n+                    } else if (!Double.isFinite(value)) {\n+                        throw badDecimationValueException(str, path, i, j, \"must be finite\");\n+                    }\n+                    rowValues[j] = value;\n+                    totalValues++;\n+                }\n+                result[i] = rowValues;\n+            }\n+            if (totalValues == 0) {\n+                throw new UserException.BadInput(\"the input decimation matrix does contain any values:\" + path);\n+            }\n+            return result;\n+        }\n+    }\n+\n+    private static RuntimeException badDecimationValueException(final String str, final String path, final int i, final int j,\n+                                                                final String details) {\n+        throw new UserException.BadInput(String.format(\"bad decimation value found in %s for period and repeats (%d, %d) with string (%s)%s\",\n+                path, i, j, str, details == null || details.isEmpty()? \"\": \": \" + details));\n+    }\n+\n+    private static long[][] calculateDecimationMask(final int[][] decimationMatrix) {\n+        Utils.nonNull(decimationMatrix);\n+        final long[][] result = new long[decimationMatrix.length][];\n+        for (int i = 0; i < result.length; i++) {\n+            final int[] row = decimationMatrix[i];\n+            result[i] = new long[row.length];\n+            for (int j = 0; j < row.length; j++) {\n+                result[i][j] = (1 << row[j]) - 1;\n+            }\n+        }\n+        return result;\n+    }\n+\n+    public long mask(final int period, final int repeats) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIwNzE1OA==", "bodyText": "I would replace the term mask here, that name implies it corresponds to a bitmask based on binary counts but that evidently is not the case but rather simply a count being used for a modulus calculation. I would suggest renaming it to something more clear.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495207158", "createdAt": "2020-09-25T20:08:04Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/STRDecimationTable.java", "diffHunk": "@@ -0,0 +1,235 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+\n+/**\n+ * Represents a decimation table.\n+ * <p>\n+ * A decimation table (DT) has one entry for each possible period and repeat length.\n+ * Each entry's value indicates the lowest \"mask\" bit (0-based) that is allowed to be set\n+ * for sampled sites.\n+ * </p>\n+ * <p>\n+ *     The \"mask\" of a site is roughly the number of sites with same period and repeat-length that", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIwODI1MQ==", "bodyText": "Perhaps this was mangled in the rebase? This doesn't seem to actually be used anywhere and this looks like perhaps its an older version of this code? Can you check to make sure you didn't mean to plug this in somewhere and remove it", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495208251", "createdAt": "2020-09-25T20:10:44Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/afcalc/AlleleFrequencyCalculator.java", "diffHunk": "@@ -74,6 +78,36 @@ public static AlleleFrequencyCalculator makeCalculator(DragstrParams dragstrParm\n         return MathUtils.normalizeLog10(log10Posteriors);\n     }\n \n+    public double[] getPriorFrequencies(final AlleleList<Allele> alleleList) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIwODc5OA==", "bodyText": "I suppose since we have the concept of an AutoCloseableCollection I can't really object to this object...", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495208798", "createdAt": "2020-09-25T20:12:00Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/AutoCloseableReference.java", "diffHunk": "@@ -0,0 +1,103 @@\n+package org.broadinstitute.hellbender.utils;\n+\n+import java.util.function.Consumer;\n+\n+/**\n+ * Reference to another object that perform some action when closed.\n+ *\n+ * <p>The object referred, the <em>subject</em>, may or may not be itself closeable and close invoke on this reference\n+ * does not necessary mean that the subject would also be closed; that depends on whether this reference closing action\n+ * actually propagates it to the referred object.\n+ * </p>\n+ * <p>This is useful for example in the following sceneraios:\n+ *  <ul>\n+ *      <li>\n+ *          the referred object does not or cannot extends/implement {@link AutoCloseable} but we want to use\n+ *          in <i>try-with-resources</i>.\n+ *      </li>\n+ *      <li>\n+ *          the referred object is auto-closeable but its close action is not the one we want to trigger\n+ *          at the end of a <i>try-with-resource</i>; is a totally different one or needs to be augmented.\n+ *      </li>\n+ *  </ul>\n+ * </p>\n+ * <p>\n+ *     This class is not thread-safe.\n+ * </p>\n+ * @param <T> the subject's type.\n+ */\n+public abstract class AutoCloseableReference<T> implements AutoCloseable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIxMTg5OA==", "bodyText": "Can you explain to me why this class cannot also be used for the DragstrLocus objects? Those objects seem to own a lot of the machinery to write/index/binary encode themselves but they arefundamentally just a short table expression for small locus objects with some attached data.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495211898", "createdAt": "2020-09-25T20:19:42Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/BinaryTableReader.java", "diffHunk": "@@ -1,37 +1,52 @@\n package org.broadinstitute.hellbender.utils;\n \n+import org.apache.commons.io.input.NullInputStream;\n import org.broadinstitute.hellbender.exceptions.UserException;\n-import org.broadinstitute.hellbender.utils.Utils;\n \n-import javax.swing.*;\n import java.io.*;\n import java.util.*;\n+import java.util.stream.Collectors;\n import java.util.stream.Stream;\n import java.util.stream.StreamSupport;\n \n /**\n- * Abstract base class to read for binary formatted table files.\n- * <p>\n- *     You need to provide a procedure to decode the record from the data input stream overriding {@link #readRecord(DataInput)},\n- *     and you are good to go.\n- * </p>\n- * @param <R> the record/row type.\n+ * Abstract base class for readers of table with records stored in binary.\n+ * @param <R> record type.\n  */\n public abstract class BinaryTableReader<R> implements AutoCloseable, Iterator<R> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIxMzkzMA==", "bodyText": "As somewhat of a stylisitc comment, I think we should think about making an interface along the lines of \"dragstrLocusFilter\" or something like that. I say this because I foresee it being necessary to come up with a more robust solution to sampling the stre occurrences than expecting users to guess at reasonable factors of 2 by which to subsample. Ideally we should be able to do the subsampling dynamically to a target number of occurrences. This approach won't work if anybody tires to use anything other than the human reference it seems to me.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495213930", "createdAt": "2020-09-25T20:24:36Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/dragstr/STRDecimationTable.java", "diffHunk": "@@ -0,0 +1,235 @@\n+package org.broadinstitute.hellbender.tools.dragstr;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+\n+/**\n+ * Represents a decimation table.\n+ * <p>\n+ * A decimation table (DT) has one entry for each possible period and repeat length.\n+ * Each entry's value indicates the lowest \"mask\" bit (0-based) that is allowed to be set\n+ * for sampled sites.\n+ * </p>\n+ * <p>\n+ *     The \"mask\" of a site is roughly the number of sites with same period and repeat-length that\n+ *     have been noted before this one; actually some number in the series are skipped at the beginning\n+ *     of each contig (as many as the contig 0-based index in the dictionary).\n+ * </p>\n+ * <p>\n+ *     Thus if the DT entry for a period and repeat-length combination is 0, all sites are sampled, if it is 1\n+ *     every second site is discarded, if it 10, one every 1024 sites is sampled and the rest discarded.\n+ * </p>\n+ */\n+public class STRDecimationTable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIyNTI5Mg==", "bodyText": "I think these methods are unnecessary and duplicated if i'm not mistaken. If you look at the method LoadIntervals() in the same class it is already doing thework of sorting and merging the provided intervals and you can simply change the intervalSetRule/padding that you feed to that method. I think you should use that on the intervals from IntervalArgumentColleciton and remove this duplicated method", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495225292", "createdAt": "2020-09-25T20:52:51Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/IntervalUtils.java", "diffHunk": "@@ -1559,4 +1560,63 @@ public static boolean isReciprocalOverlap(final SimpleInterval interval1, final\n                 (interval1.intersect(interval2).size() >= (interval2.size() * reciprocalOverlapThreshold)) &&\n                 (interval2.intersect(interval1).size() >= (interval1.size() * reciprocalOverlapThreshold));\n     }\n+\n+    /**\n+     * Given a collection of {@link SimpleInterval} instances returns a map where the key is the enclosing contig name and the value is the\n+     * list of the intervals  on that contig that result from sorting and then merging those that are contiguous or overlap.\n+     *\n+     * <p>\n+     *     The output collections (map and lists) are mutable, and any modification on them should not change the inputs as a side effect.\n+     * </p>\n+     *\n+     * @param input input collection of lacatables, may contain duplicates.\n+     * @param <L> the locatable type.\n+     * @throws IllegalArgumentException if input is {@code null}.\n+     * @return never {@code null}, but perhaps an empty map. It is guarantee that no value in the map is an empty list upon return.\n+     */\n+    public static <L extends Locatable> Map<String, List<SimpleInterval>> sortAndMergeOverlappingIntervals(final Collection<SimpleInterval> input) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIyNjM3MA==", "bodyText": "It looks like for these methods truncate() flip() and deleteFileTree() you removed references to the methods but forgot to delete these methods right here. You should remove them from this utils class.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495226370", "createdAt": "2020-09-25T20:55:41Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/Utils.java", "diffHunk": "@@ -1444,4 +1460,97 @@ public static String formattedRatio(final long num, final long denom) {\n         }\n         return patterns;\n     }\n+\n+    /**\n+     * Removes the last portion of a list so that it has a new size of at\n+     * most a given number of elements.\n+     * @param list the list to modify.\n+     * @param maxLength the intended maximum length for the list.\n+     */\n+    public static void truncate(final List<?> list, final int maxLength) {\n+        Utils.nonNull(list);\n+        ParamUtils.isPositiveOrZero(maxLength, \"new maximum length\");\n+        if (maxLength == 0) { // special quicker case when ml == 0.\n+            list.clear();\n+        } else {\n+            final int length = list.size();\n+            if (maxLength < length) {  // if not we are done.\n+                list.subList(maxLength, length).clear();\n+            }\n+        }\n+    }\n+\n+    public static void flip(final byte[] array, final int from, final int to) {\n+        for (int i = from, j = to -1; i < j; i++, j--) {\n+            final byte b = array[i];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIyOTM4NA==", "bodyText": "This looks to be unused now?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495229384", "createdAt": "2020-09-25T21:03:40Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/dragstr/DragstrParams.java", "diffHunk": "@@ -0,0 +1,478 @@\n+package org.broadinstitute.hellbender.utils.dragstr;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.dragstr.DragstrHyperParameters;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Holds the values of the DRAGstr model parameters for different combinations of repeat unit length (period) and\n+ * number of repeat units.\n+ * <p>\n+ *     This class is immutable.\n+ * </p>\n+ * <h3>Parameter matrices</h3>\n+ * <p>\n+ *     The DRAGstr model are composed of three matrices with the same dimensions (period x repeat-length):\n+ *     <dl>\n+ *         <dt>GOP</dt>\n+ *         <dd>gap-opening-penalty table where we store the Phred scale score that we use for pairhmm match to insert or\n+ *         delete transitions</dd>.\n+ *         <dt>GCP</dt>\n+ *         <dd>gap-continuation-penalty table where we store the Phred scale score that we use for pair-hmm insert to insert, and\n+ *             deletion to deletion\n+ *             </dd>.\n+ *         <dt>API</dt>\n+ *         <dd>adjusted prior probability of an indel occurring at a position in the reference with an STR.</dd>\n+ *     </dl>\n+ * </p>\n+ * <h3>Text file format</h3>\n+ * If P is the maximum period length and L is the maximum repeat-length:\n+ * <p>\n+ *     <pre>\n+ *         ############################\n+ *         # DragstrParams\n+ *         # -------------------------\n+ *         # annotation1 = value1\n+ *         # annotation2 = value2\n+ *         # ...\n+ *         ###########################\n+ *              1      2     3     4 ...     L\n+ *         GOP:\n+ *          gop11  gop12 gop13 gop14 ... gop1L\n+ *          gop21  gop22 ...\n+ *          ...\n+ *          gopP1  gopP2 ...             gopPL\n+ *         GCP:\n+ *          gcp11  gcp12 gcp13 ...\n+ *          ...\n+ *          gcpP1  apiP2 ...     ...     apiPL\n+ *         API:\n+ *          api11  api12 ...\n+ *          ..\n+ *          apiP1  apiP2 ...             apiPL\n+ *     </pre>\n+ * </p>\n+ * <p>\n+ *     Currently there is not a fix/standard set of annotations but is just a mechanism to add some additional\n+ *     metadata that might be useful.\n+ * </p>\n+ */\n+public final class DragstrParams {\n+\n+    public static final String GOP_TABLE_NAME = \"GOP\";\n+    public static final String GCP_TABLE_NAME = \"GCP\";\n+    public static final String API_TABLE_NAME = \"API\";\n+\n+    private static int LINE_BUILDER_BUFFER_SIZE = 1024;\n+\n+    /**\n+     * Gap-Open-Penalty default Phred scores.\n+     * <p>\n+     *     Each row represent a different period (index 0 is period length == 1, index 1 is period length == 2) to the default maximum of {@value DragstrHyperParameters#DEFAULT_MAX_PERIOD}.\n+     * </p>\n+     * <p>\n+     *     Then each element (column) in that row is the value for the ith repeat length in units (0-based) up to the default maximum of {@value DragstrHyperParameters#DEFAULT_MAX_REPEAT_LENGTH}.\n+     * </p>\n+     *\n+     * <p>\n+     *     This values were copied from DRAGstr documentation provided by Illumina.\n+     *     It is unknown to us how they came out with this values nor we know of any way to generate them\n+     *     on the fly.\n+     * </p>\n+     */\n+    //@formatter:off -- this disables IntelliJ code reformatting if preferences are set; Preferences > Editor > Code Style > Formatter Control\n+    private static final double[][] DEFAULT_GOP = { /* GOP */\n+     //   Repeat Length\n+     //             1,     2,     3,     4,     5,     6,     7,     8,     9,    10,    11,    12,    13,    14,    15,    16,    17,    18,    19,    20\n+     // Period\n+     /* 1 */   {45.00, 45.00, 45.00, 45.00, 45.00, 45.00, 40.50, 33.50, 28.00, 24.00, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75, 21.75},\n+     /* 2 */   {39.50, 39.50, 39.50, 39.50, 36.00, 30.00, 27.25, 25.00, 24.25, 24.75, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.25, 26.75},\n+     /* 3 */   {38.50, 41.00, 41.00, 41.00, 41.00, 37.50, 35.25, 34.75, 34.75, 33.25, 33.25, 33.25, 32.50, 30.75, 28.50, 29.00, 29.00, 29.00, 29.00, 29.00},\n+     /* 4 */   {37.50, 39.00, 39.00, 37.75, 34.00, 34.00, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 30.25, 31.75, 31.75, 31.75, 31.75, 31.75},\n+     /* 5 */   {37.00, 40.00, 40.00, 40.00, 36.00, 35.00, 24.50, 24.50, 24.50, 24.50, 22.50, 22.50, 22.50, 23.50, 23.50, 23.50, 23.50, 23.50, 23.50, 23.50},\n+     /* 6 */   {36.25, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00, 40.00},\n+     /* 7 */   {36.00, 40.50, 40.50, 40.50, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75, 20.75},\n+     /* 8 */   {36.25, 39.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75, 32.75}};\n+     //@formatter:on\n+\n+    /**\n+     * API default Phred scores.\n+     * <p>\n+     *     Each row represent a different period (index 0 is period length == 1, index 1 is period length == 2) to the default maximum of {@value DragstrHyperParameters#DEFAULT_MAX_PERIOD}.\n+     * </p>\n+     * <p>\n+     *     Then each element (column) in that row is the value for the ith repeat length in units (0-based) up to the default maximum of {@value DragstrHyperParameters#DEFAULT_MAX_REPEAT_LENGTH}.\n+     * </p>\n+     *\n+     * <p>\n+     *     This values were copied from DRAGstr documentation provided by Illumina.\n+     *     It is unknown to us how they came out with this values nor we know of any way to generate them\n+     *     on the fly.\n+     * </p>\n+     */\n+    //@formatter:off -- this disables IntelliJ code reformatting if preferences are set; Preferences > Editor > Code Style > Formatter Control\n+    private static final double[][] DEFAULT_API = { /* API */\n+     //   Repeat Length\n+     //           1,     2,     3,     4,     5,     6,     7,     8,     9,    10,    11,    12,    13,    14,    15,    16,    17,    18,    19,    20\n+     // Period\n+     /* 1 */ {39.00, 39.00, 37.00, 35.00, 32.00, 26.00, 20.00, 16.00, 12.00, 10.00,  8.00,  7.00,  7.00,  6.00,  6.00,  5.00,  5.00,  4.00,  4.00,  4.00},\n+     /* 2 */ {30.00, 30.00, 29.00, 22.00, 17.00, 14.00, 11.00,  8.00,  6.00,  5.00,  4.00,  4.00,  3.00,  3.00,  3.00,  3.00,  3.00,  3.00,  2.00,  2.00},\n+     /* 3 */ {27.00, 27.00, 25.00, 18.00, 14.00, 12.00,  9.00,  7.00,  5.00,  4.00,  3.00,  3.00,  3.00,  3.00,  2.00,  2.00,  2.00,  2.00,  2.00,  2.00},\n+     /* 4 */ {27.00, 27.00, 18.00,  9.00,  9.00,  9.00,  9.00,  3.00,  3.00,  3.00,  3.00,  3.00,  2.00,  2.00,  2.00,  2.00,  2.00,  2.00,  2.00,  2.00},\n+     /* 5 */ {29.00, 29.00, 18.00,  8.00,  8.00,  8.00,  4.00,  3.00,  3.00,  3.00,  2.00,  2.00,  2.00,  2.00,  2.00,  2.00,  2.00,  2.00,  2.00,  2.00},\n+     /* 6 */ {25.00, 25.00, 10.00, 10.00, 10.00,  4.00,  3.00,  3.00,  3.00,  3.00,  3.00,  3.00,  3.00,  3.00,  3.00,  3.00,  3.00,  3.00,  3.00,  3.00},\n+     /* 7 */ {21.00, 21.00, 11.00, 11.00,  5.00,  5.00,  5.00,  5.00,  5.00,  5.00,  5.00,  5.00,  5.00,  5.00,  5.00,  5.00,  5.00,  5.00,  5.00,  5.00},\n+     /* 8 */ {18.00, 18.00, 10.00,  6.00,  4.00,  4.00,  4.00,  4.00,  4.00,  4.00,  4.00,  4.00,  4.00,  4.00,  4.00,  4.00,  4.00,  4.00,  4.00,  4.00}};\n+    //@formatter:on\n+\n+    /**\n+     * Gap-Continuation-Penalty (GCP)\n+     * <p>\n+     *     Each row represent a different period (index 0 is period length == 1, index 1 is period length == 2) to the default maximum of {@value DragstrHyperParameters#DEFAULT_MAX_PERIOD}.\n+     * </p>\n+     * <p>\n+     *     Then each element (column) in that row is the value for the ith repeat length in units (0-based) up to the default maximum of {@value DragstrHyperParameters#DEFAULT_MAX_REPEAT_LENGTH}.\n+     * </p>\n+     *\n+     * <p>\n+     *    This value is not estimated and it rather has fixed values. They only depend on the period length.\n+     *    With a very simple formula of 10.0 / period. Since DRAGEN tables are approximated just the second\n+     *    decimal. We do that below using: {@code round(10 * 100 / period) / 100}\n+     *    It is unknown to us how they came out with this values nor we know of any way to generate them\n+     *    on the fly.\n+     * </p>\n+     * <p>\n+     *     As a result the first row (period 1, index 0) is just an array of 10.0, the second row (period 2, index 1) is filled with 5.0,\n+     *     the third 3.33, the fourth 2.25 and so forth.\n+     * </p>\n+     */\n+    private static final double[][] DEFAULT_GCP = IntStream.rangeClosed(1, DragstrHyperParameters.DEFAULT_MAX_PERIOD)\n+            .mapToDouble(period -> Math.round(1000.0 / period) / 100.0) // This way we end-up with two decimal precision.\n+            .mapToObj(value -> MathUtils.doubles(DragstrHyperParameters.DEFAULT_MAX_REPEAT_LENGTH, value))\n+            .toArray(double[][]::new);\n+\n+    /**\n+     * Default parameters when there is not enough data for estimation.\n+     */\n+    public static final DragstrParams DEFAULT = new DragstrParams(\n+                   DragstrHyperParameters.DEFAULT_MAX_PERIOD,\n+                   DragstrHyperParameters.DEFAULT_MAX_REPEAT_LENGTH,\n+                   DEFAULT_GOP,\n+                   DEFAULT_GCP,\n+                   DEFAULT_API);\n+\n+    private final int maxPeriod;\n+    private final int maxRepeats;\n+    private final double[][] gop;\n+    private final double[][] gcp;\n+    private final double[][] api;\n+\n+    private Map<Object, AlleleFrequencyCalculator> afcs;\n+\n+    public DragstrParams(final String path) {\n+        this(openBufferedReader(path), path);\n+    }\n+\n+    private static BufferedReader openBufferedReader(String path) {\n+        try {\n+            return Files.newBufferedReader(Paths.get(path));\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotReadInputFile(path, ex);\n+        }\n+    }\n+\n+    public static DragstrParams of(final int maxPeriod, final int maxRepeats, final double[][] gop, final double[][] gcp, final double[][] api) {\n+        ParamUtils.isPositive(maxPeriod, \"max period must be a positive\");\n+        ParamUtils.isPositive(maxRepeats, \"max repeats must be a positive\");\n+        Utils.nonNull(gop, \"gop cannot be null\");\n+        Utils.nonNull(gcp, \"gcp cannot be null\");\n+        Utils.nonNull(api, \"api cannot be null\");\n+        Utils.validate(gop.length == maxPeriod, \"input gop length must match maxPeriod\");\n+        Utils.validate(gcp.length == maxPeriod, \"input gcp length must match maxPeriod\");\n+        Utils.validate(api.length == maxPeriod, \"input api length must match maxPeriod\");\n+        for (int i = 0; i < maxPeriod; i++) {\n+            final double[] gopRow = gop[i];\n+            final double[] gcpRow = gcp[i];\n+            final double[] apiRow = api[i];\n+            for (int j = 0; j < maxRepeats; j++) {\n+                Utils.validate(gopRow[j] >= 0 && Double.isFinite(gopRow[j]), \"bad gop value: \" + gopRow[j]);\n+                Utils.validate(gcpRow[j] >= 0 && Double.isFinite(gcpRow[j]), \"bad gcp value: \" + gcpRow[j]);\n+                Utils.validate(apiRow[j] >= 0 && Double.isFinite(apiRow[j]), \"bad api value: \" + apiRow[j]);\n+            }\n+        }\n+        return new DragstrParams(maxPeriod, maxRepeats, gop.clone(), gcp.clone(), api.clone());\n+    }\n+\n+    private BufferedWriter openBufferedWriter(final String path) {\n+        try {\n+            return Files.newBufferedWriter(Paths.get(path));\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(path, ex);\n+        }\n+    }\n+\n+    private DragstrParams(final BufferedReader reader, final String path) {\n+        try {\n+            String header;\n+            while ((header = reader.readLine()) != null) {\n+                if (!header.startsWith(\"#\")) {\n+                    break;\n+                }\n+            }\n+            if (header == null) {\n+                throw new UserException.BadInput(\"there is no content in the dragstr-params file \" + path);\n+            }\n+            final String[] headerParts = header.split(\"\\\\s+\");\n+            final int[] repeats = Arrays.stream(headerParts)\n+                    .filter(str -> !str.isEmpty())\n+                    .mapToInt(str -> {\n+                        try {\n+                            return Integer.parseInt(str);\n+                        } catch (final NumberFormatException ex) {\n+                            throw new UserException.BadInput(\"bad format for an integer\", ex);\n+                        }\n+                    })\n+                    .toArray();\n+            final int maxRepeats = repeats.length;\n+            for (int i = 0; i < repeats.length; i++) {\n+                if (repeats[i] != i + 1) {\n+                    throw new UserException.BadInput(\"the DRAGstr parameter file header line must contain integers starting at 1 \" + Arrays.toString(repeats));\n+                }\n+            }\n+            final Map<String, double[][]> tables = new HashMap<>();\n+            String line = reader.readLine();\n+            if (line == null) {\n+                throw new UserException.BadInput(\"end of table list before expected\");\n+            }\n+            String tableName = line.replaceAll(\":$\", \"\");\n+            List<String> tableLines = new ArrayList<>();\n+            while ((line = reader.readLine()) != null) {\n+                if (line.charAt(line.length() - 1) == ':') {\n+                    tables.put(tableName, linesToMatrix(tableLines, repeats.length));\n+                    tableName = line.replaceAll(\":$\", \"\");\n+                    tableLines.clear();\n+                } else {\n+                    tableLines.add(line);\n+                }\n+            }\n+            if (tableName.isEmpty()) {\n+                throw new UserException.BadInput(\"table with no name\");\n+            }\n+            tables.put(tableName, linesToMatrix(tableLines, repeats.length));\n+            final double[][] gopMatrix = mandatoryMatrix(tables, GOP_TABLE_NAME);\n+            final double[][] gcpMatrix = mandatoryMatrix(tables, GCP_TABLE_NAME);\n+            final double[][] apiMatrix = mandatoryMatrix(tables, API_TABLE_NAME);\n+            final int maxPeriod = gopMatrix.length;\n+            checkMatricesAreValid(maxPeriod, maxRepeats, gopMatrix, gcpMatrix, apiMatrix);\n+\n+            this.maxPeriod = maxPeriod;\n+            this.maxRepeats = maxRepeats;\n+            this.gop = gopMatrix;\n+            this.gcp = gcpMatrix;\n+            this.api = apiMatrix;\n+            this.afcs = new HashMap<>(maxPeriod * maxRepeats);\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotReadInputFile(path, ex);\n+        }\n+    }\n+\n+    /**\n+     * Dump the parameters in their text file form given the destination path.\n+     * @param path where to print the parameters file.\n+     */\n+    public void print(final String path) {\n+        try (final BufferedWriter writer = openBufferedWriter(path);\n+             final PrintWriter printWriter = new PrintWriter(writer)) {\n+            print(printWriter);\n+        } catch (final IOException ex) {\n+            throw new UserException.CouldNotCreateOutputFile(path, ex);\n+        }\n+    }\n+\n+    /**\n+     * Dump the parameters in their text file form given the sink print writer.\n+     * @param printWriter the sink for the dump.\n+     */\n+    public void print(final PrintWriter printWriter) {\n+        final StringBuilder lineBuilder = new StringBuilder(LINE_BUILDER_BUFFER_SIZE);\n+        lineBuilder.append(String.format(\"%5s\", \"1\"));\n+        for (int i = 2; i <= maxRepeats; i++) {\n+            lineBuilder.append(\"  \");\n+            lineBuilder.append(String.format(\"%5s\", i));\n+        }\n+        printWriter.println(lineBuilder.toString());\n+        printTable(printWriter, lineBuilder, GOP_TABLE_NAME, gop);\n+        printTable(printWriter, lineBuilder, GCP_TABLE_NAME, gcp);\n+        printTable(printWriter, lineBuilder, API_TABLE_NAME, api);\n+    }\n+\n+    private void printTable(final PrintWriter printWriter, final StringBuilder lineBuilder, final String tableName, final double[][] table)  {\n+        printWriter.println(tableName + \":\");\n+        for (final double[] row : table) {\n+            lineBuilder.setLength(0); // i.e. clear the builder.\n+            lineBuilder.append(String.format(\"%5s\", String.format(\"%.2f\", row[0])));\n+            for (int i = 1; i < row.length; i++) {\n+                lineBuilder.append(\"  \");\n+                lineBuilder.append(String.format(\"%5s\", String.format(\"%.2f\", row[i])));\n+            }\n+            printWriter.println(lineBuilder.toString());\n+        }\n+    }\n+\n+    private DragstrParams(final int maxPeriod, final int maxRepeats, final double[][] gop, final double[][] gcp, final double[][] api) {\n+        this.maxPeriod = maxPeriod;\n+        this.maxRepeats = maxRepeats;\n+        this.gop = gop;\n+        this.gcp = gcp;\n+        this.api = api;\n+        this.afcs = new HashMap<>(maxPeriod * maxRepeats);\n+    }\n+\n+\n+    private static void checkMatricesAreValid(final int maxPeriod, final int maxRepeats, final double[][] gopMatrix,\n+                                              final double[][] gcpMatrix, final double[][] apiMatrix) {\n+        checkMatrixIsValid(maxPeriod, maxRepeats, gopMatrix, GOP_TABLE_NAME);\n+        checkMatrixIsValid(maxPeriod, maxRepeats, gcpMatrix, GCP_TABLE_NAME);\n+        checkMatrixIsValid(maxPeriod, maxRepeats, apiMatrix, API_TABLE_NAME);\n+    }\n+\n+    private static void checkMatrixIsValid(final int maxPeriod, final int maxRepeatLength, final double[][] matrix, final String name) {\n+        Utils.nonNull(matrix, \"the \" + name + \" matrix provided cannot be null\");\n+        if (matrix.length != maxPeriod) {\n+            throw new UserException.BadInput(\"the \" + name + \" matrix provided has the wrong number of rows\");\n+        } else {\n+            for (final double[] row : matrix) {\n+                Utils.nonNull(row, \"the \" + name + \" matrix contains null rows\");\n+                if (row.length != maxRepeatLength) {\n+                    throw new UserException.BadInput(\"the \" + name + \" matrix contains rows with length that does not match the max repeat length\");\n+                }\n+            }\n+        }\n+    }\n+\n+    private static double[][] mandatoryMatrix(final Map<String, double[][]> tableData, final String name) {\n+        final double[][] result = tableData.get(name);\n+        if (result == null) {\n+            throw new UserException.BadInput(\"missing matrix \" + name);\n+        } else {\n+            return result;\n+        }\n+    }\n+\n+    private static double[][] linesToMatrix(final List<String> lines, final int expectedNumberOfColumns) {\n+\n+        final double[][] result = new double[lines.size()][expectedNumberOfColumns];\n+        for (int i = 0; i < lines.size(); i++) {\n+            final String line = lines.get(i);\n+            final String[] parts = line.split(\"\\\\s+\");\n+\n+            if (parts.length < expectedNumberOfColumns) {\n+                throw new UserException.BadInput(\"line has the wrong number of columns\");\n+            }\n+            int k = 0;\n+            for (int j = 0; j < parts.length; j++) {\n+                if (!parts[j].isEmpty()) {\n+                    if (k >= expectedNumberOfColumns) {\n+                        throw new UserException.BadInput(\"line has the wrong number of columns\");\n+                    }\n+                    try {\n+                        final double val = Double.parseDouble(parts[j]);\n+                        if (Double.isNaN(val) || Double.isInfinite(val) || val < 0.0) {\n+                            throw new NullPointerException();\n+                        }\n+                        result[i][k++] = val;\n+                    } catch (final NumberFormatException ex) {\n+                        throw new UserException.BadInput(\n+                                String.format(\"score is not a valid Phred value (%d,%d) == %s\", i + 1, j + 1, parts[j]));\n+                    }\n+                }\n+            }\n+        }\n+        return result;\n+    }\n+\n+    private double lookup(final double[][] matrix, final int period, final int repeats) {\n+        ParamUtils.isPositive(period, \"period\");\n+        ParamUtils.isPositive(repeats, \"repeat length in units\");\n+        final int periodIndex = period < maxPeriod ? period - 1 : maxPeriod - 1;\n+        final int repeatIndex = repeats < maxRepeats ? repeats - 1 : maxRepeats - 1;\n+        return matrix[periodIndex][repeatIndex];\n+    }\n+\n+    /**\n+     * Returns the GOP for a specific period and repeat length in Phred scale.\n+     * @param period the query period/unit length.\n+     * @param repeats the query repeat length\n+     * @return 0 or greater.\n+     */\n+    public double gop(final int period, final int repeats) {\n+        return lookup(gop, period, repeats);\n+    }\n+\n+    /**\n+     * Returns the GCP for a specific period and repeat length in Phred scale.\n+     * @param period the query period/unit length.\n+     * @param repeats the query repeat length\n+     * @return 0 or greater.\n+     */\n+    public double gcp(final int period, final int repeats) {\n+        return lookup(gcp, period, repeats);\n+    }\n+\n+    /**\n+     * Returns the API for a specific period and repeat length in Phred scale.\n+     * @param period the query period/unit length.\n+     * @param repeats the query repeat length\n+     * @return 0 or greater.\n+     */\n+    public double api(final int period, final int repeats) {\n+        return lookup(api, period, repeats);\n+    }\n+\n+    /**\n+     * Return the maximum period this parameter collection has specified values for.\n+     * @return 1 or greater.\n+     */\n+    public int maximumPeriod() {\n+        return maxPeriod;\n+    }\n+\n+    /**\n+     * Return the maximum repeat length this parameter collection has specified values for.\n+     * @return 1 or greater.\n+     */\n+    public int maximumRepeats() {\n+        return maxRepeats;\n+    }\n+\n+    public int maximumLengthInBasePairs() {\n+        return maxPeriod * maxRepeats;\n+    }\n+\n+    /**\n+     * Return an AFC based on this DRAGstr parameters and context information on the targeted STR.\n+     *\n+     * @param period the target STR period length.\n+     * @param repeat the target STR repeat length.\n+     * @param ploidy the ploidy at that STR site.\n+     * @param snpHet the snp Heterozygosity at that site.\n+     * @param scale the scale, the weight of the prior.\n+     * @return never {@code null}.\n+     */\n+    public AlleleFrequencyCalculator getAFCalculator(final int period, final int repeat, final int ploidy, final double snpHet, final double scale) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 474}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIzNDQzMQ==", "bodyText": "This is a stylistic comment but this class is essentially a codec that handles both writing and reading of these objects from a table in that is just 3 2d arrays written out. That is somewhat obfuscated by the structure of this class. I would advocate pulling out a dragstrTableCodec that this classes constructor/writer both call out to.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495234431", "createdAt": "2020-09-25T21:12:59Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/dragstr/DragstrParams.java", "diffHunk": "@@ -0,0 +1,478 @@\n+package org.broadinstitute.hellbender.utils.dragstr;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.dragstr.DragstrHyperParameters;\n+import org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.io.BufferedReader;\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.*;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Holds the values of the DRAGstr model parameters for different combinations of repeat unit length (period) and\n+ * number of repeat units.\n+ * <p>\n+ *     This class is immutable.\n+ * </p>\n+ * <h3>Parameter matrices</h3>\n+ * <p>\n+ *     The DRAGstr model are composed of three matrices with the same dimensions (period x repeat-length):\n+ *     <dl>\n+ *         <dt>GOP</dt>\n+ *         <dd>gap-opening-penalty table where we store the Phred scale score that we use for pairhmm match to insert or\n+ *         delete transitions</dd>.\n+ *         <dt>GCP</dt>\n+ *         <dd>gap-continuation-penalty table where we store the Phred scale score that we use for pair-hmm insert to insert, and\n+ *             deletion to deletion\n+ *             </dd>.\n+ *         <dt>API</dt>\n+ *         <dd>adjusted prior probability of an indel occurring at a position in the reference with an STR.</dd>\n+ *     </dl>\n+ * </p>\n+ * <h3>Text file format</h3>\n+ * If P is the maximum period length and L is the maximum repeat-length:\n+ * <p>\n+ *     <pre>\n+ *         ############################\n+ *         # DragstrParams\n+ *         # -------------------------\n+ *         # annotation1 = value1\n+ *         # annotation2 = value2\n+ *         # ...\n+ *         ###########################\n+ *              1      2     3     4 ...     L\n+ *         GOP:\n+ *          gop11  gop12 gop13 gop14 ... gop1L\n+ *          gop21  gop22 ...\n+ *          ...\n+ *          gopP1  gopP2 ...             gopPL\n+ *         GCP:\n+ *          gcp11  gcp12 gcp13 ...\n+ *          ...\n+ *          gcpP1  apiP2 ...     ...     apiPL\n+ *         API:\n+ *          api11  api12 ...\n+ *          ..\n+ *          apiP1  apiP2 ...             apiPL\n+ *     </pre>\n+ * </p>\n+ * <p>\n+ *     Currently there is not a fix/standard set of annotations but is just a mechanism to add some additional\n+ *     metadata that might be useful.\n+ * </p>\n+ */\n+public final class DragstrParams {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIzODQ4Mg==", "bodyText": "Looking at this class now. It looks like almost none of these methods are being used anymore? It seems that you intended to rename and push all of this code into the DragstrReferenceAnalyzer class which seems tobe doing all of the work this class used to do about computing the str presence for a given bit of sequence. Perhaps you mean to delete this class and lost it in the rebase?", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495238482", "createdAt": "2020-09-25T21:18:34Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/dragstr/DragstrUtils.java", "diffHunk": "@@ -0,0 +1,283 @@\n+package org.broadinstitute.hellbender.utils.dragstr;\n+\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.variantcontext.VariantContextBuilder;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFHeaderLineType;\n+import htsjdk.variant.vcf.VCFInfoHeaderLine;\n+import org.aeonbits.owner.util.Collections;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+\n+import java.util.Arrays;\n+import java.util.Collection;\n+\n+public class DragstrUtils {\n+\n+    public static final String DRAGSTRINFO_KEY = \"DRAGstrInfo\";\n+    public static final String DRAGSTRPARAMS_KEY = \"DRAGstrParams\";\n+\n+    public static STRSequenceAnalyzer repeatPeriodAndCounts(final int maxSequenceLength, final int maxPeriod) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIzOTM5MQ==", "bodyText": "As i have said elsewhere, i think this can become a feature reader which would open these sites files to be used for any number of other applications outside of the STR code.", "url": "https://github.com/broadinstitute/gatk/pull/6634#discussion_r495239391", "createdAt": "2020-09-25T21:19:59Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/dragstr/STRTableFile.java", "diffHunk": "@@ -0,0 +1,126 @@\n+package org.broadinstitute.hellbender.utils.dragstr;\n+\n+import com.google.common.io.Files;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceDictionaryCodec;\n+import htsjdk.samtools.util.BufferedLineReader;\n+import htsjdk.samtools.util.LineReader;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.io.output.NullWriter;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.dragstr.DragstrLocus;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.ZipUtils;\n+import org.broadinstitute.hellbender.tools.dragstr.STRDecimationTable;\n+\n+import java.io.*;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Class to create and access STR table file contents.\n+ */\n+public final class STRTableFile implements AutoCloseable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c7056099d879ee90113fc33565c1cacd23570d0"}, "originalPosition": 22}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a6e4c009439ec22411fc0c6781a788a48b84aea9", "author": {"user": {"login": "vruano", "name": "Valentin Ruano Rubio"}}, "url": "https://github.com/broadinstitute/gatk/commit/a6e4c009439ec22411fc0c6781a788a48b84aea9", "committedDate": "2020-09-16T14:01:29Z", "message": "Solves james'  comment on DragstrParam toString deficiency"}, "afterCommit": {"oid": "14123e20e9f5e3257b5181ec73f0995d622c5932", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/14123e20e9f5e3257b5181ec73f0995d622c5932", "committedDate": "2020-09-28T19:00:25Z", "message": "fixed a few dangling issues with the rebase"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bba784a2168c2cf2a7cd014dc0168b20fd3ea774", "author": {"user": {"login": "vruano", "name": "Valentin Ruano Rubio"}}, "url": "https://github.com/broadinstitute/gatk/commit/bba784a2168c2cf2a7cd014dc0168b20fd3ea774", "committedDate": "2020-10-08T16:36:50Z", "message": "Addresses the rest of the comment except for the ones that will go to future tickets"}, "afterCommit": {"oid": "1126cb5891689ca7751cef2b1d5a017d802be69e", "author": {"user": {"login": "vruano", "name": "Valentin Ruano Rubio"}}, "url": "https://github.com/broadinstitute/gatk/commit/1126cb5891689ca7751cef2b1d5a017d802be69e", "committedDate": "2020-10-08T19:50:58Z", "message": "Addresses the rest of the comment except for the ones that will go to future tickets"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "35a3cda7c23004d2bb614baa2425396c12a75aec", "author": {"user": {"login": "vruano", "name": "Valentin Ruano Rubio"}}, "url": "https://github.com/broadinstitute/gatk/commit/35a3cda7c23004d2bb614baa2425396c12a75aec", "committedDate": "2020-10-09T18:48:40Z", "message": "Fixed unit test issues"}, "afterCommit": {"oid": "7748f7269429e2e0a8784b35232f8d6934babbe4", "author": {"user": {"login": "vruano", "name": "Valentin Ruano Rubio"}}, "url": "https://github.com/broadinstitute/gatk/commit/7748f7269429e2e0a8784b35232f8d6934babbe4", "committedDate": "2020-10-09T19:02:14Z", "message": "Fixed unit test issues"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7748f7269429e2e0a8784b35232f8d6934babbe4", "author": {"user": {"login": "vruano", "name": "Valentin Ruano Rubio"}}, "url": "https://github.com/broadinstitute/gatk/commit/7748f7269429e2e0a8784b35232f8d6934babbe4", "committedDate": "2020-10-09T19:02:14Z", "message": "Fixed unit test issues"}, "afterCommit": {"oid": "96a7f7436b33911c412cd94f034badb4786e76f5", "author": {"user": {"login": "vruano", "name": "Valentin Ruano Rubio"}}, "url": "https://github.com/broadinstitute/gatk/commit/96a7f7436b33911c412cd94f034badb4786e76f5", "committedDate": "2020-10-09T19:05:39Z", "message": "Fixed unit test issues"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "61e78708d69e27ab3c770ba909cf494421372d1d", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/61e78708d69e27ab3c770ba909cf494421372d1d", "committedDate": "2020-10-13T20:21:44Z", "message": "Hopefully handling softclipped bases in the likelihoods sengine in m2 without breaks"}, "afterCommit": {"oid": "3a04bb5005595d64ccaf557424c06b02c4096aaa", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/3a04bb5005595d64ccaf557424c06b02c4096aaa", "committedDate": "2020-10-14T18:04:03Z", "message": "VERY IMPORTANT PLEASE DELETE THAT TEST"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de6a4bfe703c322b0b17774a07d1c6bc04f3b4df", "author": {"user": {"login": "vruano", "name": "Valentin Ruano Rubio"}}, "url": "https://github.com/broadinstitute/gatk/commit/de6a4bfe703c322b0b17774a07d1c6bc04f3b4df", "committedDate": "2020-10-15T18:21:05Z", "message": "DRAGstr first working approximation to matlab scripts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5afb0ecc6113a7e4e4ceb34956925000d84f244c", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/5afb0ecc6113a7e4e4ceb34956925000d84f244c", "committedDate": "2020-10-15T18:21:06Z", "message": "fist go at rebasing everything correctly, now have to deal with the buggy refactor code\n\nRevert \"Rewrote leftAlignIndels code -- it now always works, even for multiple indels (#6427)\"\n\nThis reverts commit 8612288881890345d949b40ead5796183c52c08d. Which brakes the softclip aware cigar building code and has a horrible bug.\n\nbringing modernity to this branch\n\nadded a consistency test for DRAGEN mode\n\na bunch of code cleanup\n\nmore cleanup and tests?\n\nAdded the matlab files as test\n\nStrengthened trust in SoftclipBases Behavior for overlap base quality adjustments\n\nFixing a crash in alleleliklehoods and adjusting bq annotation after the quality for overlapping bases change\n\nminor fix and expanded debug output\n\nremoving excess bases before hmm scoring seems to improve the resutlts at complex sites... the FRD issue is still relevant...\n\nFixed genotype posterior calculator to not honor the unphase permutation number factor\n\nmore bugfixes to read edge cases at sites with many low quality ends and low mapping quality combined\n\nincluding reads that only overlap in hardclips (despite the obvious concerns with that)\n\nfixing an unexpected error\n\nflipping\n\nhardclippingsoftclippedBases\n\ncreating a frankenbranch with a very intersting change involving realignment of reads and apparent DRAGEN behavior\n\nfixing a bug in 'getAllVariationEvents()' code\n\nflipping the values so it actually works\n\nChanged BQD feather end calculation to account for indels in the read relative to the reference\n\nreverted the non-realignment code\n\nfixing the horrible regression I just wrought on the BQD calculations...\n\ndisabling bad debug squack\n\nParameterizing the option to use original base qualities\n\nthreading the truly original alignment through for BQD/FRD to match the bad dragen behavior we are now aware of\n\ncleaning up the branch\n\nif you like me then you should have put a pin in it\n\nresponding to reveiw comments, (still havent fixed the debug streams)\n\ndeleting a test that was for local files\n\nfixing a silly bug\n\nfixing the logic yet one more time\n\nfixing an issue with the disable-spanning-event-genotyping code that was causing false positivles (or perhaps reasonable behavior)\n\njumping the gun on takutos fix because it was causing me trouble for this test\n\nrefactored the debug output to be something very simple that should/could/maybe is better handled by making a debug level in the debugger. Anyway this is fine for now and probably could be expanded to handle the other debug streams in HC\n\nadding a scary error message to deter people from using contamination downsampling with BQD/FRD (it should be an exception but somebody would complain and it really isn't a sound thing to do unless the disqualified reads are included but they can't be inherently because they don't have scores)\n\npartial val review response work checkpoint\n:\n\nresponded to most comments for my code in val PR review\n\nFixed a couple of rare bugs in DRAGEN port code.\nNow we can reads dragstr-params values out of gcloud storage directly.\n\ntaking the changes from valentin with the removed OQ tag\n\npulling out the --use-original-alignments-for-genotyping-overlap as if it were so many weeds in my garden.\n\nRevision changes thus far\n\nFix issues with how we assign STR period and repeat lenghts to positions on a read.\nAdded more robust testing using Illumina's Matlab script to generate use cases.\n\nImpose a maximum of 40 for the GOP in DRAGstr pairhmm as seem to be the case in DRAGEN.\n\nChanges to meet recommendation except for the param estimation part that is on a separate branch\n\nreverting a mistake where i replaced every every instance of the word 'exists' with 'isEnabled'\n\nUnifying changes with the refactoring of the ReadsDataSource class\n\nA few updates on the revision:\n\n* Solved a couple of self-evaluation javadoc PR comments.\n* Added a integration test for ComposeSTRTableFile based on the DRAGEN output on some of the mini references under src/test/resources/large\n* Fix an issue that made ComposeSTRTableFile quite slow with reference with a very large number of contigs (eg. funcotator gencode trascripts ~100_000).\n* Found and fixed a bug introduced during refactoring making EstimateDragstrParameters produce \"garbage\" if run in parallel.\n* Silience a tedious debug message in SAMReaderQueryingIterator that comes out too often when the traversal intervals are just a few. Now it requires\n          like a 1000+ intervals to produce that debug, might be still too low but it works in practice.\n\nSolves james'  comment on DragstrParam toString deficiency\n\nfixed a few dangling issues with the rebase\n\nadded the argument to end all arguments (and also completely remove the users ability to interact with our toolkit as they please)\n\nFixes bug reported by Cwhelan\n\nAddresses some of James' comments and fixes a bug in a unit test for DragstrLocus class\n\nFixes Dragstr Read Str analyzer failing unit test\n\nallowing nulls in the genotyper because its actually okay\n\nFixed a bug causing the wrong priors to be taken (indel's instead of snp's in snp sites)\nReverted a couple of changes in DragstrPairHMMInputScoreImputator to make it match better\nthe corresponding HC integration-tests\n\nfixing a test i broke\n\nadding a test for the easy dragen calling mode\n\nRoll forward some of the rolled back changes Dragstr PairHMM code:\n\n* Limit GOP to 40 max.\n* byte-round rather than byte-truncate gop, gcp scores\n\nI Address most outstanding review comments\n\nFix treatment of star alleles in use genotype posteriors for qual mode (#6856)\n\n* use genotype posteriors for qual spanning deletion fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e4d95c21db40bf091428f01bad22155607a255d", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/5e4d95c21db40bf091428f01bad22155607a255d", "committedDate": "2020-10-15T18:21:06Z", "message": "Revert \"Revert \"Rewrote leftAlignIndels code -- it now always works, even for multiple indels (#6427)\"\"\n\nThis reverts commit 3347761dea6b52a22f2423e68e93d9602e17fd8d. which hopefully works better now and will be somewhat functional"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9a9884bea279cb0179fef9d89183a70eb74db14d", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/9a9884bea279cb0179fef9d89183a70eb74db14d", "committedDate": "2020-10-15T18:21:06Z", "message": "updating the tests to the current results after Cwhelans fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "725ce660107d76d9cfe1200c73e7946b4fb34f6d", "author": {"user": {"login": "vruano", "name": "Valentin Ruano Rubio"}}, "url": "https://github.com/broadinstitute/gatk/commit/725ce660107d76d9cfe1200c73e7946b4fb34f6d", "committedDate": "2020-10-15T18:21:07Z", "message": "Addresses the rest of the comment except for the ones that will go to future tickets"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dd16db35b00c953c058e5ffc44aaba542ebbfbf4", "author": {"user": {"login": "vruano", "name": "Valentin Ruano Rubio"}}, "url": "https://github.com/broadinstitute/gatk/commit/dd16db35b00c953c058e5ffc44aaba542ebbfbf4", "committedDate": "2020-10-15T18:21:07Z", "message": "Fixed unit test issues"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d868cd98885785b89daf6db804459bbc8b76590c", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/d868cd98885785b89daf6db804459bbc8b76590c", "committedDate": "2020-10-15T18:21:07Z", "message": "Hopefully handling softclipped bases in the likelihoods sengine in m2 without breaks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6add9790856ec34154f33ef72ec06ec4877bdc36", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/6add9790856ec34154f33ef72ec06ec4877bdc36", "committedDate": "2020-10-15T18:21:07Z", "message": "VERY IMPORTANT PLEASE DELETE THAT TEST"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5ead1eefb14e034526198d90bec39c0fd0678612", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/5ead1eefb14e034526198d90bec39c0fd0678612", "committedDate": "2020-10-15T18:21:07Z", "message": "removing a dummy test"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "86ca471e94a83cc5ea730880f2277a6d309365fa", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/86ca471e94a83cc5ea730880f2277a6d309365fa", "committedDate": "2020-10-15T17:18:48Z", "message": "removing a dummy test"}, "afterCommit": {"oid": "5ead1eefb14e034526198d90bec39c0fd0678612", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/5ead1eefb14e034526198d90bec39c0fd0678612", "committedDate": "2020-10-15T18:21:07Z", "message": "removing a dummy test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "260b441bcab0c585130b08e02e99ea8f80573f68", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/260b441bcab0c585130b08e02e99ea8f80573f68", "committedDate": "2020-10-16T16:58:25Z", "message": "Fixed an edge case arising from locatable vs simpleinterval overlaps methods"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c350535d228b812cccc12a1756322a342a0ec765", "author": {"user": {"login": "vruano", "name": "Valentin Ruano Rubio"}}, "url": "https://github.com/broadinstitute/gatk/commit/c350535d228b812cccc12a1756322a342a0ec765", "committedDate": "2020-10-20T04:21:20Z", "message": "Fixed a OutOfMemError issue in EstimateDragstrParameters with 2g fixed heap size hitting a very deep (coverage wise) region\nInstead of increasing memory we solve the issue by reduce memory footprint.\nAlso fixed a cosmethic bug affecting the annotations/comments printed in the dragstr-params file."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "15657738bd1ae887566f36ef013af260c222a1cf", "author": {"user": {"login": "vruano", "name": "Valentin Ruano Rubio"}}, "url": "https://github.com/broadinstitute/gatk/commit/15657738bd1ae887566f36ef013af260c222a1cf", "committedDate": "2020-10-20T04:37:32Z", "message": "Changed the name of tool EstimateDragstrParameters to CalibrateDrastrModel"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f485616fb4724892093d422714f5cc02a54965f7", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/f485616fb4724892093d422714f5cc02a54965f7", "committedDate": "2020-10-20T18:07:01Z", "message": "adding a df commnad for this one test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8f07c4649a876d536e8762bad20be6d158b956b4", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/8f07c4649a876d536e8762bad20be6d158b956b4", "committedDate": "2020-10-20T19:10:58Z", "message": "fixing an issue?"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ae1f2248617e60557abd96bc67e399c257437b8d", "author": {"user": {"login": "jamesemery", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/ae1f2248617e60557abd96bc67e399c257437b8d", "committedDate": "2020-10-21T16:28:55Z", "message": "fixed an error that accidentally broke GenotypeGVCFs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE0ODg4NTU5", "url": "https://github.com/broadinstitute/gatk/pull/6634#pullrequestreview-514888559", "createdAt": "2020-10-22T16:02:38Z", "commit": {"oid": "ae1f2248617e60557abd96bc67e399c257437b8d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2623, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}