{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM1MjQ1MzYx", "number": 6660, "title": "Update for funcotator data sources", "bodyText": "Includes latest Gencode and an implicit fix for #6564.  Had to make some code changes for latest liftover Gencode data(v34 -> hg19).\nThe associated DS test release correctly annotates data on hg19 and hg38.\nLeft to do:\n\n Update data sources downloader.\n Update data source version validation code.\n\nCode updates:\n\nNow both hg19 and hg38 have the contig names translated to chr__\nAdded 'lncRNA' to GeneTranscriptType.\nAdded \"TAGENE\" gene tag.\nAdded the MANE_SELECT tag to FeatureTag.\nAdded the STOP_CODON_READTHROUGH tag to FeatureTag.\nUpdated the GTF versions that are parseable.\nFixed a parsing error with new versions of gencode and the remap\npositions (for liftover files).\nAdded test for indexing new lifted over gencode GTF.\nAdded Gencode_34 entries to MAF output map.\nMinor changes to FuncotatorIntegrationTest.java for code syntax.\nPointed data source downloader at new data sources URL.\nMinor updates to workflows to point at new data sources.\n\nScript updates:\n\nUpdated retrieval scripts for dbSNP and Gencode.\nAdded required field to gencode config file generation.\nNow gencode retrieval script enforces double hash comments at\ntop of gencode GTF files.\n\nBug Fixes:\nRemoving erroneous trailing tab in MAF file output.\n\nFixes #6693", "createdAt": "2020-06-16T14:13:48Z", "url": "https://github.com/broadinstitute/gatk/pull/6660", "merged": true, "mergeCommit": {"oid": "3ff2a5ac37c47955e8507e9e0ee1a59891cee542"}, "closed": true, "closedAt": "2020-09-10T21:55:31Z", "author": {"login": "jonn-smith"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc1kyK5gBqjM1NTQ4OTM5OTg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdHiB0kABqjM3NTE1MjY2Nzc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2e74cf4232cac1436057d0e708dcadca9ad45458", "author": {"user": {"login": "jonn-smith", "name": "Jonn Smith"}}, "url": "https://github.com/broadinstitute/gatk/commit/2e74cf4232cac1436057d0e708dcadca9ad45458", "committedDate": "2020-07-16T19:33:32Z", "message": "Minor updates to workflows to point at new data sources."}, "afterCommit": {"oid": "c769d2193f94c5803e7b9f6d59896b93ee599763", "author": {"user": {"login": "jonn-smith", "name": "Jonn Smith"}}, "url": "https://github.com/broadinstitute/gatk/commit/c769d2193f94c5803e7b9f6d59896b93ee599763", "committedDate": "2020-07-16T19:34:22Z", "message": "Fixed issue with dbSNP source data for hg38.\n\nCode updates:\n- Now both hg19 and hg38 have the contig names translated to `chr__`\n- Added 'lncRNA' to GeneTranscriptType.\n- Added \"TAGENE\" gene tag.\n- Added the MANE_SELECT tag to FeatureTag.\n- Added the STOP_CODON_READTHROUGH tag to FeatureTag.\n- Updated the GTF versions that are parseable.\n- Fixed a parsing error with new versions of gencode and the remap\npositions (for liftover files).\n- Added test for indexing new lifted over gencode GTF.\n- Added Gencode_34 entries to MAF output map.\n- Minor changes to FuncotatorIntegrationTest.java for code syntax.\n- Pointed data source downloader at new data sources URL.\n- Minor updates to workflows to point at new data sources.\n\nScript updates:\n- Updated retrieval scripts for dbSNP and Gencode.\n- Added required field to gencode config file generation.\n- Now gencode retrieval script enforces double hash comments at\ntop of gencode GTF files.\n\nBug Fixes:\nRemoving erroneous trailing tab in MAF file output.\n\n- Fixes #6693"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU0Mzk3NzQ3", "url": "https://github.com/broadinstitute/gatk/pull/6660#pullrequestreview-454397747", "createdAt": "2020-07-23T18:37:12Z", "commit": {"oid": "54ff67a5555d4b0ca9074071c8c10658edfb3a46"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODozNzoxMlrOG2W3SQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODo1OTowMFrOG2XmqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MDg4OQ==", "bodyText": "Couldn't you download the file without modification, verify the MD5, and then change the contig names after verification?", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459650889", "createdAt": "2020-07-23T18:37:12Z", "author": {"login": "droazen"}, "path": "scripts/funcotator/data_sources/getDbSNP.sh", "diffHunk": "@@ -154,57 +154,60 @@ function downloadAndVerifyVcfFiles() {\n \n     curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n \n-    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${SRC_FILE_REGEX} )\n-    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${TBI_FILE_REGEX} )\n-    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E ${MD5_FILE_REGEX} )\n+    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${SRC_FILE_REGEX}\" )\n+    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${TBI_FILE_REGEX}\" )\n+    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${MD5_FILE_REGEX}\" )\n \n     echo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n     wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n \n     # Get the VCF file, then make sure that the contig names are correct for HG19 (if applicable)\n     echo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n-    if [[ \"${filePrefix}\" == \"hg19\" ]] ; then\n-        curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}\n-    else\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n-\n-        echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n-\n-        # We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n-        echo \"${indentSpace}Verifying VCF checksum ...\"\n-        if [[ \"$(uname)\" == \"Darwin\" ]] ; then\n-            which md5sum-lite &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n-            fi\n-        else\n-            which md5sum &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n-            fi\n-        fi\n-    fi\n+    curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}\n+\n+\t\t# We can no longer verify the MD5sum because we have to change the file contents.\n+\t\t# Uncomment the following block when we can once again verify contents:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54ff67a5555d4b0ca9074071c8c10658edfb3a46"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MTczMA==", "bodyText": "Is sed really the best mechanism to do this transformation? Is there a more robust option we could consider (like a liftover file)?", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459651730", "createdAt": "2020-07-23T18:38:50Z", "author": {"login": "droazen"}, "path": "scripts/funcotator/data_sources/getDbSNP.sh", "diffHunk": "@@ -154,57 +154,60 @@ function downloadAndVerifyVcfFiles() {\n \n     curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n \n-    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${SRC_FILE_REGEX} )\n-    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${TBI_FILE_REGEX} )\n-    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E ${MD5_FILE_REGEX} )\n+    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${SRC_FILE_REGEX}\" )\n+    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${TBI_FILE_REGEX}\" )\n+    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${MD5_FILE_REGEX}\" )\n \n     echo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n     wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n \n     # Get the VCF file, then make sure that the contig names are correct for HG19 (if applicable)\n     echo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n-    if [[ \"${filePrefix}\" == \"hg19\" ]] ; then\n-        curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}\n-    else\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n-\n-        echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n-\n-        # We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n-        echo \"${indentSpace}Verifying VCF checksum ...\"\n-        if [[ \"$(uname)\" == \"Darwin\" ]] ; then\n-            which md5sum-lite &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n-            fi\n-        else\n-            which md5sum &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n-            fi\n-        fi\n-    fi\n+    curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54ff67a5555d4b0ca9074071c8c10658edfb3a46"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MzYzOQ==", "bodyText": "This script has a lot of arguments -- should ideally document them in a comment at the top.", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459653639", "createdAt": "2020-07-23T18:42:09Z", "author": {"login": "droazen"}, "path": "scripts/funcotator/data_sources/getGencode.sh", "diffHunk": "@@ -89,9 +88,10 @@ function createConfigFile() {\n \n     local dataSourceName=$1\n     local version=$2\n-    local srcFile=$3\n-    local originLocation=$4\n-    local fastaPath=$5\n+    local refVersion=$3\n+    local srcFile=$4\n+    local originLocation=$5\n+    local fastaPath=$6", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54ff67a5555d4b0ca9074071c8c10658edfb3a46"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NDkyNg==", "bodyText": "Your indentation is a bit off in several places -- are you mixing spaces and tabs perhaps?", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459654926", "createdAt": "2020-07-23T18:44:26Z", "author": {"login": "droazen"}, "path": "scripts/funcotator/data_sources/getGencode.sh", "diffHunk": "@@ -158,25 +162,35 @@ function getGencodeFiles()\n \n     mkdir -p ${OUT_DIR_NAME}/${refVersion}\n     pushd ${OUT_DIR_NAME}/${refVersion} &> /dev/null\n-    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.annotation.gtf.gz\n-    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.pc_transcripts.fa.gz\n \n-    gunzip gencode.v${version}.annotation.gtf.gz\n-    gunzip gencode.v${version}.pc_transcripts.fa.gz\n+\t\tfileRefVersion=${version}\n+\t\tif [[ \"${refVersion}\" == \"hg38\" ]] ; then\n+\t\t\tsourceUrl=ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.annotation.gtf.gz\n+\t\t\twget ${sourceUrl} \n+    \twget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.pc_transcripts.fa.gz", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54ff67a5555d4b0ca9074071c8c10658edfb3a46"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NjA5OA==", "bodyText": "Are you guaranteed to have exactly 5 lines of header?", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459656098", "createdAt": "2020-07-23T18:46:24Z", "author": {"login": "droazen"}, "path": "scripts/funcotator/data_sources/getGencode.sh", "diffHunk": "@@ -158,25 +162,35 @@ function getGencodeFiles()\n \n     mkdir -p ${OUT_DIR_NAME}/${refVersion}\n     pushd ${OUT_DIR_NAME}/${refVersion} &> /dev/null\n-    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.annotation.gtf.gz\n-    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.pc_transcripts.fa.gz\n \n-    gunzip gencode.v${version}.annotation.gtf.gz\n-    gunzip gencode.v${version}.pc_transcripts.fa.gz\n+\t\tfileRefVersion=${version}\n+\t\tif [[ \"${refVersion}\" == \"hg38\" ]] ; then\n+\t\t\tsourceUrl=ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.annotation.gtf.gz\n+\t\t\twget ${sourceUrl} \n+    \twget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.pc_transcripts.fa.gz\n+\t\telse\n+\t\t\tfileRefVersion=\"${version}lift37\"\n+\t\t\tsourceUrl=ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/GRCh37_mapping/gencode.v${fileRefVersion}.annotation.gtf.gz\n+\t\t\twget ${sourceUrl}\n+\t\t\twget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/GRCh37_mapping/gencode.v${fileRefVersion}.pc_transcripts.fa.gz\n+\t\tfi\n+\t\t\n+    gunzip gencode.v${fileRefVersion}.annotation.gtf.gz\n+    gunzip gencode.v${fileRefVersion}.pc_transcripts.fa.gz\n \n     # We must fix the information in the gencode gtf file:\n     echo \"Reordering Gencode GTF data ...\"\n-    ${SCRIPTDIR}/fixGencodeOrdering.py gencode.v${version}.annotation.gtf > gencode.v${version}.annotation.REORDERED.gtf\n+    ${SCRIPTDIR}/fixGencodeOrdering.py gencode.v${fileRefVersion}.annotation.gtf | sed -e '1,5s$^#\\([a-z]\\)$##\\1$' > gencode.v${fileRefVersion}.annotation.REORDERED.gtf", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54ff67a5555d4b0ca9074071c8c10658edfb3a46"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1NjU3OQ==", "bodyText": "Do you create the .dict file as well?", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459656579", "createdAt": "2020-07-23T18:47:18Z", "author": {"login": "droazen"}, "path": "scripts/funcotator/data_sources/getGencode.sh", "diffHunk": "@@ -158,25 +162,35 @@ function getGencodeFiles()\n \n     mkdir -p ${OUT_DIR_NAME}/${refVersion}\n     pushd ${OUT_DIR_NAME}/${refVersion} &> /dev/null\n-    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.annotation.gtf.gz\n-    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.pc_transcripts.fa.gz\n \n-    gunzip gencode.v${version}.annotation.gtf.gz\n-    gunzip gencode.v${version}.pc_transcripts.fa.gz\n+\t\tfileRefVersion=${version}\n+\t\tif [[ \"${refVersion}\" == \"hg38\" ]] ; then\n+\t\t\tsourceUrl=ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.annotation.gtf.gz\n+\t\t\twget ${sourceUrl} \n+    \twget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.pc_transcripts.fa.gz\n+\t\telse\n+\t\t\tfileRefVersion=\"${version}lift37\"\n+\t\t\tsourceUrl=ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/GRCh37_mapping/gencode.v${fileRefVersion}.annotation.gtf.gz\n+\t\t\twget ${sourceUrl}\n+\t\t\twget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/GRCh37_mapping/gencode.v${fileRefVersion}.pc_transcripts.fa.gz\n+\t\tfi\n+\t\t\n+    gunzip gencode.v${fileRefVersion}.annotation.gtf.gz\n+    gunzip gencode.v${fileRefVersion}.pc_transcripts.fa.gz\n \n     # We must fix the information in the gencode gtf file:\n     echo \"Reordering Gencode GTF data ...\"\n-    ${SCRIPTDIR}/fixGencodeOrdering.py gencode.v${version}.annotation.gtf > gencode.v${version}.annotation.REORDERED.gtf\n+    ${SCRIPTDIR}/fixGencodeOrdering.py gencode.v${fileRefVersion}.annotation.gtf | sed -e '1,5s$^#\\([a-z]\\)$##\\1$' > gencode.v${fileRefVersion}.annotation.REORDERED.gtf\n \n     # Clean up original file:\n-    rm gencode.v${version}.annotation.gtf\n+    rm gencode.v${fileRefVersion}.annotation.gtf\n \n     echo \"Creating config file ...\"\n-    createConfigFile \"${DATA_SOURCE_NAME}\" \"${version}\" \"gencode.v${version}.annotation.REORDERED.gtf\" \"ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_${version}/gencode.v${version}.annotation.gtf.gz\" \"gencode.v${version}.pc_transcripts.fa\" > gencode.config\n+    createConfigFile \"${DATA_SOURCE_NAME}\" \"${version}\" \"${refVersion}\" \"gencode.v${fileRefVersion}.annotation.REORDERED.gtf\" \"${sourceUrl}\" \"gencode.v${fileRefVersion}.pc_transcripts.fa\" > gencode.config\n \n     if $HAS_SAMTOOLS ; then\n-        echo \"Indexing Fasta File: gencode.v${version}.pc_transcripts.fa\"\n-        samtools faidx gencode.v${version}.pc_transcripts.fa\n+        echo \"Indexing Fasta File: gencode.v${fileRefVersion}.pc_transcripts.fa\"\n+        samtools faidx gencode.v${fileRefVersion}.pc_transcripts.fa", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54ff67a5555d4b0ca9074071c8c10658edfb3a46"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1Njk2NA==", "bodyText": "Perhaps the base directory should be a script argument instead of hardcoded to your home directory? Is this meant to be used by anyone other than yourself?", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459656964", "createdAt": "2020-07-23T18:47:57Z", "author": {"login": "droazen"}, "path": "scripts/funcotator/testing/testFuncotator.sh", "diffHunk": "@@ -56,6 +57,7 @@ MANUAL_MODE=false\n ################################################################################\n \n # Change this to point to your funcotator data sources folder:\n+DATA_SOURCES_PATH_16=/Users/jonn/Development/funcotator_dataSources.v1.6.20190124s\n DATA_SOURCES_PATH=/Users/jonn/Development/funcotator_dataSources_latest\n DATA_SOURCES_PATH_GERMLINE=/Users/jonn/Development/funcotator_dataSources_germline_latest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54ff67a5555d4b0ca9074071c8c10658edfb3a46"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MTg5OQ==", "bodyText": "What's the motivation behind this IndexFeatureFile test to index this specific gencode file? Can you add a code comment with an explanation?", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459661899", "createdAt": "2020-07-23T18:56:52Z", "author": {"login": "droazen"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/IndexFeatureFileIntegrationTest.java", "diffHunk": "@@ -490,4 +492,19 @@ public void testEnsemblGtfIndexQuery(final SimpleInterval interval,\n             }\n         }\n     }\n+\n+    @Test\n+    public void testNewGencodeLiftoverGtfFile() {\n+        // First ensure that we can index the file:\n+        // Required Args:\n+        final ArgumentsBuilder arguments = new ArgumentsBuilder();\n+\n+        final File output = createTempFile(GENCODE_NEW_LIFTOVER_FILE.getName(), \".idx\");\n+\n+        arguments.addInput(GENCODE_NEW_LIFTOVER_FILE);\n+        arguments.addOutput(output.getAbsolutePath());\n+\n+        // Run the beast:\n+        runCommandLine(arguments);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54ff67a5555d4b0ca9074071c8c10658edfb3a46"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MzAxNg==", "bodyText": "Can you add new Funcotator tests to cover the fixes in this branch (the hg38 dbsnp contig names fix and the MAF tab fix)?", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r459663016", "createdAt": "2020-07-23T18:59:00Z", "author": {"login": "droazen"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/funcotator/FuncotatorIntegrationTest.java", "diffHunk": "@@ -1689,7 +1691,6 @@ public void testEColiFuncotations() {\n         arguments.add(FuncotatorArgumentDefinitions.TRANSCRIPT_SELECTION_MODE_LONG_NAME, TranscriptSelectionMode.CANONICAL.toString());\n         runCommandLine(arguments);\n         assertEqualVariantFiles(outputFile, E_COLI_EXPECTED_OUT);\n-\n     }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54ff67a5555d4b0ca9074071c8c10658edfb3a46"}, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY4NzQ4MTYw", "url": "https://github.com/broadinstitute/gatk/pull/6660#pullrequestreview-468748160", "createdAt": "2020-08-17T18:47:47Z", "commit": {"oid": "9f6221ca7b4d12c77462f2c2fea1496d6e2032fb"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxODo0Nzo0N1rOHB2vtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxOTowODo1M1rOHB3arg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcwNzU3Mg==", "bodyText": "This comment appears to be obsolete, since as far as I can tell you are now always verifying the checksum, aren't you?", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r471707572", "createdAt": "2020-08-17T18:47:47Z", "author": {"login": "droazen"}, "path": "scripts/funcotator/data_sources/getDbSNP.sh", "diffHunk": "@@ -14,211 +14,213 @@ MINARGS=0\n MAXARGS=0\n \n FTP_BASE_URL='ftp://ftp.ncbi.nih.gov/snp/organisms/'\n-BUILD_NUMBER='150'\n+BUILD_NUMBER='151'\n \n DATA_SOURCE_NAME=\"dbSNP\"\n OUT_DIR_NAME='dbsnp'\n \n SRC_FILE_BASE_NAME=\"All_\"\n #SRC_FILE_BASE_NAME=\"common_all_\"\n \n-SRC_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz\\\\s*\\$\"\n-MD5_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz.md5\\\\s*\\$\"\n-TBI_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz.tbi\\\\s*\\$\"\n+SRC_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz[ \\\\t]*\\$\"\n+MD5_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz.md5[ \\\\t]*\\$\"\n+TBI_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz.tbi[ \\\\t]*\\$\"\n \n ################################################################################\n \n function simpleUsage()\n {\n-  echo -e \"Usage: $SCRIPTNAME [OPTIONS] ...\"\n-  echo -e \"Creates the data sources folder for dbSnp for the GATK Funcotator tool.\"\n+\techo -e \"Usage: $SCRIPTNAME [OPTIONS] ...\"\n+\techo -e \"Creates the data sources folder for dbSnp for the GATK Funcotator tool.\"\n }\n \n #Define a usage function:\n function usage()\n {\n-  simpleUsage\n-  echo -e \"\"\n-  echo -e \"Will download all data sources directly from the NCBI website:\"\n-  echo -e \"    ${FTP_BASE_URL}\"\n-  echo -e \"\"\n-  echo -e \"Return values:\"\n-  echo -e \"  0  NORMAL\"\n-  echo -e \"  1  TOO MANY ARGUMENTS\"\n-  echo -e \"  2  TOO FEW ARGUMENTS\"\n-  echo -e \"  3  UNKNOWN ARGUMENT\"\n-  echo -e \"  4  BAD CHECKSUM\"\n-  echo -e \"  5  OUTPUT DIRECTORY ALREADY EXISTS\"\n-  echo -e \"  6  COULD NOT FIND BGZIP UTILITY\"\n-  echo -e \"\"\n+\tsimpleUsage\n+\techo -e \"\"\n+\techo -e \"Will download all data sources directly from the NCBI website:\"\n+\techo -e \"    ${FTP_BASE_URL}\"\n+\techo -e \"\"\n+\techo -e \"Return values:\"\n+\techo -e \"  0  NORMAL\"\n+\techo -e \"  1  TOO MANY ARGUMENTS\"\n+\techo -e \"  2  TOO FEW ARGUMENTS\"\n+\techo -e \"  3  UNKNOWN ARGUMENT\"\n+\techo -e \"  4  BAD CHECKSUM\"\n+\techo -e \"  5  OUTPUT DIRECTORY ALREADY EXISTS\"\n+\techo -e \"  6  COULD NOT FIND BGZIP UTILITY\"\n+\techo -e \"\"\n }\n \n #Display a message to std error:\n function error()\n {\n-  echo \"$1\" 2>&1\n+\techo \"$1\" 2>&1\n }\n \n TMPFILELIST=''\n function makeTemp()\n {\n-  local f\n-  f=$( mktemp )\n-  TMPFILELIST=\"${TMPFILELIST} $f\"\n-  echo $f\n+\tlocal f\n+\tf=$( mktemp )\n+\tTMPFILELIST=\"${TMPFILELIST} $f\"\n+\techo $f\n }\n \n function cleanTempVars()\n {\n-  rm -f ${TMPFILELIST}\n+\trm -f ${TMPFILELIST}\n }\n \n function at_exit()\n {\n-  cleanTempVars\n+\tcleanTempVars\n }\n \n ################################################################################\n \n \n function createConfigFile() {\n \n-    local dataSourceName=$1\n-    local version=$2\n-    local srcFile=$3\n-    local originLocation=$4\n-\n-    echo \"name = ${dataSourceName}\"\n-    echo \"version = ${version}\"\n-    echo \"src_file = ${srcFile}\"\n-    echo \"origin_location = ${originLocation}\"\n-    echo \"preprocessing_script = ${SCRIPTNAME}\"\n-    echo \"\"\n-    echo \"# Supported types:\"\n-    echo \"# simpleXSV    -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript ID\"\n-    echo \"# locatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome location\"\n-    echo \"# gencode      -- Custom datasource class for GENCODE\"\n-    echo \"# cosmic       -- Custom datasource class for COSMIC\"\n-    echo \"# vcf          -- Custom datasource class for Variant Call Format (VCF) files\"\n-    echo \"type = vcf\"\n-    echo \"\"\n-    echo \"# Required field for GENCODE files.\"\n-    echo \"# Path to the FASTA file from which to load the sequences for GENCODE transcripts:\"\n-    echo \"gencode_fasta_path =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# Valid values:\"\n-    echo \"#     GENE_NAME\"\n-    echo \"#     TRANSCRIPT_ID\"\n-    echo \"xsv_key =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# The 0-based index of the column containing the key on which to match\"\n-    echo \"xsv_key_column =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV AND locatableXSV files.\"\n-    echo \"# The delimiter by which to split the XSV file into columns.\"\n-    echo \"xsv_delimiter =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# Whether to permissively match the number of columns in the header and data rows\"\n-    echo \"# Valid values:\"\n-    echo \"#     true\"\n-    echo \"#     false\"\n-    echo \"xsv_permissive_cols =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the contig for each row\"\n-    echo \"contig_column =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the start position for each row\"\n-    echo \"start_column =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the end position for each row\"\n-    echo \"end_column =\"\n-    echo \"\"\n+\tlocal dataSourceName=$1\n+\tlocal version=$2\n+\tlocal srcFile=$3\n+\tlocal originLocation=$4\n+\n+\techo \"name = ${dataSourceName}\"\n+\techo \"version = ${version}\"\n+\techo \"src_file = ${srcFile}\"\n+\techo \"origin_location = ${originLocation}\"\n+\techo \"preprocessing_script = ${SCRIPTNAME}\"\n+\techo \"\"\n+\techo \"# Supported types:\"\n+\techo \"# simpleXSV    -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript ID\"\n+\techo \"# locatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome location\"\n+\techo \"# gencode      -- Custom datasource class for GENCODE\"\n+\techo \"# cosmic       -- Custom datasource class for COSMIC\"\n+\techo \"# vcf          -- Custom datasource class for Variant Call Format (VCF) files\"\n+\techo \"type = vcf\"\n+\techo \"\"\n+\techo \"# Required field for GENCODE files.\"\n+\techo \"# Path to the FASTA file from which to load the sequences for GENCODE transcripts:\"\n+\techo \"gencode_fasta_path =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# Valid values:\"\n+\techo \"#     GENE_NAME\"\n+\techo \"#     TRANSCRIPT_ID\"\n+\techo \"xsv_key =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# The 0-based index of the column containing the key on which to match\"\n+\techo \"xsv_key_column =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV AND locatableXSV files.\"\n+\techo \"# The delimiter by which to split the XSV file into columns.\"\n+\techo \"xsv_delimiter =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# Whether to permissively match the number of columns in the header and data rows\"\n+\techo \"# Valid values:\"\n+\techo \"#     true\"\n+\techo \"#     false\"\n+\techo \"xsv_permissive_cols =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the contig for each row\"\n+\techo \"contig_column =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the start position for each row\"\n+\techo \"start_column =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the end position for each row\"\n+\techo \"end_column =\"\n+\techo \"\"\n \n }\n \n function downloadAndVerifyVcfFiles() {\n \n-    local remoteFolder=$1\n-    local outputFolder=$2\n-    local filePrefix=$3\n-\n-    local listingFile=$( makeTemp )\n-    local indentSpace=\"    \"\n-    local version=$( echo \"${remoteFolder}\" | sed \"s#.*human_\\\\(.*b${BUILD_NUMBER}\\\\).*#\\\\1#g\" )\n-\n-    curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n-\n-    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${SRC_FILE_REGEX} )\n-    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${TBI_FILE_REGEX} )\n-    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E ${MD5_FILE_REGEX} )\n-\n-    echo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n-    wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n-\n-    # Get the VCF file, then make sure that the contig names are correct for HG19 (if applicable)\n-    echo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n-    if [[ \"${filePrefix}\" == \"hg19\" ]] ; then\n-        curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}\n-    else\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n-\n-        echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n-\n-        # We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n-        echo \"${indentSpace}Verifying VCF checksum ...\"\n-        if [[ \"$(uname)\" == \"Darwin\" ]] ; then\n-            which md5sum-lite &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n-            fi\n-        else\n-            which md5sum &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n-            fi\n-        fi\n-    fi\n-\n-    # Now put it in the right place and clean up:\n-    echo \"${indentSpace}Creating output directory ...\"\n-    mkdir -p ${outputFolder}\n-\n-    echo \"${indentSpace}Moving files to output directory ...\"\n-    mv ${vcfFile} ${outputFolder}/${filePrefix}_${vcfFile}\n-    if [[ ! \"${filePrefix}\" == \"hg19\" ]] ; then\n-        mv ${tbiFile} ${outputFolder}/${filePrefix}_${tbiFile}\n-        rm ${md5File}\n-    fi\n-\n-    echo \"${indentSpace}Creating Config File ... \"\n-    createConfigFile \"${DATA_SOURCE_NAME}\" \"${version}\" ${filePrefix}_${vcfFile} \"ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\" > ${outputFolder}/${DATA_SOURCE_NAME}.config\n+\tlocal remoteFolder=$1\n+\tlocal outputFolder=$2\n+\tlocal filePrefix=$3\n+\n+\tlocal listingFile=$( makeTemp )\n+\tlocal indentSpace=\"    \"\n+\tlocal version=$( echo \"${remoteFolder}\" | sed \"s#.*human_\\\\(.*b${BUILD_NUMBER}\\\\).*#\\\\1#g\" )\n+\n+\tlocal tmpVcfFile=$( makeTemp )\n+\n+\tcurl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n+\n+\tvcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${SRC_FILE_REGEX}\" )\n+\ttbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${TBI_FILE_REGEX}\" )\n+\tmd5File=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${MD5_FILE_REGEX}\" )\n+\n+\techo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n+\twget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n+\n+\t# Get the VCF file\n+\techo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n+\twget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n+\n+\t#echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n+\t#wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n+\n+\t# We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f6221ca7b4d12c77462f2c2fea1496d6e2032fb"}, "originalPosition": 318}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcwODEwNQ==", "bodyText": "You should consider doing the sed operation in-place (sed -i) rather than writing to a temp file, to cut down on disk usage.", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r471708105", "createdAt": "2020-08-17T18:48:56Z", "author": {"login": "droazen"}, "path": "scripts/funcotator/data_sources/getDbSNP.sh", "diffHunk": "@@ -14,211 +14,213 @@ MINARGS=0\n MAXARGS=0\n \n FTP_BASE_URL='ftp://ftp.ncbi.nih.gov/snp/organisms/'\n-BUILD_NUMBER='150'\n+BUILD_NUMBER='151'\n \n DATA_SOURCE_NAME=\"dbSNP\"\n OUT_DIR_NAME='dbsnp'\n \n SRC_FILE_BASE_NAME=\"All_\"\n #SRC_FILE_BASE_NAME=\"common_all_\"\n \n-SRC_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz\\\\s*\\$\"\n-MD5_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz.md5\\\\s*\\$\"\n-TBI_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz.tbi\\\\s*\\$\"\n+SRC_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz[ \\\\t]*\\$\"\n+MD5_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz.md5[ \\\\t]*\\$\"\n+TBI_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz.tbi[ \\\\t]*\\$\"\n \n ################################################################################\n \n function simpleUsage()\n {\n-  echo -e \"Usage: $SCRIPTNAME [OPTIONS] ...\"\n-  echo -e \"Creates the data sources folder for dbSnp for the GATK Funcotator tool.\"\n+\techo -e \"Usage: $SCRIPTNAME [OPTIONS] ...\"\n+\techo -e \"Creates the data sources folder for dbSnp for the GATK Funcotator tool.\"\n }\n \n #Define a usage function:\n function usage()\n {\n-  simpleUsage\n-  echo -e \"\"\n-  echo -e \"Will download all data sources directly from the NCBI website:\"\n-  echo -e \"    ${FTP_BASE_URL}\"\n-  echo -e \"\"\n-  echo -e \"Return values:\"\n-  echo -e \"  0  NORMAL\"\n-  echo -e \"  1  TOO MANY ARGUMENTS\"\n-  echo -e \"  2  TOO FEW ARGUMENTS\"\n-  echo -e \"  3  UNKNOWN ARGUMENT\"\n-  echo -e \"  4  BAD CHECKSUM\"\n-  echo -e \"  5  OUTPUT DIRECTORY ALREADY EXISTS\"\n-  echo -e \"  6  COULD NOT FIND BGZIP UTILITY\"\n-  echo -e \"\"\n+\tsimpleUsage\n+\techo -e \"\"\n+\techo -e \"Will download all data sources directly from the NCBI website:\"\n+\techo -e \"    ${FTP_BASE_URL}\"\n+\techo -e \"\"\n+\techo -e \"Return values:\"\n+\techo -e \"  0  NORMAL\"\n+\techo -e \"  1  TOO MANY ARGUMENTS\"\n+\techo -e \"  2  TOO FEW ARGUMENTS\"\n+\techo -e \"  3  UNKNOWN ARGUMENT\"\n+\techo -e \"  4  BAD CHECKSUM\"\n+\techo -e \"  5  OUTPUT DIRECTORY ALREADY EXISTS\"\n+\techo -e \"  6  COULD NOT FIND BGZIP UTILITY\"\n+\techo -e \"\"\n }\n \n #Display a message to std error:\n function error()\n {\n-  echo \"$1\" 2>&1\n+\techo \"$1\" 2>&1\n }\n \n TMPFILELIST=''\n function makeTemp()\n {\n-  local f\n-  f=$( mktemp )\n-  TMPFILELIST=\"${TMPFILELIST} $f\"\n-  echo $f\n+\tlocal f\n+\tf=$( mktemp )\n+\tTMPFILELIST=\"${TMPFILELIST} $f\"\n+\techo $f\n }\n \n function cleanTempVars()\n {\n-  rm -f ${TMPFILELIST}\n+\trm -f ${TMPFILELIST}\n }\n \n function at_exit()\n {\n-  cleanTempVars\n+\tcleanTempVars\n }\n \n ################################################################################\n \n \n function createConfigFile() {\n \n-    local dataSourceName=$1\n-    local version=$2\n-    local srcFile=$3\n-    local originLocation=$4\n-\n-    echo \"name = ${dataSourceName}\"\n-    echo \"version = ${version}\"\n-    echo \"src_file = ${srcFile}\"\n-    echo \"origin_location = ${originLocation}\"\n-    echo \"preprocessing_script = ${SCRIPTNAME}\"\n-    echo \"\"\n-    echo \"# Supported types:\"\n-    echo \"# simpleXSV    -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript ID\"\n-    echo \"# locatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome location\"\n-    echo \"# gencode      -- Custom datasource class for GENCODE\"\n-    echo \"# cosmic       -- Custom datasource class for COSMIC\"\n-    echo \"# vcf          -- Custom datasource class for Variant Call Format (VCF) files\"\n-    echo \"type = vcf\"\n-    echo \"\"\n-    echo \"# Required field for GENCODE files.\"\n-    echo \"# Path to the FASTA file from which to load the sequences for GENCODE transcripts:\"\n-    echo \"gencode_fasta_path =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# Valid values:\"\n-    echo \"#     GENE_NAME\"\n-    echo \"#     TRANSCRIPT_ID\"\n-    echo \"xsv_key =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# The 0-based index of the column containing the key on which to match\"\n-    echo \"xsv_key_column =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV AND locatableXSV files.\"\n-    echo \"# The delimiter by which to split the XSV file into columns.\"\n-    echo \"xsv_delimiter =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# Whether to permissively match the number of columns in the header and data rows\"\n-    echo \"# Valid values:\"\n-    echo \"#     true\"\n-    echo \"#     false\"\n-    echo \"xsv_permissive_cols =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the contig for each row\"\n-    echo \"contig_column =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the start position for each row\"\n-    echo \"start_column =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the end position for each row\"\n-    echo \"end_column =\"\n-    echo \"\"\n+\tlocal dataSourceName=$1\n+\tlocal version=$2\n+\tlocal srcFile=$3\n+\tlocal originLocation=$4\n+\n+\techo \"name = ${dataSourceName}\"\n+\techo \"version = ${version}\"\n+\techo \"src_file = ${srcFile}\"\n+\techo \"origin_location = ${originLocation}\"\n+\techo \"preprocessing_script = ${SCRIPTNAME}\"\n+\techo \"\"\n+\techo \"# Supported types:\"\n+\techo \"# simpleXSV    -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript ID\"\n+\techo \"# locatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome location\"\n+\techo \"# gencode      -- Custom datasource class for GENCODE\"\n+\techo \"# cosmic       -- Custom datasource class for COSMIC\"\n+\techo \"# vcf          -- Custom datasource class for Variant Call Format (VCF) files\"\n+\techo \"type = vcf\"\n+\techo \"\"\n+\techo \"# Required field for GENCODE files.\"\n+\techo \"# Path to the FASTA file from which to load the sequences for GENCODE transcripts:\"\n+\techo \"gencode_fasta_path =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# Valid values:\"\n+\techo \"#     GENE_NAME\"\n+\techo \"#     TRANSCRIPT_ID\"\n+\techo \"xsv_key =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# The 0-based index of the column containing the key on which to match\"\n+\techo \"xsv_key_column =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV AND locatableXSV files.\"\n+\techo \"# The delimiter by which to split the XSV file into columns.\"\n+\techo \"xsv_delimiter =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# Whether to permissively match the number of columns in the header and data rows\"\n+\techo \"# Valid values:\"\n+\techo \"#     true\"\n+\techo \"#     false\"\n+\techo \"xsv_permissive_cols =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the contig for each row\"\n+\techo \"contig_column =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the start position for each row\"\n+\techo \"start_column =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the end position for each row\"\n+\techo \"end_column =\"\n+\techo \"\"\n \n }\n \n function downloadAndVerifyVcfFiles() {\n \n-    local remoteFolder=$1\n-    local outputFolder=$2\n-    local filePrefix=$3\n-\n-    local listingFile=$( makeTemp )\n-    local indentSpace=\"    \"\n-    local version=$( echo \"${remoteFolder}\" | sed \"s#.*human_\\\\(.*b${BUILD_NUMBER}\\\\).*#\\\\1#g\" )\n-\n-    curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n-\n-    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${SRC_FILE_REGEX} )\n-    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${TBI_FILE_REGEX} )\n-    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E ${MD5_FILE_REGEX} )\n-\n-    echo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n-    wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n-\n-    # Get the VCF file, then make sure that the contig names are correct for HG19 (if applicable)\n-    echo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n-    if [[ \"${filePrefix}\" == \"hg19\" ]] ; then\n-        curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}\n-    else\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n-\n-        echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n-\n-        # We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n-        echo \"${indentSpace}Verifying VCF checksum ...\"\n-        if [[ \"$(uname)\" == \"Darwin\" ]] ; then\n-            which md5sum-lite &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n-            fi\n-        else\n-            which md5sum &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n-            fi\n-        fi\n-    fi\n-\n-    # Now put it in the right place and clean up:\n-    echo \"${indentSpace}Creating output directory ...\"\n-    mkdir -p ${outputFolder}\n-\n-    echo \"${indentSpace}Moving files to output directory ...\"\n-    mv ${vcfFile} ${outputFolder}/${filePrefix}_${vcfFile}\n-    if [[ ! \"${filePrefix}\" == \"hg19\" ]] ; then\n-        mv ${tbiFile} ${outputFolder}/${filePrefix}_${tbiFile}\n-        rm ${md5File}\n-    fi\n-\n-    echo \"${indentSpace}Creating Config File ... \"\n-    createConfigFile \"${DATA_SOURCE_NAME}\" \"${version}\" ${filePrefix}_${vcfFile} \"ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\" > ${outputFolder}/${DATA_SOURCE_NAME}.config\n+\tlocal remoteFolder=$1\n+\tlocal outputFolder=$2\n+\tlocal filePrefix=$3\n+\n+\tlocal listingFile=$( makeTemp )\n+\tlocal indentSpace=\"    \"\n+\tlocal version=$( echo \"${remoteFolder}\" | sed \"s#.*human_\\\\(.*b${BUILD_NUMBER}\\\\).*#\\\\1#g\" )\n+\n+\tlocal tmpVcfFile=$( makeTemp )\n+\n+\tcurl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n+\n+\tvcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${SRC_FILE_REGEX}\" )\n+\ttbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${TBI_FILE_REGEX}\" )\n+\tmd5File=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${MD5_FILE_REGEX}\" )\n+\n+\techo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n+\twget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n+\n+\t# Get the VCF file\n+\techo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n+\twget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n+\n+\t#echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n+\t#wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n+\n+\t# We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n+\techo \"${indentSpace}Verifying VCF checksum ...\"\n+\tif [[ \"$(uname)\" == \"Darwin\" ]] ; then\n+\t\twhich md5sum-lite &> /dev/null\n+\t\tr=$?\n+\t\tif [ $r == 0 ] ; then\n+\t\t\tchecksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\t\t\texpected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\n+\t\t\tif [[ \"${checksum}\" != \"${expected}\" ]] ; then\n+\t\t\t\terror \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n+\t\t\t\terror \"FAILING\"\n+\t\t\t\texit 4\n+\t\t\tfi\n+\t\telse\n+\t\t\terror \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n+\t\tfi\n+\telse\n+\t\twhich md5sum &> /dev/null\n+\t\tr=$?\n+\t\tif [ $r == 0 ] ; then\n+\t\t\tchecksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\t\t\texpected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\n+\t\t\tif [[ \"${checksum}\" != \"${expected}\" ]] ; then\n+\t\t\t\terror \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n+\t\t\t\terror \"FAILING\"\n+\t\t\t\texit 4\n+\t\t\tfi\n+\t\telse\n+\t\t\terror \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n+\t\tfi\n+\tfi\n+\n+\t# Now change the contigs in the file:\n+\tcat ${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${tmpVcfFile} ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f6221ca7b4d12c77462f2c2fea1496d6e2032fb"}, "originalPosition": 353}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcwOTM1Nw==", "bodyText": "Also, I reiterate my concern that the sed approach overall is a bit dangerous/brittle. What if there happened to be alt contigs that began with a digit, or the letter X or Y? Have you checked that there are no such contigs in either hg19 or hg38?", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r471709357", "createdAt": "2020-08-17T18:51:18Z", "author": {"login": "droazen"}, "path": "scripts/funcotator/data_sources/getDbSNP.sh", "diffHunk": "@@ -14,211 +14,213 @@ MINARGS=0\n MAXARGS=0\n \n FTP_BASE_URL='ftp://ftp.ncbi.nih.gov/snp/organisms/'\n-BUILD_NUMBER='150'\n+BUILD_NUMBER='151'\n \n DATA_SOURCE_NAME=\"dbSNP\"\n OUT_DIR_NAME='dbsnp'\n \n SRC_FILE_BASE_NAME=\"All_\"\n #SRC_FILE_BASE_NAME=\"common_all_\"\n \n-SRC_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz\\\\s*\\$\"\n-MD5_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz.md5\\\\s*\\$\"\n-TBI_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz.tbi\\\\s*\\$\"\n+SRC_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz[ \\\\t]*\\$\"\n+MD5_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz.md5[ \\\\t]*\\$\"\n+TBI_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz.tbi[ \\\\t]*\\$\"\n \n ################################################################################\n \n function simpleUsage()\n {\n-  echo -e \"Usage: $SCRIPTNAME [OPTIONS] ...\"\n-  echo -e \"Creates the data sources folder for dbSnp for the GATK Funcotator tool.\"\n+\techo -e \"Usage: $SCRIPTNAME [OPTIONS] ...\"\n+\techo -e \"Creates the data sources folder for dbSnp for the GATK Funcotator tool.\"\n }\n \n #Define a usage function:\n function usage()\n {\n-  simpleUsage\n-  echo -e \"\"\n-  echo -e \"Will download all data sources directly from the NCBI website:\"\n-  echo -e \"    ${FTP_BASE_URL}\"\n-  echo -e \"\"\n-  echo -e \"Return values:\"\n-  echo -e \"  0  NORMAL\"\n-  echo -e \"  1  TOO MANY ARGUMENTS\"\n-  echo -e \"  2  TOO FEW ARGUMENTS\"\n-  echo -e \"  3  UNKNOWN ARGUMENT\"\n-  echo -e \"  4  BAD CHECKSUM\"\n-  echo -e \"  5  OUTPUT DIRECTORY ALREADY EXISTS\"\n-  echo -e \"  6  COULD NOT FIND BGZIP UTILITY\"\n-  echo -e \"\"\n+\tsimpleUsage\n+\techo -e \"\"\n+\techo -e \"Will download all data sources directly from the NCBI website:\"\n+\techo -e \"    ${FTP_BASE_URL}\"\n+\techo -e \"\"\n+\techo -e \"Return values:\"\n+\techo -e \"  0  NORMAL\"\n+\techo -e \"  1  TOO MANY ARGUMENTS\"\n+\techo -e \"  2  TOO FEW ARGUMENTS\"\n+\techo -e \"  3  UNKNOWN ARGUMENT\"\n+\techo -e \"  4  BAD CHECKSUM\"\n+\techo -e \"  5  OUTPUT DIRECTORY ALREADY EXISTS\"\n+\techo -e \"  6  COULD NOT FIND BGZIP UTILITY\"\n+\techo -e \"\"\n }\n \n #Display a message to std error:\n function error()\n {\n-  echo \"$1\" 2>&1\n+\techo \"$1\" 2>&1\n }\n \n TMPFILELIST=''\n function makeTemp()\n {\n-  local f\n-  f=$( mktemp )\n-  TMPFILELIST=\"${TMPFILELIST} $f\"\n-  echo $f\n+\tlocal f\n+\tf=$( mktemp )\n+\tTMPFILELIST=\"${TMPFILELIST} $f\"\n+\techo $f\n }\n \n function cleanTempVars()\n {\n-  rm -f ${TMPFILELIST}\n+\trm -f ${TMPFILELIST}\n }\n \n function at_exit()\n {\n-  cleanTempVars\n+\tcleanTempVars\n }\n \n ################################################################################\n \n \n function createConfigFile() {\n \n-    local dataSourceName=$1\n-    local version=$2\n-    local srcFile=$3\n-    local originLocation=$4\n-\n-    echo \"name = ${dataSourceName}\"\n-    echo \"version = ${version}\"\n-    echo \"src_file = ${srcFile}\"\n-    echo \"origin_location = ${originLocation}\"\n-    echo \"preprocessing_script = ${SCRIPTNAME}\"\n-    echo \"\"\n-    echo \"# Supported types:\"\n-    echo \"# simpleXSV    -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript ID\"\n-    echo \"# locatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome location\"\n-    echo \"# gencode      -- Custom datasource class for GENCODE\"\n-    echo \"# cosmic       -- Custom datasource class for COSMIC\"\n-    echo \"# vcf          -- Custom datasource class for Variant Call Format (VCF) files\"\n-    echo \"type = vcf\"\n-    echo \"\"\n-    echo \"# Required field for GENCODE files.\"\n-    echo \"# Path to the FASTA file from which to load the sequences for GENCODE transcripts:\"\n-    echo \"gencode_fasta_path =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# Valid values:\"\n-    echo \"#     GENE_NAME\"\n-    echo \"#     TRANSCRIPT_ID\"\n-    echo \"xsv_key =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# The 0-based index of the column containing the key on which to match\"\n-    echo \"xsv_key_column =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV AND locatableXSV files.\"\n-    echo \"# The delimiter by which to split the XSV file into columns.\"\n-    echo \"xsv_delimiter =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# Whether to permissively match the number of columns in the header and data rows\"\n-    echo \"# Valid values:\"\n-    echo \"#     true\"\n-    echo \"#     false\"\n-    echo \"xsv_permissive_cols =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the contig for each row\"\n-    echo \"contig_column =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the start position for each row\"\n-    echo \"start_column =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the end position for each row\"\n-    echo \"end_column =\"\n-    echo \"\"\n+\tlocal dataSourceName=$1\n+\tlocal version=$2\n+\tlocal srcFile=$3\n+\tlocal originLocation=$4\n+\n+\techo \"name = ${dataSourceName}\"\n+\techo \"version = ${version}\"\n+\techo \"src_file = ${srcFile}\"\n+\techo \"origin_location = ${originLocation}\"\n+\techo \"preprocessing_script = ${SCRIPTNAME}\"\n+\techo \"\"\n+\techo \"# Supported types:\"\n+\techo \"# simpleXSV    -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript ID\"\n+\techo \"# locatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome location\"\n+\techo \"# gencode      -- Custom datasource class for GENCODE\"\n+\techo \"# cosmic       -- Custom datasource class for COSMIC\"\n+\techo \"# vcf          -- Custom datasource class for Variant Call Format (VCF) files\"\n+\techo \"type = vcf\"\n+\techo \"\"\n+\techo \"# Required field for GENCODE files.\"\n+\techo \"# Path to the FASTA file from which to load the sequences for GENCODE transcripts:\"\n+\techo \"gencode_fasta_path =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# Valid values:\"\n+\techo \"#     GENE_NAME\"\n+\techo \"#     TRANSCRIPT_ID\"\n+\techo \"xsv_key =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# The 0-based index of the column containing the key on which to match\"\n+\techo \"xsv_key_column =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV AND locatableXSV files.\"\n+\techo \"# The delimiter by which to split the XSV file into columns.\"\n+\techo \"xsv_delimiter =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# Whether to permissively match the number of columns in the header and data rows\"\n+\techo \"# Valid values:\"\n+\techo \"#     true\"\n+\techo \"#     false\"\n+\techo \"xsv_permissive_cols =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the contig for each row\"\n+\techo \"contig_column =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the start position for each row\"\n+\techo \"start_column =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the end position for each row\"\n+\techo \"end_column =\"\n+\techo \"\"\n \n }\n \n function downloadAndVerifyVcfFiles() {\n \n-    local remoteFolder=$1\n-    local outputFolder=$2\n-    local filePrefix=$3\n-\n-    local listingFile=$( makeTemp )\n-    local indentSpace=\"    \"\n-    local version=$( echo \"${remoteFolder}\" | sed \"s#.*human_\\\\(.*b${BUILD_NUMBER}\\\\).*#\\\\1#g\" )\n-\n-    curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n-\n-    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${SRC_FILE_REGEX} )\n-    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${TBI_FILE_REGEX} )\n-    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E ${MD5_FILE_REGEX} )\n-\n-    echo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n-    wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n-\n-    # Get the VCF file, then make sure that the contig names are correct for HG19 (if applicable)\n-    echo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n-    if [[ \"${filePrefix}\" == \"hg19\" ]] ; then\n-        curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}\n-    else\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n-\n-        echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n-\n-        # We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n-        echo \"${indentSpace}Verifying VCF checksum ...\"\n-        if [[ \"$(uname)\" == \"Darwin\" ]] ; then\n-            which md5sum-lite &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n-            fi\n-        else\n-            which md5sum &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n-            fi\n-        fi\n-    fi\n-\n-    # Now put it in the right place and clean up:\n-    echo \"${indentSpace}Creating output directory ...\"\n-    mkdir -p ${outputFolder}\n-\n-    echo \"${indentSpace}Moving files to output directory ...\"\n-    mv ${vcfFile} ${outputFolder}/${filePrefix}_${vcfFile}\n-    if [[ ! \"${filePrefix}\" == \"hg19\" ]] ; then\n-        mv ${tbiFile} ${outputFolder}/${filePrefix}_${tbiFile}\n-        rm ${md5File}\n-    fi\n-\n-    echo \"${indentSpace}Creating Config File ... \"\n-    createConfigFile \"${DATA_SOURCE_NAME}\" \"${version}\" ${filePrefix}_${vcfFile} \"ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\" > ${outputFolder}/${DATA_SOURCE_NAME}.config\n+\tlocal remoteFolder=$1\n+\tlocal outputFolder=$2\n+\tlocal filePrefix=$3\n+\n+\tlocal listingFile=$( makeTemp )\n+\tlocal indentSpace=\"    \"\n+\tlocal version=$( echo \"${remoteFolder}\" | sed \"s#.*human_\\\\(.*b${BUILD_NUMBER}\\\\).*#\\\\1#g\" )\n+\n+\tlocal tmpVcfFile=$( makeTemp )\n+\n+\tcurl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n+\n+\tvcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${SRC_FILE_REGEX}\" )\n+\ttbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${TBI_FILE_REGEX}\" )\n+\tmd5File=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${MD5_FILE_REGEX}\" )\n+\n+\techo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n+\twget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n+\n+\t# Get the VCF file\n+\techo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n+\twget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n+\n+\t#echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n+\t#wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n+\n+\t# We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n+\techo \"${indentSpace}Verifying VCF checksum ...\"\n+\tif [[ \"$(uname)\" == \"Darwin\" ]] ; then\n+\t\twhich md5sum-lite &> /dev/null\n+\t\tr=$?\n+\t\tif [ $r == 0 ] ; then\n+\t\t\tchecksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\t\t\texpected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\n+\t\t\tif [[ \"${checksum}\" != \"${expected}\" ]] ; then\n+\t\t\t\terror \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n+\t\t\t\terror \"FAILING\"\n+\t\t\t\texit 4\n+\t\t\tfi\n+\t\telse\n+\t\t\terror \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n+\t\tfi\n+\telse\n+\t\twhich md5sum &> /dev/null\n+\t\tr=$?\n+\t\tif [ $r == 0 ] ; then\n+\t\t\tchecksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\t\t\texpected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\n+\t\t\tif [[ \"${checksum}\" != \"${expected}\" ]] ; then\n+\t\t\t\terror \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n+\t\t\t\terror \"FAILING\"\n+\t\t\t\texit 4\n+\t\t\tfi\n+\t\telse\n+\t\t\terror \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n+\t\tfi\n+\tfi\n+\n+\t# Now change the contigs in the file:\n+\tcat ${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${tmpVcfFile} ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcwODEwNQ=="}, "originalCommit": {"oid": "9f6221ca7b4d12c77462f2c2fea1496d6e2032fb"}, "originalPosition": 353}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxMDAzMA==", "bodyText": "Why only delete the md5 file when we're not on hg19?", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r471710030", "createdAt": "2020-08-17T18:52:34Z", "author": {"login": "droazen"}, "path": "scripts/funcotator/data_sources/getDbSNP.sh", "diffHunk": "@@ -14,211 +14,213 @@ MINARGS=0\n MAXARGS=0\n \n FTP_BASE_URL='ftp://ftp.ncbi.nih.gov/snp/organisms/'\n-BUILD_NUMBER='150'\n+BUILD_NUMBER='151'\n \n DATA_SOURCE_NAME=\"dbSNP\"\n OUT_DIR_NAME='dbsnp'\n \n SRC_FILE_BASE_NAME=\"All_\"\n #SRC_FILE_BASE_NAME=\"common_all_\"\n \n-SRC_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz\\\\s*\\$\"\n-MD5_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz.md5\\\\s*\\$\"\n-TBI_FILE_REGEX=\"${SRC_FILE_BASE_NAME}\\\\d+.vcf.gz.tbi\\\\s*\\$\"\n+SRC_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz[ \\\\t]*\\$\"\n+MD5_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz.md5[ \\\\t]*\\$\"\n+TBI_FILE_REGEX=\"${SRC_FILE_BASE_NAME}[0-9][0-9]*.vcf.gz.tbi[ \\\\t]*\\$\"\n \n ################################################################################\n \n function simpleUsage()\n {\n-  echo -e \"Usage: $SCRIPTNAME [OPTIONS] ...\"\n-  echo -e \"Creates the data sources folder for dbSnp for the GATK Funcotator tool.\"\n+\techo -e \"Usage: $SCRIPTNAME [OPTIONS] ...\"\n+\techo -e \"Creates the data sources folder for dbSnp for the GATK Funcotator tool.\"\n }\n \n #Define a usage function:\n function usage()\n {\n-  simpleUsage\n-  echo -e \"\"\n-  echo -e \"Will download all data sources directly from the NCBI website:\"\n-  echo -e \"    ${FTP_BASE_URL}\"\n-  echo -e \"\"\n-  echo -e \"Return values:\"\n-  echo -e \"  0  NORMAL\"\n-  echo -e \"  1  TOO MANY ARGUMENTS\"\n-  echo -e \"  2  TOO FEW ARGUMENTS\"\n-  echo -e \"  3  UNKNOWN ARGUMENT\"\n-  echo -e \"  4  BAD CHECKSUM\"\n-  echo -e \"  5  OUTPUT DIRECTORY ALREADY EXISTS\"\n-  echo -e \"  6  COULD NOT FIND BGZIP UTILITY\"\n-  echo -e \"\"\n+\tsimpleUsage\n+\techo -e \"\"\n+\techo -e \"Will download all data sources directly from the NCBI website:\"\n+\techo -e \"    ${FTP_BASE_URL}\"\n+\techo -e \"\"\n+\techo -e \"Return values:\"\n+\techo -e \"  0  NORMAL\"\n+\techo -e \"  1  TOO MANY ARGUMENTS\"\n+\techo -e \"  2  TOO FEW ARGUMENTS\"\n+\techo -e \"  3  UNKNOWN ARGUMENT\"\n+\techo -e \"  4  BAD CHECKSUM\"\n+\techo -e \"  5  OUTPUT DIRECTORY ALREADY EXISTS\"\n+\techo -e \"  6  COULD NOT FIND BGZIP UTILITY\"\n+\techo -e \"\"\n }\n \n #Display a message to std error:\n function error()\n {\n-  echo \"$1\" 2>&1\n+\techo \"$1\" 2>&1\n }\n \n TMPFILELIST=''\n function makeTemp()\n {\n-  local f\n-  f=$( mktemp )\n-  TMPFILELIST=\"${TMPFILELIST} $f\"\n-  echo $f\n+\tlocal f\n+\tf=$( mktemp )\n+\tTMPFILELIST=\"${TMPFILELIST} $f\"\n+\techo $f\n }\n \n function cleanTempVars()\n {\n-  rm -f ${TMPFILELIST}\n+\trm -f ${TMPFILELIST}\n }\n \n function at_exit()\n {\n-  cleanTempVars\n+\tcleanTempVars\n }\n \n ################################################################################\n \n \n function createConfigFile() {\n \n-    local dataSourceName=$1\n-    local version=$2\n-    local srcFile=$3\n-    local originLocation=$4\n-\n-    echo \"name = ${dataSourceName}\"\n-    echo \"version = ${version}\"\n-    echo \"src_file = ${srcFile}\"\n-    echo \"origin_location = ${originLocation}\"\n-    echo \"preprocessing_script = ${SCRIPTNAME}\"\n-    echo \"\"\n-    echo \"# Supported types:\"\n-    echo \"# simpleXSV    -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript ID\"\n-    echo \"# locatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome location\"\n-    echo \"# gencode      -- Custom datasource class for GENCODE\"\n-    echo \"# cosmic       -- Custom datasource class for COSMIC\"\n-    echo \"# vcf          -- Custom datasource class for Variant Call Format (VCF) files\"\n-    echo \"type = vcf\"\n-    echo \"\"\n-    echo \"# Required field for GENCODE files.\"\n-    echo \"# Path to the FASTA file from which to load the sequences for GENCODE transcripts:\"\n-    echo \"gencode_fasta_path =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# Valid values:\"\n-    echo \"#     GENE_NAME\"\n-    echo \"#     TRANSCRIPT_ID\"\n-    echo \"xsv_key =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# The 0-based index of the column containing the key on which to match\"\n-    echo \"xsv_key_column =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV AND locatableXSV files.\"\n-    echo \"# The delimiter by which to split the XSV file into columns.\"\n-    echo \"xsv_delimiter =\"\n-    echo \"\"\n-    echo \"# Required field for simpleXSV files.\"\n-    echo \"# Whether to permissively match the number of columns in the header and data rows\"\n-    echo \"# Valid values:\"\n-    echo \"#     true\"\n-    echo \"#     false\"\n-    echo \"xsv_permissive_cols =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the contig for each row\"\n-    echo \"contig_column =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the start position for each row\"\n-    echo \"start_column =\"\n-    echo \"\"\n-    echo \"# Required field for locatableXSV files.\"\n-    echo \"# The 0-based index of the column containing the end position for each row\"\n-    echo \"end_column =\"\n-    echo \"\"\n+\tlocal dataSourceName=$1\n+\tlocal version=$2\n+\tlocal srcFile=$3\n+\tlocal originLocation=$4\n+\n+\techo \"name = ${dataSourceName}\"\n+\techo \"version = ${version}\"\n+\techo \"src_file = ${srcFile}\"\n+\techo \"origin_location = ${originLocation}\"\n+\techo \"preprocessing_script = ${SCRIPTNAME}\"\n+\techo \"\"\n+\techo \"# Supported types:\"\n+\techo \"# simpleXSV    -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript ID\"\n+\techo \"# locatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome location\"\n+\techo \"# gencode      -- Custom datasource class for GENCODE\"\n+\techo \"# cosmic       -- Custom datasource class for COSMIC\"\n+\techo \"# vcf          -- Custom datasource class for Variant Call Format (VCF) files\"\n+\techo \"type = vcf\"\n+\techo \"\"\n+\techo \"# Required field for GENCODE files.\"\n+\techo \"# Path to the FASTA file from which to load the sequences for GENCODE transcripts:\"\n+\techo \"gencode_fasta_path =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# Valid values:\"\n+\techo \"#     GENE_NAME\"\n+\techo \"#     TRANSCRIPT_ID\"\n+\techo \"xsv_key =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# The 0-based index of the column containing the key on which to match\"\n+\techo \"xsv_key_column =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV AND locatableXSV files.\"\n+\techo \"# The delimiter by which to split the XSV file into columns.\"\n+\techo \"xsv_delimiter =\"\n+\techo \"\"\n+\techo \"# Required field for simpleXSV files.\"\n+\techo \"# Whether to permissively match the number of columns in the header and data rows\"\n+\techo \"# Valid values:\"\n+\techo \"#     true\"\n+\techo \"#     false\"\n+\techo \"xsv_permissive_cols =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the contig for each row\"\n+\techo \"contig_column =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the start position for each row\"\n+\techo \"start_column =\"\n+\techo \"\"\n+\techo \"# Required field for locatableXSV files.\"\n+\techo \"# The 0-based index of the column containing the end position for each row\"\n+\techo \"end_column =\"\n+\techo \"\"\n \n }\n \n function downloadAndVerifyVcfFiles() {\n \n-    local remoteFolder=$1\n-    local outputFolder=$2\n-    local filePrefix=$3\n-\n-    local listingFile=$( makeTemp )\n-    local indentSpace=\"    \"\n-    local version=$( echo \"${remoteFolder}\" | sed \"s#.*human_\\\\(.*b${BUILD_NUMBER}\\\\).*#\\\\1#g\" )\n-\n-    curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n-\n-    vcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${SRC_FILE_REGEX} )\n-    tbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E ${TBI_FILE_REGEX} )\n-    md5File=$( cat ${listingFile} | awk '{print $9}' | grep -E ${MD5_FILE_REGEX} )\n-\n-    echo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n-    wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n-\n-    # Get the VCF file, then make sure that the contig names are correct for HG19 (if applicable)\n-    echo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n-    if [[ \"${filePrefix}\" == \"hg19\" ]] ; then\n-        curl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${vcfFile}\n-    else\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n-\n-        echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n-        wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n-\n-        # We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n-        echo \"${indentSpace}Verifying VCF checksum ...\"\n-        if [[ \"$(uname)\" == \"Darwin\" ]] ; then\n-            which md5sum-lite &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n-            fi\n-        else\n-            which md5sum &> /dev/null\n-            r=$?\n-            if [ $r == 0 ] ; then\n-                checksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-                expected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n-\n-                if [[ \"${checksum}\" != \"${expected}\" ]] ; then\n-                    error \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n-                    error \"FAILING\"\n-                    exit 4\n-                fi\n-            else\n-                error \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n-            fi\n-        fi\n-    fi\n-\n-    # Now put it in the right place and clean up:\n-    echo \"${indentSpace}Creating output directory ...\"\n-    mkdir -p ${outputFolder}\n-\n-    echo \"${indentSpace}Moving files to output directory ...\"\n-    mv ${vcfFile} ${outputFolder}/${filePrefix}_${vcfFile}\n-    if [[ ! \"${filePrefix}\" == \"hg19\" ]] ; then\n-        mv ${tbiFile} ${outputFolder}/${filePrefix}_${tbiFile}\n-        rm ${md5File}\n-    fi\n-\n-    echo \"${indentSpace}Creating Config File ... \"\n-    createConfigFile \"${DATA_SOURCE_NAME}\" \"${version}\" ${filePrefix}_${vcfFile} \"ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\" > ${outputFolder}/${DATA_SOURCE_NAME}.config\n+\tlocal remoteFolder=$1\n+\tlocal outputFolder=$2\n+\tlocal filePrefix=$3\n+\n+\tlocal listingFile=$( makeTemp )\n+\tlocal indentSpace=\"    \"\n+\tlocal version=$( echo \"${remoteFolder}\" | sed \"s#.*human_\\\\(.*b${BUILD_NUMBER}\\\\).*#\\\\1#g\" )\n+\n+\tlocal tmpVcfFile=$( makeTemp )\n+\n+\tcurl ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/ 2>/dev/null > ${listingFile}\n+\n+\tvcfFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${SRC_FILE_REGEX}\" )\n+\ttbiFile=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${TBI_FILE_REGEX}\" )\n+\tmd5File=$( cat ${listingFile} | awk '{print $9}' | grep -E \"${MD5_FILE_REGEX}\" )\n+\n+\techo \"${indentSpace}Retrieving MD5 sum file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File} ... \"\n+\twget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${md5File}\n+\n+\t# Get the VCF file\n+\techo \"${indentSpace}Retrieving VCF file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile} ... \"\n+\twget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${vcfFile}\n+\n+\t#echo \"${indentSpace}Retrieving VCF Index file: ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile} ... \"\n+\t#wget ftp://ftp.ncbi.nih.gov/snp/organisms/${remoteFolder}/VCF/${tbiFile}\n+\n+\t# We can only verify the checksum with hg38 because we modify the hg19 file as we stream it in:\n+\techo \"${indentSpace}Verifying VCF checksum ...\"\n+\tif [[ \"$(uname)\" == \"Darwin\" ]] ; then\n+\t\twhich md5sum-lite &> /dev/null\n+\t\tr=$?\n+\t\tif [ $r == 0 ] ; then\n+\t\t\tchecksum=$( md5sum-lite ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\t\t\texpected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\n+\t\t\tif [[ \"${checksum}\" != \"${expected}\" ]] ; then\n+\t\t\t\terror \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n+\t\t\t\terror \"FAILING\"\n+\t\t\t\texit 4\n+\t\t\tfi\n+\t\telse\n+\t\t\terror \"Unable to validate md5sum of file: cannot locate 'md5sum-lite'.  Use these data with caution.\"\n+\t\tfi\n+\telse\n+\t\twhich md5sum &> /dev/null\n+\t\tr=$?\n+\t\tif [ $r == 0 ] ; then\n+\t\t\tchecksum=$( md5sum ${vcfFile} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\t\t\texpected=$( head -n1 ${md5File} | awk '{print $1}' | sed -e 's#^[ \\t]*##g' -e 's#[ \\t]*$##g' )\n+\n+\t\t\tif [[ \"${checksum}\" != \"${expected}\" ]] ; then\n+\t\t\t\terror \"DOWNLOADED FILE IS CORRUPT!  (${checksum} != ${expected})\"\n+\t\t\t\terror \"FAILING\"\n+\t\t\t\texit 4\n+\t\t\tfi\n+\t\telse\n+\t\t\terror \"Unable to validate md5sum of file: cannot locate 'md5sum'.  Use these data with caution.\"\n+\t\tfi\n+\tfi\n+\n+\t# Now change the contigs in the file:\n+\tcat ${vcfFile} | gunzip | sed -e 's#^\\([0-9][0-9]*\\)#chr\\1#' -e 's#^MT#chrM#' -e 's#^X#chrX#' -e 's#^Y#chrY#' | bgzip > ${tmpVcfFile} \n+\tmv ${tmpVcfFile} ${vcfFile}\n+\n+\t# Now put it in the right place and clean up:\n+\techo \"${indentSpace}Creating output directory ...\"\n+\tmkdir -p ${outputFolder}\n+\n+\techo \"${indentSpace}Moving files to output directory ...\"\n+\tmv ${vcfFile} ${outputFolder}/${filePrefix}_${vcfFile}\n+\tif [[ ! \"${filePrefix}\" == \"hg19\" ]] ; then\n+\t\tmv ${tbiFile} ${outputFolder}/${filePrefix}_${tbiFile}\n+\t\trm ${md5File}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f6221ca7b4d12c77462f2c2fea1496d6e2032fb"}, "originalPosition": 364}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxMjkwNQ==", "bodyText": "At least declare /Users/jonn/Development/ as a named constant at the top of the script, so that someone else could potentially make the script work by editing only a single line.", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r471712905", "createdAt": "2020-08-17T18:58:05Z", "author": {"login": "droazen"}, "path": "scripts/funcotator/testing/testFuncotator.sh", "diffHunk": "@@ -56,6 +57,7 @@ MANUAL_MODE=false\n ################################################################################\n \n # Change this to point to your funcotator data sources folder:\n+DATA_SOURCES_PATH_16=/Users/jonn/Development/funcotator_dataSources.v1.6.20190124s\n DATA_SOURCES_PATH=/Users/jonn/Development/funcotator_dataSources_latest\n DATA_SOURCES_PATH_GERMLINE=/Users/jonn/Development/funcotator_dataSources_germline_latest", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1Njk2NA=="}, "originalCommit": {"oid": "54ff67a5555d4b0ca9074071c8c10658edfb3a46"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxNDQzMg==", "bodyText": "Add a comment about not ignoring trailing whitespace here.", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r471714432", "createdAt": "2020-08-17T19:00:58Z", "author": {"login": "droazen"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/funcotator/mafOutput/MafOutputRendererUnitTest.java", "diffHunk": "@@ -1163,8 +1163,9 @@ public void testWrite(final List<VariantContext> variants, final List<List<Funco\n             }\n         }\n \n+        // Make sure our files are as we expect them to be:\n         try {\n-            IntegrationTestSpec.assertEqualTextFiles(outFile, expectedFile, \"#\");\n+            IntegrationTestSpec.assertEqualTextFiles(outFile, expectedFile, MafOutputRendererConstants.COMMENT_STRING, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f6221ca7b4d12c77462f2c2fea1496d6e2032fb"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxODU3NA==", "bodyText": "You still need to add some kind of Funcotator test to cover the fix for dbsnp. Anything reasonable that fails without the fix and passes with it is fine, provided that it covers the actual files published in the latest datasource release. How about a cloud test (@Test(groups={\"cloud\"})) that does 4 small queries on the actual hosted dbsnp VCF from the latest Funcotator datasources in GCS: a tiny interval on each of chr1, chrM, chrX, and chrY. The test could just assert that you get a certain expected number of records from each query. You can do this trivially using a FeatureDataSource.", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r471718574", "createdAt": "2020-08-17T19:08:53Z", "author": {"login": "droazen"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/funcotator/FuncotatorIntegrationTest.java", "diffHunk": "@@ -498,7 +498,7 @@ private ArgumentsBuilder createBaselineArgumentsForFuncotator(final String varia\n     // DO NOT ADD THIS TO ANY TEST GROUPS!", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f6221ca7b4d12c77462f2c2fea1496d6e2032fb"}, "originalPosition": 1}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1Mjc4MDYx", "url": "https://github.com/broadinstitute/gatk/pull/6660#pullrequestreview-485278061", "createdAt": "2020-09-09T18:14:02Z", "commit": {"oid": "b92ecb99e79116699512e3f34daf7585145965dd"}, "state": "APPROVED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxODoxNDowMlrOHPUEyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxODoyODozM1rOHPUkiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgxOTU5NA==", "bodyText": "These can't be final? Also, generally when something is @VisibleForTesting, the test class is in the same package and so default/package access is sufficient rather than public access.", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485819594", "createdAt": "2020-09-09T18:14:02Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/funcotator/BaseFuncotatorArgumentCollection.java", "diffHunk": "@@ -22,9 +23,11 @@\n      * This variable is necessary to resolve the differences between b37 and hg19 when\n      * dealing with Homo Sapiens samples.\n      */\n-    protected static String FuncotatorReferenceVersionHg19 = \"hg19\";\n+    @VisibleForTesting\n+    public static String FuncotatorReferenceVersionHg19 = \"hg19\";\n     /** String representing the hg38 version of the homo sapiens reference. */\n-    protected static String FuncotatorReferenceVersionHg38 = \"hg38\";\n+    @VisibleForTesting\n+    public static String FuncotatorReferenceVersionHg38 = \"hg38\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b92ecb99e79116699512e3f34daf7585145965dd"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgxOTkxNg==", "bodyText": "Same question: can these be final and package access instead of public?", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485819916", "createdAt": "2020-09-09T18:14:35Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/funcotator/FuncotatorDataSourceDownloader.java", "diffHunk": "@@ -71,17 +71,17 @@\n     //==================================================================================================================\n     // Private Static Members:\n \n-    private static String BASE_URL = \"gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124\";\n+    private static String BASE_URL = \"gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521\";\n \n     private static String GERMLINE_GCLOUD_DATASOURCES_BASEURL     = BASE_URL + \"g\";\n     @VisibleForTesting\n     static Path   GERMLINE_GCLOUD_DATASOURCES_PATH        = IOUtils.getPath(GERMLINE_GCLOUD_DATASOURCES_BASEURL + \".tar.gz\");\n     @VisibleForTesting\n     static Path   GERMLINE_GCLOUD_DATASOURCES_SHA256_PATH = IOUtils.getPath(GERMLINE_GCLOUD_DATASOURCES_BASEURL + \".sha256\");\n \n-    private static String SOMATIC_GCLOUD_DATASOURCES_BASEURL     = BASE_URL + \"s\";\n-    @VisibleForTesting\n-    static Path   SOMATIC_GCLOUD_DATASOURCES_PATH        = IOUtils.getPath(SOMATIC_GCLOUD_DATASOURCES_BASEURL + \".tar.gz\");\n+    public static String SOMATIC_GCLOUD_DATASOURCES_BASEURL     = BASE_URL + \"s\";\n+\n+    public static Path   SOMATIC_GCLOUD_DATASOURCES_PATH        = IOUtils.getPath(SOMATIC_GCLOUD_DATASOURCES_BASEURL + \".tar.gz\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b92ecb99e79116699512e3f34daf7585145965dd"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgyNjg4MQ==", "bodyText": "Open this FeatureDataSource in a try-with-resources block to ensure that it gets closed.", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485826881", "createdAt": "2020-09-09T18:27:09Z", "author": {"login": "droazen"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/funcotator/dataSources/DbSnpIntegrationTest.java", "diffHunk": "@@ -0,0 +1,171 @@\n+package org.broadinstitute.hellbender.tools.funcotator.dataSources;\n+\n+import htsjdk.samtools.util.Locatable;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.CommandLineProgramTest;\n+import org.broadinstitute.hellbender.engine.FeatureDataSource;\n+import org.broadinstitute.hellbender.tools.funcotator.BaseFuncotatorArgumentCollection;\n+import org.broadinstitute.hellbender.tools.funcotator.FuncotatorDataSourceDownloader;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.nio.file.Path;\n+import java.util.List;\n+\n+/**\n+ * Class to hold integration tests for the dbSNP data source.\n+ */\n+public class DbSnpIntegrationTest extends CommandLineProgramTest {\n+\n+    private final Path DB_SNP_HG19_FILE_PATH       = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg19/\" + \"hg19_All_20180423.vcf.gz\"\n+    );\n+    private final Path DB_SNP_HG19_INDEX_FILE_PATH = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg19/\" + \"hg19_All_20180423.vcf.gz.tbi\"\n+    );\n+\n+    private final Path DB_SNP_HG38_FILE_PATH       = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg38/\" + \"hg38_All_20180418.vcf.gz\"\n+    );\n+    private final Path DB_SNP_HG38_INDEX_FILE_PATH = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg38/\" + \"hg38_All_20180418.vcf.gz.tbi\"\n+    );\n+\n+    @DataProvider\n+    private Object[][] provideFortestDbSnpDataSourceParsing() {\n+        return new Object[][] {\n+                // HG19 tests:\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chr1\", 10318704, 10318704),\n+                    \"rs746945770\",\n+                    Allele.create(\"G\", true),\n+                    Allele.create(\"A\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrX\", 31213723, 31213723),\n+                    \"rs5972332\",\n+                    Allele.create(\"C\", true),\n+                    Allele.create(\"T\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrY\", 8551842, 8551842),\n+                    \"rs562075277\",\n+                    Allele.create(\"G\", true),\n+                    Allele.create(\"A\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrM\", 5005, 5005),\n+                    \"rs879008075\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                // HG38 tests:\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chr1\", 84349785, 84349785),\n+                    \"rs17131617\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrX\", 80688070, 80688070),\n+                    \"rs3122407\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrY\", 13355944, 13355944),\n+                    \"rs2032654\",\n+                    Allele.create(\"A\", true),\n+                    Allele.create(\"G\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrM\", 5131, 5133),\n+                    \"rs199476116\",\n+                    Allele.create(\"TAA\", true),\n+                    Allele.create(\"T\")\n+                }\n+        };\n+    }\n+\n+    @Test(groups={\"cloud\"}, dataProvider = \"provideFortestDbSnpDataSourceParsing\")\n+    public void testDbSnpDataSourceParsing( final String refVersion,\n+                                            final Locatable interval,\n+                                            final String expectedID,\n+                                            final Allele expectedRefAllele,\n+                                            final Allele expectedAltAllele) {\n+        // 1 - Get the correct version of dbSNP from the funcotator data sources bucket:\n+        final Path dbSnpFile = (refVersion.equals(BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38))\n+                ? DB_SNP_HG38_FILE_PATH\n+                : DB_SNP_HG19_FILE_PATH;\n+\n+        // 2 - Create a FeatureDataSource from the dbSNP VCF:\n+        final FeatureDataSource<VariantContext> dbSnpDataSource = new FeatureDataSource<>(dbSnpFile.toUri().toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b92ecb99e79116699512e3f34daf7585145965dd"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgyNzM4Ng==", "bodyText": "New test looks good otherwise \ud83d\udc4d", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485827386", "createdAt": "2020-09-09T18:27:56Z", "author": {"login": "droazen"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/funcotator/dataSources/DbSnpIntegrationTest.java", "diffHunk": "@@ -0,0 +1,171 @@\n+package org.broadinstitute.hellbender.tools.funcotator.dataSources;\n+\n+import htsjdk.samtools.util.Locatable;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.CommandLineProgramTest;\n+import org.broadinstitute.hellbender.engine.FeatureDataSource;\n+import org.broadinstitute.hellbender.tools.funcotator.BaseFuncotatorArgumentCollection;\n+import org.broadinstitute.hellbender.tools.funcotator.FuncotatorDataSourceDownloader;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.nio.file.Path;\n+import java.util.List;\n+\n+/**\n+ * Class to hold integration tests for the dbSNP data source.\n+ */\n+public class DbSnpIntegrationTest extends CommandLineProgramTest {\n+\n+    private final Path DB_SNP_HG19_FILE_PATH       = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg19/\" + \"hg19_All_20180423.vcf.gz\"\n+    );\n+    private final Path DB_SNP_HG19_INDEX_FILE_PATH = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg19/\" + \"hg19_All_20180423.vcf.gz.tbi\"\n+    );\n+\n+    private final Path DB_SNP_HG38_FILE_PATH       = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg38/\" + \"hg38_All_20180418.vcf.gz\"\n+    );\n+    private final Path DB_SNP_HG38_INDEX_FILE_PATH = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg38/\" + \"hg38_All_20180418.vcf.gz.tbi\"\n+    );\n+\n+    @DataProvider\n+    private Object[][] provideFortestDbSnpDataSourceParsing() {\n+        return new Object[][] {\n+                // HG19 tests:\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chr1\", 10318704, 10318704),\n+                    \"rs746945770\",\n+                    Allele.create(\"G\", true),\n+                    Allele.create(\"A\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrX\", 31213723, 31213723),\n+                    \"rs5972332\",\n+                    Allele.create(\"C\", true),\n+                    Allele.create(\"T\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrY\", 8551842, 8551842),\n+                    \"rs562075277\",\n+                    Allele.create(\"G\", true),\n+                    Allele.create(\"A\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrM\", 5005, 5005),\n+                    \"rs879008075\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                // HG38 tests:\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chr1\", 84349785, 84349785),\n+                    \"rs17131617\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrX\", 80688070, 80688070),\n+                    \"rs3122407\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrY\", 13355944, 13355944),\n+                    \"rs2032654\",\n+                    Allele.create(\"A\", true),\n+                    Allele.create(\"G\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrM\", 5131, 5133),\n+                    \"rs199476116\",\n+                    Allele.create(\"TAA\", true),\n+                    Allele.create(\"T\")\n+                }\n+        };\n+    }\n+\n+    @Test(groups={\"cloud\"}, dataProvider = \"provideFortestDbSnpDataSourceParsing\")\n+    public void testDbSnpDataSourceParsing( final String refVersion,\n+                                            final Locatable interval,\n+                                            final String expectedID,\n+                                            final Allele expectedRefAllele,\n+                                            final Allele expectedAltAllele) {\n+        // 1 - Get the correct version of dbSNP from the funcotator data sources bucket:\n+        final Path dbSnpFile = (refVersion.equals(BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38))\n+                ? DB_SNP_HG38_FILE_PATH\n+                : DB_SNP_HG19_FILE_PATH;\n+\n+        // 2 - Create a FeatureDataSource from the dbSNP VCF:\n+        final FeatureDataSource<VariantContext> dbSnpDataSource = new FeatureDataSource<>(dbSnpFile.toUri().toString());\n+\n+        // Do a dummy check here:\n+        Assert.assertNotNull(dbSnpDataSource);\n+\n+        // 3 - Attempt to read sites and features from the FeatureDataSource:\n+        final List<VariantContext> features = dbSnpDataSource.queryAndPrefetch(interval);\n+        Assert.assertEquals(features.size(), 1);\n+\n+        final VariantContext dbSnpVariant = features.get(0);\n+        Assert.assertEquals(dbSnpVariant.getContig(), interval.getContig());\n+        Assert.assertEquals(dbSnpVariant.getStart(), interval.getStart());\n+        Assert.assertEquals(dbSnpVariant.getEnd(), interval.getEnd());\n+        Assert.assertEquals(dbSnpVariant.getID(), expectedID);\n+        Assert.assertEquals(dbSnpVariant.getAlleles().size(), 2);\n+        Assert.assertEquals(dbSnpVariant.getAlleles().get(0), expectedRefAllele, \"Variant has incorrect ref allele: \" + dbSnpVariant.getAlleles().get(0)  + \" != \" + expectedRefAllele + \" [\" + interval + \" in \" + dbSnpFile + \"]\");\n+        Assert.assertEquals(dbSnpVariant.getAlleles().get(1), expectedAltAllele);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b92ecb99e79116699512e3f34daf7585145965dd"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgyNzcyMw==", "bodyText": "Remove commented-out code", "url": "https://github.com/broadinstitute/gatk/pull/6660#discussion_r485827723", "createdAt": "2020-09-09T18:28:33Z", "author": {"login": "droazen"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/funcotator/dataSources/DbSnpIntegrationTest.java", "diffHunk": "@@ -0,0 +1,171 @@\n+package org.broadinstitute.hellbender.tools.funcotator.dataSources;\n+\n+import htsjdk.samtools.util.Locatable;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.CommandLineProgramTest;\n+import org.broadinstitute.hellbender.engine.FeatureDataSource;\n+import org.broadinstitute.hellbender.tools.funcotator.BaseFuncotatorArgumentCollection;\n+import org.broadinstitute.hellbender.tools.funcotator.FuncotatorDataSourceDownloader;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.io.IOUtils;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.nio.file.Path;\n+import java.util.List;\n+\n+/**\n+ * Class to hold integration tests for the dbSNP data source.\n+ */\n+public class DbSnpIntegrationTest extends CommandLineProgramTest {\n+\n+    private final Path DB_SNP_HG19_FILE_PATH       = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg19/\" + \"hg19_All_20180423.vcf.gz\"\n+    );\n+    private final Path DB_SNP_HG19_INDEX_FILE_PATH = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg19/\" + \"hg19_All_20180423.vcf.gz.tbi\"\n+    );\n+\n+    private final Path DB_SNP_HG38_FILE_PATH       = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg38/\" + \"hg38_All_20180418.vcf.gz\"\n+    );\n+    private final Path DB_SNP_HG38_INDEX_FILE_PATH = IOUtils.getPath(\n+            FuncotatorDataSourceDownloader.SOMATIC_GCLOUD_DATASOURCES_BASEURL + \"/\"\n+                    + \"dbsnp/hg38/\" + \"hg38_All_20180418.vcf.gz.tbi\"\n+    );\n+\n+    @DataProvider\n+    private Object[][] provideFortestDbSnpDataSourceParsing() {\n+        return new Object[][] {\n+                // HG19 tests:\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chr1\", 10318704, 10318704),\n+                    \"rs746945770\",\n+                    Allele.create(\"G\", true),\n+                    Allele.create(\"A\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrX\", 31213723, 31213723),\n+                    \"rs5972332\",\n+                    Allele.create(\"C\", true),\n+                    Allele.create(\"T\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrY\", 8551842, 8551842),\n+                    \"rs562075277\",\n+                    Allele.create(\"G\", true),\n+                    Allele.create(\"A\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg19,\n+                    new SimpleInterval(\"chrM\", 5005, 5005),\n+                    \"rs879008075\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                // HG38 tests:\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chr1\", 84349785, 84349785),\n+                    \"rs17131617\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrX\", 80688070, 80688070),\n+                    \"rs3122407\",\n+                    Allele.create(\"T\", true),\n+                    Allele.create(\"C\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrY\", 13355944, 13355944),\n+                    \"rs2032654\",\n+                    Allele.create(\"A\", true),\n+                    Allele.create(\"G\")\n+                },\n+                {\n+                    BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38,\n+                    new SimpleInterval(\"chrM\", 5131, 5133),\n+                    \"rs199476116\",\n+                    Allele.create(\"TAA\", true),\n+                    Allele.create(\"T\")\n+                }\n+        };\n+    }\n+\n+    @Test(groups={\"cloud\"}, dataProvider = \"provideFortestDbSnpDataSourceParsing\")\n+    public void testDbSnpDataSourceParsing( final String refVersion,\n+                                            final Locatable interval,\n+                                            final String expectedID,\n+                                            final Allele expectedRefAllele,\n+                                            final Allele expectedAltAllele) {\n+        // 1 - Get the correct version of dbSNP from the funcotator data sources bucket:\n+        final Path dbSnpFile = (refVersion.equals(BaseFuncotatorArgumentCollection.FuncotatorReferenceVersionHg38))\n+                ? DB_SNP_HG38_FILE_PATH\n+                : DB_SNP_HG19_FILE_PATH;\n+\n+        // 2 - Create a FeatureDataSource from the dbSNP VCF:\n+        final FeatureDataSource<VariantContext> dbSnpDataSource = new FeatureDataSource<>(dbSnpFile.toUri().toString());\n+\n+        // Do a dummy check here:\n+        Assert.assertNotNull(dbSnpDataSource);\n+\n+        // 3 - Attempt to read sites and features from the FeatureDataSource:\n+        final List<VariantContext> features = dbSnpDataSource.queryAndPrefetch(interval);\n+        Assert.assertEquals(features.size(), 1);\n+\n+        final VariantContext dbSnpVariant = features.get(0);\n+        Assert.assertEquals(dbSnpVariant.getContig(), interval.getContig());\n+        Assert.assertEquals(dbSnpVariant.getStart(), interval.getStart());\n+        Assert.assertEquals(dbSnpVariant.getEnd(), interval.getEnd());\n+        Assert.assertEquals(dbSnpVariant.getID(), expectedID);\n+        Assert.assertEquals(dbSnpVariant.getAlleles().size(), 2);\n+        Assert.assertEquals(dbSnpVariant.getAlleles().get(0), expectedRefAllele, \"Variant has incorrect ref allele: \" + dbSnpVariant.getAlleles().get(0)  + \" != \" + expectedRefAllele + \" [\" + interval + \" in \" + dbSnpFile + \"]\");\n+        Assert.assertEquals(dbSnpVariant.getAlleles().get(1), expectedAltAllele);\n+    }\n+\n+\n+////    @Test(groups={\"cloud\"})\n+//    @Test\n+//    public void testDbSnpDataSourceParsing() {\n+//        // 1 - Extract the dbSNP file from the current datasources for Funcotator:\n+//        logger.info(\"Creating input stream from gcloud file:\");\n+//        try (final InputStream dataSourcesInputStream = new BufferedInputStream(Files.newInputStream(DB_SNP_FILE_NAME))) {\n+//            final Path dbSnpPath = extractAndReturnDbSnpPath(dataSourcesInputStream);\n+//\n+//            // 2 - Create a FeatureDataSource from the dbSNP VCF:\n+//            final FeatureDataSource<VariantContext> dbSnpDataSource = new FeatureDataSource<>(dbSnpPath.toUri().toString());\n+//\n+//            // Do a dummy check here:\n+//            Assert.assertNotNull(dbSnpDataSource);\n+//\n+//            // 3 - Attempt to read sites and features from the FeatureDataSource that would fail with the old code:\n+//            final List<Locatable> intervalsToQuery = Arrays.asList(\n+//                    new SimpleInterval(\"chr1\", 84349784, 84349786), // rs17131617 T/C\n+//                    new SimpleInterval(\"chrX\", 80688069, 80688071), // rs3122407 T/C\n+//                    new SimpleInterval(\"chrY\", 13355943, 13355945), // rs2032654 A/G\n+//                    new SimpleInterval(\"chrM\", 5131, 5134)    // rs199476116 2-BP DEL, 5132AA\n+//            );\n+//\n+//            for (int i = 0; i < intervalsToQuery.size(); ++i) {\n+//                final Locatable interval = intervalsToQuery.get(i);\n+//                final List<VariantContext> features = dbSnpDataSource.queryAndPrefetch(interval);\n+//\n+//                Assert.assertEquals(features.size(), 1);\n+//            }\n+//        }\n+//        catch (final IOException ex) {\n+//            throw new UserException(\"Unable to open data sources from gcloud!\", ex);\n+//        }\n+//    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b92ecb99e79116699512e3f34daf7585145965dd"}, "originalPosition": 170}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f910b260717c8b8a1d2da6092bff7e1aede78961", "author": {"user": {"login": "jonn-smith", "name": "Jonn Smith"}}, "url": "https://github.com/broadinstitute/gatk/commit/f910b260717c8b8a1d2da6092bff7e1aede78961", "committedDate": "2020-09-10T14:53:14Z", "message": "Fixed issue with dbSNP source data for hg38.\n\nCode updates:\n- Now both hg19 and hg38 have the contig names translated to `chr__`\n- Added 'lncRNA' to GeneTranscriptType.\n- Added \"TAGENE\" gene tag.\n- Added the MANE_SELECT tag to FeatureTag.\n- Added the STOP_CODON_READTHROUGH tag to FeatureTag.\n- Updated the GTF versions that are parseable.\n- Fixed a parsing error with new versions of gencode and the remap\npositions (for liftover files).\n- Added test for indexing new lifted over gencode GTF.\n- Added Gencode_34 entries to MAF output map.\n- Minor changes to FuncotatorIntegrationTest.java for code syntax.\n- Pointed data source downloader at new data sources URL.\n- Minor updates to workflows to point at new data sources.\n\nScript updates:\n- Updated retrieval scripts for dbSNP and Gencode.\n- Added required field to gencode config file generation.\n- Now gencode retrieval script enforces double hash comments at\ntop of gencode GTF files.\n\nBug Fixes:\nRemoving erroneous trailing tab in MAF file output.\n\n- Fixes #6693"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "33b1e51397ab7e4400a2056acda5172bd6965be7", "author": {"user": {"login": "jonn-smith", "name": "Jonn Smith"}}, "url": "https://github.com/broadinstitute/gatk/commit/33b1e51397ab7e4400a2056acda5172bd6965be7", "committedDate": "2020-09-10T14:53:14Z", "message": "Adding missing resource file."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de266c79f528014909a296b651913c71489483a8", "author": {"user": {"login": "jonn-smith", "name": "Jonn Smith"}}, "url": "https://github.com/broadinstitute/gatk/commit/de266c79f528014909a296b651913c71489483a8", "committedDate": "2020-09-10T14:53:15Z", "message": "Addressing code review comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "087c6c8e35fad15a6c8f7e62047b1ef321b4628b", "author": {"user": {"login": "jonn-smith", "name": "Jonn Smith"}}, "url": "https://github.com/broadinstitute/gatk/commit/087c6c8e35fad15a6c8f7e62047b1ef321b4628b", "committedDate": "2020-09-10T14:53:15Z", "message": "Addressing code review comments.  Need to fix test."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b0b144c9badf44389a0521a878b753fb9ea5d01", "author": {"user": {"login": "jonn-smith", "name": "Jonn Smith"}}, "url": "https://github.com/broadinstitute/gatk/commit/6b0b144c9badf44389a0521a878b753fb9ea5d01", "committedDate": "2020-09-10T14:53:16Z", "message": "Added tests for accessing / reading from hg38 dbSNP."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1906b06fb57e169a43dbbc18d1248eba0bc18ca5", "author": {"user": {"login": "jonn-smith", "name": "Jonn Smith"}}, "url": "https://github.com/broadinstitute/gatk/commit/1906b06fb57e169a43dbbc18d1248eba0bc18ca5", "committedDate": "2020-09-10T14:53:16Z", "message": "Adding final tests for dbSNP."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fe7f0c4470501f8e0437d94ee9669f997ef40e2d", "author": {"user": {"login": "jonn-smith", "name": "Jonn Smith"}}, "url": "https://github.com/broadinstitute/gatk/commit/fe7f0c4470501f8e0437d94ee9669f997ef40e2d", "committedDate": "2020-09-10T14:53:17Z", "message": "Minor test fixes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "99711688bb1a6c7e7bfbac7a4fa3ccd1fb904386", "author": {"user": {"login": "jonn-smith", "name": "Jonn Smith"}}, "url": "https://github.com/broadinstitute/gatk/commit/99711688bb1a6c7e7bfbac7a4fa3ccd1fb904386", "committedDate": "2020-09-10T14:53:17Z", "message": "Finishing the last round of comment responses."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "be7daea9872500bec7decd7abb14abfc850a8425", "author": {"user": {"login": "jonn-smith", "name": "Jonn Smith"}}, "url": "https://github.com/broadinstitute/gatk/commit/be7daea9872500bec7decd7abb14abfc850a8425", "committedDate": "2020-09-10T14:53:17Z", "message": "Missed a comment.  Actually last commit."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bf2e2c888a0d717b88423c3c3ffe11f7bc6f9aec", "author": {"user": {"login": "jonn-smith", "name": "Jonn Smith"}}, "url": "https://github.com/broadinstitute/gatk/commit/bf2e2c888a0d717b88423c3c3ffe11f7bc6f9aec", "committedDate": "2020-09-09T18:59:14Z", "message": "Missed a comment.  Actually last commit."}, "afterCommit": {"oid": "be7daea9872500bec7decd7abb14abfc850a8425", "author": {"user": {"login": "jonn-smith", "name": "Jonn Smith"}}, "url": "https://github.com/broadinstitute/gatk/commit/be7daea9872500bec7decd7abb14abfc850a8425", "committedDate": "2020-09-10T14:53:17Z", "message": "Missed a comment.  Actually last commit."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2643, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}