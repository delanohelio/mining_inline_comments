{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkyNTQ5MDEw", "number": 6520, "title": "Make adaptive pruner smarter in complex graphs", "bodyText": "@jamesemery This fixes the case you found, hopefully bringing us closer to turning on linked de Bruijn graphs.  I will start testing M2.  If you test in HC, continue to keep in mind that adaptive pruning is not default.\nThis change will be most important for rare complex graphs and in combination with junction trees but I did see modest improvements to indel sensitivity even with the old assembly.", "createdAt": "2020-03-23T18:01:04Z", "url": "https://github.com/broadinstitute/gatk/pull/6520", "merged": true, "mergeCommit": {"oid": "07aed754995717c02408517f8def57a8b8713ed7"}, "closed": true, "closedAt": "2020-09-11T01:40:28Z", "author": {"login": "davidbenjamin"}, "timelineItems": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcQkeNbgFqTM3OTgwMzY1Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdHlNX6AH2gAyMzkyNTQ5MDEwOjg3N2VjM2U5YWQ0MjllNjIzYmU1ZGY4ODBmOWEyZTJiZjkxOTBjYTk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc5ODAzNjU3", "url": "https://github.com/broadinstitute/gatk/pull/6520#pullrequestreview-379803657", "createdAt": "2020-03-23T20:38:26Z", "commit": {"oid": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QyMDozODoyNlrOF6XNCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QyMDozODoyNlrOF6XNCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0MTg5OA==", "bodyText": "So my understanding, now we are saving all chains with a MAX edge weight of (graph max / 10) and then growing outwards from there based on the same log odds ratio we had before correct? Except now we are demanding that at least one edge stems from both an already blessed \"good\" edge instead of both edges needing to satisfy the log  odds test. This seems reasonable I think but I want to think a little more on it. Have some comments I noticed. I will give this a try on haplotype caller.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r396741898", "createdAt": "2020-03-23T20:38:26Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +30,73 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+\n+        // start from set of really obvious good chains, defined as those with an edge with multiplicity above some fraction of the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc5ODA1MzAz", "url": "https://github.com/broadinstitute/gatk/pull/6520#pullrequestreview-379805303", "createdAt": "2020-03-23T20:40:59Z", "commit": {"oid": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QyMDo0MDo1OVrOF6XSRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QyMDo0MDo1OVrOF6XSRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0MzIzOQ==", "bodyText": "Parameterize the \"10\". Also I would make the comment that the maxEdgeWeight / 10 will probably falter somewhat around homopolymers in the graph, as a 40 base homopolomyer at 25mer weight without any issues in coverage should have (40 - 25) or 15x the coverage of any of the sequence around it. This doesn't matter for the old graph code but it might for the new graph code and I suspect this will call everything \"bad\" and either be very slow or possibly throw almost everything away.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r396743239", "createdAt": "2020-03-23T20:40:59Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +30,73 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+\n+        // start from set of really obvious good chains, defined as those with an edge with multiplicity above some fraction of the\n+        // maximum multiplicity edge.  Then grow the set of good chains by looking at the log odds of junctions between\n+        // good chains and unknown chains.\n+        final int maxEdgeWeight = graph.edgeSet().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0);\n+        final int thresholdEdgeWeight = maxEdgeWeight / 10;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc5ODA1NDM5", "url": "https://github.com/broadinstitute/gatk/pull/6520#pullrequestreview-379805439", "createdAt": "2020-03-23T20:41:11Z", "commit": {"oid": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QyMDo0MToxMlrOF6XSsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QyMDo0MToxMlrOF6XSsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0MzM0Ng==", "bodyText": "Why not recover reference chains here?", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r396743346", "createdAt": "2020-03-23T20:41:12Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +30,73 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+\n+        // start from set of really obvious good chains, defined as those with an edge with multiplicity above some fraction of the\n+        // maximum multiplicity edge.  Then grow the set of good chains by looking at the log odds of junctions between\n+        // good chains and unknown chains.\n+        final int maxEdgeWeight = graph.edgeSet().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0);\n+        final int thresholdEdgeWeight = maxEdgeWeight / 10;\n+        final Set<Path<V, E>> definiteGoodChains = chains.stream()\n+                .filter(chain -> chain.getEdges().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0) > thresholdEdgeWeight)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc5ODA2NTkz", "url": "https://github.com/broadinstitute/gatk/pull/6520#pullrequestreview-379806593", "createdAt": "2020-03-23T20:42:56Z", "commit": {"oid": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QyMDo0Mjo1NlrOF6XWDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QyMDo1Nzo0NlrOF6X0LA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0NDIwNQ==", "bodyText": "Put a cap on the number of iterations here. I'm worried this might loop functionally forever on very pathological/crazy high coverage sites with lots of chains and a high disparity of depths.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r396744205", "createdAt": "2020-03-23T20:42:56Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +30,73 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+\n+        // start from set of really obvious good chains, defined as those with an edge with multiplicity above some fraction of the\n+        // maximum multiplicity edge.  Then grow the set of good chains by looking at the log odds of junctions between\n+        // good chains and unknown chains.\n+        final int maxEdgeWeight = graph.edgeSet().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0);\n+        final int thresholdEdgeWeight = maxEdgeWeight / 10;\n+        final Set<Path<V, E>> definiteGoodChains = chains.stream()\n+                .filter(chain -> chain.getEdges().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0) > thresholdEdgeWeight)\n+                .collect(Collectors.toSet());\n+\n+        final Set<V> terminalVerticesOfGoodChains = definiteGoodChains.stream()\n+                .flatMap(c -> Stream.of(c.getFirstVertex(), c.getLastVertex())).collect(Collectors.toSet());\n+\n+        // initialize result to all chains that aren't definitely good and iteratively reduce this set as chains in\n+        // question are found to have good log odds at junctions with good chains\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !definiteGoodChains.contains(c)).collect(Collectors.toSet());\n+\n+        // we only need to calculate log odds for questionable chains\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = errorChains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        while (true) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0NTk0NQ==", "bodyText": "I'm worried about this sort, it seems to me that the LogOdds would have discrete values (since they are based on comparing counts that are descrete) so it seems likely there would be a tie here. Without a tiebreaker this seems prone to non-deterministic behavior. I would recommend either putting a relatively bulletproof set of tiebreakers in here or making all of the underlying sets Linked.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r396745945", "createdAt": "2020-03-23T20:46:23Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +30,73 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+\n+        // start from set of really obvious good chains, defined as those with an edge with multiplicity above some fraction of the\n+        // maximum multiplicity edge.  Then grow the set of good chains by looking at the log odds of junctions between\n+        // good chains and unknown chains.\n+        final int maxEdgeWeight = graph.edgeSet().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0);\n+        final int thresholdEdgeWeight = maxEdgeWeight / 10;\n+        final Set<Path<V, E>> definiteGoodChains = chains.stream()\n+                .filter(chain -> chain.getEdges().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0) > thresholdEdgeWeight)\n+                .collect(Collectors.toSet());\n+\n+        final Set<V> terminalVerticesOfGoodChains = definiteGoodChains.stream()\n+                .flatMap(c -> Stream.of(c.getFirstVertex(), c.getLastVertex())).collect(Collectors.toSet());\n+\n+        // initialize result to all chains that aren't definitely good and iteratively reduce this set as chains in\n+        // question are found to have good log odds at junctions with good chains\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !definiteGoodChains.contains(c)).collect(Collectors.toSet());\n+\n+        // we only need to calculate log odds for questionable chains\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = errorChains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        while (true) {\n+            final Set<Path<V, E>> moreGoodChains = new HashSet<>();\n+\n+            for (final Path<V, E> chain : errorChains) {\n+                if (terminalVerticesOfGoodChains.contains(chain.getFirstVertex()) && chainLogOdds.get(chain).getLeft() > logOddsThreshold) {\n+                    moreGoodChains.add(chain);\n+                    terminalVerticesOfGoodChains.add(chain.getLastVertex());\n+                } else if (terminalVerticesOfGoodChains.contains(chain.getLastVertex()) && chainLogOdds.get(chain).getRight() > logOddsThreshold) {\n+                    moreGoodChains.add(chain);\n+                    terminalVerticesOfGoodChains.add(chain.getFirstVertex());\n+                }\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            if (moreGoodChains.isEmpty()) {\n+                break;\n+            } else {\n+                errorChains.removeAll(moreGoodChains);\n             }\n-        });\n+        }\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n+        // add non-error chains to error chains if maximum number of variants has been exceeded\n+        chains.stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0Nzg3Nw==", "bodyText": "I would recommend including a test where the reference has \"badMultiplicity\" and making sure that we aren't accidentally pruning that and causing potential problems.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r396747877", "createdAt": "2020-03-23T20:50:13Z", "author": {"login": "jamesemery"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPrunerUnitTest.java", "diffHunk": "@@ -175,6 +176,47 @@ public void testAdaptivePruning(final int kmerSize, final byte[] ref, final byte\n         Assert.assertTrue(bestPaths.size() < 15);\n     }\n \n+    // test that in graph with good path A -> B -> C and bad edges A -> D -> C and D -> B that the adjacency of bad edges --\n+    // such that when bad edges meet the multiplicities do not indicate an error - does not harm pruning.\n+    // we test with and without a true variant path A -> E -> C\n+    @Test\n+    public void testAdaptivePruningWithAdjacentBadEdges() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc1MTkxNg==", "bodyText": "On second thought, i think there might be a more efficient way to do this. If you preprocess every chain into a map of end vertex-> list of adjacent chains and then just poll them in a queue looking at each vertex exactly once (or putting it into the queue if we discover a new vertex from our processing) i think should avoid the problem of iterating over the same gazillion error chains forever and only ever making progress on one or two vertices at a time. Indeed it seems to me that once a vertex is \"good\" we need only check its incoming and outgoing neighbors exactly once and then we can skip ever looking at its neighboring vertexes ever again.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r396751916", "createdAt": "2020-03-23T20:57:46Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +30,73 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+\n+        // start from set of really obvious good chains, defined as those with an edge with multiplicity above some fraction of the\n+        // maximum multiplicity edge.  Then grow the set of good chains by looking at the log odds of junctions between\n+        // good chains and unknown chains.\n+        final int maxEdgeWeight = graph.edgeSet().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0);\n+        final int thresholdEdgeWeight = maxEdgeWeight / 10;\n+        final Set<Path<V, E>> definiteGoodChains = chains.stream()\n+                .filter(chain -> chain.getEdges().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0) > thresholdEdgeWeight)\n+                .collect(Collectors.toSet());\n+\n+        final Set<V> terminalVerticesOfGoodChains = definiteGoodChains.stream()\n+                .flatMap(c -> Stream.of(c.getFirstVertex(), c.getLastVertex())).collect(Collectors.toSet());\n+\n+        // initialize result to all chains that aren't definitely good and iteratively reduce this set as chains in\n+        // question are found to have good log odds at junctions with good chains\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !definiteGoodChains.contains(c)).collect(Collectors.toSet());\n+\n+        // we only need to calculate log odds for questionable chains\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = errorChains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        while (true) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0NDIwNQ=="}, "originalCommit": {"oid": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035"}, "originalPosition": 56}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035", "committedDate": "2020-03-23T17:49:36Z", "message": "bug fix for trimming haplotype with insertion at trim start"}, "afterCommit": {"oid": "abcd704a9d283548eba0d1186754fbfce636ad25", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/abcd704a9d283548eba0d1186754fbfce636ad25", "committedDate": "2020-04-30T05:29:05Z", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "abcd704a9d283548eba0d1186754fbfce636ad25", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/abcd704a9d283548eba0d1186754fbfce636ad25", "committedDate": "2020-04-30T05:29:05Z", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors"}, "afterCommit": {"oid": "908b7f35f3080265c583b72ef0fa23c0e6925347", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/908b7f35f3080265c583b72ef0fa23c0e6925347", "committedDate": "2020-04-30T05:33:35Z", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "908b7f35f3080265c583b72ef0fa23c0e6925347", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/908b7f35f3080265c583b72ef0fa23c0e6925347", "committedDate": "2020-04-30T05:33:35Z", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors"}, "afterCommit": {"oid": "5806cefcc10c6073fc770c5dfebbde0ba787e28b", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/5806cefcc10c6073fc770c5dfebbde0ba787e28b", "committedDate": "2020-04-30T05:39:18Z", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5806cefcc10c6073fc770c5dfebbde0ba787e28b", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/5806cefcc10c6073fc770c5dfebbde0ba787e28b", "committedDate": "2020-04-30T05:39:18Z", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors"}, "afterCommit": {"oid": "5f399678db389bd479d253bc5abf19514941330f", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/5f399678db389bd479d253bc5abf19514941330f", "committedDate": "2020-04-30T05:53:04Z", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5f399678db389bd479d253bc5abf19514941330f", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/5f399678db389bd479d253bc5abf19514941330f", "committedDate": "2020-04-30T05:53:04Z", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors"}, "afterCommit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "committedDate": "2020-05-04T20:39:19Z", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzNzg4MzIy", "url": "https://github.com/broadinstitute/gatk/pull/6520#pullrequestreview-423788322", "createdAt": "2020-06-03T17:44:04Z", "commit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzo0NDowNFrOGemtlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzo0NDowNFrOGemtlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0NDcyNQ==", "bodyText": "This is good, I think this resolves my previous concern about potentially pruning ref edges. I still think it would be prudent to adapt the test you have below to include a reference path that could/should be pruned otherwise just to be sure.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434744725", "createdAt": "2020-06-03T17:44:04Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +33,105 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzNzkzMjky", "url": "https://github.com/broadinstitute/gatk/pull/6520#pullrequestreview-423793292", "createdAt": "2020-06-03T17:50:41Z", "commit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxODowMTozNlrOGenWAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxODo1MzowMFrOGepGbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1NTA3NA==", "bodyText": "Interesting, I do think you are right that this solves the problem of drastically differing edge weights being commonplace.  I am a little concerned though, it seems that having no ties whatsoever to the depth of good/reference will run the risk of graphs that look something like this rescuing edges:\n........................../ (1x coverage) B ---\n........................./ (1xcoverage)  A -----\\\n......................../ (1x coverage)...............\nREF SOURCE / ---- (100 coverage) C ------REFSink\nUnder your last proposal we would only have selected C or ref sinks as seeds and then we would have thrown away every other path, whereas now its possible (certain?) that both A and B would initially be marked as seed vertexes and consequently we would still end up recovering those paths. That seems problematic, as i'm sure there are other strange edge cases where low weight error kmers \"rescue\" each-other.  Still though I think this fixes a bunch of the cases.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434755074", "createdAt": "2020-06-03T18:01:36Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +33,105 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        /////// NEW\n+\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n+\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n+            }\n+\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2MDQ2Nw==", "bodyText": "I guess part of the question is about the LOD score and how its computed, perhaps we could harden ourselves to some of these edge cases if it weren't the case that a bubble where both ends are anchored by 1 base verses an alternative path that is also anchored by 1 base. Perhaps by weighting against edges with below some minimum of coverage anyway.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434760467", "createdAt": "2020-06-03T18:11:09Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +33,105 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        /////// NEW\n+\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n+\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n+            }\n+\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1NTA3NA=="}, "originalCommit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2NDA5NA==", "bodyText": "I wouldn't use Path::hashCode to break ties here as the underlying Path objects are LinkedLists with the java object hash function which is based on the memory address for the object and consequently could still vary from run to run. If you want to do something quick and dirty you could use the lexicographic ordering for the first kmer in the chain (which should never match (well maybe they could in the edge case where we duplicate repeated kmers...)).", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434764094", "createdAt": "2020-06-03T18:17:41Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +33,105 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        /////// NEW\n+\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n+\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n+            }\n+\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n+\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new HashSet<>();\n+        final Set<Path<V,E>> goodChains = new HashSet<>();\n+\n+        // starting from the high-confidence seed vertices, grow the \"good\" subgraph along chains with above-threshold log odds,\n+        // discovering good chains as we go.\n+        while (!verticesToProcess.isEmpty()) {\n+            final V vertex = verticesToProcess.poll();\n+            processedVertices.add(vertex);\n+            for (final Path<V,E> outgoingChain : vertexToGoodOutgoingChains.get(vertex)) {\n+                goodChains.add(outgoingChain);\n+                if(!processedVertices.contains(outgoingChain.getLastVertex())) {\n+                    verticesToProcess.add(outgoingChain.getLastVertex());\n+                }\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            for (final Path<V,E> incomingChain : vertexToGoodIncomingChains.get(vertex)) {\n+                goodChains.add(incomingChain);\n+                if(!processedVertices.contains(incomingChain.getFirstVertex())) {\n+                    verticesToProcess.add(incomingChain.getFirstVertex());\n+                }\n             }\n-        });\n+        }\n+\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !goodChains.contains(c)).collect(Collectors.toSet());\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n+        // add non-error chains to error chains if maximum number of variants has been exceeded\n+        chains.stream()\n+                .filter(c -> !errorChains.contains(c))\n+                .filter(c -> isChainPossibleVariant(c, graph))\n+                .sorted(Comparator.comparingDouble(c -> Math.min(chainLogOdds.get(c).getLeft(), chainLogOdds.get(c).getLeft())).reversed())\n                 .skip(maxUnprunedVariants)\n-                .forEach(result::add);\n+                .forEach(errorChains::add);\n \n-        return result;\n+        return errorChains;\n \n     }\n \n-    private double chainLogOdds(final Path<V,E> chain, final BaseGraph<V,E> graph, final double errorRate) {\n-        if (chain.getEdges().stream().anyMatch(E::isRef)) {\n-            return Double.POSITIVE_INFINITY;\n-        }\n+    // find the chain containing the edge of greatest weight, taking care to break ties deterministically\n+    private Path<V, E> getMaxWeightChain(final Collection<Path<V, E>> chains) {\n+        return chains.stream()\n+                .max(Comparator.comparingInt((Path<V, E> chain) -> chain.getEdges().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0))\n+                        .thenComparingInt(Path::length)\n+                        .thenComparingInt(Path::hashCode)).get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2ODE0Nw==", "bodyText": "Is there a reason we are using a flat chain/seed pruning log-odds here? Why not test with the defaults laid out in the mutect2 argument collection?", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434768147", "createdAt": "2020-06-03T18:24:38Z", "author": {"login": "jamesemery"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPrunerUnitTest.java", "diffHunk": "@@ -175,6 +175,47 @@ public void testAdaptivePruning(final int kmerSize, final byte[] ref, final byte\n         Assert.assertTrue(bestPaths.size() < 15);\n     }\n \n+    // test that in graph with good path A -> B -> C and bad edges A -> D -> C and D -> B that the adjacency of bad edges --\n+    // such that when bad edges meet the multiplicities do not indicate an error - does not harm pruning.\n+    // we test with and without a true variant path A -> E -> C\n+    @Test\n+    public void testAdaptivePruningWithAdjacentBadEdges() {\n+        final int goodMultiplicity = 1000;\n+        final int variantMultiplicity = 50;\n+        final int badMultiplicity = 5;\n+\n+        final SeqVertex source = new SeqVertex(\"source\");\n+        final SeqVertex sink = new SeqVertex(\"sink\");\n+        final SeqVertex A = new SeqVertex(\"A\");\n+        final SeqVertex B = new SeqVertex(\"B\");\n+        final SeqVertex C = new SeqVertex(\"C\");\n+        final SeqVertex D = new SeqVertex(\"D\");\n+        final SeqVertex E = new SeqVertex(\"E\");\n+\n+\n+        for (boolean variantPresent : new boolean[] {false, true}) {\n+            final SeqGraph graph = new SeqGraph(20);\n+\n+            graph.addVertices(source, A, B, C, D, sink);\n+            graph.addEdges(() -> new BaseEdge(true, goodMultiplicity), source, A, B, C, sink);\n+            graph.addEdges(() -> new BaseEdge(false, badMultiplicity), A, D, C);\n+            graph.addEdges(() -> new BaseEdge(false, badMultiplicity), D, B);\n+\n+            if (variantPresent) {\n+                graph.addVertices(E);\n+                graph.addEdges(() -> new BaseEdge(false, variantMultiplicity), A, E, C);\n+            }\n+\n+            final ChainPruner<SeqVertex, BaseEdge> pruner = new AdaptiveChainPruner<>(0.01, 2.0, 2.0, 50);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2OTI5MQ==", "bodyText": "You should add a test along the lines of the example I laid out above (where there is a high confidence reference path and a bubble with bad connections to the root of the tree and demonstrate that a strict seed LOD ratio will prevent that case from being recovered.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434769291", "createdAt": "2020-06-03T18:26:40Z", "author": {"login": "jamesemery"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPrunerUnitTest.java", "diffHunk": "@@ -175,6 +175,47 @@ public void testAdaptivePruning(final int kmerSize, final byte[] ref, final byte\n         Assert.assertTrue(bestPaths.size() < 15);\n     }\n \n+    // test that in graph with good path A -> B -> C and bad edges A -> D -> C and D -> B that the adjacency of bad edges --\n+    // such that when bad edges meet the multiplicities do not indicate an error - does not harm pruning.\n+    // we test with and without a true variant path A -> E -> C\n+    @Test\n+    public void testAdaptivePruningWithAdjacentBadEdges() {\n+        final int goodMultiplicity = 1000;\n+        final int variantMultiplicity = 50;\n+        final int badMultiplicity = 5;\n+\n+        final SeqVertex source = new SeqVertex(\"source\");\n+        final SeqVertex sink = new SeqVertex(\"sink\");\n+        final SeqVertex A = new SeqVertex(\"A\");\n+        final SeqVertex B = new SeqVertex(\"B\");\n+        final SeqVertex C = new SeqVertex(\"C\");\n+        final SeqVertex D = new SeqVertex(\"D\");\n+        final SeqVertex E = new SeqVertex(\"E\");\n+\n+\n+        for (boolean variantPresent : new boolean[] {false, true}) {\n+            final SeqGraph graph = new SeqGraph(20);\n+\n+            graph.addVertices(source, A, B, C, D, sink);\n+            graph.addEdges(() -> new BaseEdge(true, goodMultiplicity), source, A, B, C, sink);\n+            graph.addEdges(() -> new BaseEdge(false, badMultiplicity), A, D, C);\n+            graph.addEdges(() -> new BaseEdge(false, badMultiplicity), D, B);\n+\n+            if (variantPresent) {\n+                graph.addVertices(E);\n+                graph.addEdges(() -> new BaseEdge(false, variantMultiplicity), A, E, C);\n+            }\n+\n+            final ChainPruner<SeqVertex, BaseEdge> pruner = new AdaptiveChainPruner<>(0.01, 2.0, 2.0, 50);\n+            pruner.pruneLowWeightChains(graph);\n+\n+            Assert.assertFalse(graph.containsVertex(D));\n+            if (variantPresent) {\n+                Assert.assertTrue(graph.containsVertex(E));\n+            }\n+        }\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2OTU4MQ==", "bodyText": "Or some other such unique feature of the chains (perhaps compare more edge weights?)", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434769581", "createdAt": "2020-06-03T18:27:11Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +33,105 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        /////// NEW\n+\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n+\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n+            }\n+\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n+\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new HashSet<>();\n+        final Set<Path<V,E>> goodChains = new HashSet<>();\n+\n+        // starting from the high-confidence seed vertices, grow the \"good\" subgraph along chains with above-threshold log odds,\n+        // discovering good chains as we go.\n+        while (!verticesToProcess.isEmpty()) {\n+            final V vertex = verticesToProcess.poll();\n+            processedVertices.add(vertex);\n+            for (final Path<V,E> outgoingChain : vertexToGoodOutgoingChains.get(vertex)) {\n+                goodChains.add(outgoingChain);\n+                if(!processedVertices.contains(outgoingChain.getLastVertex())) {\n+                    verticesToProcess.add(outgoingChain.getLastVertex());\n+                }\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            for (final Path<V,E> incomingChain : vertexToGoodIncomingChains.get(vertex)) {\n+                goodChains.add(incomingChain);\n+                if(!processedVertices.contains(incomingChain.getFirstVertex())) {\n+                    verticesToProcess.add(incomingChain.getFirstVertex());\n+                }\n             }\n-        });\n+        }\n+\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !goodChains.contains(c)).collect(Collectors.toSet());\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n+        // add non-error chains to error chains if maximum number of variants has been exceeded\n+        chains.stream()\n+                .filter(c -> !errorChains.contains(c))\n+                .filter(c -> isChainPossibleVariant(c, graph))\n+                .sorted(Comparator.comparingDouble(c -> Math.min(chainLogOdds.get(c).getLeft(), chainLogOdds.get(c).getLeft())).reversed())\n                 .skip(maxUnprunedVariants)\n-                .forEach(result::add);\n+                .forEach(errorChains::add);\n \n-        return result;\n+        return errorChains;\n \n     }\n \n-    private double chainLogOdds(final Path<V,E> chain, final BaseGraph<V,E> graph, final double errorRate) {\n-        if (chain.getEdges().stream().anyMatch(E::isRef)) {\n-            return Double.POSITIVE_INFINITY;\n-        }\n+    // find the chain containing the edge of greatest weight, taking care to break ties deterministically\n+    private Path<V, E> getMaxWeightChain(final Collection<Path<V, E>> chains) {\n+        return chains.stream()\n+                .max(Comparator.comparingInt((Path<V, E> chain) -> chain.getEdges().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0))\n+                        .thenComparingInt(Path::length)\n+                        .thenComparingInt(Path::hashCode)).get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2NDA5NA=="}, "originalCommit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3NDgzMw==", "bodyText": "Interesting, I still think this can be fooled if there is a bubble of low weight in an error branch for which two low weight edges of equal weight split and merge and ultimately are connected to the reference only by bogus chains.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434774833", "createdAt": "2020-06-03T18:36:20Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +33,105 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        /////// NEW\n+\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n+\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n+            }\n+\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n+\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3NTk3NA==", "bodyText": "I would advocate this threshold be made very strict and that we do some reasoning about how probable it is for this to come up. It should be fairly unlikely for the seeding vertexes to ever fall off of good variation by accident due to low coverage I think.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434775974", "createdAt": "2020-06-03T18:38:37Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/ReadThreadingAssemblerArgumentCollection.java", "diffHunk": "@@ -18,6 +18,7 @@\n     private static final long serialVersionUID = 1L;\n \n     public static final double DEFAULT_PRUNING_LOG_ODDS_THRESHOLD = MathUtils.log10ToLog(1.0);\n+    public static final double DEFAULT_PRUNING_SEEDING_LOG_ODDS_THRESHOLD = MathUtils.log10ToLog(2.0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3NjkzMQ==", "bodyText": "I actually do not think this matters based on how you use these but for extra safety and defense against future alterations to this code I would make these linkedHashSets, especially since you are still iterating over the first one.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434776931", "createdAt": "2020-06-03T18:40:17Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +33,105 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        /////// NEW\n+\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n+\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n+            }\n+\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n+\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new HashSet<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4Mzg1NA==", "bodyText": "You know, it occurs to me that the maxUnprunedVariants limit here probably doesn't make sense. If you think about it there is not a 1:1 mapping of chains to variants after pruning. Its very possible for a \"variant\" to be a single chain in the simple case or possible correspond to a few distinct \"chains\" because there could be forked paths with low LOD that get pruned that end up splitting that one good \"chain\" into a few smaller ones. Furthermore it seems possible that a perfectly good variant has part a part of its chain removed at this stage as a result resulting in 1 or potentially 2 dangling ends after pruning and deleting the connection. I don't suspect this happens often enough to worry about here though. Worth keeping in mind for future changes though.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434783854", "createdAt": "2020-06-03T18:53:00Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +33,105 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        /////// NEW\n+\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n+\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n+            }\n+\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n+\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new HashSet<>();\n+        final Set<Path<V,E>> goodChains = new HashSet<>();\n+\n+        // starting from the high-confidence seed vertices, grow the \"good\" subgraph along chains with above-threshold log odds,\n+        // discovering good chains as we go.\n+        while (!verticesToProcess.isEmpty()) {\n+            final V vertex = verticesToProcess.poll();\n+            processedVertices.add(vertex);\n+            for (final Path<V,E> outgoingChain : vertexToGoodOutgoingChains.get(vertex)) {\n+                goodChains.add(outgoingChain);\n+                if(!processedVertices.contains(outgoingChain.getLastVertex())) {\n+                    verticesToProcess.add(outgoingChain.getLastVertex());\n+                }\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            for (final Path<V,E> incomingChain : vertexToGoodIncomingChains.get(vertex)) {\n+                goodChains.add(incomingChain);\n+                if(!processedVertices.contains(incomingChain.getFirstVertex())) {\n+                    verticesToProcess.add(incomingChain.getFirstVertex());\n+                }\n             }\n-        });\n+        }\n+\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !goodChains.contains(c)).collect(Collectors.toSet());\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n+        // add non-error chains to error chains if maximum number of variants has been exceeded\n+        chains.stream()\n+                .filter(c -> !errorChains.contains(c))\n+                .filter(c -> isChainPossibleVariant(c, graph))\n+                .sorted(Comparator.comparingDouble(c -> Math.min(chainLogOdds.get(c).getLeft(), chainLogOdds.get(c).getLeft())).reversed())\n                 .skip(maxUnprunedVariants)\n-                .forEach(result::add);\n+                .forEach(errorChains::add);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3"}, "originalPosition": 129}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "committedDate": "2020-05-04T20:39:19Z", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors"}, "afterCommit": {"oid": "cfa1b7b7f60a17deaaf4b8663ac0c91e60ad1d6a", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/cfa1b7b7f60a17deaaf4b8663ac0c91e60ad1d6a", "committedDate": "2020-07-02T16:06:13Z", "message": "review edits"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cfa1b7b7f60a17deaaf4b8663ac0c91e60ad1d6a", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/cfa1b7b7f60a17deaaf4b8663ac0c91e60ad1d6a", "committedDate": "2020-07-02T16:06:13Z", "message": "review edits"}, "afterCommit": {"oid": "2f0079dc7ef0588dbcb9c0143ee4a30319bf6c91", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/2f0079dc7ef0588dbcb9c0143ee4a30319bf6c91", "committedDate": "2020-07-06T19:58:06Z", "message": "review edits"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2f0079dc7ef0588dbcb9c0143ee4a30319bf6c91", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/2f0079dc7ef0588dbcb9c0143ee4a30319bf6c91", "committedDate": "2020-07-06T19:58:06Z", "message": "review edits"}, "afterCommit": {"oid": "e10dada478d42f1a28ad84878c7ec036a09706ed", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/e10dada478d42f1a28ad84878c7ec036a09706ed", "committedDate": "2020-07-08T15:11:07Z", "message": "review edits"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e10dada478d42f1a28ad84878c7ec036a09706ed", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/e10dada478d42f1a28ad84878c7ec036a09706ed", "committedDate": "2020-07-08T15:11:07Z", "message": "review edits"}, "afterCommit": {"oid": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7", "committedDate": "2020-07-08T15:20:25Z", "message": "review edits"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwMDcxNjg1", "url": "https://github.com/broadinstitute/gatk/pull/6520#pullrequestreview-450071685", "createdAt": "2020-07-16T17:40:00Z", "commit": {"oid": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7"}, "state": "APPROVED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxNzo0MDowMVrOGy1iOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxODoyODowM1rOGy3TNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk1OTA5Ng==", "bodyText": "Commented code", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r455959096", "createdAt": "2020-07-16T17:40:01Z", "author": {"login": "jamesemery"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPrunerUnitTest.java", "diffHunk": "@@ -167,12 +170,19 @@ public void testAdaptivePruning(final int kmerSize, final byte[] ref, final byte\n         final List<KBestHaplotype<SeqVertex, BaseEdge>> bestPaths = new GraphBasedKBestHaplotypeFinder<>(seqGraph).findBestHaplotypes(10);\n \n         final OptionalInt altIndex = IntStream.range(0, bestPaths.size()).filter(n -> bestPaths.get(n).haplotype().basesMatch(alt)).findFirst();\n-        Assert.assertTrue(altIndex.isPresent());\n+        //Assert.assertTrue(altIndex.isPresent());\n+        if (!altIndex.isPresent()) {\n+            int g = 90;\n+        }\n+\n+        // ref path should not be pruned even if all reads are alt\n+        final OptionalInt refIndex = IntStream.range(0, bestPaths.size()).filter(n -> bestPaths.get(n).haplotype().basesMatch(ref)).findFirst();\n+        Assert.assertTrue(refIndex.isPresent());\n \n         // the haplotype score is the sum of the log-10 of all branching fractions, so the alt haplotype score should come out to\n-        // around the log-10 of the allele fraction up to some fudge factor, assumign we didn't do any dumb pruning\n-        Assert.assertEquals(bestPaths.get(altIndex.getAsInt()).score(), Math.log10(altFraction), 0.5);\n-        Assert.assertTrue(bestPaths.size() < 15);\n+        // around the log-10 of the allele fraction up to some fudge factor, assuming we didn't do any dumb pruning\n+        //Assert.assertEquals(bestPaths.get(altIndex.getAsInt()).score(), Math.log10(altFraction), 0.5);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk2Njc2Nw==", "bodyText": "This G is unused.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r455966767", "createdAt": "2020-07-16T17:52:32Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +34,136 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n             }\n-        });\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n-                .skip(maxUnprunedVariants)\n-                .forEach(result::add);\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n \n-        return result;\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new LinkedHashSet<>();\n+        final Set<Path<V,E>> goodChains = new LinkedHashSet<>();\n+\n+        // starting from the high-confidence seed vertices, grow the \"good\" subgraph along chains with above-threshold log odds,\n+        // discovering good chains as we go.\n+        while (!verticesToProcess.isEmpty()) {\n+            final V vertex = verticesToProcess.poll();\n+            processedVertices.add(vertex);\n+            for (final Path<V,E> outgoingChain : vertexToGoodOutgoingChains.get(vertex)) {\n+                goodChains.add(outgoingChain);\n+                if(!processedVertices.contains(outgoingChain.getLastVertex())) {\n+                    verticesToProcess.add(outgoingChain.getLastVertex());\n+                }\n+            }\n \n-    }\n+            for (final Path<V,E> incomingChain : vertexToGoodIncomingChains.get(vertex)) {\n+                goodChains.add(incomingChain);\n+                if(!processedVertices.contains(incomingChain.getFirstVertex())) {\n+                    verticesToProcess.add(incomingChain.getFirstVertex());\n+                }\n+            }\n+        }\n+\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !goodChains.contains(c)).collect(Collectors.toSet());\n+\n+        // A vertex with N > 0 outgoing good chains corresponds to N - 1 variants\n+        int numberOfVariantsInGraph = vertexToGoodOutgoingChains.keySet().stream()\n+                .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n+\n+        if (numberOfVariantsInGraph > maxUnprunedVariants) {\n+            // the vertex-to-good-incoming/outgoing chain maps contain all chains with log odds passing the threshold, even if\n+            // the seeding and extending process revealed them to be errors.  We need to cull such chains for the following steps\n+            for (final Path<V,E> chain : errorChains) {\n+                vertexToGoodOutgoingChains.remove(chain.getFirstVertex(), chain);\n+                vertexToGoodIncomingChains.remove(chain.getLastVertex(), chain);\n+            }\n+\n+            // recalculate now that we have more accurate vertex-to-good-chains maps\n+            numberOfVariantsInGraph = vertexToGoodOutgoingChains.keySet().stream()\n+                    .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n \n-    private double chainLogOdds(final Path<V,E> chain, final BaseGraph<V,E> graph, final double errorRate) {\n-        if (chain.getEdges().stream().anyMatch(E::isRef)) {\n-            return Double.POSITIVE_INFINITY;\n+            // start with the worst good variants\n+            final PriorityQueue<Path<V,E>> excessGoodChainsToPrune = new PriorityQueue<>(\n+                    Comparator.comparingDouble((Path<V,E> c) -> Math.min(chainLogOdds.get(c).getLeft(), chainLogOdds.get(c).getLeft()))\n+                            .thenComparing((Path<V,E> c) -> c.getFirstVertex().getSequence(), BaseUtils.BASES_COMPARATOR));\n+\n+            excessGoodChainsToPrune.addAll(goodChains);\n+\n+            while (numberOfVariantsInGraph > maxUnprunedVariants) {\n+                final Path<V,E> worstGoodChain = excessGoodChainsToPrune.poll();\n+                errorChains.add(worstGoodChain);\n+                //if removing this chain pops a bubble, we have pruned a variant\n+                if (vertexToGoodOutgoingChains.get(worstGoodChain.getFirstVertex()).size() > 1) {\n+                    numberOfVariantsInGraph--;\n+                }\n+                // remove the chain\n+                vertexToGoodOutgoingChains.remove(worstGoodChain.getFirstVertex(), worstGoodChain);\n+                vertexToGoodIncomingChains.remove(worstGoodChain.getLastVertex(), worstGoodChain);\n+\n+                int numberOfVariantsInGraphRecalculated = vertexToGoodOutgoingChains.keySet().stream()\n+                        .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n+\n+                int g = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk2NzExMg==", "bodyText": "This value is not actually used anywhere.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r455967112", "createdAt": "2020-07-16T17:53:04Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +34,136 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n             }\n-        });\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n-                .skip(maxUnprunedVariants)\n-                .forEach(result::add);\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n \n-        return result;\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new LinkedHashSet<>();\n+        final Set<Path<V,E>> goodChains = new LinkedHashSet<>();\n+\n+        // starting from the high-confidence seed vertices, grow the \"good\" subgraph along chains with above-threshold log odds,\n+        // discovering good chains as we go.\n+        while (!verticesToProcess.isEmpty()) {\n+            final V vertex = verticesToProcess.poll();\n+            processedVertices.add(vertex);\n+            for (final Path<V,E> outgoingChain : vertexToGoodOutgoingChains.get(vertex)) {\n+                goodChains.add(outgoingChain);\n+                if(!processedVertices.contains(outgoingChain.getLastVertex())) {\n+                    verticesToProcess.add(outgoingChain.getLastVertex());\n+                }\n+            }\n \n-    }\n+            for (final Path<V,E> incomingChain : vertexToGoodIncomingChains.get(vertex)) {\n+                goodChains.add(incomingChain);\n+                if(!processedVertices.contains(incomingChain.getFirstVertex())) {\n+                    verticesToProcess.add(incomingChain.getFirstVertex());\n+                }\n+            }\n+        }\n+\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !goodChains.contains(c)).collect(Collectors.toSet());\n+\n+        // A vertex with N > 0 outgoing good chains corresponds to N - 1 variants\n+        int numberOfVariantsInGraph = vertexToGoodOutgoingChains.keySet().stream()\n+                .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n+\n+        if (numberOfVariantsInGraph > maxUnprunedVariants) {\n+            // the vertex-to-good-incoming/outgoing chain maps contain all chains with log odds passing the threshold, even if\n+            // the seeding and extending process revealed them to be errors.  We need to cull such chains for the following steps\n+            for (final Path<V,E> chain : errorChains) {\n+                vertexToGoodOutgoingChains.remove(chain.getFirstVertex(), chain);\n+                vertexToGoodIncomingChains.remove(chain.getLastVertex(), chain);\n+            }\n+\n+            // recalculate now that we have more accurate vertex-to-good-chains maps\n+            numberOfVariantsInGraph = vertexToGoodOutgoingChains.keySet().stream()\n+                    .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n \n-    private double chainLogOdds(final Path<V,E> chain, final BaseGraph<V,E> graph, final double errorRate) {\n-        if (chain.getEdges().stream().anyMatch(E::isRef)) {\n-            return Double.POSITIVE_INFINITY;\n+            // start with the worst good variants\n+            final PriorityQueue<Path<V,E>> excessGoodChainsToPrune = new PriorityQueue<>(\n+                    Comparator.comparingDouble((Path<V,E> c) -> Math.min(chainLogOdds.get(c).getLeft(), chainLogOdds.get(c).getLeft()))\n+                            .thenComparing((Path<V,E> c) -> c.getFirstVertex().getSequence(), BaseUtils.BASES_COMPARATOR));\n+\n+            excessGoodChainsToPrune.addAll(goodChains);\n+\n+            while (numberOfVariantsInGraph > maxUnprunedVariants) {\n+                final Path<V,E> worstGoodChain = excessGoodChainsToPrune.poll();\n+                errorChains.add(worstGoodChain);\n+                //if removing this chain pops a bubble, we have pruned a variant\n+                if (vertexToGoodOutgoingChains.get(worstGoodChain.getFirstVertex()).size() > 1) {\n+                    numberOfVariantsInGraph--;\n+                }\n+                // remove the chain\n+                vertexToGoodOutgoingChains.remove(worstGoodChain.getFirstVertex(), worstGoodChain);\n+                vertexToGoodIncomingChains.remove(worstGoodChain.getLastVertex(), worstGoodChain);\n+\n+                int numberOfVariantsInGraphRecalculated = vertexToGoodOutgoingChains.keySet().stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7"}, "originalPosition": 162}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk2ODE1MQ==", "bodyText": "Again what does this g mean?", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r455968151", "createdAt": "2020-07-16T17:54:52Z", "author": {"login": "jamesemery"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPrunerUnitTest.java", "diffHunk": "@@ -167,12 +170,108 @@ public void testAdaptivePruning(final int kmerSize, final byte[] ref, final byte\n         final List<KBestHaplotype<SeqVertex, BaseEdge>> bestPaths = new GraphBasedKBestHaplotypeFinder<>(seqGraph).findBestHaplotypes(10);\n \n         final OptionalInt altIndex = IntStream.range(0, bestPaths.size()).filter(n -> bestPaths.get(n).haplotype().basesMatch(alt)).findFirst();\n-        Assert.assertTrue(altIndex.isPresent());\n+        //Assert.assertTrue(altIndex.isPresent());\n+        if (!altIndex.isPresent()) {\n+            int g = 90;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk3MzcxOQ==", "bodyText": "I would decompose this part of the method into a new method \"markWorstGoodChainsToSatisfyMaxVariants\" just to make it clearer that this is doing something demonstrably different from the rest of the method and to clean things up a little.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r455973719", "createdAt": "2020-07-16T18:04:07Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +34,136 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n             }\n-        });\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n-                .skip(maxUnprunedVariants)\n-                .forEach(result::add);\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n \n-        return result;\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new LinkedHashSet<>();\n+        final Set<Path<V,E>> goodChains = new LinkedHashSet<>();\n+\n+        // starting from the high-confidence seed vertices, grow the \"good\" subgraph along chains with above-threshold log odds,\n+        // discovering good chains as we go.\n+        while (!verticesToProcess.isEmpty()) {\n+            final V vertex = verticesToProcess.poll();\n+            processedVertices.add(vertex);\n+            for (final Path<V,E> outgoingChain : vertexToGoodOutgoingChains.get(vertex)) {\n+                goodChains.add(outgoingChain);\n+                if(!processedVertices.contains(outgoingChain.getLastVertex())) {\n+                    verticesToProcess.add(outgoingChain.getLastVertex());\n+                }\n+            }\n \n-    }\n+            for (final Path<V,E> incomingChain : vertexToGoodIncomingChains.get(vertex)) {\n+                goodChains.add(incomingChain);\n+                if(!processedVertices.contains(incomingChain.getFirstVertex())) {\n+                    verticesToProcess.add(incomingChain.getFirstVertex());\n+                }\n+            }\n+        }\n+\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !goodChains.contains(c)).collect(Collectors.toSet());\n+\n+        // A vertex with N > 0 outgoing good chains corresponds to N - 1 variants\n+        int numberOfVariantsInGraph = vertexToGoodOutgoingChains.keySet().stream()\n+                .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n+\n+        if (numberOfVariantsInGraph > maxUnprunedVariants) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk4ODAyMw==", "bodyText": "Hmm... I still think this code carries a risk, it seems possible to me that at this stage you could end up leaving a damaged graph by pruning good chains as part of a bubble that were not zipped together but appear in different orders.\nImagine you have two variants A and B and they are both \"good\" that means 2 bubbles in the graph after puning but both variants had a second bubble that was pruned and thus they are sepearted by chaings A1, A2, B1, and B2. Now say that the order in which the lod scores shakes out the best LOD scores are A1, B1, B2, A2 and we only allow 1 maxUnprunedVariant. In this circumstance it seems possible to me that we might over prune the graph in a destructive manner. The graph will look at A2 first, remove it (wihtout decrementing numberOfVariantsInGraph), then B2 (again without decrementing numberOfVariantsInGraph) then hit B1 and remove it and arrive on the \"corect\" number of variants leftover in the graph (1) with the only chain left being that A1 chain. In this situation the pruning code will delete the chains A2, B1, and B2 which could have left the A1 bubble as a dangling end potentially without enough information to accurately recover it. It seems the absolute safest thing to do in this circumstance would be to \"zip\" the chains before calling this good chain bubble pruning in some capacity. This could either be done by reprocessing everything before pruning good chains so it will never be split across multiple chains that could be zipped or by fixing the LOD scores when you prune down to a single path.", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r455988023", "createdAt": "2020-07-16T18:28:03Z", "author": {"login": "jamesemery"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +34,136 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n             }\n-        });\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n-                .skip(maxUnprunedVariants)\n-                .forEach(result::add);\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n \n-        return result;\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new LinkedHashSet<>();\n+        final Set<Path<V,E>> goodChains = new LinkedHashSet<>();\n+\n+        // starting from the high-confidence seed vertices, grow the \"good\" subgraph along chains with above-threshold log odds,\n+        // discovering good chains as we go.\n+        while (!verticesToProcess.isEmpty()) {\n+            final V vertex = verticesToProcess.poll();\n+            processedVertices.add(vertex);\n+            for (final Path<V,E> outgoingChain : vertexToGoodOutgoingChains.get(vertex)) {\n+                goodChains.add(outgoingChain);\n+                if(!processedVertices.contains(outgoingChain.getLastVertex())) {\n+                    verticesToProcess.add(outgoingChain.getLastVertex());\n+                }\n+            }\n \n-    }\n+            for (final Path<V,E> incomingChain : vertexToGoodIncomingChains.get(vertex)) {\n+                goodChains.add(incomingChain);\n+                if(!processedVertices.contains(incomingChain.getFirstVertex())) {\n+                    verticesToProcess.add(incomingChain.getFirstVertex());\n+                }\n+            }\n+        }\n+\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !goodChains.contains(c)).collect(Collectors.toSet());\n+\n+        // A vertex with N > 0 outgoing good chains corresponds to N - 1 variants\n+        int numberOfVariantsInGraph = vertexToGoodOutgoingChains.keySet().stream()\n+                .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n+\n+        if (numberOfVariantsInGraph > maxUnprunedVariants) {\n+            // the vertex-to-good-incoming/outgoing chain maps contain all chains with log odds passing the threshold, even if\n+            // the seeding and extending process revealed them to be errors.  We need to cull such chains for the following steps\n+            for (final Path<V,E> chain : errorChains) {\n+                vertexToGoodOutgoingChains.remove(chain.getFirstVertex(), chain);\n+                vertexToGoodIncomingChains.remove(chain.getLastVertex(), chain);\n+            }\n+\n+            // recalculate now that we have more accurate vertex-to-good-chains maps\n+            numberOfVariantsInGraph = vertexToGoodOutgoingChains.keySet().stream()\n+                    .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n \n-    private double chainLogOdds(final Path<V,E> chain, final BaseGraph<V,E> graph, final double errorRate) {\n-        if (chain.getEdges().stream().anyMatch(E::isRef)) {\n-            return Double.POSITIVE_INFINITY;\n+            // start with the worst good variants\n+            final PriorityQueue<Path<V,E>> excessGoodChainsToPrune = new PriorityQueue<>(\n+                    Comparator.comparingDouble((Path<V,E> c) -> Math.min(chainLogOdds.get(c).getLeft(), chainLogOdds.get(c).getLeft()))\n+                            .thenComparing((Path<V,E> c) -> c.getFirstVertex().getSequence(), BaseUtils.BASES_COMPARATOR));\n+\n+            excessGoodChainsToPrune.addAll(goodChains);\n+\n+            while (numberOfVariantsInGraph > maxUnprunedVariants) {\n+                final Path<V,E> worstGoodChain = excessGoodChainsToPrune.poll();\n+                errorChains.add(worstGoodChain);\n+                //if removing this chain pops a bubble, we have pruned a variant\n+                if (vertexToGoodOutgoingChains.get(worstGoodChain.getFirstVertex()).size() > 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7"}, "originalPosition": 155}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5f05b956315863eeb37356e7122ce8c5ac25927a", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/5f05b956315863eeb37356e7122ce8c5ac25927a", "committedDate": "2020-09-10T02:55:02Z", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c0e99ae3e0c38845748a109ba3cad88ab300ac0", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/0c0e99ae3e0c38845748a109ba3cad88ab300ac0", "committedDate": "2020-09-10T02:55:02Z", "message": "review edits"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de7cdf669fd42bf60a1995eee3427ce8ce5c6496", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/de7cdf669fd42bf60a1995eee3427ce8ce5c6496", "committedDate": "2020-09-10T02:55:02Z", "message": "handled James' concern"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "672cf461ec3faa201716a271cf9f4d0751cf0dbd", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/672cf461ec3faa201716a271cf9f4d0751cf0dbd", "committedDate": "2020-09-10T02:55:03Z", "message": "in the thick of things"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e35a0311ccada14f1e078f6538b5a264df444be", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/0e35a0311ccada14f1e078f6538b5a264df444be", "committedDate": "2020-09-10T02:55:03Z", "message": "oh boy"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4067d9425c5705aeb9e615896118c952f6d1da5b", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/4067d9425c5705aeb9e615896118c952f6d1da5b", "committedDate": "2020-09-10T02:55:03Z", "message": "commit"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "507e23d94efa185384e44f39cc0e0047dda4d9e9", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/507e23d94efa185384e44f39cc0e0047dda4d9e9", "committedDate": "2020-09-10T03:00:55Z", "message": "seems like it works"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "03b1efe400f097d88671fe452eea4f7de0f3742d", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/03b1efe400f097d88671fe452eea4f7de0f3742d", "committedDate": "2020-09-08T16:02:02Z", "message": "seems like it works"}, "afterCommit": {"oid": "507e23d94efa185384e44f39cc0e0047dda4d9e9", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/507e23d94efa185384e44f39cc0e0047dda4d9e9", "committedDate": "2020-09-10T03:00:55Z", "message": "seems like it works"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "877ec3e9ad429e623be5df880f9a2e2bf9190ca9", "author": {"user": {"login": "davidbenjamin", "name": "David Benjamin"}}, "url": "https://github.com/broadinstitute/gatk/commit/877ec3e9ad429e623be5df880f9a2e2bf9190ca9", "committedDate": "2020-09-10T18:35:48Z", "message": "post-rebase crud"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2828, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}