{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAwNzg1MzI4", "number": 6881, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxMzoyOToyNVrOEvxxEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDo1NDowNVrOEv0pVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NTMzOTA2OnYy", "diffSide": "RIGHT", "path": "scripts/variantstore_wdl/ImportArrays.wdl", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxMzoyOToyNVrOHk8iew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxMzoyOToyNVrOHk8iew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUwMjY1MQ==", "bodyText": "i changed the name of the file from metadata_.tsv to sample_.tsv. it might make it more clear to update this output param. (if you do, also change it in the inputs to LoadArrays)", "url": "https://github.com/broadinstitute/gatk/pull/6881#discussion_r508502651", "createdAt": "2020-10-20T13:29:25Z", "author": {"login": "ahaessly"}, "path": "scripts/variantstore_wdl/ImportArrays.wdl", "diffHunk": "@@ -0,0 +1,222 @@\n+version 1.0\n+\n+workflow ImportArrays {\n+\n+  input {\n+    Array[File] input_vcfs\n+    Array[File]? input_metrics\n+    String? probe_info_table\n+    File? probe_info_file\n+    String output_directory\n+    File sample_map\n+    String project_id\n+    String dataset_name\n+    File raw_schema\n+    File sample_list_schema\n+    #TODO: determine table_id from input sample_map (including looping over multiple table_ids)\n+    Int table_id\n+\n+    Int? preemptible_tries\n+    File? gatk_override\n+    String? docker\n+  }\n+\n+  String docker_final = select_first([docker, \"us.gcr.io/broad-gatk/gatk:4.1.7.0\"])\n+\n+  scatter (i in range(length(input_vcfs))) {\n+    if (defined(input_metrics)) {\n+      File input_metric = select_first([input_metrics])[i]\n+    }\n+\n+    call CreateImportTsvs {\n+      input:\n+        input_vcf = input_vcfs[i],\n+        input_metrics = input_metric,\n+        probe_info_table = probe_info_table,\n+        probe_info_file = probe_info_file,\n+        sample_map = sample_map,\n+        output_directory = output_directory,\n+        gatk_override = gatk_override,\n+        docker = docker_final,\n+        preemptible_tries = preemptible_tries\n+    }\n+  }\n+\n+  call LoadArrays {\n+    input:\n+      metadata_tsvs = CreateImportTsvs.metadata_tsv,\n+      project_id = project_id,\n+      dataset_name = dataset_name,\n+      storage_location = output_directory,\n+      table_id = table_id,\n+      raw_schema = raw_schema,\n+      sample_list_schema = sample_list_schema,\n+      preemptible_tries = preemptible_tries,\n+      docker = docker_final\n+  }\n+}\n+\n+\n+task CreateImportTsvs {\n+  input {\n+    File input_vcf\n+    File? input_metrics\n+    String? probe_info_table\n+    File? probe_info_file\n+    String output_directory\n+    File sample_map\n+\n+    # runtime\n+    Int? preemptible_tries\n+    File? gatk_override\n+    String docker\n+\n+    String? for_testing_only\n+  }\n+\n+  Int disk_size = ceil(size(input_vcf, \"GB\") * 2.5) + 20\n+\n+  meta {\n+    description: \"Creates a tsv file for imort into BigQuery\"\n+  }\n+  parameter_meta {\n+    input_vcf: {\n+      localization_optional: true\n+    }\n+  }\n+  command <<<\n+      set -e\n+\n+      #workaround for https://github.com/broadinstitute/cromwell/issues/3647\n+      export TMPDIR=/tmp\n+      export GATK_LOCAL_JAR=~{default=\"/root/gatk.jar\" gatk_override}\n+      ~{for_testing_only}\n+\n+      gatk --java-options \"-Xmx2500m\" CreateArrayIngestFiles \\\n+        -V ~{input_vcf} \\\n+        ~{\"-QCF \" + input_metrics} \\\n+        ~{\"--probe-info-file \" + probe_info_file} \\\n+        ~{\"--probe-info-table \" + probe_info_table} \\\n+        -SNM ~{sample_map} \\\n+        --ref-version 37\n+        \n+      gsutil cp sample_*.tsv ~{output_directory}/sample_tsvs/\n+      gsutil cp raw_*.tsv ~{output_directory}/raw_tsvs/\n+  >>>\n+  runtime {\n+      docker: docker\n+      memory: \"4 GB\"\n+      disks: \"local-disk \" + disk_size + \" HDD\"\n+      preemptible: select_first([preemptible_tries, 5])\n+      cpu: 2\n+  }\n+  output {\n+      File metadata_tsv = glob(\"sample_*.tsv\")[0]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9d4f77bf68040a568c938e1a976ac4b3e050a78"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NTQyOTQyOnYy", "diffSide": "RIGHT", "path": "scripts/variantstore_wdl/ImportArrays.wdl", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxMzo0NjowMFrOHk9cqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNTozMzoxM1rOHlDdUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUxNzU0NA==", "bodyText": "is false the right default?", "url": "https://github.com/broadinstitute/gatk/pull/6881#discussion_r508517544", "createdAt": "2020-10-20T13:46:00Z", "author": {"login": "ahaessly"}, "path": "scripts/variantstore_wdl/ImportArrays.wdl", "diffHunk": "@@ -0,0 +1,222 @@\n+version 1.0\n+\n+workflow ImportArrays {\n+\n+  input {\n+    Array[File] input_vcfs\n+    Array[File]? input_metrics\n+    String? probe_info_table\n+    File? probe_info_file\n+    String output_directory\n+    File sample_map\n+    String project_id\n+    String dataset_name\n+    File raw_schema\n+    File sample_list_schema\n+    #TODO: determine table_id from input sample_map (including looping over multiple table_ids)\n+    Int table_id\n+\n+    Int? preemptible_tries\n+    File? gatk_override\n+    String? docker\n+  }\n+\n+  String docker_final = select_first([docker, \"us.gcr.io/broad-gatk/gatk:4.1.7.0\"])\n+\n+  scatter (i in range(length(input_vcfs))) {\n+    if (defined(input_metrics)) {\n+      File input_metric = select_first([input_metrics])[i]\n+    }\n+\n+    call CreateImportTsvs {\n+      input:\n+        input_vcf = input_vcfs[i],\n+        input_metrics = input_metric,\n+        probe_info_table = probe_info_table,\n+        probe_info_file = probe_info_file,\n+        sample_map = sample_map,\n+        output_directory = output_directory,\n+        gatk_override = gatk_override,\n+        docker = docker_final,\n+        preemptible_tries = preemptible_tries\n+    }\n+  }\n+\n+  call LoadArrays {\n+    input:\n+      metadata_tsvs = CreateImportTsvs.metadata_tsv,\n+      project_id = project_id,\n+      dataset_name = dataset_name,\n+      storage_location = output_directory,\n+      table_id = table_id,\n+      raw_schema = raw_schema,\n+      sample_list_schema = sample_list_schema,\n+      preemptible_tries = preemptible_tries,\n+      docker = docker_final\n+  }\n+}\n+\n+\n+task CreateImportTsvs {\n+  input {\n+    File input_vcf\n+    File? input_metrics\n+    String? probe_info_table\n+    File? probe_info_file\n+    String output_directory\n+    File sample_map\n+\n+    # runtime\n+    Int? preemptible_tries\n+    File? gatk_override\n+    String docker\n+\n+    String? for_testing_only\n+  }\n+\n+  Int disk_size = ceil(size(input_vcf, \"GB\") * 2.5) + 20\n+\n+  meta {\n+    description: \"Creates a tsv file for imort into BigQuery\"\n+  }\n+  parameter_meta {\n+    input_vcf: {\n+      localization_optional: true\n+    }\n+  }\n+  command <<<\n+      set -e\n+\n+      #workaround for https://github.com/broadinstitute/cromwell/issues/3647\n+      export TMPDIR=/tmp\n+      export GATK_LOCAL_JAR=~{default=\"/root/gatk.jar\" gatk_override}\n+      ~{for_testing_only}\n+\n+      gatk --java-options \"-Xmx2500m\" CreateArrayIngestFiles \\\n+        -V ~{input_vcf} \\\n+        ~{\"-QCF \" + input_metrics} \\\n+        ~{\"--probe-info-file \" + probe_info_file} \\\n+        ~{\"--probe-info-table \" + probe_info_table} \\\n+        -SNM ~{sample_map} \\\n+        --ref-version 37\n+        \n+      gsutil cp sample_*.tsv ~{output_directory}/sample_tsvs/\n+      gsutil cp raw_*.tsv ~{output_directory}/raw_tsvs/\n+  >>>\n+  runtime {\n+      docker: docker\n+      memory: \"4 GB\"\n+      disks: \"local-disk \" + disk_size + \" HDD\"\n+      preemptible: select_first([preemptible_tries, 5])\n+      cpu: 2\n+  }\n+  output {\n+      File metadata_tsv = glob(\"sample_*.tsv\")[0]\n+      File arraydata_tsv = glob(\"raw_*.tsv\")[0] \n+  }\n+}\n+\n+task LoadArrays {\n+  input {\n+    String project_id\n+    String dataset_name\n+    String storage_location\n+    Int table_id\n+    File raw_schema\n+    File sample_list_schema\n+    String load = \"false\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9d4f77bf68040a568c938e1a976ac4b3e050a78"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODYxNjAxOA==", "bodyText": "I'll change it to true.", "url": "https://github.com/broadinstitute/gatk/pull/6881#discussion_r508616018", "createdAt": "2020-10-20T15:33:13Z", "author": {"login": "meganshand"}, "path": "scripts/variantstore_wdl/ImportArrays.wdl", "diffHunk": "@@ -0,0 +1,222 @@\n+version 1.0\n+\n+workflow ImportArrays {\n+\n+  input {\n+    Array[File] input_vcfs\n+    Array[File]? input_metrics\n+    String? probe_info_table\n+    File? probe_info_file\n+    String output_directory\n+    File sample_map\n+    String project_id\n+    String dataset_name\n+    File raw_schema\n+    File sample_list_schema\n+    #TODO: determine table_id from input sample_map (including looping over multiple table_ids)\n+    Int table_id\n+\n+    Int? preemptible_tries\n+    File? gatk_override\n+    String? docker\n+  }\n+\n+  String docker_final = select_first([docker, \"us.gcr.io/broad-gatk/gatk:4.1.7.0\"])\n+\n+  scatter (i in range(length(input_vcfs))) {\n+    if (defined(input_metrics)) {\n+      File input_metric = select_first([input_metrics])[i]\n+    }\n+\n+    call CreateImportTsvs {\n+      input:\n+        input_vcf = input_vcfs[i],\n+        input_metrics = input_metric,\n+        probe_info_table = probe_info_table,\n+        probe_info_file = probe_info_file,\n+        sample_map = sample_map,\n+        output_directory = output_directory,\n+        gatk_override = gatk_override,\n+        docker = docker_final,\n+        preemptible_tries = preemptible_tries\n+    }\n+  }\n+\n+  call LoadArrays {\n+    input:\n+      metadata_tsvs = CreateImportTsvs.metadata_tsv,\n+      project_id = project_id,\n+      dataset_name = dataset_name,\n+      storage_location = output_directory,\n+      table_id = table_id,\n+      raw_schema = raw_schema,\n+      sample_list_schema = sample_list_schema,\n+      preemptible_tries = preemptible_tries,\n+      docker = docker_final\n+  }\n+}\n+\n+\n+task CreateImportTsvs {\n+  input {\n+    File input_vcf\n+    File? input_metrics\n+    String? probe_info_table\n+    File? probe_info_file\n+    String output_directory\n+    File sample_map\n+\n+    # runtime\n+    Int? preemptible_tries\n+    File? gatk_override\n+    String docker\n+\n+    String? for_testing_only\n+  }\n+\n+  Int disk_size = ceil(size(input_vcf, \"GB\") * 2.5) + 20\n+\n+  meta {\n+    description: \"Creates a tsv file for imort into BigQuery\"\n+  }\n+  parameter_meta {\n+    input_vcf: {\n+      localization_optional: true\n+    }\n+  }\n+  command <<<\n+      set -e\n+\n+      #workaround for https://github.com/broadinstitute/cromwell/issues/3647\n+      export TMPDIR=/tmp\n+      export GATK_LOCAL_JAR=~{default=\"/root/gatk.jar\" gatk_override}\n+      ~{for_testing_only}\n+\n+      gatk --java-options \"-Xmx2500m\" CreateArrayIngestFiles \\\n+        -V ~{input_vcf} \\\n+        ~{\"-QCF \" + input_metrics} \\\n+        ~{\"--probe-info-file \" + probe_info_file} \\\n+        ~{\"--probe-info-table \" + probe_info_table} \\\n+        -SNM ~{sample_map} \\\n+        --ref-version 37\n+        \n+      gsutil cp sample_*.tsv ~{output_directory}/sample_tsvs/\n+      gsutil cp raw_*.tsv ~{output_directory}/raw_tsvs/\n+  >>>\n+  runtime {\n+      docker: docker\n+      memory: \"4 GB\"\n+      disks: \"local-disk \" + disk_size + \" HDD\"\n+      preemptible: select_first([preemptible_tries, 5])\n+      cpu: 2\n+  }\n+  output {\n+      File metadata_tsv = glob(\"sample_*.tsv\")[0]\n+      File arraydata_tsv = glob(\"raw_*.tsv\")[0] \n+  }\n+}\n+\n+task LoadArrays {\n+  input {\n+    String project_id\n+    String dataset_name\n+    String storage_location\n+    Int table_id\n+    File raw_schema\n+    File sample_list_schema\n+    String load = \"false\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUxNzU0NA=="}, "originalCommit": {"oid": "f9d4f77bf68040a568c938e1a976ac4b3e050a78"}, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NTgxMDc3OnYy", "diffSide": "RIGHT", "path": "scripts/variantstore_wdl/bq_ingest_arrays.sh", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDo1NDowNVrOHlBOqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNTozMzowMVrOHlDcbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU3OTQ5OQ==", "bodyText": "when do you not want to load?", "url": "https://github.com/broadinstitute/gatk/pull/6881#discussion_r508579499", "createdAt": "2020-10-20T14:54:05Z", "author": {"login": "ahaessly"}, "path": "scripts/variantstore_wdl/bq_ingest_arrays.sh", "diffHunk": "@@ -0,0 +1,97 @@\n+#!/usr/bin/env bash\n+set -e\n+\n+if [ $# -lt 5 ]; then\n+  echo \"usage: $0 <project-id> <dataset-name> <storage-location> <table-id> <load> <uuid>\"\n+  exit 1\n+fi\n+\n+PROJECT_ID=$1\n+DATASET_NAME=$2\n+STORAGE_LOCATION=$3\n+TABLE_ID=$4\n+if [ $5 == \"true\" ]; then\n+  LOAD=true\n+else\n+  LOAD=false\n+fi", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9d4f77bf68040a568c938e1a976ac4b3e050a78"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODYxNTc4OA==", "bodyText": "My thought was that if we end up using Google Data Transfer we need a script that will create the tables but not actually load the data. Ideally we'd add the generation of the Transfers to this script too, but I didn't get around to doing that.\nNow that I look at this though, I think the code for this script is all contained within the WDL and I shouldn't have committed this extra file. I'll add a comment about the Google Data Transfer to the WDL and delete this bash script.", "url": "https://github.com/broadinstitute/gatk/pull/6881#discussion_r508615788", "createdAt": "2020-10-20T15:33:01Z", "author": {"login": "meganshand"}, "path": "scripts/variantstore_wdl/bq_ingest_arrays.sh", "diffHunk": "@@ -0,0 +1,97 @@\n+#!/usr/bin/env bash\n+set -e\n+\n+if [ $# -lt 5 ]; then\n+  echo \"usage: $0 <project-id> <dataset-name> <storage-location> <table-id> <load> <uuid>\"\n+  exit 1\n+fi\n+\n+PROJECT_ID=$1\n+DATASET_NAME=$2\n+STORAGE_LOCATION=$3\n+TABLE_ID=$4\n+if [ $5 == \"true\" ]; then\n+  LOAD=true\n+else\n+  LOAD=false\n+fi", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU3OTQ5OQ=="}, "originalCommit": {"oid": "f9d4f77bf68040a568c938e1a976ac4b3e050a78"}, "originalPosition": 17}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 821, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}