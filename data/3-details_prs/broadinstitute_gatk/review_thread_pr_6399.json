{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY1NDQxNTc3", "number": 6399, "reviewThreads": {"totalCount": 47, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDowNjo1M1rODZ9c3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxODozMTo1M1rOD0YDaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTQ3ODA3OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDowNjo1M1rOFgpXaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDowNjo1M1rOFgpXaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc3NjQ4OQ==", "bodyText": "I believe you can return String.join(\",\", alleleValues)", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369776489", "createdAt": "2020-01-22T20:06:53Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "diffHunk": "@@ -0,0 +1,160 @@\n+package org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.engine.filters.VariantFilter;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.AnnotationUtils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class StrandBiasUtils {\n+    public static final int FORWARD = 0;\n+    public static final int REVERSE = 1;\n+    public static final int MIN_COUNT = 2;\n+    public static final String PRINT_DELIM = \"|\";\n+    private static final List<Integer> ZERO_LIST = new ArrayList<>(Arrays.asList(0,0));\n+\n+    public static Map<String, Object> computeSBAnnotation(VariantContext vc, AlleleLikelihoods<GATKRead, Allele> likelihoods, String key) {\n+        // calculate the annotation from the likelihoods\n+        // likelihoods can come from HaplotypeCaller call to VariantAnnotatorEngine\n+        final Map<String, Object> annotations = new HashMap<>();\n+        final ReducibleAnnotationData<List<Integer>> myData = new AlleleSpecificAnnotationData<>(vc.getAlleles(),null);\n+        getStrandCountsFromLikelihoodMap(vc, likelihoods, myData, MIN_COUNT);\n+        final Map<Allele, List<Integer>> perAlleleValues = myData.getAttributeMap();\n+        final String annotationString = makeRawAnnotationString(vc.getAlleles(), perAlleleValues);\n+        annotations.put(key, annotationString);\n+        return annotations;\n+    }\n+\n+    protected static String makeRawAnnotationString(final List<Allele> vcAlleles, final Map<Allele, List<Integer>> perAlleleValues) {\n+        String annotationString = \"\";\n+        for (final Allele a : vcAlleles) {\n+            if (!annotationString.isEmpty()) {\n+                annotationString += PRINT_DELIM;\n+            }\n+            List<Integer> alleleValues = perAlleleValues.get(a);\n+            if (alleleValues == null) {\n+                alleleValues = ZERO_LIST;\n+            }\n+            annotationString += encode(alleleValues);\n+        }\n+        return annotationString;\n+    }\n+\n+    protected static String encode(List<Integer> alleleValues) {\n+        String annotationString = \"\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTQ5MDU2OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDoxMjowMFrOFgpf-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDoxMjowMFrOFgpf-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc3ODY4MA==", "bodyText": "This method could be written as\nfinal List<String> alleleStrings = vcAlleles.stream()\n    .map(a -> perAlleleValues.getOrDefault(a, ZERO_LIST))\n    .map(encode)\n    .collect(Collectors.toList());\nreturn String.join(PRINT_DELIM, alleleStrings);", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369778680", "createdAt": "2020-01-22T20:12:00Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "diffHunk": "@@ -0,0 +1,160 @@\n+package org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.engine.filters.VariantFilter;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.AnnotationUtils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class StrandBiasUtils {\n+    public static final int FORWARD = 0;\n+    public static final int REVERSE = 1;\n+    public static final int MIN_COUNT = 2;\n+    public static final String PRINT_DELIM = \"|\";\n+    private static final List<Integer> ZERO_LIST = new ArrayList<>(Arrays.asList(0,0));\n+\n+    public static Map<String, Object> computeSBAnnotation(VariantContext vc, AlleleLikelihoods<GATKRead, Allele> likelihoods, String key) {\n+        // calculate the annotation from the likelihoods\n+        // likelihoods can come from HaplotypeCaller call to VariantAnnotatorEngine\n+        final Map<String, Object> annotations = new HashMap<>();\n+        final ReducibleAnnotationData<List<Integer>> myData = new AlleleSpecificAnnotationData<>(vc.getAlleles(),null);\n+        getStrandCountsFromLikelihoodMap(vc, likelihoods, myData, MIN_COUNT);\n+        final Map<Allele, List<Integer>> perAlleleValues = myData.getAttributeMap();\n+        final String annotationString = makeRawAnnotationString(vc.getAlleles(), perAlleleValues);\n+        annotations.put(key, annotationString);\n+        return annotations;\n+    }\n+\n+    protected static String makeRawAnnotationString(final List<Allele> vcAlleles, final Map<Allele, List<Integer>> perAlleleValues) {\n+        String annotationString = \"\";\n+        for (final Allele a : vcAlleles) {\n+            if (!annotationString.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTQ5NDc0OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDoxMzozNlrOFgpisA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDoxMzozNlrOFgpisA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc3OTM3Ng==", "bodyText": "This is already defined as AnnotationUtils.ALLELE_SPECIFIC_PRINT_DELIM", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369779376", "createdAt": "2020-01-22T20:13:36Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "diffHunk": "@@ -0,0 +1,160 @@\n+package org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.engine.filters.VariantFilter;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.AnnotationUtils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class StrandBiasUtils {\n+    public static final int FORWARD = 0;\n+    public static final int REVERSE = 1;\n+    public static final int MIN_COUNT = 2;\n+    public static final String PRINT_DELIM = \"|\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTQ5OTYxOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDoxNTozMFrOFgpl4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDoxNTozMFrOFgpl4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4MDE5NQ==", "bodyText": "How about:\nfinal int strand = read.isReverseStrand() ? REVERSE : FORWARD;\nalleleStrandCounts.set(strand, alleleStrandCounts.get(strand) + 1);", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369780195", "createdAt": "2020-01-22T20:15:30Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "diffHunk": "@@ -0,0 +1,160 @@\n+package org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.engine.filters.VariantFilter;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.AnnotationUtils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class StrandBiasUtils {\n+    public static final int FORWARD = 0;\n+    public static final int REVERSE = 1;\n+    public static final int MIN_COUNT = 2;\n+    public static final String PRINT_DELIM = \"|\";\n+    private static final List<Integer> ZERO_LIST = new ArrayList<>(Arrays.asList(0,0));\n+\n+    public static Map<String, Object> computeSBAnnotation(VariantContext vc, AlleleLikelihoods<GATKRead, Allele> likelihoods, String key) {\n+        // calculate the annotation from the likelihoods\n+        // likelihoods can come from HaplotypeCaller call to VariantAnnotatorEngine\n+        final Map<String, Object> annotations = new HashMap<>();\n+        final ReducibleAnnotationData<List<Integer>> myData = new AlleleSpecificAnnotationData<>(vc.getAlleles(),null);\n+        getStrandCountsFromLikelihoodMap(vc, likelihoods, myData, MIN_COUNT);\n+        final Map<Allele, List<Integer>> perAlleleValues = myData.getAttributeMap();\n+        final String annotationString = makeRawAnnotationString(vc.getAlleles(), perAlleleValues);\n+        annotations.put(key, annotationString);\n+        return annotations;\n+    }\n+\n+    protected static String makeRawAnnotationString(final List<Allele> vcAlleles, final Map<Allele, List<Integer>> perAlleleValues) {\n+        String annotationString = \"\";\n+        for (final Allele a : vcAlleles) {\n+            if (!annotationString.isEmpty()) {\n+                annotationString += PRINT_DELIM;\n+            }\n+            List<Integer> alleleValues = perAlleleValues.get(a);\n+            if (alleleValues == null) {\n+                alleleValues = ZERO_LIST;\n+            }\n+            annotationString += encode(alleleValues);\n+        }\n+        return annotationString;\n+    }\n+\n+    protected static String encode(List<Integer> alleleValues) {\n+        String annotationString = \"\";\n+        for (int j =0; j < alleleValues.size(); j++) {\n+            annotationString += alleleValues.get(j);\n+            if (j < alleleValues.size()-1) {\n+                annotationString += \",\";\n+            }\n+        }\n+        return annotationString;\n+    }\n+\n+\n+    /**\n+     Allocate and fill a 2x2 strand contingency table.  In the end, it'll look something like this:\n+     *             fw      rc\n+     *   allele1   #       #\n+     *   allele2   #       #\n+     * @return a 2x2 contingency table\n+     */\n+    public static void getStrandCountsFromLikelihoodMap( final VariantContext vc,\n+                                                  final AlleleLikelihoods<GATKRead, Allele> likelihoods,\n+                                                  final ReducibleAnnotationData<List<Integer>> perAlleleValues,\n+                                                  final int minCount) {\n+        if( likelihoods == null || vc == null ) {\n+            return;\n+        }\n+\n+        final Allele ref = vc.getReference();\n+        final List<Allele> allAlts = vc.getAlternateAlleles();\n+\n+        for (final String sample : likelihoods.samples()) {\n+            final ReducibleAnnotationData<List<Integer>> sampleTable = new AlleleSpecificAnnotationData<>(vc.getAlleles(),null);\n+            likelihoods.bestAllelesBreakingTies(sample).stream()\n+                    .filter(ba -> ba.isInformative())\n+                    .forEach(ba -> updateTable(ba.allele, ba.evidence, ref, allAlts, sampleTable));\n+            if (passesMinimumThreshold(sampleTable, minCount)) {\n+                combineAttributeMap(sampleTable, perAlleleValues);\n+            }\n+        }\n+    }\n+\n+    protected static void combineAttributeMap(final ReducibleAnnotationData<List<Integer>> toAdd, final ReducibleAnnotationData<List<Integer>> combined) {\n+        for (final Allele a : combined.getAlleles()) {\n+            if (toAdd.hasAttribute(a) && toAdd.getAttribute(a) != null) {\n+                if (combined.getAttribute(a) != null) {\n+                    combined.getAttribute(a).set(FORWARD, (int) combined.getAttribute(a).get(FORWARD) + (int) toAdd.getAttribute(a).get(FORWARD));\n+                    combined.getAttribute(a).set(REVERSE, (int) combined.getAttribute(a).get(REVERSE) + (int) toAdd.getAttribute(a).get(REVERSE));\n+                }\n+                else {\n+                    List<Integer> alleleData = new ArrayList<>();\n+                    alleleData.add(FORWARD, toAdd.getAttribute(a).get(FORWARD));\n+                    alleleData.add(REVERSE, toAdd.getAttribute(a).get(REVERSE));\n+                    combined.putAttribute(a,alleleData);\n+                }\n+            }\n+        }\n+    }\n+\n+    private static void updateTable(final Allele bestAllele, final GATKRead read, final Allele ref, final List<Allele> allAlts, final ReducibleAnnotationData<List<Integer>> perAlleleValues) {\n+\n+        final boolean matchesRef = bestAllele.equals(ref, true);\n+        final boolean matchesAnyAlt = allAlts.contains(bestAllele);\n+\n+        //can happen if a read's most likely allele has been removed when --max_alternate_alleles is exceeded\n+        if (!( matchesRef || matchesAnyAlt )) {\n+            return;\n+        }\n+\n+        final List<Integer> alleleStrandCounts;\n+        if (perAlleleValues.hasAttribute(bestAllele) && perAlleleValues.getAttribute(bestAllele) != null) {\n+            alleleStrandCounts = perAlleleValues.getAttribute(bestAllele);\n+        } else {\n+            alleleStrandCounts = new ArrayList<>();\n+            alleleStrandCounts.add(0,0);\n+            alleleStrandCounts.add(1,0);\n+        }\n+        final boolean isForward = !read.isReverseStrand();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 124}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTUyMTI5OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDoyMzowMVrOFgpzlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QyMTowNDo0NVrOFhMkoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4MzcwMA==", "bodyText": "As long as you're touching this code, could you put in javadoc that I should have done earlier, specifying that\n\ntumorADs are by alt allele, summed over samples\ntumor logOdds are by alt allele\nartifactProbabilities are by alelle and specifically technical artifact probabilities not including sequencing error, contamination, or germline variation\nnonSomatic probabilities are probabilitie that the variants are real but not somatic ie germline or contamination", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369783700", "createdAt": "2020-01-22T20:23:01Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "diffHunk": "@@ -96,11 +96,16 @@ public double probabilityOfSequencingError(final Datum datum) {\n         return clusterProbabilities(datum)[SEQUENCING_ERROR_INDEX];\n     }\n \n-    public void record(final int[] tumorADs, final double[] tumorLogOdds, final double artifactProbability, final double nonSomaticProbability, final VariantContext vc) {\n+    public void record(final int[] tumorADs, final double[] tumorLogOdds, final List<Double> artifactProbabilities, final List<Double> nonSomaticProbabilities, final VariantContext vc) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDMzMjA3Ng==", "bodyText": "i thought tumor ADs were for all alleles. that's why we sum them for the total AD and then fetch them by i+1 to skip the ref.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r370332076", "createdAt": "2020-01-23T20:14:41Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "diffHunk": "@@ -96,11 +96,16 @@ public double probabilityOfSequencingError(final Datum datum) {\n         return clusterProbabilities(datum)[SEQUENCING_ERROR_INDEX];\n     }\n \n-    public void record(final int[] tumorADs, final double[] tumorLogOdds, final double artifactProbability, final double nonSomaticProbability, final VariantContext vc) {\n+    public void record(final int[] tumorADs, final double[] tumorLogOdds, final List<Double> artifactProbabilities, final List<Double> nonSomaticProbabilities, final VariantContext vc) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4MzcwMA=="}, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM1MzMxMg==", "bodyText": "My mistake.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r370353312", "createdAt": "2020-01-23T21:04:45Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "diffHunk": "@@ -96,11 +96,16 @@ public double probabilityOfSequencingError(final Datum datum) {\n         return clusterProbabilities(datum)[SEQUENCING_ERROR_INDEX];\n     }\n \n-    public void record(final int[] tumorADs, final double[] tumorLogOdds, final double artifactProbability, final double nonSomaticProbability, final VariantContext vc) {\n+    public void record(final int[] tumorADs, final double[] tumorLogOdds, final List<Double> artifactProbabilities, final List<Double> nonSomaticProbabilities, final VariantContext vc) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4MzcwMA=="}, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTUyNzAxOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDoyNToxNlrOFgp3Yw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDoyNToxNlrOFgp3Yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4NDY3NQ==", "bodyText": "We should skip symbolic alleles -- don't record them in the clustering model or the threshold calculator, don't involve them in any filters that learn their parameters, and don't include them in the sum of ADs here, either.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369784675", "createdAt": "2020-01-22T20:25:16Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "diffHunk": "@@ -96,11 +96,16 @@ public double probabilityOfSequencingError(final Datum datum) {\n         return clusterProbabilities(datum)[SEQUENCING_ERROR_INDEX];\n     }\n \n-    public void record(final int[] tumorADs, final double[] tumorLogOdds, final double artifactProbability, final double nonSomaticProbability, final VariantContext vc) {\n+    public void record(final int[] tumorADs, final double[] tumorLogOdds, final List<Double> artifactProbabilities, final List<Double> nonSomaticProbabilities, final VariantContext vc) {\n         final int totalAD = (int) MathUtils.sum(tumorADs);\n+        // TODO: david b: is it important to have data for symbolic alleles?\n+        int numAltAlleles = tumorLogOdds.length;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTUyOTc0OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDoyNjoxN1rOFgp49w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDoyNjoxN1rOFgp49w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4NTA3OQ==", "bodyText": "There could conceivably be multiple symbolic alleles (spanning deletion in GVCF mode, for example), so\nfinal int numAltAlleles = tumorLogOdds.alleles().stream().filter(a -> !a.isSymbolic()).count() is safer.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369785079", "createdAt": "2020-01-22T20:26:17Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "diffHunk": "@@ -96,11 +96,16 @@ public double probabilityOfSequencingError(final Datum datum) {\n         return clusterProbabilities(datum)[SEQUENCING_ERROR_INDEX];\n     }\n \n-    public void record(final int[] tumorADs, final double[] tumorLogOdds, final double artifactProbability, final double nonSomaticProbability, final VariantContext vc) {\n+    public void record(final int[] tumorADs, final double[] tumorLogOdds, final List<Double> artifactProbabilities, final List<Double> nonSomaticProbabilities, final VariantContext vc) {\n         final int totalAD = (int) MathUtils.sum(tumorADs);\n+        // TODO: david b: is it important to have data for symbolic alleles?\n+        int numAltAlleles = tumorLogOdds.length;\n+        if (vc.hasSymbolicAlleles()) {\n+            numAltAlleles--;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTUzMzYyOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDoyNzo1NFrOFgp7cQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDoyNzo1NFrOFgp7cQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4NTcxMw==", "bodyText": "And we shouldn't assume that symbolic alleles are at the end, so perhaps\nfor (int i = 0; i < tumorLogOdds.length; i++) {\n   if (allele i is not symbolic) {\n      data.add. . .\n   }\n}", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369785713", "createdAt": "2020-01-22T20:27:54Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "diffHunk": "@@ -96,11 +96,16 @@ public double probabilityOfSequencingError(final Datum datum) {\n         return clusterProbabilities(datum)[SEQUENCING_ERROR_INDEX];\n     }\n \n-    public void record(final int[] tumorADs, final double[] tumorLogOdds, final double artifactProbability, final double nonSomaticProbability, final VariantContext vc) {\n+    public void record(final int[] tumorADs, final double[] tumorLogOdds, final List<Double> artifactProbabilities, final List<Double> nonSomaticProbabilities, final VariantContext vc) {\n         final int totalAD = (int) MathUtils.sum(tumorADs);\n+        // TODO: david b: is it important to have data for symbolic alleles?\n+        int numAltAlleles = tumorLogOdds.length;\n+        if (vc.hasSymbolicAlleles()) {\n+            numAltAlleles--;\n+        }\n         // split into one-vs-all biallelics for clustering\n-        for (int i = 0; i < tumorLogOdds.length; i++) {\n-            data.add(new Datum(tumorLogOdds[i], artifactProbability, nonSomaticProbability, tumorADs[i+1], totalAD, indelLength(vc, i)));\n+        for (int i = 0; i < numAltAlleles; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTU0MTY5OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/BaseQualityFilter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDozMTowMlrOFgqAng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDozMTowMlrOFgqAng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4NzAzOA==", "bodyText": "How about\nreturn vc.getAttributeAsIntList(GATKVCFConstants.MEDIAN_BASE_QUALITY_KEY, 0).stream().skip(1)  // skip ref\n   .map(qual -> qual < minMedianBaseQuality).collect(Collectors.toList());", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369787038", "createdAt": "2020-01-22T20:31:02Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/BaseQualityFilter.java", "diffHunk": "@@ -19,12 +19,10 @@ public BaseQualityFilter(final double minMedianBaseQuality) {\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n     @Override\n-    public boolean isArtifact(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n-        final List<Integer> baseQualityByAllele = vc.getAttributeAsIntList(GATKVCFConstants.MEDIAN_BASE_QUALITY_KEY, 0);\n-        final double[] tumorLods = Mutect2FilteringEngine.getTumorLogOdds(vc);\n-        final int indexOfMaxTumorLod = MathUtils.maxElementIndex(tumorLods);\n-\n-        return baseQualityByAllele.get(indexOfMaxTumorLod + 1) < minMedianBaseQuality;\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        List<Integer> baseQualityByAllele = vc.getAttributeAsIntList(GATKVCFConstants.MEDIAN_BASE_QUALITY_KEY, 0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTU0NDU2OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ChimericOriginalAlignmentFilter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDozMjowOVrOFgqCZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMTo1MTo0MlrOFgsP0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4NzQ5Mg==", "bodyText": "Why does this need to be a method and not a static constant predicate?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369787492", "createdAt": "2020-01-22T20:32:09Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ChimericOriginalAlignmentFilter.java", "diffHunk": "@@ -16,15 +20,24 @@ public ChimericOriginalAlignmentFilter(final double maxNuMTFraction) {\n     @Override\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n+    public Predicate<Genotype> checkPreconditions() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgyMzY5Nw==", "bodyText": "ah, left over from different design. thanks", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369823697", "createdAt": "2020-01-22T21:51:42Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ChimericOriginalAlignmentFilter.java", "diffHunk": "@@ -16,15 +20,24 @@ public ChimericOriginalAlignmentFilter(final double maxNuMTFraction) {\n     @Override\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n+    public Predicate<Genotype> checkPreconditions() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4NzQ5Mg=="}, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTU0OTM4OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ChimericOriginalAlignmentFilter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDozNDoxN1rOFgqFjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDozNDoxN1rOFgqFjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4ODMwMA==", "bodyText": "Do we still need to skip multiallelic sites?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369788300", "createdAt": "2020-01-22T20:34:17Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ChimericOriginalAlignmentFilter.java", "diffHunk": "@@ -16,15 +20,24 @@ public ChimericOriginalAlignmentFilter(final double maxNuMTFraction) {\n     @Override\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n+    public Predicate<Genotype> checkPreconditions() {\n+        return Genotype::hasAD;\n+    }\n+\n+    public List<Integer> getData(Genotype g) {\n+        return Arrays.stream(g.getAD()).boxed().collect(Collectors.toList());\n+    }\n+\n     @Override\n-    public boolean isArtifact(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n         if(!vc.isBiallelic()) {\n-            return false;\n+            return Collections.emptyList();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTU1MjU5OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ContaminationFilter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDozNTozOFrOFgqHtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDozNTozOFrOFgqHtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4ODg1NA==", "bodyText": "a list of the depths and posterior pairs of each sample", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369788854", "createdAt": "2020-01-22T20:35:38Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ContaminationFilter.java", "diffHunk": "@@ -31,8 +32,10 @@ public ContaminationFilter(final List<File> contaminationTables, final double co\n     public ErrorType errorType() { return ErrorType.NON_SOMATIC; }\n \n     @Override\n-    public double calculateErrorProbability(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n-        final List<ImmutablePair<Integer, Double>> depthsAndPosteriors = new ArrayList<>();\n+    public List<Double> calculateErrorProbabilityForAlleles(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        // for every alt allele, a list of the depth and posterior pair", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTU1NzUxOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ContaminationFilter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDozNzoxOFrOFgqKvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDozNzoxOFrOFgqKvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4OTYzMA==", "bodyText": "The +1 offset of the comment seems not to be needed any more now that you copy the array starting at 1, right?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369789630", "createdAt": "2020-01-22T20:37:18Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ContaminationFilter.java", "diffHunk": "@@ -41,29 +44,38 @@ public double calculateErrorProbability(final VariantContext vc, final Mutect2Fi\n \n             final double contaminationFromFile = contaminationBySample.getOrDefault(tumorGenotype.getSampleName(), defaultContamination);\n             final double contamination = Math.max(0, Math.min(contaminationFromFile, 1 - EPSILON)); // handle file with contamination == 1\n-            final double[] alleleFractions = GATKProtectedVariantContextUtils.getAttributeAsDoubleArray(tumorGenotype, VCFConstants.ALLELE_FREQUENCY_KEY,\n-                    () -> new double[] {1.0}, 1.0);\n-            final int maxFractionIndex = MathUtils.maxElementIndex(alleleFractions);\n-            final int[] ADs = tumorGenotype.getAD();\n-            final int altCount = ADs[maxFractionIndex + 1];   // AD is all alleles, while AF is alts only, hence the +1 offset\n-            final int depth = (int) MathUtils.sum(ADs);\n+            final int[] ADs = tumorGenotype.getAD(); // AD is all alleles, while AF is alts only, hence the +1 offset", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTU3MDgzOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/DuplicatedAltReadFilter.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDo0MjowN1rOFgqTGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMDo1NTozNFrOFljJEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc5MTc3MA==", "bodyText": "This always returns a singleton list, even if there is more than one alt allele. . .", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369791770", "createdAt": "2020-01-22T20:42:07Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/DuplicatedAltReadFilter.java", "diffHunk": "@@ -20,11 +21,11 @@ public DuplicatedAltReadFilter(final int uniqueAltReadCount) {\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n     @Override\n-    public boolean isArtifact(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n-        return vc.getAttributeAsInt(UniqueAltReadCount.KEY, 1) <= uniqueAltReadCount;\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgyMTg5MQ==", "bodyText": "good catch. So, UniqueAltReadCount is actually assuming a single alt allele. I guess I should change that class to write an allele specific annotation that can be used in an allele specific way in the filter.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369821891", "createdAt": "2020-01-22T21:47:51Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/DuplicatedAltReadFilter.java", "diffHunk": "@@ -20,11 +21,11 @@ public DuplicatedAltReadFilter(final int uniqueAltReadCount) {\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n     @Override\n-    public boolean isArtifact(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n-        return vc.getAttributeAsInt(UniqueAltReadCount.KEY, 1) <= uniqueAltReadCount;\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc5MTc3MA=="}, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkxNzM5Mw==", "bodyText": "Yes.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r374917393", "createdAt": "2020-02-04T20:55:34Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/DuplicatedAltReadFilter.java", "diffHunk": "@@ -20,11 +21,11 @@ public DuplicatedAltReadFilter(final int uniqueAltReadCount) {\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n     @Override\n-    public boolean isArtifact(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n-        return vc.getAttributeAsInt(UniqueAltReadCount.KEY, 1) <= uniqueAltReadCount;\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc5MTc3MA=="}, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTYxNjkxOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ErrorProbabilities.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDo1Nzo1OVrOFgqvbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMDo1Nzo1OVrOFgqvbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc5OTAyMA==", "bodyText": "The VCF spec doesn't require it, so let's be cautious and not assume.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369799020", "createdAt": "2020-01-22T20:57:59Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ErrorProbabilities.java", "diffHunk": "@@ -3,36 +3,95 @@\n import htsjdk.variant.variantcontext.VariantContext;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n \n-import java.util.Arrays;\n-import java.util.EnumMap;\n-import java.util.List;\n-import java.util.Map;\n+import java.util.*;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static java.util.stream.Collectors.*;\n \n public final class ErrorProbabilities {\n-    private final Map<Mutect2VariantFilter, Double> probabilitiesByFilter;\n-    private final EnumMap<ErrorType, Double> probabilitiesByType;\n-    private final double errorProbability;\n+    private  LinkedHashMap<Mutect2Filter, List<Double>> alleleProbabilitiesByFilter;\n+    private final Map<ErrorType, List<Double>> probabilitiesByTypeAndAllele;\n+    private final List<Double> combinedErrorProbabilitiesByAllele;\n+    private final int numAltAlleles;\n \n \n-    public ErrorProbabilities(final List<Mutect2VariantFilter> filters, final VariantContext vc, final Mutect2FilteringEngine filteringEngine, final ReferenceContext referenceContext) {\n-        probabilitiesByFilter = filters.stream().collect(Collectors.toMap(f -> f, f -> f.errorProbability(vc, filteringEngine, referenceContext)));\n-        probabilitiesByType = Arrays.stream(ErrorType.values()).collect(Collectors.toMap(v -> v, v -> 0.0, (a,b) -> a, () -> new EnumMap<>(ErrorType.class)));\n-        filters.forEach(f -> probabilitiesByType.compute(f.errorType(), (type,prob) -> Math.max(prob, probabilitiesByFilter.get(f))));\n+    public ErrorProbabilities(final List<Mutect2Filter> filters, final VariantContext vc, final Mutect2FilteringEngine filteringEngine, final ReferenceContext referenceContext) {\n+        numAltAlleles = vc.getAlternateAlleles().size();\n+        alleleProbabilitiesByFilter = filters.stream()\n+                .collect(toMap(\n+                        Function.identity(),\n+                        f -> f.errorProbabilities(vc, filteringEngine, referenceContext),\n+                        (a, b) -> a, LinkedHashMap::new))\n+                // remove filters that were not applied. i.e. returned empty list\n+                .entrySet().stream().filter(entry -> !entry.getValue().isEmpty())\n+                .collect(toMap(Map.Entry::getKey, Map.Entry::getValue, (a, b) -> a, LinkedHashMap::new));\n \n-        // treat errors of different types as independent\n-        double trueProbability = 1;\n-        for (final double errorProb : probabilitiesByType.values()) {\n-            trueProbability *= (1 - errorProb);\n+        // if vc has symbolic allele, remove it\n+        if (vc.hasSymbolicAlleles()) {\n+            // can we assume it's the last allele?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NTYyNjY0OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ErrorProbabilities.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQyMTowMToyM1rOFgq1eQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxNToxMjoxOVrOFhBvwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgwMDU2OQ==", "bodyText": "Should there be an error thrown inside this block?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369800569", "createdAt": "2020-01-22T21:01:23Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ErrorProbabilities.java", "diffHunk": "@@ -3,36 +3,95 @@\n import htsjdk.variant.variantcontext.VariantContext;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n \n-import java.util.Arrays;\n-import java.util.EnumMap;\n-import java.util.List;\n-import java.util.Map;\n+import java.util.*;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static java.util.stream.Collectors.*;\n \n public final class ErrorProbabilities {\n-    private final Map<Mutect2VariantFilter, Double> probabilitiesByFilter;\n-    private final EnumMap<ErrorType, Double> probabilitiesByType;\n-    private final double errorProbability;\n+    private  LinkedHashMap<Mutect2Filter, List<Double>> alleleProbabilitiesByFilter;\n+    private final Map<ErrorType, List<Double>> probabilitiesByTypeAndAllele;\n+    private final List<Double> combinedErrorProbabilitiesByAllele;\n+    private final int numAltAlleles;\n \n \n-    public ErrorProbabilities(final List<Mutect2VariantFilter> filters, final VariantContext vc, final Mutect2FilteringEngine filteringEngine, final ReferenceContext referenceContext) {\n-        probabilitiesByFilter = filters.stream().collect(Collectors.toMap(f -> f, f -> f.errorProbability(vc, filteringEngine, referenceContext)));\n-        probabilitiesByType = Arrays.stream(ErrorType.values()).collect(Collectors.toMap(v -> v, v -> 0.0, (a,b) -> a, () -> new EnumMap<>(ErrorType.class)));\n-        filters.forEach(f -> probabilitiesByType.compute(f.errorType(), (type,prob) -> Math.max(prob, probabilitiesByFilter.get(f))));\n+    public ErrorProbabilities(final List<Mutect2Filter> filters, final VariantContext vc, final Mutect2FilteringEngine filteringEngine, final ReferenceContext referenceContext) {\n+        numAltAlleles = vc.getAlternateAlleles().size();\n+        alleleProbabilitiesByFilter = filters.stream()\n+                .collect(toMap(\n+                        Function.identity(),\n+                        f -> f.errorProbabilities(vc, filteringEngine, referenceContext),\n+                        (a, b) -> a, LinkedHashMap::new))\n+                // remove filters that were not applied. i.e. returned empty list\n+                .entrySet().stream().filter(entry -> !entry.getValue().isEmpty())\n+                .collect(toMap(Map.Entry::getKey, Map.Entry::getValue, (a, b) -> a, LinkedHashMap::new));\n \n-        // treat errors of different types as independent\n-        double trueProbability = 1;\n-        for (final double errorProb : probabilitiesByType.values()) {\n-            trueProbability *= (1 - errorProb);\n+        // if vc has symbolic allele, remove it\n+        if (vc.hasSymbolicAlleles()) {\n+            // can we assume it's the last allele?\n+            int symIndex = numAltAlleles - 1;\n+            alleleProbabilitiesByFilter.values().stream().forEach(probList -> probList.remove(symIndex));\n         }\n+        LinkedHashMap<ErrorType, List<List<Double>>> probabilitiesByAllelesForEachFilter = alleleProbabilitiesByFilter.entrySet().stream().collect(\n+                groupingBy(entry -> entry.getKey().errorType(), LinkedHashMap::new, mapping(entry -> entry.getValue(), toList())));\n+        // convert the data so we have a list of probabilities by allele instead of filter\n+        probabilitiesByAllelesForEachFilter.replaceAll((k, v) -> ErrorProbabilities.transpose(v));\n+\n+        // foreach error type, get the max probability for each allele\n+        probabilitiesByTypeAndAllele = probabilitiesByAllelesForEachFilter.entrySet().stream().collect(toMap(\n+                Map.Entry::getKey,\n+                entry -> entry.getValue().stream().map(alleleList -> alleleList.stream().max(Double::compare).orElse(0.0)).collect(Collectors.toList()),\n+                (a,b) -> a, HashMap::new));\n+\n+\n+        // treat errors of different types as independent\n+        // transpose the lists of allele probabilities, so it is now a list per allele that contains the prob for each type\n+        // combine allele-wise\n+        combinedErrorProbabilitiesByAllele = transpose(probabilitiesByTypeAndAllele.values().stream().collect(toList()))\n+                .stream().map(\n+                        alleleProbabilities -> alleleProbabilities.stream().map(p -> 1.0 - p).reduce(1.0, (a, b) -> a * b)).collect(Collectors.toList());\n+        combinedErrorProbabilitiesByAllele.replaceAll(trueProb -> Mutect2FilteringEngine.roundFinitePrecisionErrors(1.0 - trueProb));\n+    }\n+\n+    public List<Double> getCombinedErrorProbabilities() { return combinedErrorProbabilitiesByAllele; }\n+    public List<Double> getTechnicalArtifactProbabilities() { return probabilitiesByTypeAndAllele.get(ErrorType.ARTIFACT); }\n+    public List<Double> getNonSomaticProbabilities() { return probabilitiesByTypeAndAllele.get(ErrorType.NON_SOMATIC); }\n+    public Map<Mutect2Filter, List<Double>> getProbabilitiesByFilter() { return alleleProbabilitiesByFilter; }\n+\n+    // helper functions for the few operations that still differ depending on whether the filter\n+    // is per variant or allele\n+    public Map<Mutect2Filter, List<Double>> getProbabilitiesForAlleleFilters() {\n+        return getPartitionedProbabilitiesByFilter(false);\n+    }\n \n-        errorProbability = Mutect2FilteringEngine.roundFinitePrecisionErrors(1 - trueProbability);\n+    public Map<Mutect2Filter, Double> getProbabilitiesForVariantFilters() {\n+        return getPartitionedProbabilitiesByFilter(true).entrySet().stream()\n+                .filter(entry -> entry.getValue() != null && !entry.getValue().isEmpty())\n+                .collect(toMap(entry -> entry.getKey(), entry -> entry.getValue().get(0)));\n     }\n \n-    public double getErrorProbability() { return errorProbability; }\n-    public double getTechnicalArtifactProbability() { return probabilitiesByType.get(ErrorType.ARTIFACT); }\n-    public double getNonSomaticProbability() { return probabilitiesByType.get(ErrorType.NON_SOMATIC); }\n-    public Map<Mutect2VariantFilter, Double> getProbabilitiesByFilter() { return probabilitiesByFilter; }\n+    private Map<Mutect2Filter, List<Double>> getPartitionedProbabilitiesByFilter(boolean variantOnly) {\n+        Map<Boolean, LinkedHashMap<Mutect2Filter, List<Double>>> groups =\n+                alleleProbabilitiesByFilter.entrySet().stream().collect(Collectors.partitioningBy(\n+                        entry -> Mutect2VariantFilter.class.isAssignableFrom(entry.getKey().getClass()),\n+                        toMap(Map.Entry::getKey, Map.Entry::getValue, (a,b) -> a, LinkedHashMap::new)));\n+        return groups.get(variantOnly);\n+    }\n \n+    // TODO would this be useful in a util class somewhere?\n+    private static <T> List<List<T>> transpose(List<List<T>> list) {\n+        // all lists need to be the same size\n+        final int N = list.stream().mapToInt(l -> l.size()).max().orElse(-1);\n+        if (list.stream().anyMatch(l -> l.size() != N)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgyMjIyNw==", "bodyText": "yes, is there an example you can refer me to for the best way to throw this error?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369822227", "createdAt": "2020-01-22T21:48:37Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ErrorProbabilities.java", "diffHunk": "@@ -3,36 +3,95 @@\n import htsjdk.variant.variantcontext.VariantContext;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n \n-import java.util.Arrays;\n-import java.util.EnumMap;\n-import java.util.List;\n-import java.util.Map;\n+import java.util.*;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static java.util.stream.Collectors.*;\n \n public final class ErrorProbabilities {\n-    private final Map<Mutect2VariantFilter, Double> probabilitiesByFilter;\n-    private final EnumMap<ErrorType, Double> probabilitiesByType;\n-    private final double errorProbability;\n+    private  LinkedHashMap<Mutect2Filter, List<Double>> alleleProbabilitiesByFilter;\n+    private final Map<ErrorType, List<Double>> probabilitiesByTypeAndAllele;\n+    private final List<Double> combinedErrorProbabilitiesByAllele;\n+    private final int numAltAlleles;\n \n \n-    public ErrorProbabilities(final List<Mutect2VariantFilter> filters, final VariantContext vc, final Mutect2FilteringEngine filteringEngine, final ReferenceContext referenceContext) {\n-        probabilitiesByFilter = filters.stream().collect(Collectors.toMap(f -> f, f -> f.errorProbability(vc, filteringEngine, referenceContext)));\n-        probabilitiesByType = Arrays.stream(ErrorType.values()).collect(Collectors.toMap(v -> v, v -> 0.0, (a,b) -> a, () -> new EnumMap<>(ErrorType.class)));\n-        filters.forEach(f -> probabilitiesByType.compute(f.errorType(), (type,prob) -> Math.max(prob, probabilitiesByFilter.get(f))));\n+    public ErrorProbabilities(final List<Mutect2Filter> filters, final VariantContext vc, final Mutect2FilteringEngine filteringEngine, final ReferenceContext referenceContext) {\n+        numAltAlleles = vc.getAlternateAlleles().size();\n+        alleleProbabilitiesByFilter = filters.stream()\n+                .collect(toMap(\n+                        Function.identity(),\n+                        f -> f.errorProbabilities(vc, filteringEngine, referenceContext),\n+                        (a, b) -> a, LinkedHashMap::new))\n+                // remove filters that were not applied. i.e. returned empty list\n+                .entrySet().stream().filter(entry -> !entry.getValue().isEmpty())\n+                .collect(toMap(Map.Entry::getKey, Map.Entry::getValue, (a, b) -> a, LinkedHashMap::new));\n \n-        // treat errors of different types as independent\n-        double trueProbability = 1;\n-        for (final double errorProb : probabilitiesByType.values()) {\n-            trueProbability *= (1 - errorProb);\n+        // if vc has symbolic allele, remove it\n+        if (vc.hasSymbolicAlleles()) {\n+            // can we assume it's the last allele?\n+            int symIndex = numAltAlleles - 1;\n+            alleleProbabilitiesByFilter.values().stream().forEach(probList -> probList.remove(symIndex));\n         }\n+        LinkedHashMap<ErrorType, List<List<Double>>> probabilitiesByAllelesForEachFilter = alleleProbabilitiesByFilter.entrySet().stream().collect(\n+                groupingBy(entry -> entry.getKey().errorType(), LinkedHashMap::new, mapping(entry -> entry.getValue(), toList())));\n+        // convert the data so we have a list of probabilities by allele instead of filter\n+        probabilitiesByAllelesForEachFilter.replaceAll((k, v) -> ErrorProbabilities.transpose(v));\n+\n+        // foreach error type, get the max probability for each allele\n+        probabilitiesByTypeAndAllele = probabilitiesByAllelesForEachFilter.entrySet().stream().collect(toMap(\n+                Map.Entry::getKey,\n+                entry -> entry.getValue().stream().map(alleleList -> alleleList.stream().max(Double::compare).orElse(0.0)).collect(Collectors.toList()),\n+                (a,b) -> a, HashMap::new));\n+\n+\n+        // treat errors of different types as independent\n+        // transpose the lists of allele probabilities, so it is now a list per allele that contains the prob for each type\n+        // combine allele-wise\n+        combinedErrorProbabilitiesByAllele = transpose(probabilitiesByTypeAndAllele.values().stream().collect(toList()))\n+                .stream().map(\n+                        alleleProbabilities -> alleleProbabilities.stream().map(p -> 1.0 - p).reduce(1.0, (a, b) -> a * b)).collect(Collectors.toList());\n+        combinedErrorProbabilitiesByAllele.replaceAll(trueProb -> Mutect2FilteringEngine.roundFinitePrecisionErrors(1.0 - trueProb));\n+    }\n+\n+    public List<Double> getCombinedErrorProbabilities() { return combinedErrorProbabilitiesByAllele; }\n+    public List<Double> getTechnicalArtifactProbabilities() { return probabilitiesByTypeAndAllele.get(ErrorType.ARTIFACT); }\n+    public List<Double> getNonSomaticProbabilities() { return probabilitiesByTypeAndAllele.get(ErrorType.NON_SOMATIC); }\n+    public Map<Mutect2Filter, List<Double>> getProbabilitiesByFilter() { return alleleProbabilitiesByFilter; }\n+\n+    // helper functions for the few operations that still differ depending on whether the filter\n+    // is per variant or allele\n+    public Map<Mutect2Filter, List<Double>> getProbabilitiesForAlleleFilters() {\n+        return getPartitionedProbabilitiesByFilter(false);\n+    }\n \n-        errorProbability = Mutect2FilteringEngine.roundFinitePrecisionErrors(1 - trueProbability);\n+    public Map<Mutect2Filter, Double> getProbabilitiesForVariantFilters() {\n+        return getPartitionedProbabilitiesByFilter(true).entrySet().stream()\n+                .filter(entry -> entry.getValue() != null && !entry.getValue().isEmpty())\n+                .collect(toMap(entry -> entry.getKey(), entry -> entry.getValue().get(0)));\n     }\n \n-    public double getErrorProbability() { return errorProbability; }\n-    public double getTechnicalArtifactProbability() { return probabilitiesByType.get(ErrorType.ARTIFACT); }\n-    public double getNonSomaticProbability() { return probabilitiesByType.get(ErrorType.NON_SOMATIC); }\n-    public Map<Mutect2VariantFilter, Double> getProbabilitiesByFilter() { return probabilitiesByFilter; }\n+    private Map<Mutect2Filter, List<Double>> getPartitionedProbabilitiesByFilter(boolean variantOnly) {\n+        Map<Boolean, LinkedHashMap<Mutect2Filter, List<Double>>> groups =\n+                alleleProbabilitiesByFilter.entrySet().stream().collect(Collectors.partitioningBy(\n+                        entry -> Mutect2VariantFilter.class.isAssignableFrom(entry.getKey().getClass()),\n+                        toMap(Map.Entry::getKey, Map.Entry::getValue, (a,b) -> a, LinkedHashMap::new)));\n+        return groups.get(variantOnly);\n+    }\n \n+    // TODO would this be useful in a util class somewhere?\n+    private static <T> List<List<T>> transpose(List<List<T>> list) {\n+        // all lists need to be the same size\n+        final int N = list.stream().mapToInt(l -> l.size()).max().orElse(-1);\n+        if (list.stream().anyMatch(l -> l.size() != N)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgwMDU2OQ=="}, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3NTkzNw==", "bodyText": "I think something like\nUtils.validateArg(list.stream().mapToInt(List::size).distinct().count() == 1, \"lists are not the same size\");\n\nThis throws an IllegalArgumentException, which seems like the right choice to me.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r370175937", "createdAt": "2020-01-23T15:12:19Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ErrorProbabilities.java", "diffHunk": "@@ -3,36 +3,95 @@\n import htsjdk.variant.variantcontext.VariantContext;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n \n-import java.util.Arrays;\n-import java.util.EnumMap;\n-import java.util.List;\n-import java.util.Map;\n+import java.util.*;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static java.util.stream.Collectors.*;\n \n public final class ErrorProbabilities {\n-    private final Map<Mutect2VariantFilter, Double> probabilitiesByFilter;\n-    private final EnumMap<ErrorType, Double> probabilitiesByType;\n-    private final double errorProbability;\n+    private  LinkedHashMap<Mutect2Filter, List<Double>> alleleProbabilitiesByFilter;\n+    private final Map<ErrorType, List<Double>> probabilitiesByTypeAndAllele;\n+    private final List<Double> combinedErrorProbabilitiesByAllele;\n+    private final int numAltAlleles;\n \n \n-    public ErrorProbabilities(final List<Mutect2VariantFilter> filters, final VariantContext vc, final Mutect2FilteringEngine filteringEngine, final ReferenceContext referenceContext) {\n-        probabilitiesByFilter = filters.stream().collect(Collectors.toMap(f -> f, f -> f.errorProbability(vc, filteringEngine, referenceContext)));\n-        probabilitiesByType = Arrays.stream(ErrorType.values()).collect(Collectors.toMap(v -> v, v -> 0.0, (a,b) -> a, () -> new EnumMap<>(ErrorType.class)));\n-        filters.forEach(f -> probabilitiesByType.compute(f.errorType(), (type,prob) -> Math.max(prob, probabilitiesByFilter.get(f))));\n+    public ErrorProbabilities(final List<Mutect2Filter> filters, final VariantContext vc, final Mutect2FilteringEngine filteringEngine, final ReferenceContext referenceContext) {\n+        numAltAlleles = vc.getAlternateAlleles().size();\n+        alleleProbabilitiesByFilter = filters.stream()\n+                .collect(toMap(\n+                        Function.identity(),\n+                        f -> f.errorProbabilities(vc, filteringEngine, referenceContext),\n+                        (a, b) -> a, LinkedHashMap::new))\n+                // remove filters that were not applied. i.e. returned empty list\n+                .entrySet().stream().filter(entry -> !entry.getValue().isEmpty())\n+                .collect(toMap(Map.Entry::getKey, Map.Entry::getValue, (a, b) -> a, LinkedHashMap::new));\n \n-        // treat errors of different types as independent\n-        double trueProbability = 1;\n-        for (final double errorProb : probabilitiesByType.values()) {\n-            trueProbability *= (1 - errorProb);\n+        // if vc has symbolic allele, remove it\n+        if (vc.hasSymbolicAlleles()) {\n+            // can we assume it's the last allele?\n+            int symIndex = numAltAlleles - 1;\n+            alleleProbabilitiesByFilter.values().stream().forEach(probList -> probList.remove(symIndex));\n         }\n+        LinkedHashMap<ErrorType, List<List<Double>>> probabilitiesByAllelesForEachFilter = alleleProbabilitiesByFilter.entrySet().stream().collect(\n+                groupingBy(entry -> entry.getKey().errorType(), LinkedHashMap::new, mapping(entry -> entry.getValue(), toList())));\n+        // convert the data so we have a list of probabilities by allele instead of filter\n+        probabilitiesByAllelesForEachFilter.replaceAll((k, v) -> ErrorProbabilities.transpose(v));\n+\n+        // foreach error type, get the max probability for each allele\n+        probabilitiesByTypeAndAllele = probabilitiesByAllelesForEachFilter.entrySet().stream().collect(toMap(\n+                Map.Entry::getKey,\n+                entry -> entry.getValue().stream().map(alleleList -> alleleList.stream().max(Double::compare).orElse(0.0)).collect(Collectors.toList()),\n+                (a,b) -> a, HashMap::new));\n+\n+\n+        // treat errors of different types as independent\n+        // transpose the lists of allele probabilities, so it is now a list per allele that contains the prob for each type\n+        // combine allele-wise\n+        combinedErrorProbabilitiesByAllele = transpose(probabilitiesByTypeAndAllele.values().stream().collect(toList()))\n+                .stream().map(\n+                        alleleProbabilities -> alleleProbabilities.stream().map(p -> 1.0 - p).reduce(1.0, (a, b) -> a * b)).collect(Collectors.toList());\n+        combinedErrorProbabilitiesByAllele.replaceAll(trueProb -> Mutect2FilteringEngine.roundFinitePrecisionErrors(1.0 - trueProb));\n+    }\n+\n+    public List<Double> getCombinedErrorProbabilities() { return combinedErrorProbabilitiesByAllele; }\n+    public List<Double> getTechnicalArtifactProbabilities() { return probabilitiesByTypeAndAllele.get(ErrorType.ARTIFACT); }\n+    public List<Double> getNonSomaticProbabilities() { return probabilitiesByTypeAndAllele.get(ErrorType.NON_SOMATIC); }\n+    public Map<Mutect2Filter, List<Double>> getProbabilitiesByFilter() { return alleleProbabilitiesByFilter; }\n+\n+    // helper functions for the few operations that still differ depending on whether the filter\n+    // is per variant or allele\n+    public Map<Mutect2Filter, List<Double>> getProbabilitiesForAlleleFilters() {\n+        return getPartitionedProbabilitiesByFilter(false);\n+    }\n \n-        errorProbability = Mutect2FilteringEngine.roundFinitePrecisionErrors(1 - trueProbability);\n+    public Map<Mutect2Filter, Double> getProbabilitiesForVariantFilters() {\n+        return getPartitionedProbabilitiesByFilter(true).entrySet().stream()\n+                .filter(entry -> entry.getValue() != null && !entry.getValue().isEmpty())\n+                .collect(toMap(entry -> entry.getKey(), entry -> entry.getValue().get(0)));\n     }\n \n-    public double getErrorProbability() { return errorProbability; }\n-    public double getTechnicalArtifactProbability() { return probabilitiesByType.get(ErrorType.ARTIFACT); }\n-    public double getNonSomaticProbability() { return probabilitiesByType.get(ErrorType.NON_SOMATIC); }\n-    public Map<Mutect2VariantFilter, Double> getProbabilitiesByFilter() { return probabilitiesByFilter; }\n+    private Map<Mutect2Filter, List<Double>> getPartitionedProbabilitiesByFilter(boolean variantOnly) {\n+        Map<Boolean, LinkedHashMap<Mutect2Filter, List<Double>>> groups =\n+                alleleProbabilitiesByFilter.entrySet().stream().collect(Collectors.partitioningBy(\n+                        entry -> Mutect2VariantFilter.class.isAssignableFrom(entry.getKey().getClass()),\n+                        toMap(Map.Entry::getKey, Map.Entry::getValue, (a,b) -> a, LinkedHashMap::new)));\n+        return groups.get(variantOnly);\n+    }\n \n+    // TODO would this be useful in a util class somewhere?\n+    private static <T> List<List<T>> transpose(List<List<T>> list) {\n+        // all lists need to be the same size\n+        final int N = list.stream().mapToInt(l -> l.size()).max().orElse(-1);\n+        if (list.stream().anyMatch(l -> l.size() != N)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgwMDU2OQ=="}, "originalCommit": {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjMxMTcxOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/MinAlleleFractionFilter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMjozNTo0OFrOFgxaDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMjozNTo0OFrOFgxaDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkwODIzOQ==", "bodyText": "As above, why is this a method and not a constant predicate?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369908239", "createdAt": "2020-01-23T02:35:48Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/MinAlleleFractionFilter.java", "diffHunk": "@@ -18,16 +21,21 @@ public MinAlleleFractionFilter(final double minAf) {\n     @Override\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n+    public Predicate<Genotype> checkPreconditions() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjMxMjA3OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/MinAlleleFractionFilter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMjozNjowOVrOFgxaRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMjozNjowOVrOFgxaRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkwODI5Mw==", "bodyText": "final Genotype g", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369908293", "createdAt": "2020-01-23T02:36:09Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/MinAlleleFractionFilter.java", "diffHunk": "@@ -18,16 +21,21 @@ public MinAlleleFractionFilter(final double minAf) {\n     @Override\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n+    public Predicate<Genotype> checkPreconditions() {\n+        return g -> g.hasExtendedAttribute(GATKVCFConstants.ALLELE_FRACTION_KEY);\n+    }\n+\n+    public List<Double> getAltData(Genotype g) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjMxMzc0OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/MinAlleleFractionFilter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMjozNzo0MFrOFgxbUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMjozNzo0MFrOFgxbUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkwODU2MA==", "bodyText": "Instead of the stream, return Doubles.asList(data).", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369908560", "createdAt": "2020-01-23T02:37:40Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/MinAlleleFractionFilter.java", "diffHunk": "@@ -18,16 +21,21 @@ public MinAlleleFractionFilter(final double minAf) {\n     @Override\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n+    public Predicate<Genotype> checkPreconditions() {\n+        return g -> g.hasExtendedAttribute(GATKVCFConstants.ALLELE_FRACTION_KEY);\n+    }\n+\n+    public List<Double> getAltData(Genotype g) {\n+        double[] data = GATKProtectedVariantContextUtils.getAttributeAsDoubleArray(g, GATKVCFConstants.ALLELE_FRACTION_KEY, () -> null, 1.0);\n+        return Arrays.stream(data).boxed().collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjMzMjU0OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMjo1MzowOFrOFgxmrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMjo1MzowOFrOFgxmrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMTQ2OA==", "bodyText": "There should be logic to \"promote\" an allele-specific filter that applies to every allele to a site-level filter.  Also, if all alleles fail, but for different reasons, what happens to the site filter?  Apologies if those features are here and I missed them.  It looks to me like perhaps this is a side-effect of addFilterStrings, but I'm having trouble following.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369911468", "createdAt": "2020-01-23T02:53:08Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "diffHunk": "@@ -175,28 +177,89 @@ public VariantContext applyFiltersAndAccumulateOutputStats(final VariantContext\n         final ErrorProbabilities errorProbabilities = new ErrorProbabilities(filters, vc, this, referenceContext);\n         filteringOutputStats.recordCall(errorProbabilities, getThreshold() - EPSILON);\n \n-        final boolean variantFailsFilters = errorProbabilities.getErrorProbability() > Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n-        final double maxErrorProb = errorProbabilities.getProbabilitiesByFilter().values().stream().mapToDouble(p->p).max().orElse(1);\n+        // error probability must exceed threshold, and just in case threshold is bad, probabilities close to 1 must be filtered\n+        // and probabilities close to 0 must not be filtered\n+        double errorThreshold = Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n+\n+        Map<String, Double> siteFiltersWithErrorProb = new LinkedHashMap<>();\n+\n+        // apply allele specific filters\n+        List<Iterator<String>> ASFilters =\n+                errorProbabilities.getProbabilitiesForAlleleFilters().entrySet().stream()\n+                        .filter(entry -> !entry.getValue().isEmpty())\n+                        .map(entry -> addFilterStrings(entry.getValue(), siteFiltersWithErrorProb, errorThreshold, entry.getKey().filterName())).collect(Collectors.toList());\n+\n+        List<String> orderedASFilterStrings = vc.getAlleles().stream().map(allele -> allele.isReference() || allele.isSymbolic() ?\n+                VCFConstants.EMPTY_INFO_FIELD : getMergedFilterStringForAllele(ASFilters)).collect(Collectors.toList());\n+        String finalAttrString = AnnotationUtils.encodeAnyASList(orderedASFilterStrings);\n+\n+        vcb.putAttributes(Collections.singletonMap(GATKVCFConstants.AS_FILTER_STATUS_KEY, finalAttrString));\n+\n+\n+        // compute site-only filters\n+        errorProbabilities.getProbabilitiesForVariantFilters().entrySet().stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjMzNDQxOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMjo1NDo0NlrOFgxnzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMjo1NDo0NlrOFgxnzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMTc1OA==", "bodyText": "I think there should not be such an option.  MIN_REPORTABLE_ERROR_PROBABILITY is fairly low.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369911758", "createdAt": "2020-01-23T02:54:46Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "diffHunk": "@@ -175,28 +177,89 @@ public VariantContext applyFiltersAndAccumulateOutputStats(final VariantContext\n         final ErrorProbabilities errorProbabilities = new ErrorProbabilities(filters, vc, this, referenceContext);\n         filteringOutputStats.recordCall(errorProbabilities, getThreshold() - EPSILON);\n \n-        final boolean variantFailsFilters = errorProbabilities.getErrorProbability() > Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n-        final double maxErrorProb = errorProbabilities.getProbabilitiesByFilter().values().stream().mapToDouble(p->p).max().orElse(1);\n+        // error probability must exceed threshold, and just in case threshold is bad, probabilities close to 1 must be filtered\n+        // and probabilities close to 0 must not be filtered\n+        double errorThreshold = Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n+\n+        Map<String, Double> siteFiltersWithErrorProb = new LinkedHashMap<>();\n+\n+        // apply allele specific filters\n+        List<Iterator<String>> ASFilters =\n+                errorProbabilities.getProbabilitiesForAlleleFilters().entrySet().stream()\n+                        .filter(entry -> !entry.getValue().isEmpty())\n+                        .map(entry -> addFilterStrings(entry.getValue(), siteFiltersWithErrorProb, errorThreshold, entry.getKey().filterName())).collect(Collectors.toList());\n+\n+        List<String> orderedASFilterStrings = vc.getAlleles().stream().map(allele -> allele.isReference() || allele.isSymbolic() ?\n+                VCFConstants.EMPTY_INFO_FIELD : getMergedFilterStringForAllele(ASFilters)).collect(Collectors.toList());\n+        String finalAttrString = AnnotationUtils.encodeAnyASList(orderedASFilterStrings);\n+\n+        vcb.putAttributes(Collections.singletonMap(GATKVCFConstants.AS_FILTER_STATUS_KEY, finalAttrString));\n+\n+\n+        // compute site-only filters\n+        errorProbabilities.getProbabilitiesForVariantFilters().entrySet().stream()\n+                .forEach(entry -> {\n+                    entry.getKey().phredScaledPosteriorAnnotationName().ifPresent(annotation -> {\n+                        if (entry.getKey().requiredAnnotations().stream().allMatch(vc::hasAttribute)) {\n+                            vcb.attribute(annotation, QualityUtils.errorProbToQual(entry.getValue()));\n+                        }\n+                    });\n+                    if (entry.getValue() > errorThreshold) {\n+                        siteFiltersWithErrorProb.put(entry.getKey().filterName(), entry.getValue());\n+                    }\n \n-        for (final Map.Entry<Mutect2VariantFilter, Double> entry : errorProbabilities.getProbabilitiesByFilter().entrySet()) {\n-            final double errorProbability = entry.getValue();\n+                });\n \n-            entry.getKey().phredScaledPosteriorAnnotationName().ifPresent(annotation -> {\n-                if (entry.getKey().requiredAnnotations().stream().allMatch(vc::hasAttribute)) {\n-                    vcb.attribute(annotation, QualityUtils.errorProbToQual(errorProbability));\n-                }\n-            });\n+        // TO reviewers - should there be a flag where this is skipped and all filters are in the output vcf?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjMzOTY2OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMjo1OToyMVrOFgxrWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMjo1OToyMVrOFgxrWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMjY2NA==", "bodyText": "I think it's the correct default.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369912664", "createdAt": "2020-01-23T02:59:21Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "diffHunk": "@@ -175,28 +177,89 @@ public VariantContext applyFiltersAndAccumulateOutputStats(final VariantContext\n         final ErrorProbabilities errorProbabilities = new ErrorProbabilities(filters, vc, this, referenceContext);\n         filteringOutputStats.recordCall(errorProbabilities, getThreshold() - EPSILON);\n \n-        final boolean variantFailsFilters = errorProbabilities.getErrorProbability() > Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n-        final double maxErrorProb = errorProbabilities.getProbabilitiesByFilter().values().stream().mapToDouble(p->p).max().orElse(1);\n+        // error probability must exceed threshold, and just in case threshold is bad, probabilities close to 1 must be filtered\n+        // and probabilities close to 0 must not be filtered\n+        double errorThreshold = Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n+\n+        Map<String, Double> siteFiltersWithErrorProb = new LinkedHashMap<>();\n+\n+        // apply allele specific filters\n+        List<Iterator<String>> ASFilters =\n+                errorProbabilities.getProbabilitiesForAlleleFilters().entrySet().stream()\n+                        .filter(entry -> !entry.getValue().isEmpty())\n+                        .map(entry -> addFilterStrings(entry.getValue(), siteFiltersWithErrorProb, errorThreshold, entry.getKey().filterName())).collect(Collectors.toList());\n+\n+        List<String> orderedASFilterStrings = vc.getAlleles().stream().map(allele -> allele.isReference() || allele.isSymbolic() ?\n+                VCFConstants.EMPTY_INFO_FIELD : getMergedFilterStringForAllele(ASFilters)).collect(Collectors.toList());\n+        String finalAttrString = AnnotationUtils.encodeAnyASList(orderedASFilterStrings);\n+\n+        vcb.putAttributes(Collections.singletonMap(GATKVCFConstants.AS_FILTER_STATUS_KEY, finalAttrString));\n+\n+\n+        // compute site-only filters\n+        errorProbabilities.getProbabilitiesForVariantFilters().entrySet().stream()\n+                .forEach(entry -> {\n+                    entry.getKey().phredScaledPosteriorAnnotationName().ifPresent(annotation -> {\n+                        if (entry.getKey().requiredAnnotations().stream().allMatch(vc::hasAttribute)) {\n+                            vcb.attribute(annotation, QualityUtils.errorProbToQual(entry.getValue()));\n+                        }\n+                    });\n+                    if (entry.getValue() > errorThreshold) {\n+                        siteFiltersWithErrorProb.put(entry.getKey().filterName(), entry.getValue());\n+                    }\n \n-        for (final Map.Entry<Mutect2VariantFilter, Double> entry : errorProbabilities.getProbabilitiesByFilter().entrySet()) {\n-            final double errorProbability = entry.getValue();\n+                });\n \n-            entry.getKey().phredScaledPosteriorAnnotationName().ifPresent(annotation -> {\n-                if (entry.getKey().requiredAnnotations().stream().allMatch(vc::hasAttribute)) {\n-                    vcb.attribute(annotation, QualityUtils.errorProbToQual(errorProbability));\n-                }\n-            });\n+        // TO reviewers - should there be a flag where this is skipped and all filters are in the output vcf?\n+        // otherwise things may seem erroneous. and should we apply this type of limit on the allele specific filters too?\n \n-            // error probability must exceed threshold, and just in case threshold is bad, probabilities close to 1 must be filtered\n-            // and probabilities close to 0 must not be filtered\n-            if (variantFailsFilters && errorProbability >= Math.min(maxErrorProb, MIN_REPORTABLE_ERROR_PROBABILITY)) {\n-                vcb.filter(entry.getKey().filterName());\n+        // this code limits the number of filters specified for any variant to the highest probability filters\n+        // this will not change the status of whether a variant is actually filtered or not\n+        final double maxErrorProb = siteFiltersWithErrorProb.values().stream().mapToDouble(p->p).max().orElse(1);\n+        siteFiltersWithErrorProb.entrySet().stream().forEach(entry -> {\n+            if (entry.getValue() >= Math.min(maxErrorProb, MIN_REPORTABLE_ERROR_PROBABILITY)) {\n+                vcb.filter(entry.getKey());\n             }\n-        }\n+        });\n \n         return vcb.make();\n     }\n \n+    /**\n+     * Creates a comma separated string of all the filters that apply to the allele. This is basically\n+     * a pivot of the data. we have filterlist -> allele -> filterName. and we want allele -> list of filterName\n+     * @param alleleSpecificFilters all of the allele specific filters with the allele filter info\n+     * @return encoded (comma separated) list of filters that apply to the allele\n+     */\n+    private String getMergedFilterStringForAllele(List<Iterator<String>> alleleSpecificFilters) {\n+        // loop through each filter and pull out the filters the specified allele\n+        List<String> results = alleleSpecificFilters.stream().map(alleleValuesIterator -> alleleValuesIterator.next()).distinct().collect(Collectors.toList());\n+        if (results.size() > 1 && results.contains(VCFConstants.PASSES_FILTERS_v4)) {\n+            results.remove(VCFConstants.PASSES_FILTERS_v4);\n+        } else if (results.isEmpty()) {\n+            results.add(VCFConstants.PASSES_FILTERS_v4);\n+        }\n+        return AnnotationUtils.encodeStringList(results);\n+    }\n+\n+    /**\n+     * For each allele, determine whether the filter should be applied. also determine if the filter should apply to the site\n+     * @param probabilities the probability computed by the filter for the allele\n+     * @param siteFiltersWithErrorProb in/out parameter that is collecting site level filters with the max error probability\n+     * @param errorThreshold the theshold to use to determine whether filter applies\n+     * @param filterName the name of the filter used in the vcf\n+     * @return Iterator of filters for an allele\n+     */\n+    private Iterator<String> addFilterStrings(List<Double> probabilities, Map<String, Double> siteFiltersWithErrorProb, double errorThreshold, String filterName) {\n+        List<String> results = probabilities.stream().map(value -> value > errorThreshold ?\n+                        filterName : VCFConstants.PASSES_FILTERS_v4).collect(Collectors.toList());\n+        if (!results.isEmpty() && results.stream().allMatch(x -> x.equals(filterName))) {\n+            // TODO is this the correct default", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjM0MDIxOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMjo1OTo1M1rOFgxrsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMjo1OTo1M1rOFgxrsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMjc1NQ==", "bodyText": "What are these test gvcf comments about?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369912755", "createdAt": "2020-01-23T02:59:53Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "diffHunk": "@@ -214,15 +277,21 @@ private void buildFiltersList(final M2FiltersArgumentCollection MTFAC) {\n         filters.add(new BaseQualityFilter(MTFAC.minMedianBaseQuality));\n         filters.add(new MappingQualityFilter(MTFAC.minMedianMappingQuality, MTFAC.longIndelLength));\n         filters.add(new DuplicatedAltReadFilter(MTFAC.uniqueAltReadCount));\n-        filters.add(new StrandArtifactFilter());\n+        filters.add(new StrandArtifactFilter());  // test gvcf", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 151}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjM0MTA1OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzowMDozM1rOFgxsLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzowMDozM1rOFgxsLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMjg3Ng==", "bodyText": "Address the TODO", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369912876", "createdAt": "2020-01-23T03:00:33Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "diffHunk": "@@ -234,8 +303,8 @@ private void buildFiltersList(final M2FiltersArgumentCollection MTFAC) {\n         }\n \n         if (MTFAC.mitochondria) {\n-            filters.add(new ChimericOriginalAlignmentFilter(MTFAC.maxNuMTFraction));\n-            filters.add(new PolymorphicNuMTFilter(MTFAC.medianAutosomalCoverage));\n+            filters.add(new ChimericOriginalAlignmentFilter(MTFAC.maxNuMTFraction));  // TODO convert!!", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 179}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjM0Mjc5OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2VariantFilter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzowMjowOVrOFgxtSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzowMjowOVrOFgxtSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMzE2Mg==", "bodyText": "This can be achieved with Collections.nCopies()", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369913162", "createdAt": "2020-01-23T03:02:09Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2VariantFilter.java", "diffHunk": "@@ -1,56 +1,25 @@\n package org.broadinstitute.hellbender.tools.walkers.mutect.filtering;\n \n import htsjdk.variant.variantcontext.VariantContext;\n-import org.apache.commons.lang3.tuple.ImmutablePair;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n \n-import java.util.Comparator;\n+import java.util.ArrayList;\n import java.util.List;\n-import java.util.Optional;\n \n-public abstract class Mutect2VariantFilter {\n+public abstract class Mutect2VariantFilter extends Mutect2Filter {\n     public Mutect2VariantFilter() { }\n \n-    public double errorProbability(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n-        final double result = requiredAnnotations().stream().allMatch(vc::hasAttribute) ? calculateErrorProbability(vc, filteringEngine, referenceContext) : 0;\n-        return Mutect2FilteringEngine.roundFinitePrecisionErrors(result);\n-    }\n-\n-    protected abstract double calculateErrorProbability(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext);\n+    @Override\n+    public List<Double> errorProbabilities(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        int numAltAlleles = vc.getNAlleles() - 1;\n+        final double result = Mutect2FilteringEngine.roundFinitePrecisionErrors(requiredAnnotations().stream().allMatch(vc::hasAttribute) ?\n+                calculateErrorProbability(vc, filteringEngine, referenceContext) : 0.0);\n+        ArrayList<Double> resultList = new ArrayList<>(numAltAlleles);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjM0NjE1OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/StrandArtifactFilter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzowNTowNVrOFgxvcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzowNTowNVrOFgxvcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMzcxMw==", "bodyText": "Is .map(   ).reduce(0, Math::addExact) equivalent to .mapToInt(  ).sum()?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369913713", "createdAt": "2020-01-23T03:05:05Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/StrandArtifactFilter.java", "diffHunk": "@@ -43,30 +44,40 @@\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n     @Override\n-    public double calculateErrorProbability(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n-        final EStep probabilities = calculateArtifactProbabilities(vc, filteringEngine);\n-        return probabilities.forwardArtifactResponsibility + probabilities.reverseArtifactResponsibility;\n+    public List<Double> calculateErrorProbabilityForAlleles(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        final List<EStep> alleleProbs = calculateArtifactProbabilities(vc, filteringEngine);\n+        return alleleProbs.isEmpty() ? Collections.emptyList() :\n+                alleleProbs.stream().map(probabilities -> probabilities.forwardArtifactResponsibility + probabilities.reverseArtifactResponsibility).collect(Collectors.toList());\n     }\n \n-    public EStep calculateArtifactProbabilities(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n-        // {fwd ref, rev ref, fwd alt, rev alt}\n-        final int[] counts = filteringEngine.sumStrandCountsOverSamples(vc, true, false);\n-\n-        final int indelSize = Math.abs(vc.getReference().length() - vc.getAlternateAllele(0).length());\n-        if (counts[2] + counts[3] == 0 || indelSize > LONGEST_STRAND_ARTIFACT_INDEL_SIZE) {\n-            return new EStep(0, 0, counts[0] + counts[2], counts[1] + counts[3], counts[2], counts[3]);\n+    public List<EStep> calculateArtifactProbabilities(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n+        // for each allele, forward and reverse count\n+        List<List<Integer>> sbs = StrandBiasUtils.getSBsForAlleles(vc);\n+        if (sbs == null || sbs.isEmpty() || sbs.size() <= 1) {\n+            return Collections.emptyList();\n         }\n \n-\n-        return strandArtifactProbability(strandArtifactPrior, counts[0] + counts[2], counts[1] + counts[3], counts[2], counts[3], indelSize);\n-\n+        final ListIterator<Integer> indelSizeIterator = vc.getAlternateAlleles().stream().map(alt -> Math.abs(vc.getReference().length() - alt.length())).collect(Collectors.toList()).listIterator();\n+        int totalFwd = sbs.stream().map(sb -> sb.get(0)).reduce(0, Math::addExact);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjM0OTUwOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/StrandArtifactFilter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzowNzo1NFrOFgxxhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzowNzo1NFrOFgxxhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDI0NA==", "bodyText": "Incrementing an iterator as a side-effect of a lambda spooks me.  It feels like it's adhering to the letter of the law that requires variables inside a lambda to be final but violating the spirit of functional programming.  I would rather have an indel size list and have an IntStream even at the cost of an extra line altSB = altSBs.get(n).", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369914244", "createdAt": "2020-01-23T03:07:54Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/StrandArtifactFilter.java", "diffHunk": "@@ -43,30 +44,40 @@\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n     @Override\n-    public double calculateErrorProbability(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n-        final EStep probabilities = calculateArtifactProbabilities(vc, filteringEngine);\n-        return probabilities.forwardArtifactResponsibility + probabilities.reverseArtifactResponsibility;\n+    public List<Double> calculateErrorProbabilityForAlleles(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        final List<EStep> alleleProbs = calculateArtifactProbabilities(vc, filteringEngine);\n+        return alleleProbs.isEmpty() ? Collections.emptyList() :\n+                alleleProbs.stream().map(probabilities -> probabilities.forwardArtifactResponsibility + probabilities.reverseArtifactResponsibility).collect(Collectors.toList());\n     }\n \n-    public EStep calculateArtifactProbabilities(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n-        // {fwd ref, rev ref, fwd alt, rev alt}\n-        final int[] counts = filteringEngine.sumStrandCountsOverSamples(vc, true, false);\n-\n-        final int indelSize = Math.abs(vc.getReference().length() - vc.getAlternateAllele(0).length());\n-        if (counts[2] + counts[3] == 0 || indelSize > LONGEST_STRAND_ARTIFACT_INDEL_SIZE) {\n-            return new EStep(0, 0, counts[0] + counts[2], counts[1] + counts[3], counts[2], counts[3]);\n+    public List<EStep> calculateArtifactProbabilities(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n+        // for each allele, forward and reverse count\n+        List<List<Integer>> sbs = StrandBiasUtils.getSBsForAlleles(vc);\n+        if (sbs == null || sbs.isEmpty() || sbs.size() <= 1) {\n+            return Collections.emptyList();\n         }\n \n-\n-        return strandArtifactProbability(strandArtifactPrior, counts[0] + counts[2], counts[1] + counts[3], counts[2], counts[3], indelSize);\n-\n+        final ListIterator<Integer> indelSizeIterator = vc.getAlternateAlleles().stream().map(alt -> Math.abs(vc.getReference().length() - alt.length())).collect(Collectors.toList()).listIterator();\n+        int totalFwd = sbs.stream().map(sb -> sb.get(0)).reduce(0, Math::addExact);\n+        int totalRev = sbs.stream().map(sb -> sb.get(1)).reduce(0, Math::addExact);\n+        // skip the reference\n+        List<List<Integer>> altSBs = sbs.subList(1, sbs.size());\n+\n+        return altSBs.stream().map(altSB -> {\n+            final int altIndelSize = indelSizeIterator.next();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjM1MDgxOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ThresholdCalculator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzowOTowMlrOFgxyRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzowOTowMlrOFgxyRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDQzNw==", "bodyText": "Thank you.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369914437", "createdAt": "2020-01-23T03:09:02Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ThresholdCalculator.java", "diffHunk": "@@ -20,7 +20,7 @@\n \n     private double threshold;\n \n-    final List<Double> artifactProbabilities = new ArrayList<>();\n+    final List<Double> errorProbabilities = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjM1MjI5OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/TumorEvidenceFilter.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxMDozNFrOFgxzOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxNToyMDozOFrOFhCDxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDY4Mg==", "bodyText": "Why not\nreturn new IndexRange(0, tumorLods.length).mapToDouble(i -> _____);", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369914682", "createdAt": "2020-01-23T03:10:34Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/TumorEvidenceFilter.java", "diffHunk": "@@ -4,27 +4,30 @@\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n import org.broadinstitute.hellbender.tools.walkers.mutect.clustering.Datum;\n import org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n import org.broadinstitute.hellbender.utils.MathUtils;\n import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n \n-import java.util.Collections;\n-import java.util.List;\n-import java.util.Optional;\n+import java.util.*;\n \n-public class TumorEvidenceFilter extends Mutect2VariantFilter {\n+public class TumorEvidenceFilter extends Mutect2AlleleFilter {\n     @Override\n     public ErrorType errorType() { return ErrorType.SEQUENCING; }\n \n     @Override\n-    public double calculateErrorProbability(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+    protected List<Double> calculateErrorProbabilityForAlleles(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext)\n+    {\n         final double[] tumorLods = Mutect2FilteringEngine.getTumorLogOdds(vc);\n         final int[] ADs = filteringEngine.sumADsOverSamples(vc, true, false);\n-        final int maxIndex = MathUtils.maxElementIndex(tumorLods);\n-        final int altCount = ADs[maxIndex + 1];\n         final int totalCount = (int) MathUtils.sum(ADs);\n+        SomaticClusteringModel model = filteringEngine.getSomaticClusteringModel();\n \n-        return filteringEngine.getSomaticClusteringModel()\n-                .probabilityOfSequencingError(new Datum(tumorLods[maxIndex], 0, 0, altCount, totalCount, SomaticClusteringModel.indelLength(vc, maxIndex)));\n+        List<Double> altResults = new ArrayList<>();\n+        // 0 is the correct value. problem with threshold\n+        new IndexRange(0, tumorLods.length).forEach(i ->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE1OTg3NA==", "bodyText": "this is what i had to do. can you think of a way to simplify?\nreturn Arrays.stream(new IndexRange(0, tumorLods.length).mapToDouble(i ->\n    model.probabilityOfSequencingError(new Datum(tumorLods[i], 0, 0, ADs[i+1],\n        totalCount, SomaticClusteringModel.indelLength(vc, i)))))\n                .boxed().collect(Collectors.toList());", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r370159874", "createdAt": "2020-01-23T14:46:48Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/TumorEvidenceFilter.java", "diffHunk": "@@ -4,27 +4,30 @@\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n import org.broadinstitute.hellbender.tools.walkers.mutect.clustering.Datum;\n import org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n import org.broadinstitute.hellbender.utils.MathUtils;\n import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n \n-import java.util.Collections;\n-import java.util.List;\n-import java.util.Optional;\n+import java.util.*;\n \n-public class TumorEvidenceFilter extends Mutect2VariantFilter {\n+public class TumorEvidenceFilter extends Mutect2AlleleFilter {\n     @Override\n     public ErrorType errorType() { return ErrorType.SEQUENCING; }\n \n     @Override\n-    public double calculateErrorProbability(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+    protected List<Double> calculateErrorProbabilityForAlleles(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext)\n+    {\n         final double[] tumorLods = Mutect2FilteringEngine.getTumorLogOdds(vc);\n         final int[] ADs = filteringEngine.sumADsOverSamples(vc, true, false);\n-        final int maxIndex = MathUtils.maxElementIndex(tumorLods);\n-        final int altCount = ADs[maxIndex + 1];\n         final int totalCount = (int) MathUtils.sum(ADs);\n+        SomaticClusteringModel model = filteringEngine.getSomaticClusteringModel();\n \n-        return filteringEngine.getSomaticClusteringModel()\n-                .probabilityOfSequencingError(new Datum(tumorLods[maxIndex], 0, 0, altCount, totalCount, SomaticClusteringModel.indelLength(vc, maxIndex)));\n+        List<Double> altResults = new ArrayList<>();\n+        // 0 is the correct value. problem with threshold\n+        new IndexRange(0, tumorLods.length).forEach(i ->", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDY4Mg=="}, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE4MTA2MQ==", "bodyText": "Ah, I see, because IndexRange.mapToDouble gives you a double[], not a list.  In that case how about\nreturn IntStream.range(0, tumorLods.length)\n   .mapToObj(i -> new Datum(tumorLods[i], 0, 0, ADs[i+1], totalCount, SomaticClusteringModel.indelLength(vc, i)))\n   .map(model::probabilityOfSequencingError)\n   .collect(Collectors.toList());", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r370181061", "createdAt": "2020-01-23T15:20:38Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/TumorEvidenceFilter.java", "diffHunk": "@@ -4,27 +4,30 @@\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n import org.broadinstitute.hellbender.tools.walkers.mutect.clustering.Datum;\n import org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n import org.broadinstitute.hellbender.utils.MathUtils;\n import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n \n-import java.util.Collections;\n-import java.util.List;\n-import java.util.Optional;\n+import java.util.*;\n \n-public class TumorEvidenceFilter extends Mutect2VariantFilter {\n+public class TumorEvidenceFilter extends Mutect2AlleleFilter {\n     @Override\n     public ErrorType errorType() { return ErrorType.SEQUENCING; }\n \n     @Override\n-    public double calculateErrorProbability(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+    protected List<Double> calculateErrorProbabilityForAlleles(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext)\n+    {\n         final double[] tumorLods = Mutect2FilteringEngine.getTumorLogOdds(vc);\n         final int[] ADs = filteringEngine.sumADsOverSamples(vc, true, false);\n-        final int maxIndex = MathUtils.maxElementIndex(tumorLods);\n-        final int altCount = ADs[maxIndex + 1];\n         final int totalCount = (int) MathUtils.sum(ADs);\n+        SomaticClusteringModel model = filteringEngine.getSomaticClusteringModel();\n \n-        return filteringEngine.getSomaticClusteringModel()\n-                .probabilityOfSequencingError(new Datum(tumorLods[maxIndex], 0, 0, altCount, totalCount, SomaticClusteringModel.indelLength(vc, maxIndex)));\n+        List<Double> altResults = new ArrayList<>();\n+        // 0 is the correct value. problem with threshold\n+        new IndexRange(0, tumorLods.length).forEach(i ->", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDY4Mg=="}, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjM1NDAxOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKVCFConstants.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxMjoyMFrOFgx0Ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxMjoyMFrOFgx0Ug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDk2Mg==", "bodyText": "I try to keep every M2 filter name at least 8 characters and less than 16.  That way the filtering stats file and, with luck, the VCF, looks better when viewed in a terminal.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369914962", "createdAt": "2020-01-23T03:12:20Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKVCFConstants.java", "diffHunk": "@@ -162,7 +162,8 @@ their names (or descriptions) depend on some threshold.  Those filters are not i\n     public final static String N_RATIO_FILTER_NAME =                           \"n_ratio\";\n     public final static String CHIMERIC_ORIGINAL_ALIGNMENT_FILTER_NAME =       \"numt_chimera\"; //mitochondria\n     public final static String ALLELE_FRACTION_FILTER_NAME =                   \"low_allele_frac\";\n-    public static final String POTENTIAL_POLYMORPHIC_NUMT_FILTER_NAME =        \"numt_novel\";\n+    public static final String POSSIBLE_NUMT_FILTER_NAME =                     \"possible_numt\";\n+    public static final String LOW_HET_FILTER_NAME =                            \"low_het\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjM1NTM0OnYy", "diffSide": "RIGHT", "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2IntegrationTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxMzozN1rOFgx1Hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxMzozN1rOFgx1Hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNTE2Ng==", "bodyText": "What's the status of this comment?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369915166", "createdAt": "2020-01-23T03:13:37Z", "author": {"login": "davidbenjamin"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2IntegrationTest.java", "diffHunk": "@@ -532,21 +550,28 @@ public void testFilterMitochondria(File unfiltered, final double minAlleleFracti\n                 args -> args.addBooleanArgument(StandardArgumentDefinitions.DISABLE_SEQUENCE_DICT_VALIDATION_NAME, true),\n                 args -> args.addNumericArgument(M2FiltersArgumentCollection.MIN_AF_LONG_NAME, minAlleleFraction),\n                 args -> args.addNumericArgument(M2FiltersArgumentCollection.MEDIAN_AUTOSOMAL_COVERAGE_LONG_NAME, autosomalCoverage),\n+                args -> args.addNumericArgument(M2FiltersArgumentCollection.MAX_NUMT_COPIES_IN_AUTOSOME_LONG_NAME, 4.0),\n+                args -> args.addNumericArgument(M2FiltersArgumentCollection.MIN_READS_ON_EACH_STRAND_LONG_NAME, 1),\n                 args -> {\n                     intervals.stream().map(SimpleInterval::new).forEach(args::addInterval);\n                     return args;\n                 });\n \n+        // add tests for DUPLICATE", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjM1NjM5OnYy", "diffSide": "RIGHT", "path": "src/test/resources/org/broadinstitute/hellbender/tools/mutect/mito/expected_LowHetNone_output.txt", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxNDo0M1rOFgx1vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxNDo0M1rOFgx1vg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNTMyNg==", "bodyText": "This file extension should be .vcf, not .txt", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369915326", "createdAt": "2020-01-23T03:14:43Z", "author": {"login": "davidbenjamin"}, "path": "src/test/resources/org/broadinstitute/hellbender/tools/mutect/mito/expected_LowHetNone_output.txt", "diffHunk": "@@ -0,0 +1,69 @@\n+##fileformat=VCFv4.2", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjM1NjUyOnYy", "diffSide": "RIGHT", "path": "src/test/resources/org/broadinstitute/hellbender/tools/mutect/mito/expected_LowHetVariantWalkerIntegrationTest_output.txt", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxNDo1NFrOFgx11g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxNDo1NFrOFgx11g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNTM1MA==", "bodyText": "Ditto about .vcf", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369915350", "createdAt": "2020-01-23T03:14:54Z", "author": {"login": "davidbenjamin"}, "path": "src/test/resources/org/broadinstitute/hellbender/tools/mutect/mito/expected_LowHetVariantWalkerIntegrationTest_output.txt", "diffHunk": "@@ -0,0 +1,69 @@\n+##fileformat=VCFv4.2", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4Nzg3MzkwOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/AS_StrandBiasMutectAnnotation.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxNDoyNjoxMlrOFhAAQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxOToyODozNVrOFk9JuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE0NzM5NA==", "bodyText": "I'm on the fence about putting this in the allelespecific subpackage.  On the one hand, it is allele-specific, but on the other hand, all those classes have interfaces and/or parent classes that this one doesn't.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r370147394", "createdAt": "2020-01-23T14:26:12Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/AS_StrandBiasMutectAnnotation.java", "diffHunk": "@@ -0,0 +1,44 @@\n+package org.broadinstitute.hellbender.tools.walkers.annotator;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI5NDk2OA==", "bodyText": "hmm, now that I look at that class again, I think I'll add the AlleleSpecificAnnotation interface to it. but, yeah, I wasn't really sure which package it belonged in.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r374294968", "createdAt": "2020-02-03T19:28:35Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/AS_StrandBiasMutectAnnotation.java", "diffHunk": "@@ -0,0 +1,44 @@\n+package org.broadinstitute.hellbender.tools.walkers.annotator;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE0NzM5NA=="}, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NzkxMzQ1OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxNDozNjozN1rOFhAYlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxNDozNjozN1rOFhAYlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE1MzYyMg==", "bodyText": "Well, sometimes likelihoods come from Mutect2, right?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r370153622", "createdAt": "2020-01-23T14:36:37Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "diffHunk": "@@ -0,0 +1,160 @@\n+package org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.engine.filters.VariantFilter;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.AnnotationUtils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class StrandBiasUtils {\n+    public static final int FORWARD = 0;\n+    public static final int REVERSE = 1;\n+    public static final int MIN_COUNT = 2;\n+    public static final String PRINT_DELIM = \"|\";\n+    private static final List<Integer> ZERO_LIST = new ArrayList<>(Arrays.asList(0,0));\n+\n+    public static Map<String, Object> computeSBAnnotation(VariantContext vc, AlleleLikelihoods<GATKRead, Allele> likelihoods, String key) {\n+        // calculate the annotation from the likelihoods\n+        // likelihoods can come from HaplotypeCaller call to VariantAnnotatorEngine", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdbbe7f71162e09cfdef50a4a42c22463374519"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwOTcxOTg4OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxNTozMjozMFrOFkPDBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxODo1NjozNVrOFk8MQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzUzOTU4OQ==", "bodyText": "Does it really create a comma separated string?  Also, it looks like the only duplicate filter you're checking for is PASS, so I would call the method \"removeExtraPassFilters\" or something less general than what's there now.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r373539589", "createdAt": "2020-01-31T15:32:30Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "diffHunk": "@@ -176,28 +178,105 @@ public VariantContext applyFiltersAndAccumulateOutputStats(final VariantContext\n         final ErrorProbabilities errorProbabilities = new ErrorProbabilities(filters, vc, this, referenceContext);\n         filteringOutputStats.recordCall(errorProbabilities, getThreshold() - EPSILON);\n \n-        final boolean variantFailsFilters = errorProbabilities.getErrorProbability() > Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n-        final double maxErrorProb = errorProbabilities.getProbabilitiesByFilter().values().stream().mapToDouble(p->p).max().orElse(1);\n+        // error probability must exceed threshold, and just in case threshold is bad, probabilities close to 1 must be filtered\n+        // and probabilities close to 0 must not be filtered\n+        double errorThreshold = Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n+\n+        Map<String, Double> siteFiltersWithErrorProb = new LinkedHashMap<>();\n+\n+        // apply allele specific filters\n+        List<List<String>> alleleStatusByFilter =\n+                errorProbabilities.getProbabilitiesForAlleleFilters().entrySet().stream()\n+                        .filter(entry -> !entry.getValue().isEmpty())\n+                        .map(entry -> addFilterStrings(entry.getValue(), errorThreshold, entry.getKey().filterName())).collect(Collectors.toList());\n+\n+        // for each allele, merge all allele specific filters\n+//      List<Iterator<String>> ASFiltersIterator = ASFilters.stream().map(list -> list.listIterator()).collect(Collectors.toList());\n+        List<List<String>> filtersByAllele = ErrorProbabilities.transpose(alleleStatusByFilter);\n+        List<List<String>> distinctFiltersByAllele = filtersByAllele.stream().map(this::getDistinctFiltersForAllele).collect(Collectors.toList());\n+        ListIterator<String> mergedFilterStringByAllele = distinctFiltersByAllele.stream().map(AnnotationUtils::encodeStringList).collect(Collectors.toList()).listIterator();\n+\n+        List<String> orderedASFilterStrings = vc.getAlleles().stream().map(allele -> allele.isReference() || allele.isSymbolic() ?\n+                VCFConstants.EMPTY_INFO_FIELD : mergedFilterStringByAllele.next()).collect(Collectors.toList());\n+        String finalAttrString = AnnotationUtils.encodeAnyASList(orderedASFilterStrings);\n+\n+        vcb.putAttributes(Collections.singletonMap(GATKVCFConstants.AS_FILTER_STATUS_KEY, finalAttrString));\n \n-        for (final Map.Entry<Mutect2VariantFilter, Double> entry : errorProbabilities.getProbabilitiesByFilter().entrySet()) {\n-            final double errorProbability = entry.getValue();\n \n-            entry.getKey().phredScaledPosteriorAnnotationName().ifPresent(annotation -> {\n-                if (entry.getKey().requiredAnnotations().stream().allMatch(vc::hasAttribute)) {\n-                    vcb.attribute(annotation, QualityUtils.errorProbToQual(errorProbability));\n-                }\n-            });\n+        // compute site-only filters\n+        // from allele specific filters\n+         alleleStatusByFilter.stream().forEachOrdered(alleleStatusForFilter -> {\n+            if (!alleleStatusForFilter.isEmpty() && alleleStatusForFilter.stream().distinct().count() == 1 && !alleleStatusForFilter.contains(VCFConstants.PASSES_FILTERS_v4)) {\n+                siteFiltersWithErrorProb.put(alleleStatusForFilter.get(0), 1.0);\n+            }\n+        });\n \n-            // error probability must exceed threshold, and just in case threshold is bad, probabilities close to 1 must be filtered\n-            // and probabilities close to 0 must not be filtered\n-            if (variantFailsFilters && errorProbability >= Math.min(maxErrorProb, MIN_REPORTABLE_ERROR_PROBABILITY)) {\n-                vcb.filter(entry.getKey().filterName());\n+\n+        // from variant filters\n+        errorProbabilities.getProbabilitiesForVariantFilters().entrySet().stream()\n+                .forEach(entry -> {\n+                    entry.getKey().phredScaledPosteriorAnnotationName().ifPresent(annotation -> {\n+                        if (entry.getKey().requiredAnnotations().stream().allMatch(vc::hasAttribute)) {\n+                            vcb.attribute(annotation, QualityUtils.errorProbToQual(entry.getValue()));\n+                        }\n+                    });\n+                    if (entry.getValue() > errorThreshold) {\n+                        siteFiltersWithErrorProb.put(entry.getKey().filterName(), entry.getValue());\n+                    }\n+\n+                });\n+\n+        // if all alleles have been filtered out, but for different reasons, fail the site.\n+        // if the site is only ref and symbolic, no filters will be applied so don't fail\n+        if (siteFiltersWithErrorProb.isEmpty() && !distinctFiltersByAllele.stream().allMatch(List::isEmpty)) {\n+            // if any allele passed, don't fail the site\n+            if (!distinctFiltersByAllele.stream().flatMap(List::stream).anyMatch(f -> f.equals(VCFConstants.PASSES_FILTERS_v4))) {\n+                // we know the allele level filters exceeded their threshold - so set this prob to 1\n+                siteFiltersWithErrorProb.put(GATKVCFConstants.FAIL, 1.0);\n             }\n         }\n \n+        // this code limits the number of filters specified for any variant to the highest probability filters\n+        // this will not change the status of whether a variant is actually filtered or not\n+        final double maxErrorProb = siteFiltersWithErrorProb.values().stream().mapToDouble(p->p).max().orElse(1);\n+        siteFiltersWithErrorProb.entrySet().stream().forEach(entry -> {\n+            if (entry.getValue() >= Math.min(maxErrorProb, MIN_REPORTABLE_ERROR_PROBABILITY)) {\n+                vcb.filter(entry.getKey());\n+            }\n+        });\n+\n         return vcb.make();\n     }\n \n+    /**\n+     * Creates a comma separated string of all the filters that apply to the allele.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65d5e19b62f1d768ae4139cb2d13e50e97ec3b2b"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI3OTIzNA==", "bodyText": "thanks - didn't update the doc after a refactor.\nthe call to distinct removes any duplicate filters (of which the only possibility should be PASS). the first if statement removes the PASS if there are filters that did not pass.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r374279234", "createdAt": "2020-02-03T18:56:35Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "diffHunk": "@@ -176,28 +178,105 @@ public VariantContext applyFiltersAndAccumulateOutputStats(final VariantContext\n         final ErrorProbabilities errorProbabilities = new ErrorProbabilities(filters, vc, this, referenceContext);\n         filteringOutputStats.recordCall(errorProbabilities, getThreshold() - EPSILON);\n \n-        final boolean variantFailsFilters = errorProbabilities.getErrorProbability() > Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n-        final double maxErrorProb = errorProbabilities.getProbabilitiesByFilter().values().stream().mapToDouble(p->p).max().orElse(1);\n+        // error probability must exceed threshold, and just in case threshold is bad, probabilities close to 1 must be filtered\n+        // and probabilities close to 0 must not be filtered\n+        double errorThreshold = Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n+\n+        Map<String, Double> siteFiltersWithErrorProb = new LinkedHashMap<>();\n+\n+        // apply allele specific filters\n+        List<List<String>> alleleStatusByFilter =\n+                errorProbabilities.getProbabilitiesForAlleleFilters().entrySet().stream()\n+                        .filter(entry -> !entry.getValue().isEmpty())\n+                        .map(entry -> addFilterStrings(entry.getValue(), errorThreshold, entry.getKey().filterName())).collect(Collectors.toList());\n+\n+        // for each allele, merge all allele specific filters\n+//      List<Iterator<String>> ASFiltersIterator = ASFilters.stream().map(list -> list.listIterator()).collect(Collectors.toList());\n+        List<List<String>> filtersByAllele = ErrorProbabilities.transpose(alleleStatusByFilter);\n+        List<List<String>> distinctFiltersByAllele = filtersByAllele.stream().map(this::getDistinctFiltersForAllele).collect(Collectors.toList());\n+        ListIterator<String> mergedFilterStringByAllele = distinctFiltersByAllele.stream().map(AnnotationUtils::encodeStringList).collect(Collectors.toList()).listIterator();\n+\n+        List<String> orderedASFilterStrings = vc.getAlleles().stream().map(allele -> allele.isReference() || allele.isSymbolic() ?\n+                VCFConstants.EMPTY_INFO_FIELD : mergedFilterStringByAllele.next()).collect(Collectors.toList());\n+        String finalAttrString = AnnotationUtils.encodeAnyASList(orderedASFilterStrings);\n+\n+        vcb.putAttributes(Collections.singletonMap(GATKVCFConstants.AS_FILTER_STATUS_KEY, finalAttrString));\n \n-        for (final Map.Entry<Mutect2VariantFilter, Double> entry : errorProbabilities.getProbabilitiesByFilter().entrySet()) {\n-            final double errorProbability = entry.getValue();\n \n-            entry.getKey().phredScaledPosteriorAnnotationName().ifPresent(annotation -> {\n-                if (entry.getKey().requiredAnnotations().stream().allMatch(vc::hasAttribute)) {\n-                    vcb.attribute(annotation, QualityUtils.errorProbToQual(errorProbability));\n-                }\n-            });\n+        // compute site-only filters\n+        // from allele specific filters\n+         alleleStatusByFilter.stream().forEachOrdered(alleleStatusForFilter -> {\n+            if (!alleleStatusForFilter.isEmpty() && alleleStatusForFilter.stream().distinct().count() == 1 && !alleleStatusForFilter.contains(VCFConstants.PASSES_FILTERS_v4)) {\n+                siteFiltersWithErrorProb.put(alleleStatusForFilter.get(0), 1.0);\n+            }\n+        });\n \n-            // error probability must exceed threshold, and just in case threshold is bad, probabilities close to 1 must be filtered\n-            // and probabilities close to 0 must not be filtered\n-            if (variantFailsFilters && errorProbability >= Math.min(maxErrorProb, MIN_REPORTABLE_ERROR_PROBABILITY)) {\n-                vcb.filter(entry.getKey().filterName());\n+\n+        // from variant filters\n+        errorProbabilities.getProbabilitiesForVariantFilters().entrySet().stream()\n+                .forEach(entry -> {\n+                    entry.getKey().phredScaledPosteriorAnnotationName().ifPresent(annotation -> {\n+                        if (entry.getKey().requiredAnnotations().stream().allMatch(vc::hasAttribute)) {\n+                            vcb.attribute(annotation, QualityUtils.errorProbToQual(entry.getValue()));\n+                        }\n+                    });\n+                    if (entry.getValue() > errorThreshold) {\n+                        siteFiltersWithErrorProb.put(entry.getKey().filterName(), entry.getValue());\n+                    }\n+\n+                });\n+\n+        // if all alleles have been filtered out, but for different reasons, fail the site.\n+        // if the site is only ref and symbolic, no filters will be applied so don't fail\n+        if (siteFiltersWithErrorProb.isEmpty() && !distinctFiltersByAllele.stream().allMatch(List::isEmpty)) {\n+            // if any allele passed, don't fail the site\n+            if (!distinctFiltersByAllele.stream().flatMap(List::stream).anyMatch(f -> f.equals(VCFConstants.PASSES_FILTERS_v4))) {\n+                // we know the allele level filters exceeded their threshold - so set this prob to 1\n+                siteFiltersWithErrorProb.put(GATKVCFConstants.FAIL, 1.0);\n             }\n         }\n \n+        // this code limits the number of filters specified for any variant to the highest probability filters\n+        // this will not change the status of whether a variant is actually filtered or not\n+        final double maxErrorProb = siteFiltersWithErrorProb.values().stream().mapToDouble(p->p).max().orElse(1);\n+        siteFiltersWithErrorProb.entrySet().stream().forEach(entry -> {\n+            if (entry.getValue() >= Math.min(maxErrorProb, MIN_REPORTABLE_ERROR_PROBABILITY)) {\n+                vcb.filter(entry.getKey());\n+            }\n+        });\n+\n         return vcb.make();\n     }\n \n+    /**\n+     * Creates a comma separated string of all the filters that apply to the allele.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzUzOTU4OQ=="}, "originalCommit": {"oid": "65d5e19b62f1d768ae4139cb2d13e50e97ec3b2b"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwOTczNTk2OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/NuMTFilter.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxNTozNzo0NlrOFkPNPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQxNTozNToxOVrOFl8U0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzU0MjIwNw==", "bodyText": "This does have required annotations, they're just not info annotations.  How important is this check?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r373542207", "createdAt": "2020-01-31T15:37:46Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/NuMTFilter.java", "diffHunk": "@@ -0,0 +1,50 @@\n+package org.broadinstitute.hellbender.tools.walkers.mutect.filtering;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.apache.commons.math3.distribution.PoissonDistribution;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+\n+public class NuMTFilter extends HardAlleleFilter {\n+    private static final double LOWER_BOUND_PROB = .01;\n+    private final int maxAltDepthCutoff;\n+\n+    public NuMTFilter(final double medianAutosomalCoverage, final double maxNuMTCopies){\n+        if (maxNuMTCopies > 0 && medianAutosomalCoverage > 0) {\n+            final PoissonDistribution autosomalCoverage = new PoissonDistribution(medianAutosomalCoverage * maxNuMTCopies / 2.0);\n+            maxAltDepthCutoff = autosomalCoverage.inverseCumulativeProbability(1 - LOWER_BOUND_PROB);\n+        } else {\n+            maxAltDepthCutoff = 0;\n+        }\n+    }\n+\n+    @Override\n+    public ErrorType errorType() { return ErrorType.NON_SOMATIC; }\n+\n+    public List<Integer> getData(Genotype g) {\n+        return Arrays.stream(g.getAD()).boxed().collect(Collectors.toList());\n+    }\n+\n+    @Override\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        LinkedHashMap<Allele, List<Integer>> dataByAllele = getDataByAllele(vc, Genotype::hasAD, this::getData, filteringEngine);\n+        return dataByAllele.entrySet().stream()\n+                .filter(entry -> !vc.getReference().equals(entry.getKey()))\n+                .map(entry -> entry.getValue().stream().max(Integer::compare).orElse(0) < maxAltDepthCutoff).collect(Collectors.toList());\n+    }\n+\n+    @Override\n+    public String filterName() {\n+        return GATKVCFConstants.POSSIBLE_NUMT_FILTER_NAME;\n+    }\n+\n+    @Override\n+    protected List<String> requiredAnnotations() { return Collections.emptyList(); }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65d5e19b62f1d768ae4139cb2d13e50e97ec3b2b"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI5NzYxNA==", "bodyText": "well the code that uses this sees if the Variant Context has the annotations - so this wouldn't work here because it's on the genotype. If there are no required annotations the check passes and the error probabilities are calculated. I could rename the method to requiredInfoAnnotations or requiredVariantAnnotations?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r374297614", "createdAt": "2020-02-03T19:34:02Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/NuMTFilter.java", "diffHunk": "@@ -0,0 +1,50 @@\n+package org.broadinstitute.hellbender.tools.walkers.mutect.filtering;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.apache.commons.math3.distribution.PoissonDistribution;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+\n+public class NuMTFilter extends HardAlleleFilter {\n+    private static final double LOWER_BOUND_PROB = .01;\n+    private final int maxAltDepthCutoff;\n+\n+    public NuMTFilter(final double medianAutosomalCoverage, final double maxNuMTCopies){\n+        if (maxNuMTCopies > 0 && medianAutosomalCoverage > 0) {\n+            final PoissonDistribution autosomalCoverage = new PoissonDistribution(medianAutosomalCoverage * maxNuMTCopies / 2.0);\n+            maxAltDepthCutoff = autosomalCoverage.inverseCumulativeProbability(1 - LOWER_BOUND_PROB);\n+        } else {\n+            maxAltDepthCutoff = 0;\n+        }\n+    }\n+\n+    @Override\n+    public ErrorType errorType() { return ErrorType.NON_SOMATIC; }\n+\n+    public List<Integer> getData(Genotype g) {\n+        return Arrays.stream(g.getAD()).boxed().collect(Collectors.toList());\n+    }\n+\n+    @Override\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        LinkedHashMap<Allele, List<Integer>> dataByAllele = getDataByAllele(vc, Genotype::hasAD, this::getData, filteringEngine);\n+        return dataByAllele.entrySet().stream()\n+                .filter(entry -> !vc.getReference().equals(entry.getKey()))\n+                .map(entry -> entry.getValue().stream().max(Integer::compare).orElse(0) < maxAltDepthCutoff).collect(Collectors.toList());\n+    }\n+\n+    @Override\n+    public String filterName() {\n+        return GATKVCFConstants.POSSIBLE_NUMT_FILTER_NAME;\n+    }\n+\n+    @Override\n+    protected List<String> requiredAnnotations() { return Collections.emptyList(); }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzU0MjIwNw=="}, "originalCommit": {"oid": "65d5e19b62f1d768ae4139cb2d13e50e97ec3b2b"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg0ODY4MQ==", "bodyText": "If the upstream method has a check for INFO annotations, then I'm happy.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r374848681", "createdAt": "2020-02-04T18:35:04Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/NuMTFilter.java", "diffHunk": "@@ -0,0 +1,50 @@\n+package org.broadinstitute.hellbender.tools.walkers.mutect.filtering;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.apache.commons.math3.distribution.PoissonDistribution;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+\n+public class NuMTFilter extends HardAlleleFilter {\n+    private static final double LOWER_BOUND_PROB = .01;\n+    private final int maxAltDepthCutoff;\n+\n+    public NuMTFilter(final double medianAutosomalCoverage, final double maxNuMTCopies){\n+        if (maxNuMTCopies > 0 && medianAutosomalCoverage > 0) {\n+            final PoissonDistribution autosomalCoverage = new PoissonDistribution(medianAutosomalCoverage * maxNuMTCopies / 2.0);\n+            maxAltDepthCutoff = autosomalCoverage.inverseCumulativeProbability(1 - LOWER_BOUND_PROB);\n+        } else {\n+            maxAltDepthCutoff = 0;\n+        }\n+    }\n+\n+    @Override\n+    public ErrorType errorType() { return ErrorType.NON_SOMATIC; }\n+\n+    public List<Integer> getData(Genotype g) {\n+        return Arrays.stream(g.getAD()).boxed().collect(Collectors.toList());\n+    }\n+\n+    @Override\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        LinkedHashMap<Allele, List<Integer>> dataByAllele = getDataByAllele(vc, Genotype::hasAD, this::getData, filteringEngine);\n+        return dataByAllele.entrySet().stream()\n+                .filter(entry -> !vc.getReference().equals(entry.getKey()))\n+                .map(entry -> entry.getValue().stream().max(Integer::compare).orElse(0) < maxAltDepthCutoff).collect(Collectors.toList());\n+    }\n+\n+    @Override\n+    public String filterName() {\n+        return GATKVCFConstants.POSSIBLE_NUMT_FILTER_NAME;\n+    }\n+\n+    @Override\n+    protected List<String> requiredAnnotations() { return Collections.emptyList(); }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzU0MjIwNw=="}, "originalCommit": {"oid": "65d5e19b62f1d768ae4139cb2d13e50e97ec3b2b"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTMzMDAwMQ==", "bodyText": "ok - renaming the method to requiredInfoAnnotations so it's more explicit", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r375330001", "createdAt": "2020-02-05T15:35:19Z", "author": {"login": "ahaessly"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/NuMTFilter.java", "diffHunk": "@@ -0,0 +1,50 @@\n+package org.broadinstitute.hellbender.tools.walkers.mutect.filtering;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.apache.commons.math3.distribution.PoissonDistribution;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+\n+public class NuMTFilter extends HardAlleleFilter {\n+    private static final double LOWER_BOUND_PROB = .01;\n+    private final int maxAltDepthCutoff;\n+\n+    public NuMTFilter(final double medianAutosomalCoverage, final double maxNuMTCopies){\n+        if (maxNuMTCopies > 0 && medianAutosomalCoverage > 0) {\n+            final PoissonDistribution autosomalCoverage = new PoissonDistribution(medianAutosomalCoverage * maxNuMTCopies / 2.0);\n+            maxAltDepthCutoff = autosomalCoverage.inverseCumulativeProbability(1 - LOWER_BOUND_PROB);\n+        } else {\n+            maxAltDepthCutoff = 0;\n+        }\n+    }\n+\n+    @Override\n+    public ErrorType errorType() { return ErrorType.NON_SOMATIC; }\n+\n+    public List<Integer> getData(Genotype g) {\n+        return Arrays.stream(g.getAD()).boxed().collect(Collectors.toList());\n+    }\n+\n+    @Override\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        LinkedHashMap<Allele, List<Integer>> dataByAllele = getDataByAllele(vc, Genotype::hasAD, this::getData, filteringEngine);\n+        return dataByAllele.entrySet().stream()\n+                .filter(entry -> !vc.getReference().equals(entry.getKey()))\n+                .map(entry -> entry.getValue().stream().max(Integer::compare).orElse(0) < maxAltDepthCutoff).collect(Collectors.toList());\n+    }\n+\n+    @Override\n+    public String filterName() {\n+        return GATKVCFConstants.POSSIBLE_NUMT_FILTER_NAME;\n+    }\n+\n+    @Override\n+    protected List<String> requiredAnnotations() { return Collections.emptyList(); }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzU0MjIwNw=="}, "originalCommit": {"oid": "65d5e19b62f1d768ae4139cb2d13e50e97ec3b2b"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwOTc0MDc0OnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/StrictStrandBiasFilter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxNTozOToyMFrOFkPQdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxNTozOToyMFrOFkPQdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzU0MzAzMA==", "bodyText": "Thanks for the cleanup!", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r373543030", "createdAt": "2020-01-31T15:39:20Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/StrictStrandBiasFilter.java", "diffHunk": "@@ -20,25 +21,14 @@ public StrictStrandBiasFilter(final int minReadsOnEachStrand) {\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n     @Override\n-    public boolean isArtifact(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n-        if (minReadsOnEachStrand == 0) {\n-            return false;\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        List<List<Integer>> sbs = StrandBiasUtils.getSBsForAlleles(vc);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65d5e19b62f1d768ae4139cb2d13e50e97ec3b2b"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDE5NjU2OnYy", "diffSide": "RIGHT", "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/MTLowHeteroplasmyFilterToolTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODoxODozM1rOFkTvGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODoxODozM1rOFkTvGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYxNjQwOA==", "bodyText": "I would really much rather have these tests read in the VCs and then check the filters (and check the relevant FILTER line in the header) rather than doing an exact match, because annoying things happen like someone changes the VCF header description of something unrelated and now these tests fail.  If that's going to be a huge pain, then we can live with this.  They're very thorough, I just want to save someone a little potential pain the future.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r373616408", "createdAt": "2020-01-31T18:18:33Z", "author": {"login": "ldgauthier"}, "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/MTLowHeteroplasmyFilterToolTest.java", "diffHunk": "@@ -0,0 +1,35 @@\n+package org.broadinstitute.hellbender.tools.walkers.mutect.filtering;\n+\n+import org.broadinstitute.hellbender.CommandLineProgramTest;\n+import org.broadinstitute.hellbender.testutils.IntegrationTestSpec;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+public class MTLowHeteroplasmyFilterToolTest extends CommandLineProgramTest {\n+    private static final File MITO_REF = new File(toolsTestDir, \"mutect/mito/Homo_sapiens_assembly38.mt_only.fasta\");\n+    private static final File NA12878_MITO_FILTERED_VCF = new File(toolsTestDir, \"mutect/mito/filtered.vcf\");\n+\n+    @Test\n+    public void testLowHetVariantWalker() throws IOException {\n+        final IntegrationTestSpec testSpec = new IntegrationTestSpec(\n+                        \" -R \" + MITO_REF.getAbsolutePath()  +\n+                        \" -V \" + NA12878_MITO_FILTERED_VCF +\n+                        \" -O %s\",\n+                Arrays.asList(toolsTestDir + \"mutect/mito/expected_LowHetVariantWalkerIntegrationTest_output.vcf\")\n+        );\n+        testSpec.executeTest(\"testLowHetVariantWalker\", this);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65d5e19b62f1d768ae4139cb2d13e50e97ec3b2b"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2ODExOTQyOnYy", "diffSide": "RIGHT", "path": "scripts/mitochondria_m2_wdl/AlignAndCall.wdl", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMDowNTo1NVrOF7sP_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMDowNTo1NVrOF7sP_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODEzNTI5NA==", "bodyText": "I think for the tests to work this needs to be the file name \"AlignmentPipeline.wdl\". If there's a better solution for moving things back and forth between Terra and the gatk repo though I'm all for it.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r398135294", "createdAt": "2020-03-25T20:05:55Z", "author": {"login": "meganshand"}, "path": "scripts/mitochondria_m2_wdl/AlignAndCall.wdl", "diffHunk": "@@ -1,6 +1,6 @@\n version 1.0\n \n-import \"AlignmentPipeline.wdl\" as AlignAndMarkDuplicates\n+import \"https://api.firecloud.org/ga4gh/v1/tools/mitochondria:AlignmentPipeline/versions/1/plain-WDL/descriptor\" as AlignAndMarkDuplicates", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "74fff72231f9e1094cd79010e9547da85d1db76e"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2ODE2MTA5OnYy", "diffSide": "RIGHT", "path": "scripts/mitochondria_m2_wdl/AlignAndCall.wdl", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMDoxODoxOFrOF7sqoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMDoxODoxOFrOF7sqoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE0MjExMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  # runtime\n          \n          \n            \n                # runtime", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r398142113", "createdAt": "2020-03-25T20:18:18Z", "author": {"login": "meganshand"}, "path": "scripts/mitochondria_m2_wdl/AlignAndCall.wdl", "diffHunk": "@@ -169,93 +202,101 @@ workflow AlignAndCall {\n       gatk_override = gatk_override,\n       m2_extra_filtering_args = m2_filter_extra_args,\n       max_alt_allele_count = 4,\n-      contamination = GetContamination.minor_level,\n-      autosomal_coverage = autosomal_coverage,\n       vaf_filter_threshold = vaf_filter_threshold,\n       blacklisted_sites = blacklisted_sites,\n       blacklisted_sites_index = blacklisted_sites_index,\n       f_score_beta = f_score_beta,\n       preemptible_tries = preemptible_tries\n+ }\n+\n+  if ( defined(autosomal_coverage) ) {\n+    call FilterNuMTs {\n+      input:\n+        filtered_vcf = FilterContamination.filtered_vcf,\n+        ref_fasta = mt_fasta,\n+        ref_fai = mt_fasta_index,\n+        ref_dict = mt_dict,\n+        autosomal_coverage = autosomal_coverage,\n+        gatk_override = gatk_override,\n+        compress = compress_output_vcf,\n+        preemptible_tries = preemptible_tries\n+    }\n   }\n \n+  File low_het_vcf = select_first([FilterNuMTs.numt_filtered_vcf, FilterContamination.filtered_vcf])\n+\n+  call FilterLowHetSites {\n+    input:\n+      filtered_vcf = low_het_vcf,\n+      ref_fasta = mt_fasta,\n+      ref_fai = mt_fasta_index,\n+      ref_dict = mt_dict,\n+      max_low_het_sites = max_low_het_sites,\n+      gatk_override = gatk_override,\n+      compress = compress_output_vcf,\n+      preemptible_tries = preemptible_tries\n+  }\n+\n+\n   output {\n     File mt_aligned_bam = AlignToMt.mt_aligned_bam\n     File mt_aligned_bai = AlignToMt.mt_aligned_bai\n     File mt_aligned_shifted_bam = AlignToShiftedMt.mt_aligned_bam\n     File mt_aligned_shifted_bai = AlignToShiftedMt.mt_aligned_bai\n-    File out_vcf = Filter.filtered_vcf\n-    File out_vcf_index = Filter.filtered_vcf_idx\n+    File out_vcf = FilterLowHetSites.final_filtered_vcf\n+    File out_vcf_index = FilterLowHetSites.final_filtered_vcf_idx\n+    File input_vcf_for_haplochecker = SplitMultiAllelicsAndRemoveNonPassSites.vcf_for_haplochecker\n     File duplicate_metrics = AlignToMt.duplicate_metrics\n     File coverage_metrics = CollectWgsMetrics.metrics\n     File theoretical_sensitivity_metrics = CollectWgsMetrics.theoretical_sensitivity\n     File contamination_metrics = GetContamination.contamination_file\n     Int mean_coverage = CollectWgsMetrics.mean_coverage\n     String major_haplogroup = GetContamination.major_hg\n-    Float contamination = GetContamination.minor_level\n+    Float contamination = FilterContamination.contamination\n   }\n }\n \n+\n task GetContamination {\n   input {\n-    File input_bam\n-    File input_bam_index\n-    File ref_fasta\n-    File ref_fasta_index\n-    Int qual = 20\n-    Int map_qual = 30\n-    Float vaf = 0.01\n+    File input_vcf\n+      # runtime", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "74fff72231f9e1094cd79010e9547da85d1db76e"}, "originalPosition": 173}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2ODE2MjU0OnYy", "diffSide": "RIGHT", "path": "scripts/mitochondria_m2_wdl/AlignAndCall.wdl", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMDoxODo0MVrOF7srdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMDoxODo0MVrOF7srdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE0MjMyNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                Int? preemptible_tries}\n          \n          \n            \n                Int? preemptible_tries\n          \n          \n            \n              }", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r398142325", "createdAt": "2020-03-25T20:18:41Z", "author": {"login": "meganshand"}, "path": "scripts/mitochondria_m2_wdl/AlignAndCall.wdl", "diffHunk": "@@ -169,93 +202,101 @@ workflow AlignAndCall {\n       gatk_override = gatk_override,\n       m2_extra_filtering_args = m2_filter_extra_args,\n       max_alt_allele_count = 4,\n-      contamination = GetContamination.minor_level,\n-      autosomal_coverage = autosomal_coverage,\n       vaf_filter_threshold = vaf_filter_threshold,\n       blacklisted_sites = blacklisted_sites,\n       blacklisted_sites_index = blacklisted_sites_index,\n       f_score_beta = f_score_beta,\n       preemptible_tries = preemptible_tries\n+ }\n+\n+  if ( defined(autosomal_coverage) ) {\n+    call FilterNuMTs {\n+      input:\n+        filtered_vcf = FilterContamination.filtered_vcf,\n+        ref_fasta = mt_fasta,\n+        ref_fai = mt_fasta_index,\n+        ref_dict = mt_dict,\n+        autosomal_coverage = autosomal_coverage,\n+        gatk_override = gatk_override,\n+        compress = compress_output_vcf,\n+        preemptible_tries = preemptible_tries\n+    }\n   }\n \n+  File low_het_vcf = select_first([FilterNuMTs.numt_filtered_vcf, FilterContamination.filtered_vcf])\n+\n+  call FilterLowHetSites {\n+    input:\n+      filtered_vcf = low_het_vcf,\n+      ref_fasta = mt_fasta,\n+      ref_fai = mt_fasta_index,\n+      ref_dict = mt_dict,\n+      max_low_het_sites = max_low_het_sites,\n+      gatk_override = gatk_override,\n+      compress = compress_output_vcf,\n+      preemptible_tries = preemptible_tries\n+  }\n+\n+\n   output {\n     File mt_aligned_bam = AlignToMt.mt_aligned_bam\n     File mt_aligned_bai = AlignToMt.mt_aligned_bai\n     File mt_aligned_shifted_bam = AlignToShiftedMt.mt_aligned_bam\n     File mt_aligned_shifted_bai = AlignToShiftedMt.mt_aligned_bai\n-    File out_vcf = Filter.filtered_vcf\n-    File out_vcf_index = Filter.filtered_vcf_idx\n+    File out_vcf = FilterLowHetSites.final_filtered_vcf\n+    File out_vcf_index = FilterLowHetSites.final_filtered_vcf_idx\n+    File input_vcf_for_haplochecker = SplitMultiAllelicsAndRemoveNonPassSites.vcf_for_haplochecker\n     File duplicate_metrics = AlignToMt.duplicate_metrics\n     File coverage_metrics = CollectWgsMetrics.metrics\n     File theoretical_sensitivity_metrics = CollectWgsMetrics.theoretical_sensitivity\n     File contamination_metrics = GetContamination.contamination_file\n     Int mean_coverage = CollectWgsMetrics.mean_coverage\n     String major_haplogroup = GetContamination.major_hg\n-    Float contamination = GetContamination.minor_level\n+    Float contamination = FilterContamination.contamination\n   }\n }\n \n+\n task GetContamination {\n   input {\n-    File input_bam\n-    File input_bam_index\n-    File ref_fasta\n-    File ref_fasta_index\n-    Int qual = 20\n-    Int map_qual = 30\n-    Float vaf = 0.01\n+    File input_vcf\n+      # runtime\n+    Int? preemptible_tries}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "74fff72231f9e1094cd79010e9547da85d1db76e"}, "originalPosition": 174}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2ODE3Mjc2OnYy", "diffSide": "RIGHT", "path": "scripts/mitochondria_m2_wdl/AlignAndCall.wdl", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMDoyMTo0NlrOF7syGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMDoyMTo0NlrOF7syGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE0NDAyNg==", "bodyText": "Do you think it's worth adding any sanity checks here to make sure this doesn't break in the future since it's relying on column order to extract the values?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r398144026", "createdAt": "2020-03-25T20:21:46Z", "author": {"login": "meganshand"}, "path": "scripts/mitochondria_m2_wdl/AlignAndCall.wdl", "diffHunk": "@@ -169,93 +202,101 @@ workflow AlignAndCall {\n       gatk_override = gatk_override,\n       m2_extra_filtering_args = m2_filter_extra_args,\n       max_alt_allele_count = 4,\n-      contamination = GetContamination.minor_level,\n-      autosomal_coverage = autosomal_coverage,\n       vaf_filter_threshold = vaf_filter_threshold,\n       blacklisted_sites = blacklisted_sites,\n       blacklisted_sites_index = blacklisted_sites_index,\n       f_score_beta = f_score_beta,\n       preemptible_tries = preemptible_tries\n+ }\n+\n+  if ( defined(autosomal_coverage) ) {\n+    call FilterNuMTs {\n+      input:\n+        filtered_vcf = FilterContamination.filtered_vcf,\n+        ref_fasta = mt_fasta,\n+        ref_fai = mt_fasta_index,\n+        ref_dict = mt_dict,\n+        autosomal_coverage = autosomal_coverage,\n+        gatk_override = gatk_override,\n+        compress = compress_output_vcf,\n+        preemptible_tries = preemptible_tries\n+    }\n   }\n \n+  File low_het_vcf = select_first([FilterNuMTs.numt_filtered_vcf, FilterContamination.filtered_vcf])\n+\n+  call FilterLowHetSites {\n+    input:\n+      filtered_vcf = low_het_vcf,\n+      ref_fasta = mt_fasta,\n+      ref_fai = mt_fasta_index,\n+      ref_dict = mt_dict,\n+      max_low_het_sites = max_low_het_sites,\n+      gatk_override = gatk_override,\n+      compress = compress_output_vcf,\n+      preemptible_tries = preemptible_tries\n+  }\n+\n+\n   output {\n     File mt_aligned_bam = AlignToMt.mt_aligned_bam\n     File mt_aligned_bai = AlignToMt.mt_aligned_bai\n     File mt_aligned_shifted_bam = AlignToShiftedMt.mt_aligned_bam\n     File mt_aligned_shifted_bai = AlignToShiftedMt.mt_aligned_bai\n-    File out_vcf = Filter.filtered_vcf\n-    File out_vcf_index = Filter.filtered_vcf_idx\n+    File out_vcf = FilterLowHetSites.final_filtered_vcf\n+    File out_vcf_index = FilterLowHetSites.final_filtered_vcf_idx\n+    File input_vcf_for_haplochecker = SplitMultiAllelicsAndRemoveNonPassSites.vcf_for_haplochecker\n     File duplicate_metrics = AlignToMt.duplicate_metrics\n     File coverage_metrics = CollectWgsMetrics.metrics\n     File theoretical_sensitivity_metrics = CollectWgsMetrics.theoretical_sensitivity\n     File contamination_metrics = GetContamination.contamination_file\n     Int mean_coverage = CollectWgsMetrics.mean_coverage\n     String major_haplogroup = GetContamination.major_hg\n-    Float contamination = GetContamination.minor_level\n+    Float contamination = FilterContamination.contamination\n   }\n }\n \n+\n task GetContamination {\n   input {\n-    File input_bam\n-    File input_bam_index\n-    File ref_fasta\n-    File ref_fasta_index\n-    Int qual = 20\n-    Int map_qual = 30\n-    Float vaf = 0.01\n+    File input_vcf\n+      # runtime\n+    Int? preemptible_tries}\n \n-    # runtime\n-    Int? preemptible_tries\n-  }\n-\n-  String basename = basename(input_bam, \".bam\")\n-  Float ref_size = size(ref_fasta, \"GB\") + size(ref_fasta_index, \"GB\")\n-  Int disk_size = ceil(size(input_bam, \"GB\") + ref_size) + 20\n+  Int disk_size = ceil(size(input_vcf, \"GB\")) + 20\n \n   meta {\n-    description: \"Uses Haplochecker to estimate levels of contamination in mitochondria\"\n+    description: \"Uses new Haplochecker to estimate levels of contamination in mitochondria\"\n   }\n   parameter_meta {\n-    input_bam: \"Bam aligned to chrM\"\n-    ref_fasta: \"chrM reference\"\n+    input_vcf: \"Filtered and split multi-allelic sites VCF for mitochondria\"\n   }\n-  command {\n+  command <<<\n   set -e\n-\n-  java -jar /usr/mtdnaserver/mitolib.jar haplochecker \\\n-    --in ~{input_bam} \\\n-    --ref ~{ref_fasta} \\\n-    --out haplochecker_out \\\n-    --QUAL ~{qual} \\\n-    --MAPQ ~{map_qual} \\\n-    --VAF ~{vaf}\n-\n-python3 <<CODE\n-\n-import csv\n-\n-with open(\"haplochecker_out/~{basename}.contamination.txt\") as output:\n-    reader = csv.DictReader(output, delimiter='\\t')\n-    for row in reader:\n-        print(row[\"MajorHG\"], file=open(\"major_hg.txt\", 'w'))\n-        print(row[\"MajorLevel\"], file=open(\"major_level.txt\", 'w'))\n-        print(row[\"MinorHG\"], file=open(\"minor_hg.txt\", 'w'))\n-        print(row[\"MinorLevel\"], file=open(\"minor_level.txt\", 'w'))\n-CODE\n-  }\n+  PARENT_DIR=\"$(dirname \"~{input_vcf}\")\"\n+  java -jar /usr/mtdnaserver/haplocheckCLI.jar \"${PARENT_DIR}\"\n+\n+  sed 's/\\\"//g' output > output-noquotes\n+  grep -v \"SampleID\" output-noquotes > output-data\n+  awk '{print $2}' output-data > contamination.txt\n+  awk '{print $6}' output-data > major_hg.txt\n+  awk '{print $8}' output-data > minor_hg.txt\n+  awk '{print $14}' output-data > mean_het_major.txt\n+  awk '{print $15}' output-data > mean_het_minor.txt", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "74fff72231f9e1094cd79010e9547da85d1db76e"}, "originalPosition": 228}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2ODIxNzM4OnYy", "diffSide": "RIGHT", "path": "scripts/mitochondria_m2_wdl/AlignAndCall.wdl", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMDozNDoyM1rOF7tOZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo0NToxOVrOGAdnDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE1MTI2OA==", "bodyText": "I'm confused, I thought we got rid of the \"genotyping-mode\" argument in later versions not the other way around. Can you try running this with a gga_vcf just to make sure it doesn't throw a user error? I don't think we're testing this argument in the WDL tests, but you could also add it there (it's left out now because it's an optional parameter).", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r398151268", "createdAt": "2020-03-25T20:34:23Z", "author": {"login": "meganshand"}, "path": "scripts/mitochondria_m2_wdl/AlignAndCall.wdl", "diffHunk": "@@ -417,7 +458,9 @@ task M2 {\n       gatk --java-options \"-Xmx~{command_mem}m\" Mutect2 \\\n         -R ~{ref_fasta} \\\n         -I ~{input_bam} \\\n-        ~{\"--alleles \" + gga_vcf} \\\n+        ~{\"--genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles \" + gga_vcf} \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "74fff72231f9e1094cd79010e9547da85d1db76e"}, "originalPosition": 284}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzODMxNw==", "bodyText": "removing this argument all together. it can be added to the m2_extra_args if needed", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r403138317", "createdAt": "2020-04-03T16:45:19Z", "author": {"login": "ahaessly"}, "path": "scripts/mitochondria_m2_wdl/AlignAndCall.wdl", "diffHunk": "@@ -417,7 +458,9 @@ task M2 {\n       gatk --java-options \"-Xmx~{command_mem}m\" Mutect2 \\\n         -R ~{ref_fasta} \\\n         -I ~{input_bam} \\\n-        ~{\"--alleles \" + gga_vcf} \\\n+        ~{\"--genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles \" + gga_vcf} \\", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE1MTI2OA=="}, "originalCommit": {"oid": "74fff72231f9e1094cd79010e9547da85d1db76e"}, "originalPosition": 284}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNjg0NTYzOnYy", "diffSide": "RIGHT", "path": "scripts/mitochondria_m2_wdl/AlignAndCall.wdl", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNDo0MjozNVrOGCyh4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNDo0MjozNVrOGCyh4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTU3ODIwOQ==", "bodyText": "I'm not sure what the solution here is, but does keeping this as the default version mean that we'll have to keep updating it when there are new releases? That's the way it already was, so it's not necessarily a problem, I just want to remember that once this PR is merged and released we should go back and update this default.", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r405578209", "createdAt": "2020-04-08T14:42:35Z", "author": {"login": "meganshand"}, "path": "scripts/mitochondria_m2_wdl/AlignAndCall.wdl", "diffHunk": "@@ -470,7 +498,7 @@ task M2 {\n         --max-mnp-distance 0\n   >>>\n   runtime {\n-      docker: \"us.gcr.io/broad-gatk/gatk:4.1.1.0\"\n+      docker: select_first([gatk_docker_override, \"us.gcr.io/broad-gatk/gatk:4.1.1.0\"])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "717d87fab4fb938aa44737019f62a4ae7fed1172"}, "originalPosition": 155}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNzI2MDIyOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMlQwNjoxNjoxMlrOGESnUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMlQwNjoxNjoxMlrOGESnUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzE1MjQ2Ng==", "bodyText": "Can you go directly compute symbolic indices via new IndexRange(0, vc.getNAlleles()).filter(n -> vc.getAllele(n).isSymbolic());?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r407152466", "createdAt": "2020-04-12T06:16:12Z", "author": {"login": "davidbenjamin"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "diffHunk": "@@ -88,19 +89,33 @@ public SomaticClusteringModel(final M2FiltersArgumentCollection MTFAC, final Lis\n         logClusterWeights = new double[] {Math.log1p(INITIAL_HIGH_AF_WEIGHT), Math.log(INITIAL_HIGH_AF_WEIGHT)};\n     }\n \n-    public void record(final int[] tumorADs, final double[] tumorLogOdds, final double artifactProbability, final double nonSomaticProbability, final VariantContext vc) {\n-        // things that are definitely not somatic don't need to go in the somatic clustering model\n-        if (artifactProbability > OBVIOUS_ARTIFACT_PROBABILITY_THRESHOLD) {\n-            obviousArtifactCount.increment();\n-            return;\n-        } else if (nonSomaticProbability > OBVIOUS_ARTIFACT_PROBABILITY_THRESHOLD) {\n-            return;\n-        }\n-\n+    /**\n+     * Adds data to the model for every alternate allele\n+     * @param tumorADs for all alleles, summed over samples\n+     * @param tumorLogOdds for alt alleles only\n+     * @param artifactProbabilities by alt allele, specifically technical artifact probabilities not including sequencing error, contamination, or germline variation\n+     * @param nonSomaticProbabilities by alt allele, probabilities that the variants are real but not somatic ie germline or contamination\n+     * @param vc the variant context the data apply to\n+     */\n+    public void record(int[] tumorADs, final double[] tumorLogOdds, final List<Double> artifactProbabilities, final List<Double> nonSomaticProbabilities, final VariantContext vc) {\n+        // set tumorAD to 0 for symbolic alleles so it won't contribute to overall AD\n+        List<Allele> symbolicAlleles = vc.getAlternateAlleles().stream().filter(allele -> allele.isSymbolic()).collect(Collectors.toList());\n+        // convert allele index to alt allele index\n+        List<Integer> symIndexes = vc.getAlleleIndices(symbolicAlleles).stream().map(i -> i-1).collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "717d87fab4fb938aa44737019f62a4ae7fed1172"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2MjQ2NjMzOnYy", "diffSide": "RIGHT", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2AlleleFilter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxODozMTo1M1rOGJSmvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxODozMTo1M1rOGJSmvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5NTE5OA==", "bodyText": "\"a list with one element for each allele\"?", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r412395198", "createdAt": "2020-04-21T18:31:53Z", "author": {"login": "ldgauthier"}, "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2AlleleFilter.java", "diffHunk": "@@ -73,7 +73,7 @@\n \n \n     /**\n-     *\n+     * Returns a list for each alternate allele which is the probability that the allele should be filtered out.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "59f3dc9a2fc244fe4f0e7e64ee69055d79bc0018"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1201, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}