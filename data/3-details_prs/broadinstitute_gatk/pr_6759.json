{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDcwMzQ1OTAw", "number": 6759, "title": "ah - big query utils and local sort", "bodyText": "Add bq util classes and update dependencies\nadd local sort classes", "createdAt": "2020-08-19T18:43:31Z", "url": "https://github.com/broadinstitute/gatk/pull/6759", "merged": true, "mergeCommit": {"oid": "febd7cbbd1fc2a631fa9d7dcde52383e3134e88c"}, "closed": true, "closedAt": "2020-10-02T18:30:49Z", "author": {"login": "ahaessly"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdAgBMVAH2gAyNDcwMzQ1OTAwOjI3MmY1ODJiNmZkMTk1ZGM3YzI3ZmVkMjllNDhlNjFlYTg2N2Y5Mjk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdOqSRqgFqTUwMTM1Nzk2OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "272f582b6fd195dc7c27fed29e48e61ea867f929", "author": {"user": {"login": "ahaessly", "name": "Andrea Haessly"}}, "url": "https://github.com/broadinstitute/gatk/commit/272f582b6fd195dc7c27fed29e48e61ea867f929", "committedDate": "2020-08-19T18:35:30Z", "message": "add big query utils and update dependencies"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "42823f240fb461e67f9188331aaec64a17484630", "author": {"user": {"login": "ahaessly", "name": "Andrea Haessly"}}, "url": "https://github.com/broadinstitute/gatk/commit/42823f240fb461e67f9188331aaec64a17484630", "committedDate": "2020-08-19T20:03:36Z", "message": "update math import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918", "author": {"user": {"login": "ahaessly", "name": "Andrea Haessly"}}, "url": "https://github.com/broadinstitute/gatk/commit/82e4cf10713b4a4be419496491e08f4329d79918", "committedDate": "2020-08-19T21:13:23Z", "message": "remove commented out todo"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwMDE4Mzg2", "url": "https://github.com/broadinstitute/gatk/pull/6759#pullrequestreview-480018386", "createdAt": "2020-09-01T18:41:58Z", "commit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxODo0MTo1OFrOHLDfsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxOToyMzozOFrOHLFAgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM1MzY0OA==", "bodyText": "Is there a reason you had to switch the GATK release bundle to zip64 in this branch? There are certain relatively-recent versions of OS X that ship with an archive manager that doesn't support zip64, and can actually corrupt zip64 archives (not sure how much of an issue this still is in practice...).", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481353648", "createdAt": "2020-09-01T18:41:58Z", "author": {"login": "droazen"}, "path": "build.gradle", "diffHunk": "@@ -527,6 +533,7 @@ task collectBundleIntoDir(type: Copy) {\n \n task bundle(type: Zip) {\n     dependsOn collectBundleIntoDir\n+    zip64 true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM1NDk1MA==", "bodyText": "The build.gradle changes in this branch look good to me. The protobuf version here is slightly older than the one in #6736, but we can just rebase that PR onto this one once it goes in.", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481354950", "createdAt": "2020-09-01T18:44:27Z", "author": {"login": "droazen"}, "path": "build.gradle", "diffHunk": "@@ -66,14 +66,17 @@ final picardVersion = System.getProperty('picard.version','2.23.3')\n final barclayVersion = System.getProperty('barclay.version','3.0.0')\n final sparkVersion = System.getProperty('spark.version', '2.4.5')\n final scalaVersion = System.getProperty('scala.version', '2.11')\n-final hadoopVersion = System.getProperty('hadoop.version', '2.8.2')\n+final hadoopVersion = System.getProperty('hadoop.version', '3.2.1')\n final disqVersion = System.getProperty('disq.version','0.3.6')\n final genomicsdbVersion = System.getProperty('genomicsdb.version','1.3.0')\n+final bigQueryVersion = System.getProperty('bigQuery.version', '1.69.0')\n+final guavaVersion = System.getProperty('guava.version', '27.1-jre')", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM1NjQyMQ==", "bodyText": "\"on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\"\nshould be:\n\"on the provided BigQuery instance\"\nfor this overload", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481356421", "createdAt": "2020-09-01T18:47:03Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtils.java", "diffHunk": "@@ -0,0 +1,449 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.ivy.util.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+/**\n+ * Utility class for dealing with BigQuery connections / tables / queries /etc.\n+ *\n+ * Created by jonn on 4/17/19.\n+ */\n+public class BigQueryUtils {\n+    private static final Logger logger = LogManager.getLogger(BigQueryUtils.class);\n+\n+    //==================================================================================================================\n+    // Static Methods:\n+\n+    /**\n+     * @return A {@link BigQuery} object that can be used to interact with a BigQuery data set.\n+     */\n+    public static BigQuery getBigQueryEndPoint() {\n+        return BigQueryOptions.getDefaultInstance().getService();\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, false);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString, final boolean runQueryInBatchMode) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, runQueryInBatchMode);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM1NzEwOQ==", "bodyText": "\"on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\"\nshould be:\n\"on the provided BigQuery instance\"\nfor this overload", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481357109", "createdAt": "2020-09-01T18:48:20Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtils.java", "diffHunk": "@@ -0,0 +1,449 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.ivy.util.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+/**\n+ * Utility class for dealing with BigQuery connections / tables / queries /etc.\n+ *\n+ * Created by jonn on 4/17/19.\n+ */\n+public class BigQueryUtils {\n+    private static final Logger logger = LogManager.getLogger(BigQueryUtils.class);\n+\n+    //==================================================================================================================\n+    // Static Methods:\n+\n+    /**\n+     * @return A {@link BigQuery} object that can be used to interact with a BigQuery data set.\n+     */\n+    public static BigQuery getBigQueryEndPoint() {\n+        return BigQueryOptions.getDefaultInstance().getService();\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, false);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString, final boolean runQueryInBatchMode) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, runQueryInBatchMode);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param bigQuery The {@link BigQuery} instance against which to execute the given {@code queryString}.\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final BigQuery bigQuery, final String queryString, final boolean runQueryInBatchMode) {\n+\n+        // Create a query configuration we can run based on our query string:\n+        final QueryJobConfiguration queryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setPriority(runQueryInBatchMode ? QueryJobConfiguration.Priority.BATCH : QueryJobConfiguration.Priority.INTERACTIVE)\n+                        .build();\n+\n+        logger.info(\"Executing Query: \\n\\n\" + queryString);\n+        final TableResult result = submitQueryAndWaitForResults( bigQuery, queryConfig );\n+        logger.info(\"Query returned \" + result.getTotalRows() + \" results.\");\n+        return result;\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NzI2Mg==", "bodyText": "Keep fractions of a MB, and reword slightly:\n            logger.info(String.format(\"%.2f MB actually scanned\", bytesProcessed / 1000000.0));", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481367262", "createdAt": "2020-09-01T19:02:03Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtils.java", "diffHunk": "@@ -0,0 +1,449 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.ivy.util.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+/**\n+ * Utility class for dealing with BigQuery connections / tables / queries /etc.\n+ *\n+ * Created by jonn on 4/17/19.\n+ */\n+public class BigQueryUtils {\n+    private static final Logger logger = LogManager.getLogger(BigQueryUtils.class);\n+\n+    //==================================================================================================================\n+    // Static Methods:\n+\n+    /**\n+     * @return A {@link BigQuery} object that can be used to interact with a BigQuery data set.\n+     */\n+    public static BigQuery getBigQueryEndPoint() {\n+        return BigQueryOptions.getDefaultInstance().getService();\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, false);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString, final boolean runQueryInBatchMode) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, runQueryInBatchMode);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param bigQuery The {@link BigQuery} instance against which to execute the given {@code queryString}.\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final BigQuery bigQuery, final String queryString, final boolean runQueryInBatchMode) {\n+\n+        // Create a query configuration we can run based on our query string:\n+        final QueryJobConfiguration queryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setPriority(runQueryInBatchMode ? QueryJobConfiguration.Priority.BATCH : QueryJobConfiguration.Priority.INTERACTIVE)\n+                        .build();\n+\n+        logger.info(\"Executing Query: \\n\\n\" + queryString);\n+        final TableResult result = submitQueryAndWaitForResults( bigQuery, queryConfig );\n+        logger.info(\"Query returned \" + result.getTotalRows() + \" results.\");\n+        return result;\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param bigQuery The {@link BigQuery} instance against which to execute the given {@code queryString}.  Must contain the table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param projectID The BigQuery {@code project ID} containing the {@code dataSet} and table from which to query data.\n+     * @param dataSet The BigQuery {@code dataSet} containing the table from which to query data.\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table ID in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final BigQuery bigQuery,\n+                                           final String projectID,\n+                                           final String dataSet,\n+                                           final String queryString) {\n+\n+        // Create a query configuration we can run based on our query string:\n+        final QueryJobConfiguration queryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setDefaultDataset(DatasetId.of(projectID, dataSet))\n+                        .build();\n+\n+        return submitQueryAndWaitForResults( bigQuery, queryConfig );\n+    }\n+\n+    /**\n+     * Creates a {@link String} containing the given results in a pretty ascii-art table.\n+     * @param result A {@link TableResult} object containing the results of a query that generated some data.\n+     * @return A {@link String} containing the contents of the given query result pretty-printed in an ascii-art table.\n+     */\n+    public static String getResultDataPrettyString(final TableResult result){\n+        final Schema schema = result.getSchema();\n+\n+        final List<Integer> columnWidths = calculateColumnWidths( result );\n+        final boolean rowsAllPrimitive =\n+                StreamSupport.stream(result.iterateAll().spliterator(), false)\n+                        .flatMap( row -> row.stream().map(v -> v.getAttribute() == FieldValue.Attribute.PRIMITIVE) )\n+                        .allMatch( b -> b );\n+\n+        // Create a separator string for the header and boarders:\n+        final String headerFooter = \"+\" + columnWidths.stream().map(\n+                l -> StringUtils.repeat(\"=\", l+2) + \"+\"\n+        ).collect(Collectors.joining(\"\"));\n+\n+        // Create a Row Separator:\n+        final String rowSeparator = headerFooter.replace('=', '-');\n+\n+        // Create a string builder to keep the pretty table:\n+        final StringBuilder sb = new StringBuilder();\n+\n+        // Now we can write our schema header and rows:\n+        addHeaderToStringBuilder(schema, columnWidths, headerFooter, sb);\n+\n+        // Write our data to the string builder:\n+        for ( final FieldValueList row : result.iterateAll() ) {\n+\n+            // If the row fields are all simple, then we can do the simple thing:\n+            if ( rowsAllPrimitive ) {\n+                addPrimitiveRowToStringBuilder(row, columnWidths, sb);\n+            }\n+            else {\n+                addComplexRowToStringBuilder(row, schema, columnWidths, sb);\n+                sb.append(rowSeparator);\n+            }\n+            sb.append(\"\\n\");\n+        }\n+\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+\n+        return sb.toString();\n+    }\n+\n+    private static void addHeaderToStringBuilder(final Schema schema, final List<Integer> columnWidths, final String headerFooter, final StringBuilder sb) {\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+        sb.append(\"|\");\n+        sb.append(\n+                IntStream.range(0, columnWidths.size()).boxed().map(\n+                        i -> String.format(\" %-\"+ columnWidths.get(i) +\"s |\", schema.getFields().get(i).getName())\n+                ).collect(Collectors.joining()) );\n+        sb.append(\"\\n\");\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+    }\n+\n+    private static void addComplexRowToStringBuilder(final FieldValueList row, final Schema schema, final List<Integer> columnWidths, final StringBuilder sb) {\n+\n+        // TODO: Clean this up... Probably make a getStringValue(FIELD) method to use here, addPrimitiveRowToStringBuilder, and calculateColumnWidths\n+\n+        // For fields that have multiple values, we need to do something special.\n+        // In fact, we need to go through each value of each row and track how many fields it has.\n+        int maxNumValuesInRow = 1;\n+        for ( int i = 0; i < row.size(); ++i ) {\n+            final FieldValue value = row.get(schema.getFields().get(i).getName());\n+            if ( !value.isNull() && (value.getAttribute() != FieldValue.Attribute.PRIMITIVE) ) {\n+                if (maxNumValuesInRow <= value.getRecordValue().size()) {\n+                    maxNumValuesInRow = value.getRecordValue().size();\n+                }\n+            }\n+        }\n+\n+        for ( int currentFieldNum = 0; currentFieldNum < maxNumValuesInRow ; ++currentFieldNum ) {\n+            sb.append(\"|\");\n+            for ( int i = 0; i < row.size(); ++i ) {\n+                final FieldValue value = row.get(i);\n+                if ( value.isNull() ) {\n+                    sb.append(getEmptyColumnString(columnWidths, i));\n+                }\n+                else if (value.getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                    if ( currentFieldNum == 0 ) {\n+                        sb.append( String.format(\" %-\" + columnWidths.get(i) + \"s |\", value.getStringValue()) );\n+                    }\n+                    else {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                }\n+                else {\n+                    if ( value.getRepeatedValue().size() == 0 ) {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                    else if ( currentFieldNum < value.getRepeatedValue().size() ) {\n+                        if ( value.getRepeatedValue().get(currentFieldNum).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                            sb.append(String.format(\" %-\" + columnWidths.get(i) + \"s |\",\n+                                    value.getRepeatedValue().get(currentFieldNum).getStringValue())\n+                            );\n+                        }\n+                        else {\n+                            // This is kind of gross, but it seems to be the only way to get a particular\n+                            // value of a field that is in an array:\n+                            sb.append(String.format(\" %-\" + columnWidths.get(i) + \"s |\",\n+                                    value.getRepeatedValue().get(currentFieldNum).getRecordValue().get(0).getStringValue())\n+                            );\n+                        }\n+                    }\n+                    else {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                }\n+            }\n+            sb.append(\"\\n\");\n+        }\n+    }\n+\n+    private static String getEmptyColumnString(final List<Integer> columnWidths, final int i) {\n+        return String.format(\" %-\" + columnWidths.get(i) + \"s |\", \"\");\n+    }\n+\n+    private static void addPrimitiveRowToStringBuilder(final FieldValueList row, final List<Integer> columnWidths, final StringBuilder sb) {\n+        sb.append(\"|\");\n+        sb.append(IntStream.range(0, row.size()).boxed().map(\n+                i -> String.format(\" %-\" + columnWidths.get(i) + \"s |\", row.get(i).getStringValue())\n+        ).collect(Collectors.joining()));\n+    }\n+\n+    /**\n+     * Creates a {@link String} containing the given results in a pretty ascii-art table.\n+     * @param result A {@link TableResult} object containing the results of a query that generated some data.\n+     * @param theLogger A {@link Logger} object with which to log the results contained in {@code result}.\n+     */\n+    public static void logResultDataPretty( final TableResult result, final Logger theLogger ){\n+        for ( final String line : getResultDataPrettyString(result).split(\"\\n\") ) {\n+            theLogger.info( line );\n+        }\n+    }\n+\n+    //==================================================================================================================\n+    // Helper Methods:\n+\n+    private static List<Integer> calculateColumnWidths( final TableResult result ) {\n+        // Go through all rows and get the length of each column:\n+        final List<Integer> columnWidths = new ArrayList<>(result.getSchema().getFields().size());\n+\n+        // Start with schema names:\n+        for ( final Field field : result.getSchema().getFields() ) {\n+            columnWidths.add( field.getName().length() );\n+        }\n+\n+        // Check each row and each row's array values (if applicable):\n+        for ( final FieldValueList row : result.iterateAll() ) {\n+            for ( int i = 0; i < row.size() ; ++i ) {\n+                // Only get the row size if it's not null:\n+                if ( !row.get(i).isNull() ) {\n+                    if ( row.get(i).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                        if ( columnWidths.get(i) < row.get(i).getStringValue().length() ) {\n+                            columnWidths.set(i, row.get(i).getStringValue().length());\n+                        }\n+                    }\n+                    else {\n+                        for ( int j = 0; j < row.get(i).getRepeatedValue().size(); ++j ) {\n+                            final String stringValue;\n+                            if ( row.get(i).getRepeatedValue().get(j).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                                stringValue = row.get(i).getRepeatedValue().get(j).getStringValue();\n+                            }\n+                            else {\n+                                stringValue = row.get(i).getRepeatedValue().get(j).getRecordValue().get(0).getStringValue();\n+                            }\n+                            if ( columnWidths.get(i) < stringValue.length() ) {\n+                                columnWidths.set(i, stringValue.length());\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        return columnWidths;\n+    }\n+\n+    /**\n+     * Executes the given {@code queryJobConfiguration} on the given {@code bigQuery} instance.\n+     * @param bigQuery The instance of {@link BigQuery} to use to connect to BigQuery.\n+     * @param queryJobConfiguration The {@link QueryJobConfiguration} object containing all required information to retrieve data from a BigQuery table.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    private static TableResult submitQueryAndWaitForResults( final BigQuery bigQuery,\n+                                                             final QueryJobConfiguration queryJobConfiguration ) {\n+        // Create a job ID so that we can safely retry:\n+        final JobId jobId = JobId.of(UUID.randomUUID().toString());\n+\n+        logger.info(\"Sending query to server...\");\n+        Job   queryJob       = bigQuery.create(JobInfo.newBuilder(queryJobConfiguration).setJobId(jobId).build());\n+\n+        // Wait for the query to complete.\n+        try {\n+            logger.info(\"Waiting for query to complete...\");\n+            queryJob = queryJob.waitFor();\n+        }\n+        catch (final InterruptedException ex) {\n+            throw new GATKException(\"Interrupted while waiting for query job to complete\", ex);\n+        }\n+\n+        // Check for errors:\n+        if (queryJob == null) {\n+            throw new GATKException(\"Query job no longer exists\");\n+        } else if (queryJob.getStatus().getError() != null) {\n+\n+            // Get all the errors we found and log them:\n+            for ( final BigQueryError bigQueryError : queryJob.getStatus().getExecutionErrors() ) {\n+                logger.error( \"Encountered BigQuery Execution Error: \" + bigQueryError.toString() );\n+            }\n+\n+            // Since we found an error, we should stop and alert the user:\n+            throw new GATKException(queryJob.getStatus().getError().toString());\n+        }\n+\n+        // Get the results.\n+        logger.info(\"Retrieving query results...\");\n+        final QueryResponse response = bigQuery.getQueryResults(jobId);\n+        final TableResult result;\n+        try {\n+            result = queryJob.getQueryResults();\n+\n+            long bytesProcessed = ((JobStatistics.QueryStatistics) queryJob.getStatistics()).getTotalBytesProcessed();\n+            logger.info(String.format(\"Actual %s MB scanned\", bytesProcessed/1000000));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 342}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2OTIyOQ==", "bodyText": "You include the StorageAPIAvroReader in this branch, and yet you commented out the BigQueryUtils methods that actually use it. Can you explain? As I'm sure you know, the Storage API provides much better performance than the regular API, so it seems useful to have these utilities here. Are you going to be using the Storage API in your subsequent PRs?", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481369229", "createdAt": "2020-09-01T19:05:55Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtils.java", "diffHunk": "@@ -0,0 +1,449 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.ivy.util.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+/**\n+ * Utility class for dealing with BigQuery connections / tables / queries /etc.\n+ *\n+ * Created by jonn on 4/17/19.\n+ */\n+public class BigQueryUtils {\n+    private static final Logger logger = LogManager.getLogger(BigQueryUtils.class);\n+\n+    //==================================================================================================================\n+    // Static Methods:\n+\n+    /**\n+     * @return A {@link BigQuery} object that can be used to interact with a BigQuery data set.\n+     */\n+    public static BigQuery getBigQueryEndPoint() {\n+        return BigQueryOptions.getDefaultInstance().getService();\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, false);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString, final boolean runQueryInBatchMode) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, runQueryInBatchMode);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param bigQuery The {@link BigQuery} instance against which to execute the given {@code queryString}.\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final BigQuery bigQuery, final String queryString, final boolean runQueryInBatchMode) {\n+\n+        // Create a query configuration we can run based on our query string:\n+        final QueryJobConfiguration queryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setPriority(runQueryInBatchMode ? QueryJobConfiguration.Priority.BATCH : QueryJobConfiguration.Priority.INTERACTIVE)\n+                        .build();\n+\n+        logger.info(\"Executing Query: \\n\\n\" + queryString);\n+        final TableResult result = submitQueryAndWaitForResults( bigQuery, queryConfig );\n+        logger.info(\"Query returned \" + result.getTotalRows() + \" results.\");\n+        return result;\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param bigQuery The {@link BigQuery} instance against which to execute the given {@code queryString}.  Must contain the table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param projectID The BigQuery {@code project ID} containing the {@code dataSet} and table from which to query data.\n+     * @param dataSet The BigQuery {@code dataSet} containing the table from which to query data.\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table ID in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final BigQuery bigQuery,\n+                                           final String projectID,\n+                                           final String dataSet,\n+                                           final String queryString) {\n+\n+        // Create a query configuration we can run based on our query string:\n+        final QueryJobConfiguration queryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setDefaultDataset(DatasetId.of(projectID, dataSet))\n+                        .build();\n+\n+        return submitQueryAndWaitForResults( bigQuery, queryConfig );\n+    }\n+\n+    /**\n+     * Creates a {@link String} containing the given results in a pretty ascii-art table.\n+     * @param result A {@link TableResult} object containing the results of a query that generated some data.\n+     * @return A {@link String} containing the contents of the given query result pretty-printed in an ascii-art table.\n+     */\n+    public static String getResultDataPrettyString(final TableResult result){\n+        final Schema schema = result.getSchema();\n+\n+        final List<Integer> columnWidths = calculateColumnWidths( result );\n+        final boolean rowsAllPrimitive =\n+                StreamSupport.stream(result.iterateAll().spliterator(), false)\n+                        .flatMap( row -> row.stream().map(v -> v.getAttribute() == FieldValue.Attribute.PRIMITIVE) )\n+                        .allMatch( b -> b );\n+\n+        // Create a separator string for the header and boarders:\n+        final String headerFooter = \"+\" + columnWidths.stream().map(\n+                l -> StringUtils.repeat(\"=\", l+2) + \"+\"\n+        ).collect(Collectors.joining(\"\"));\n+\n+        // Create a Row Separator:\n+        final String rowSeparator = headerFooter.replace('=', '-');\n+\n+        // Create a string builder to keep the pretty table:\n+        final StringBuilder sb = new StringBuilder();\n+\n+        // Now we can write our schema header and rows:\n+        addHeaderToStringBuilder(schema, columnWidths, headerFooter, sb);\n+\n+        // Write our data to the string builder:\n+        for ( final FieldValueList row : result.iterateAll() ) {\n+\n+            // If the row fields are all simple, then we can do the simple thing:\n+            if ( rowsAllPrimitive ) {\n+                addPrimitiveRowToStringBuilder(row, columnWidths, sb);\n+            }\n+            else {\n+                addComplexRowToStringBuilder(row, schema, columnWidths, sb);\n+                sb.append(rowSeparator);\n+            }\n+            sb.append(\"\\n\");\n+        }\n+\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+\n+        return sb.toString();\n+    }\n+\n+    private static void addHeaderToStringBuilder(final Schema schema, final List<Integer> columnWidths, final String headerFooter, final StringBuilder sb) {\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+        sb.append(\"|\");\n+        sb.append(\n+                IntStream.range(0, columnWidths.size()).boxed().map(\n+                        i -> String.format(\" %-\"+ columnWidths.get(i) +\"s |\", schema.getFields().get(i).getName())\n+                ).collect(Collectors.joining()) );\n+        sb.append(\"\\n\");\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+    }\n+\n+    private static void addComplexRowToStringBuilder(final FieldValueList row, final Schema schema, final List<Integer> columnWidths, final StringBuilder sb) {\n+\n+        // TODO: Clean this up... Probably make a getStringValue(FIELD) method to use here, addPrimitiveRowToStringBuilder, and calculateColumnWidths\n+\n+        // For fields that have multiple values, we need to do something special.\n+        // In fact, we need to go through each value of each row and track how many fields it has.\n+        int maxNumValuesInRow = 1;\n+        for ( int i = 0; i < row.size(); ++i ) {\n+            final FieldValue value = row.get(schema.getFields().get(i).getName());\n+            if ( !value.isNull() && (value.getAttribute() != FieldValue.Attribute.PRIMITIVE) ) {\n+                if (maxNumValuesInRow <= value.getRecordValue().size()) {\n+                    maxNumValuesInRow = value.getRecordValue().size();\n+                }\n+            }\n+        }\n+\n+        for ( int currentFieldNum = 0; currentFieldNum < maxNumValuesInRow ; ++currentFieldNum ) {\n+            sb.append(\"|\");\n+            for ( int i = 0; i < row.size(); ++i ) {\n+                final FieldValue value = row.get(i);\n+                if ( value.isNull() ) {\n+                    sb.append(getEmptyColumnString(columnWidths, i));\n+                }\n+                else if (value.getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                    if ( currentFieldNum == 0 ) {\n+                        sb.append( String.format(\" %-\" + columnWidths.get(i) + \"s |\", value.getStringValue()) );\n+                    }\n+                    else {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                }\n+                else {\n+                    if ( value.getRepeatedValue().size() == 0 ) {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                    else if ( currentFieldNum < value.getRepeatedValue().size() ) {\n+                        if ( value.getRepeatedValue().get(currentFieldNum).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                            sb.append(String.format(\" %-\" + columnWidths.get(i) + \"s |\",\n+                                    value.getRepeatedValue().get(currentFieldNum).getStringValue())\n+                            );\n+                        }\n+                        else {\n+                            // This is kind of gross, but it seems to be the only way to get a particular\n+                            // value of a field that is in an array:\n+                            sb.append(String.format(\" %-\" + columnWidths.get(i) + \"s |\",\n+                                    value.getRepeatedValue().get(currentFieldNum).getRecordValue().get(0).getStringValue())\n+                            );\n+                        }\n+                    }\n+                    else {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                }\n+            }\n+            sb.append(\"\\n\");\n+        }\n+    }\n+\n+    private static String getEmptyColumnString(final List<Integer> columnWidths, final int i) {\n+        return String.format(\" %-\" + columnWidths.get(i) + \"s |\", \"\");\n+    }\n+\n+    private static void addPrimitiveRowToStringBuilder(final FieldValueList row, final List<Integer> columnWidths, final StringBuilder sb) {\n+        sb.append(\"|\");\n+        sb.append(IntStream.range(0, row.size()).boxed().map(\n+                i -> String.format(\" %-\" + columnWidths.get(i) + \"s |\", row.get(i).getStringValue())\n+        ).collect(Collectors.joining()));\n+    }\n+\n+    /**\n+     * Creates a {@link String} containing the given results in a pretty ascii-art table.\n+     * @param result A {@link TableResult} object containing the results of a query that generated some data.\n+     * @param theLogger A {@link Logger} object with which to log the results contained in {@code result}.\n+     */\n+    public static void logResultDataPretty( final TableResult result, final Logger theLogger ){\n+        for ( final String line : getResultDataPrettyString(result).split(\"\\n\") ) {\n+            theLogger.info( line );\n+        }\n+    }\n+\n+    //==================================================================================================================\n+    // Helper Methods:\n+\n+    private static List<Integer> calculateColumnWidths( final TableResult result ) {\n+        // Go through all rows and get the length of each column:\n+        final List<Integer> columnWidths = new ArrayList<>(result.getSchema().getFields().size());\n+\n+        // Start with schema names:\n+        for ( final Field field : result.getSchema().getFields() ) {\n+            columnWidths.add( field.getName().length() );\n+        }\n+\n+        // Check each row and each row's array values (if applicable):\n+        for ( final FieldValueList row : result.iterateAll() ) {\n+            for ( int i = 0; i < row.size() ; ++i ) {\n+                // Only get the row size if it's not null:\n+                if ( !row.get(i).isNull() ) {\n+                    if ( row.get(i).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                        if ( columnWidths.get(i) < row.get(i).getStringValue().length() ) {\n+                            columnWidths.set(i, row.get(i).getStringValue().length());\n+                        }\n+                    }\n+                    else {\n+                        for ( int j = 0; j < row.get(i).getRepeatedValue().size(); ++j ) {\n+                            final String stringValue;\n+                            if ( row.get(i).getRepeatedValue().get(j).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                                stringValue = row.get(i).getRepeatedValue().get(j).getStringValue();\n+                            }\n+                            else {\n+                                stringValue = row.get(i).getRepeatedValue().get(j).getRecordValue().get(0).getStringValue();\n+                            }\n+                            if ( columnWidths.get(i) < stringValue.length() ) {\n+                                columnWidths.set(i, stringValue.length());\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        return columnWidths;\n+    }\n+\n+    /**\n+     * Executes the given {@code queryJobConfiguration} on the given {@code bigQuery} instance.\n+     * @param bigQuery The instance of {@link BigQuery} to use to connect to BigQuery.\n+     * @param queryJobConfiguration The {@link QueryJobConfiguration} object containing all required information to retrieve data from a BigQuery table.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    private static TableResult submitQueryAndWaitForResults( final BigQuery bigQuery,\n+                                                             final QueryJobConfiguration queryJobConfiguration ) {\n+        // Create a job ID so that we can safely retry:\n+        final JobId jobId = JobId.of(UUID.randomUUID().toString());\n+\n+        logger.info(\"Sending query to server...\");\n+        Job   queryJob       = bigQuery.create(JobInfo.newBuilder(queryJobConfiguration).setJobId(jobId).build());\n+\n+        // Wait for the query to complete.\n+        try {\n+            logger.info(\"Waiting for query to complete...\");\n+            queryJob = queryJob.waitFor();\n+        }\n+        catch (final InterruptedException ex) {\n+            throw new GATKException(\"Interrupted while waiting for query job to complete\", ex);\n+        }\n+\n+        // Check for errors:\n+        if (queryJob == null) {\n+            throw new GATKException(\"Query job no longer exists\");\n+        } else if (queryJob.getStatus().getError() != null) {\n+\n+            // Get all the errors we found and log them:\n+            for ( final BigQueryError bigQueryError : queryJob.getStatus().getExecutionErrors() ) {\n+                logger.error( \"Encountered BigQuery Execution Error: \" + bigQueryError.toString() );\n+            }\n+\n+            // Since we found an error, we should stop and alert the user:\n+            throw new GATKException(queryJob.getStatus().getError().toString());\n+        }\n+\n+        // Get the results.\n+        logger.info(\"Retrieving query results...\");\n+        final QueryResponse response = bigQuery.getQueryResults(jobId);\n+        final TableResult result;\n+        try {\n+            result = queryJob.getQueryResults();\n+\n+            long bytesProcessed = ((JobStatistics.QueryStatistics) queryJob.getStatistics()).getTotalBytesProcessed();\n+            logger.info(String.format(\"Actual %s MB scanned\", bytesProcessed/1000000));\n+\n+        }\n+        catch (final InterruptedException ex) {\n+            throw new GATKException(\"Interrupted while waiting for query job to complete\", ex);\n+        }\n+\n+        return result;\n+    }\n+\n+    private static long getQueryCostBytesProcessedEstimate(String queryString) {\n+        final QueryJobConfiguration dryRunQueryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setDryRun(true)\n+                        .setUseQueryCache(false)\n+                        .setPriority(QueryJobConfiguration.Priority.INTERACTIVE)\n+                        .build();\n+\n+        Job dryRunJob = getBigQueryEndPoint().create(JobInfo.newBuilder(dryRunQueryConfig).build());\n+        long bytesProcessed = ((JobStatistics.QueryStatistics) dryRunJob.getStatistics()).getTotalBytesProcessed();\n+        return bytesProcessed;\n+    }\n+\n+//    public static StorageAPIAvroReader executeQueryWithStorageAPI(final String queryString, final List<String> fieldsToRetrieve, final String projectID) {\n+//\n+//        return executeQueryWithStorageAPI(queryString, fieldsToRetrieve, projectID, false);\n+//    }\n+//\n+//    public static StorageAPIAvroReader executeQueryWithStorageAPI(final String queryString, final List<String> fieldsToRetrieve, final String projectID, final boolean runQueryInBatchMode) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 371}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3MDExMA==", "bodyText": "Is this class used anywhere?", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481370110", "createdAt": "2020-09-01T19:07:31Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtils.java", "diffHunk": "@@ -0,0 +1,449 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.ivy.util.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+/**\n+ * Utility class for dealing with BigQuery connections / tables / queries /etc.\n+ *\n+ * Created by jonn on 4/17/19.\n+ */\n+public class BigQueryUtils {\n+    private static final Logger logger = LogManager.getLogger(BigQueryUtils.class);\n+\n+    //==================================================================================================================\n+    // Static Methods:\n+\n+    /**\n+     * @return A {@link BigQuery} object that can be used to interact with a BigQuery data set.\n+     */\n+    public static BigQuery getBigQueryEndPoint() {\n+        return BigQueryOptions.getDefaultInstance().getService();\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, false);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString, final boolean runQueryInBatchMode) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, runQueryInBatchMode);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param bigQuery The {@link BigQuery} instance against which to execute the given {@code queryString}.\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final BigQuery bigQuery, final String queryString, final boolean runQueryInBatchMode) {\n+\n+        // Create a query configuration we can run based on our query string:\n+        final QueryJobConfiguration queryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setPriority(runQueryInBatchMode ? QueryJobConfiguration.Priority.BATCH : QueryJobConfiguration.Priority.INTERACTIVE)\n+                        .build();\n+\n+        logger.info(\"Executing Query: \\n\\n\" + queryString);\n+        final TableResult result = submitQueryAndWaitForResults( bigQuery, queryConfig );\n+        logger.info(\"Query returned \" + result.getTotalRows() + \" results.\");\n+        return result;\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param bigQuery The {@link BigQuery} instance against which to execute the given {@code queryString}.  Must contain the table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param projectID The BigQuery {@code project ID} containing the {@code dataSet} and table from which to query data.\n+     * @param dataSet The BigQuery {@code dataSet} containing the table from which to query data.\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table ID in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final BigQuery bigQuery,\n+                                           final String projectID,\n+                                           final String dataSet,\n+                                           final String queryString) {\n+\n+        // Create a query configuration we can run based on our query string:\n+        final QueryJobConfiguration queryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setDefaultDataset(DatasetId.of(projectID, dataSet))\n+                        .build();\n+\n+        return submitQueryAndWaitForResults( bigQuery, queryConfig );\n+    }\n+\n+    /**\n+     * Creates a {@link String} containing the given results in a pretty ascii-art table.\n+     * @param result A {@link TableResult} object containing the results of a query that generated some data.\n+     * @return A {@link String} containing the contents of the given query result pretty-printed in an ascii-art table.\n+     */\n+    public static String getResultDataPrettyString(final TableResult result){\n+        final Schema schema = result.getSchema();\n+\n+        final List<Integer> columnWidths = calculateColumnWidths( result );\n+        final boolean rowsAllPrimitive =\n+                StreamSupport.stream(result.iterateAll().spliterator(), false)\n+                        .flatMap( row -> row.stream().map(v -> v.getAttribute() == FieldValue.Attribute.PRIMITIVE) )\n+                        .allMatch( b -> b );\n+\n+        // Create a separator string for the header and boarders:\n+        final String headerFooter = \"+\" + columnWidths.stream().map(\n+                l -> StringUtils.repeat(\"=\", l+2) + \"+\"\n+        ).collect(Collectors.joining(\"\"));\n+\n+        // Create a Row Separator:\n+        final String rowSeparator = headerFooter.replace('=', '-');\n+\n+        // Create a string builder to keep the pretty table:\n+        final StringBuilder sb = new StringBuilder();\n+\n+        // Now we can write our schema header and rows:\n+        addHeaderToStringBuilder(schema, columnWidths, headerFooter, sb);\n+\n+        // Write our data to the string builder:\n+        for ( final FieldValueList row : result.iterateAll() ) {\n+\n+            // If the row fields are all simple, then we can do the simple thing:\n+            if ( rowsAllPrimitive ) {\n+                addPrimitiveRowToStringBuilder(row, columnWidths, sb);\n+            }\n+            else {\n+                addComplexRowToStringBuilder(row, schema, columnWidths, sb);\n+                sb.append(rowSeparator);\n+            }\n+            sb.append(\"\\n\");\n+        }\n+\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+\n+        return sb.toString();\n+    }\n+\n+    private static void addHeaderToStringBuilder(final Schema schema, final List<Integer> columnWidths, final String headerFooter, final StringBuilder sb) {\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+        sb.append(\"|\");\n+        sb.append(\n+                IntStream.range(0, columnWidths.size()).boxed().map(\n+                        i -> String.format(\" %-\"+ columnWidths.get(i) +\"s |\", schema.getFields().get(i).getName())\n+                ).collect(Collectors.joining()) );\n+        sb.append(\"\\n\");\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+    }\n+\n+    private static void addComplexRowToStringBuilder(final FieldValueList row, final Schema schema, final List<Integer> columnWidths, final StringBuilder sb) {\n+\n+        // TODO: Clean this up... Probably make a getStringValue(FIELD) method to use here, addPrimitiveRowToStringBuilder, and calculateColumnWidths\n+\n+        // For fields that have multiple values, we need to do something special.\n+        // In fact, we need to go through each value of each row and track how many fields it has.\n+        int maxNumValuesInRow = 1;\n+        for ( int i = 0; i < row.size(); ++i ) {\n+            final FieldValue value = row.get(schema.getFields().get(i).getName());\n+            if ( !value.isNull() && (value.getAttribute() != FieldValue.Attribute.PRIMITIVE) ) {\n+                if (maxNumValuesInRow <= value.getRecordValue().size()) {\n+                    maxNumValuesInRow = value.getRecordValue().size();\n+                }\n+            }\n+        }\n+\n+        for ( int currentFieldNum = 0; currentFieldNum < maxNumValuesInRow ; ++currentFieldNum ) {\n+            sb.append(\"|\");\n+            for ( int i = 0; i < row.size(); ++i ) {\n+                final FieldValue value = row.get(i);\n+                if ( value.isNull() ) {\n+                    sb.append(getEmptyColumnString(columnWidths, i));\n+                }\n+                else if (value.getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                    if ( currentFieldNum == 0 ) {\n+                        sb.append( String.format(\" %-\" + columnWidths.get(i) + \"s |\", value.getStringValue()) );\n+                    }\n+                    else {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                }\n+                else {\n+                    if ( value.getRepeatedValue().size() == 0 ) {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                    else if ( currentFieldNum < value.getRepeatedValue().size() ) {\n+                        if ( value.getRepeatedValue().get(currentFieldNum).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                            sb.append(String.format(\" %-\" + columnWidths.get(i) + \"s |\",\n+                                    value.getRepeatedValue().get(currentFieldNum).getStringValue())\n+                            );\n+                        }\n+                        else {\n+                            // This is kind of gross, but it seems to be the only way to get a particular\n+                            // value of a field that is in an array:\n+                            sb.append(String.format(\" %-\" + columnWidths.get(i) + \"s |\",\n+                                    value.getRepeatedValue().get(currentFieldNum).getRecordValue().get(0).getStringValue())\n+                            );\n+                        }\n+                    }\n+                    else {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                }\n+            }\n+            sb.append(\"\\n\");\n+        }\n+    }\n+\n+    private static String getEmptyColumnString(final List<Integer> columnWidths, final int i) {\n+        return String.format(\" %-\" + columnWidths.get(i) + \"s |\", \"\");\n+    }\n+\n+    private static void addPrimitiveRowToStringBuilder(final FieldValueList row, final List<Integer> columnWidths, final StringBuilder sb) {\n+        sb.append(\"|\");\n+        sb.append(IntStream.range(0, row.size()).boxed().map(\n+                i -> String.format(\" %-\" + columnWidths.get(i) + \"s |\", row.get(i).getStringValue())\n+        ).collect(Collectors.joining()));\n+    }\n+\n+    /**\n+     * Creates a {@link String} containing the given results in a pretty ascii-art table.\n+     * @param result A {@link TableResult} object containing the results of a query that generated some data.\n+     * @param theLogger A {@link Logger} object with which to log the results contained in {@code result}.\n+     */\n+    public static void logResultDataPretty( final TableResult result, final Logger theLogger ){\n+        for ( final String line : getResultDataPrettyString(result).split(\"\\n\") ) {\n+            theLogger.info( line );\n+        }\n+    }\n+\n+    //==================================================================================================================\n+    // Helper Methods:\n+\n+    private static List<Integer> calculateColumnWidths( final TableResult result ) {\n+        // Go through all rows and get the length of each column:\n+        final List<Integer> columnWidths = new ArrayList<>(result.getSchema().getFields().size());\n+\n+        // Start with schema names:\n+        for ( final Field field : result.getSchema().getFields() ) {\n+            columnWidths.add( field.getName().length() );\n+        }\n+\n+        // Check each row and each row's array values (if applicable):\n+        for ( final FieldValueList row : result.iterateAll() ) {\n+            for ( int i = 0; i < row.size() ; ++i ) {\n+                // Only get the row size if it's not null:\n+                if ( !row.get(i).isNull() ) {\n+                    if ( row.get(i).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                        if ( columnWidths.get(i) < row.get(i).getStringValue().length() ) {\n+                            columnWidths.set(i, row.get(i).getStringValue().length());\n+                        }\n+                    }\n+                    else {\n+                        for ( int j = 0; j < row.get(i).getRepeatedValue().size(); ++j ) {\n+                            final String stringValue;\n+                            if ( row.get(i).getRepeatedValue().get(j).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                                stringValue = row.get(i).getRepeatedValue().get(j).getStringValue();\n+                            }\n+                            else {\n+                                stringValue = row.get(i).getRepeatedValue().get(j).getRecordValue().get(0).getStringValue();\n+                            }\n+                            if ( columnWidths.get(i) < stringValue.length() ) {\n+                                columnWidths.set(i, stringValue.length());\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        return columnWidths;\n+    }\n+\n+    /**\n+     * Executes the given {@code queryJobConfiguration} on the given {@code bigQuery} instance.\n+     * @param bigQuery The instance of {@link BigQuery} to use to connect to BigQuery.\n+     * @param queryJobConfiguration The {@link QueryJobConfiguration} object containing all required information to retrieve data from a BigQuery table.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    private static TableResult submitQueryAndWaitForResults( final BigQuery bigQuery,\n+                                                             final QueryJobConfiguration queryJobConfiguration ) {\n+        // Create a job ID so that we can safely retry:\n+        final JobId jobId = JobId.of(UUID.randomUUID().toString());\n+\n+        logger.info(\"Sending query to server...\");\n+        Job   queryJob       = bigQuery.create(JobInfo.newBuilder(queryJobConfiguration).setJobId(jobId).build());\n+\n+        // Wait for the query to complete.\n+        try {\n+            logger.info(\"Waiting for query to complete...\");\n+            queryJob = queryJob.waitFor();\n+        }\n+        catch (final InterruptedException ex) {\n+            throw new GATKException(\"Interrupted while waiting for query job to complete\", ex);\n+        }\n+\n+        // Check for errors:\n+        if (queryJob == null) {\n+            throw new GATKException(\"Query job no longer exists\");\n+        } else if (queryJob.getStatus().getError() != null) {\n+\n+            // Get all the errors we found and log them:\n+            for ( final BigQueryError bigQueryError : queryJob.getStatus().getExecutionErrors() ) {\n+                logger.error( \"Encountered BigQuery Execution Error: \" + bigQueryError.toString() );\n+            }\n+\n+            // Since we found an error, we should stop and alert the user:\n+            throw new GATKException(queryJob.getStatus().getError().toString());\n+        }\n+\n+        // Get the results.\n+        logger.info(\"Retrieving query results...\");\n+        final QueryResponse response = bigQuery.getQueryResults(jobId);\n+        final TableResult result;\n+        try {\n+            result = queryJob.getQueryResults();\n+\n+            long bytesProcessed = ((JobStatistics.QueryStatistics) queryJob.getStatistics()).getTotalBytesProcessed();\n+            logger.info(String.format(\"Actual %s MB scanned\", bytesProcessed/1000000));\n+\n+        }\n+        catch (final InterruptedException ex) {\n+            throw new GATKException(\"Interrupted while waiting for query job to complete\", ex);\n+        }\n+\n+        return result;\n+    }\n+\n+    private static long getQueryCostBytesProcessedEstimate(String queryString) {\n+        final QueryJobConfiguration dryRunQueryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setDryRun(true)\n+                        .setUseQueryCache(false)\n+                        .setPriority(QueryJobConfiguration.Priority.INTERACTIVE)\n+                        .build();\n+\n+        Job dryRunJob = getBigQueryEndPoint().create(JobInfo.newBuilder(dryRunQueryConfig).build());\n+        long bytesProcessed = ((JobStatistics.QueryStatistics) dryRunJob.getStatistics()).getTotalBytesProcessed();\n+        return bytesProcessed;\n+    }\n+\n+//    public static StorageAPIAvroReader executeQueryWithStorageAPI(final String queryString, final List<String> fieldsToRetrieve, final String projectID) {\n+//\n+//        return executeQueryWithStorageAPI(queryString, fieldsToRetrieve, projectID, false);\n+//    }\n+//\n+//    public static StorageAPIAvroReader executeQueryWithStorageAPI(final String queryString, final List<String> fieldsToRetrieve, final String projectID, final boolean runQueryInBatchMode) {\n+//        final String tempTableDataset = \"temp_tables\";\n+//        final String tempTableName = UUID.randomUUID().toString().replace('-', '_');\n+//        final String tempTableFullyQualified = String.format(\"%s.%s.%s\", projectID, tempTableDataset, tempTableName);\n+//\n+//        long bytesProcessed = getQueryCostBytesProcessedEstimate(queryString);\n+//        logger.info(String.format(\"Estimated %s MB scanned\", bytesProcessed/1000000));\n+//\n+//        final String queryStringIntoTempTable = \"CREATE TABLE `\" + tempTableFullyQualified + \"`\\n\" +\n+//                \"OPTIONS(\\n\" +\n+//                \"  expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)\\n\" +\n+//                \") AS\\n\" +\n+//                queryString;\n+//\n+//        final TableResult result = executeQuery(queryStringIntoTempTable, runQueryInBatchMode);\n+//\n+//        final Table tableInfo = getBigQueryEndPoint().getTable( TableId.of(projectID, tempTableDataset, tempTableName) );\n+//        logger.info(String.format(\"Query temp table created with %s rows and %s bytes in size\", tableInfo.getNumRows(), tableInfo.getNumBytes()));\n+//\n+//        return new StorageAPIAvroReader(projectID, tempTableDataset, tempTableName, fieldsToRetrieve);\n+//    }\n+\n+    private static class SimpleRowReader {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 393}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3MDM2OA==", "bodyText": "Make final, and add a private default constructor to prevent instantiation.", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481370368", "createdAt": "2020-09-01T19:07:55Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtils.java", "diffHunk": "@@ -0,0 +1,449 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.ivy.util.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+/**\n+ * Utility class for dealing with BigQuery connections / tables / queries /etc.\n+ *\n+ * Created by jonn on 4/17/19.\n+ */\n+public class BigQueryUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3MjcxMg==", "bodyText": "In the original Evoquer PR #6011 there was a basic unit test for this branch:\nhttps://github.com/broadinstitute/gatk/blob/111e6875cb29da5e7c75a5d113d217931ecd2c10/src/test/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtilsUnitTest.java\nWe need some test coverage on this class, so we should either restore and expand BigQueryUtilsUnitTest or write a new set of tests from scratch. If we restore BigQueryUtilsUnitTest you should expand it slightly to cover additional overloads of executeQuery().", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481372712", "createdAt": "2020-09-01T19:12:25Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtils.java", "diffHunk": "@@ -0,0 +1,449 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.ivy.util.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+/**\n+ * Utility class for dealing with BigQuery connections / tables / queries /etc.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3NDEwMg==", "bodyText": "iterator() should return a new iterator on each call, not an existing iterator instance", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481374102", "createdAt": "2020-09-01T19:15:04Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/QueryAPIRowReader.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.Iterator;\n+\n+public class QueryAPIRowReader implements Iterable<FieldValueList>, Iterator<FieldValueList> {\n+    private static final Logger logger = LogManager.getLogger(QueryAPIRowReader.class);\n+\n+    private Iterator<FieldValueList> rowIterator;\n+\n+    public QueryAPIRowReader(TableResult tableResult) {\n+        rowIterator = tableResult.iterateAll().iterator();\n+\n+    }\n+    @Override\n+    public Iterator<FieldValueList> iterator() {\n+        return rowIterator;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3NTA5Nw==", "bodyText": "Can you explain why this class is needed? Eg., why can't you just use the TableResult iterator directly instead of having this wrapper?", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481375097", "createdAt": "2020-09-01T19:17:01Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/QueryAPIRowReader.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.Iterator;\n+\n+public class QueryAPIRowReader implements Iterable<FieldValueList>, Iterator<FieldValueList> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3NTIyNA==", "bodyText": "Add javadoc for this class, explaining how it's different from GenericRecord", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481375224", "createdAt": "2020-09-01T19:17:15Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/QueryRecord.java", "diffHunk": "@@ -0,0 +1,40 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+\n+public class QueryRecord implements GenericRecord {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3NTYzOQ==", "bodyText": "Remove commented-out code", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481375639", "createdAt": "2020-09-01T19:18:12Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/QueryRecord.java", "diffHunk": "@@ -0,0 +1,40 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+\n+public class QueryRecord implements GenericRecord {\n+    private final FieldValueList fields;\n+\n+    public QueryRecord(FieldValueList fields) {\n+        this.fields = fields;\n+    }\n+    @Override\n+    public void put(String key, Object v) {\n+        throw new RuntimeException(\"Not implemented\");\n+    }\n+\n+    @Override\n+    public Object get(String key) {\n+        return fields.get(key).getStringValue();\n+    }\n+\n+//    public long getLong(String key) { return fields.get(key).getLongValue(); }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3Njc1OQ==", "bodyText": "If this class is used in subsequent branches, we'll need to add a test for it. A basic unit test for BigQueryUtils that exercises the (currently commented-out) executeQueryWithStorageAPI() method would suffice.", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481376759", "createdAt": "2020-09-01T19:20:20Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/StorageAPIAvroReader.java", "diffHunk": "@@ -0,0 +1,161 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+\n+public class StorageAPIAvroReader implements GATKAvroReader {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3NzQxNQ==", "bodyText": "Do we know the answer to this?", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481377415", "createdAt": "2020-09-01T19:21:37Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/StorageAPIAvroReader.java", "diffHunk": "@@ -0,0 +1,161 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+\n+public class StorageAPIAvroReader implements GATKAvroReader {\n+\n+    private static final Logger logger = LogManager.getLogger(StorageAPIAvroReader.class);\n+\n+    private static int rowCount = 0;\n+\n+    private BigQueryStorageClient client;\n+\n+    private Iterator<Storage.ReadRowsResponse> serverStream;\n+\n+    private org.apache.avro.Schema schema;\n+\n+    private DatumReader<GenericRecord> datumReader;\n+\n+    // Decoder object will be reused to avoid re-allocation and too much garbage collection.\n+    private BinaryDecoder decoder = null;\n+\n+    private AvroProto.AvroRows currentAvroRows;\n+\n+    // GenericRecord object will be reused.\n+    private GenericRecord nextRow = null;\n+\n+    public StorageAPIAvroReader(final TableReference tableRef) {\n+\n+        try {\n+            this.client = BigQueryStorageClient.create();\n+\n+            final String parent = String.format(\"projects/%s\", tableRef.tableProject);\n+\n+            final TableReferenceProto.TableReference tableReference = TableReferenceProto.TableReference.newBuilder()\n+                    .setProjectId(tableRef.tableProject)\n+                    .setDatasetId(tableRef.tableDataset)\n+                    .setTableId(tableRef.tableName)\n+                    .build();\n+\n+            final ReadOptions.TableReadOptions tableReadOptions =\n+                    ReadOptions.TableReadOptions.newBuilder()\n+                            .addAllSelectedFields(tableRef.fields)\n+                            .build();\n+\n+            final Storage.CreateReadSessionRequest.Builder builder = Storage.CreateReadSessionRequest.newBuilder()\n+                    .setParent(parent)\n+                    .setTableReference(tableReference)\n+                    .setReadOptions(tableReadOptions)\n+                    .setRequestedStreams(1)\n+                    .setFormat(Storage.DataFormat.AVRO);\n+\n+            final Storage.ReadSession session = client.createReadSession(builder.build());\n+            Preconditions.checkState(session.getStreamsCount() > 0);\n+\n+            this.schema = new org.apache.avro.Schema.Parser().parse(session.getAvroSchema().getSchema());\n+\n+            this.datumReader = new GenericDatumReader<>(\n+                    new org.apache.avro.Schema.Parser().parse(session.getAvroSchema().getSchema()));\n+\n+            // Use the first stream to perform reading.\n+            Storage.StreamPosition readPosition = Storage.StreamPosition.newBuilder()\n+                    .setStream(session.getStreams(0))\n+                    .build();\n+\n+            Storage.ReadRowsRequest readRowsRequest = Storage.ReadRowsRequest.newBuilder()\n+                    .setReadPosition(readPosition)\n+                    .build();\n+\n+            this.serverStream = client.readRowsCallable().call(readRowsRequest).iterator();\n+\n+            loadNextRow();\n+        } catch ( IOException e ) {\n+            throw new GATKException(\"I/O Error\", e);\n+        }\n+    }\n+\n+    private void loadNextRow() {\n+        try {\n+            if ( decoder != null && ! decoder.isEnd() ) {\n+                nextRow = datumReader.read(null, decoder);\n+            } else {\n+                fetchNextAvroRows();\n+\n+                if ( decoder != null && ! decoder.isEnd() ) {\n+                    nextRow = datumReader.read(null, decoder);\n+                } else {\n+                    nextRow = null; // end of traversal\n+                }\n+            }\n+        } catch ( IOException e ) {\n+            throw new GATKException(\"I/O error\", e);\n+        }\n+    }\n+\n+    private void fetchNextAvroRows() {\n+        if ( serverStream.hasNext() ) {\n+            currentAvroRows = serverStream.next().getAvroRows();\n+            decoder = DecoderFactory.get()\n+                    .binaryDecoder(currentAvroRows.getSerializedBinaryRows().toByteArray(), decoder);\n+        } else {\n+            currentAvroRows = null;\n+            decoder = null;\n+        }\n+    }\n+\n+    @Override\n+    public org.apache.avro.Schema getSchema() {\n+        return schema;\n+    }\n+\n+    @Override\n+    public Iterator<GenericRecord> iterator() {\n+        return this;\n+    }\n+\n+    @Override\n+    public boolean hasNext() {\n+        return nextRow != null;\n+    }\n+\n+    @Override\n+    public GenericRecord next() {\n+        if ( ! hasNext() ) {\n+            throw new NoSuchElementException(\"next() called when ! hasNext()\");\n+        }\n+\n+        final GenericRecord recordToReturn = nextRow;\n+        loadNextRow();\n+        return recordToReturn;\n+    }\n+\n+    @Override\n+    public void close() {\n+        client.shutdownNow();\n+        /* TODO: do we need to wait for termination here?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3NzUzNA==", "bodyText": "Add javadoc for this class", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481377534", "createdAt": "2020-09-01T19:21:50Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/TableReference.java", "diffHunk": "@@ -0,0 +1,29 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import java.util.List;\n+\n+public class TableReference {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3Nzg4MA==", "bodyText": "Add javadoc explaining the format of fullyQualifiedTableName", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481377880", "createdAt": "2020-09-01T19:22:31Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/TableReference.java", "diffHunk": "@@ -0,0 +1,29 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import java.util.List;\n+\n+public class TableReference {\n+    public final String tableProject;\n+    public final String tableDataset;\n+    public final String tableName;\n+    public final List<String> fields;\n+\n+    public TableReference(String fullyQualifiedTableName, List<String> fields) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3ODE1NQ==", "bodyText": "Handle the case where there aren't enough tokens after splitting by throwing an IllegalArgumentException. Also check for empty tokens.", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481378155", "createdAt": "2020-09-01T19:23:05Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/TableReference.java", "diffHunk": "@@ -0,0 +1,29 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import java.util.List;\n+\n+public class TableReference {\n+    public final String tableProject;\n+    public final String tableDataset;\n+    public final String tableName;\n+    public final List<String> fields;\n+\n+    public TableReference(String fullyQualifiedTableName, List<String> fields) {\n+        String[] vals = fullyQualifiedTableName.split(\"\\\\.\");\n+        tableProject = vals[0];\n+        tableDataset = vals[1];\n+        tableName = vals[2];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3ODQzNA==", "bodyText": "If fields is going to be public like this, you should wrap it in an ImmutableList or similar.", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r481378434", "createdAt": "2020-09-01T19:23:38Z", "author": {"login": "droazen"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/TableReference.java", "diffHunk": "@@ -0,0 +1,29 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import java.util.List;\n+\n+public class TableReference {\n+    public final String tableProject;\n+    public final String tableDataset;\n+    public final String tableName;\n+    public final List<String> fields;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 9}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b70d89a771b1395cc923edc938d36199427b7eeb", "author": {"user": {"login": "ahaessly", "name": "Andrea Haessly"}}, "url": "https://github.com/broadinstitute/gatk/commit/b70d89a771b1395cc923edc938d36199427b7eeb", "committedDate": "2020-09-11T18:57:37Z", "message": "updates from ah_var_store branch"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e4121ea39d070485660bee75aa23fdd7243cd39d", "author": {"user": {"login": "meganshand", "name": null}}, "url": "https://github.com/broadinstitute/gatk/commit/e4121ea39d070485660bee75aa23fdd7243cd39d", "committedDate": "2020-09-23T18:43:10Z", "message": "Addressing comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzNzU4MjA5", "url": "https://github.com/broadinstitute/gatk/pull/6759#pullrequestreview-493758209", "createdAt": "2020-09-22T18:57:25Z", "commit": {"oid": "b70d89a771b1395cc923edc938d36199427b7eeb"}, "state": "COMMENTED", "comments": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQxODo1NzoyNVrOHWILtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QxODozMzo1MFrOHW7cfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk2NDc5MA==", "bodyText": "For what it's worth it worked without zip64 for me.", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r492964790", "createdAt": "2020-09-22T18:57:25Z", "author": {"login": "meganshand"}, "path": "build.gradle", "diffHunk": "@@ -527,6 +533,7 @@ task collectBundleIntoDir(type: Copy) {\n \n task bundle(type: Zip) {\n     dependsOn collectBundleIntoDir\n+    zip64 true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM1MzY0OA=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk4NzY2MA==", "bodyText": "I copied that unit test file into this branch and added more tests for executeQuery(). I don't know how to change the author in git, but I'm happy to do it if you can point me to a reference.", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r492987660", "createdAt": "2020-09-22T19:39:44Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtils.java", "diffHunk": "@@ -0,0 +1,449 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.ivy.util.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+/**\n+ * Utility class for dealing with BigQuery connections / tables / queries /etc.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3MjcxMg=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk4ODk2MA==", "bodyText": "done", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r492988960", "createdAt": "2020-09-22T19:42:18Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtils.java", "diffHunk": "@@ -0,0 +1,449 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.ivy.util.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+/**\n+ * Utility class for dealing with BigQuery connections / tables / queries /etc.\n+ *\n+ * Created by jonn on 4/17/19.\n+ */\n+public class BigQueryUtils {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3MDM2OA=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk4OTAzMQ==", "bodyText": "done", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r492989031", "createdAt": "2020-09-22T19:42:28Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtils.java", "diffHunk": "@@ -0,0 +1,449 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.ivy.util.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+/**\n+ * Utility class for dealing with BigQuery connections / tables / queries /etc.\n+ *\n+ * Created by jonn on 4/17/19.\n+ */\n+public class BigQueryUtils {\n+    private static final Logger logger = LogManager.getLogger(BigQueryUtils.class);\n+\n+    //==================================================================================================================\n+    // Static Methods:\n+\n+    /**\n+     * @return A {@link BigQuery} object that can be used to interact with a BigQuery data set.\n+     */\n+    public static BigQuery getBigQueryEndPoint() {\n+        return BigQueryOptions.getDefaultInstance().getService();\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, false);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString, final boolean runQueryInBatchMode) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, runQueryInBatchMode);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM1NjQyMQ=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk4OTMwOQ==", "bodyText": "done", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r492989309", "createdAt": "2020-09-22T19:43:02Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtils.java", "diffHunk": "@@ -0,0 +1,449 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.ivy.util.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+/**\n+ * Utility class for dealing with BigQuery connections / tables / queries /etc.\n+ *\n+ * Created by jonn on 4/17/19.\n+ */\n+public class BigQueryUtils {\n+    private static final Logger logger = LogManager.getLogger(BigQueryUtils.class);\n+\n+    //==================================================================================================================\n+    // Static Methods:\n+\n+    /**\n+     * @return A {@link BigQuery} object that can be used to interact with a BigQuery data set.\n+     */\n+    public static BigQuery getBigQueryEndPoint() {\n+        return BigQueryOptions.getDefaultInstance().getService();\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, false);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString, final boolean runQueryInBatchMode) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, runQueryInBatchMode);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param bigQuery The {@link BigQuery} instance against which to execute the given {@code queryString}.\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final BigQuery bigQuery, final String queryString, final boolean runQueryInBatchMode) {\n+\n+        // Create a query configuration we can run based on our query string:\n+        final QueryJobConfiguration queryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setPriority(runQueryInBatchMode ? QueryJobConfiguration.Priority.BATCH : QueryJobConfiguration.Priority.INTERACTIVE)\n+                        .build();\n+\n+        logger.info(\"Executing Query: \\n\\n\" + queryString);\n+        final TableResult result = submitQueryAndWaitForResults( bigQuery, queryConfig );\n+        logger.info(\"Query returned \" + result.getTotalRows() + \" results.\");\n+        return result;\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM1NzEwOQ=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk4OTUwNg==", "bodyText": "done", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r492989506", "createdAt": "2020-09-22T19:43:23Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtils.java", "diffHunk": "@@ -0,0 +1,449 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.ivy.util.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+/**\n+ * Utility class for dealing with BigQuery connections / tables / queries /etc.\n+ *\n+ * Created by jonn on 4/17/19.\n+ */\n+public class BigQueryUtils {\n+    private static final Logger logger = LogManager.getLogger(BigQueryUtils.class);\n+\n+    //==================================================================================================================\n+    // Static Methods:\n+\n+    /**\n+     * @return A {@link BigQuery} object that can be used to interact with a BigQuery data set.\n+     */\n+    public static BigQuery getBigQueryEndPoint() {\n+        return BigQueryOptions.getDefaultInstance().getService();\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, false);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString, final boolean runQueryInBatchMode) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, runQueryInBatchMode);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param bigQuery The {@link BigQuery} instance against which to execute the given {@code queryString}.\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final BigQuery bigQuery, final String queryString, final boolean runQueryInBatchMode) {\n+\n+        // Create a query configuration we can run based on our query string:\n+        final QueryJobConfiguration queryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setPriority(runQueryInBatchMode ? QueryJobConfiguration.Priority.BATCH : QueryJobConfiguration.Priority.INTERACTIVE)\n+                        .build();\n+\n+        logger.info(\"Executing Query: \\n\\n\" + queryString);\n+        final TableResult result = submitQueryAndWaitForResults( bigQuery, queryConfig );\n+        logger.info(\"Query returned \" + result.getTotalRows() + \" results.\");\n+        return result;\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param bigQuery The {@link BigQuery} instance against which to execute the given {@code queryString}.  Must contain the table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param projectID The BigQuery {@code project ID} containing the {@code dataSet} and table from which to query data.\n+     * @param dataSet The BigQuery {@code dataSet} containing the table from which to query data.\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table ID in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final BigQuery bigQuery,\n+                                           final String projectID,\n+                                           final String dataSet,\n+                                           final String queryString) {\n+\n+        // Create a query configuration we can run based on our query string:\n+        final QueryJobConfiguration queryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setDefaultDataset(DatasetId.of(projectID, dataSet))\n+                        .build();\n+\n+        return submitQueryAndWaitForResults( bigQuery, queryConfig );\n+    }\n+\n+    /**\n+     * Creates a {@link String} containing the given results in a pretty ascii-art table.\n+     * @param result A {@link TableResult} object containing the results of a query that generated some data.\n+     * @return A {@link String} containing the contents of the given query result pretty-printed in an ascii-art table.\n+     */\n+    public static String getResultDataPrettyString(final TableResult result){\n+        final Schema schema = result.getSchema();\n+\n+        final List<Integer> columnWidths = calculateColumnWidths( result );\n+        final boolean rowsAllPrimitive =\n+                StreamSupport.stream(result.iterateAll().spliterator(), false)\n+                        .flatMap( row -> row.stream().map(v -> v.getAttribute() == FieldValue.Attribute.PRIMITIVE) )\n+                        .allMatch( b -> b );\n+\n+        // Create a separator string for the header and boarders:\n+        final String headerFooter = \"+\" + columnWidths.stream().map(\n+                l -> StringUtils.repeat(\"=\", l+2) + \"+\"\n+        ).collect(Collectors.joining(\"\"));\n+\n+        // Create a Row Separator:\n+        final String rowSeparator = headerFooter.replace('=', '-');\n+\n+        // Create a string builder to keep the pretty table:\n+        final StringBuilder sb = new StringBuilder();\n+\n+        // Now we can write our schema header and rows:\n+        addHeaderToStringBuilder(schema, columnWidths, headerFooter, sb);\n+\n+        // Write our data to the string builder:\n+        for ( final FieldValueList row : result.iterateAll() ) {\n+\n+            // If the row fields are all simple, then we can do the simple thing:\n+            if ( rowsAllPrimitive ) {\n+                addPrimitiveRowToStringBuilder(row, columnWidths, sb);\n+            }\n+            else {\n+                addComplexRowToStringBuilder(row, schema, columnWidths, sb);\n+                sb.append(rowSeparator);\n+            }\n+            sb.append(\"\\n\");\n+        }\n+\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+\n+        return sb.toString();\n+    }\n+\n+    private static void addHeaderToStringBuilder(final Schema schema, final List<Integer> columnWidths, final String headerFooter, final StringBuilder sb) {\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+        sb.append(\"|\");\n+        sb.append(\n+                IntStream.range(0, columnWidths.size()).boxed().map(\n+                        i -> String.format(\" %-\"+ columnWidths.get(i) +\"s |\", schema.getFields().get(i).getName())\n+                ).collect(Collectors.joining()) );\n+        sb.append(\"\\n\");\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+    }\n+\n+    private static void addComplexRowToStringBuilder(final FieldValueList row, final Schema schema, final List<Integer> columnWidths, final StringBuilder sb) {\n+\n+        // TODO: Clean this up... Probably make a getStringValue(FIELD) method to use here, addPrimitiveRowToStringBuilder, and calculateColumnWidths\n+\n+        // For fields that have multiple values, we need to do something special.\n+        // In fact, we need to go through each value of each row and track how many fields it has.\n+        int maxNumValuesInRow = 1;\n+        for ( int i = 0; i < row.size(); ++i ) {\n+            final FieldValue value = row.get(schema.getFields().get(i).getName());\n+            if ( !value.isNull() && (value.getAttribute() != FieldValue.Attribute.PRIMITIVE) ) {\n+                if (maxNumValuesInRow <= value.getRecordValue().size()) {\n+                    maxNumValuesInRow = value.getRecordValue().size();\n+                }\n+            }\n+        }\n+\n+        for ( int currentFieldNum = 0; currentFieldNum < maxNumValuesInRow ; ++currentFieldNum ) {\n+            sb.append(\"|\");\n+            for ( int i = 0; i < row.size(); ++i ) {\n+                final FieldValue value = row.get(i);\n+                if ( value.isNull() ) {\n+                    sb.append(getEmptyColumnString(columnWidths, i));\n+                }\n+                else if (value.getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                    if ( currentFieldNum == 0 ) {\n+                        sb.append( String.format(\" %-\" + columnWidths.get(i) + \"s |\", value.getStringValue()) );\n+                    }\n+                    else {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                }\n+                else {\n+                    if ( value.getRepeatedValue().size() == 0 ) {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                    else if ( currentFieldNum < value.getRepeatedValue().size() ) {\n+                        if ( value.getRepeatedValue().get(currentFieldNum).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                            sb.append(String.format(\" %-\" + columnWidths.get(i) + \"s |\",\n+                                    value.getRepeatedValue().get(currentFieldNum).getStringValue())\n+                            );\n+                        }\n+                        else {\n+                            // This is kind of gross, but it seems to be the only way to get a particular\n+                            // value of a field that is in an array:\n+                            sb.append(String.format(\" %-\" + columnWidths.get(i) + \"s |\",\n+                                    value.getRepeatedValue().get(currentFieldNum).getRecordValue().get(0).getStringValue())\n+                            );\n+                        }\n+                    }\n+                    else {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                }\n+            }\n+            sb.append(\"\\n\");\n+        }\n+    }\n+\n+    private static String getEmptyColumnString(final List<Integer> columnWidths, final int i) {\n+        return String.format(\" %-\" + columnWidths.get(i) + \"s |\", \"\");\n+    }\n+\n+    private static void addPrimitiveRowToStringBuilder(final FieldValueList row, final List<Integer> columnWidths, final StringBuilder sb) {\n+        sb.append(\"|\");\n+        sb.append(IntStream.range(0, row.size()).boxed().map(\n+                i -> String.format(\" %-\" + columnWidths.get(i) + \"s |\", row.get(i).getStringValue())\n+        ).collect(Collectors.joining()));\n+    }\n+\n+    /**\n+     * Creates a {@link String} containing the given results in a pretty ascii-art table.\n+     * @param result A {@link TableResult} object containing the results of a query that generated some data.\n+     * @param theLogger A {@link Logger} object with which to log the results contained in {@code result}.\n+     */\n+    public static void logResultDataPretty( final TableResult result, final Logger theLogger ){\n+        for ( final String line : getResultDataPrettyString(result).split(\"\\n\") ) {\n+            theLogger.info( line );\n+        }\n+    }\n+\n+    //==================================================================================================================\n+    // Helper Methods:\n+\n+    private static List<Integer> calculateColumnWidths( final TableResult result ) {\n+        // Go through all rows and get the length of each column:\n+        final List<Integer> columnWidths = new ArrayList<>(result.getSchema().getFields().size());\n+\n+        // Start with schema names:\n+        for ( final Field field : result.getSchema().getFields() ) {\n+            columnWidths.add( field.getName().length() );\n+        }\n+\n+        // Check each row and each row's array values (if applicable):\n+        for ( final FieldValueList row : result.iterateAll() ) {\n+            for ( int i = 0; i < row.size() ; ++i ) {\n+                // Only get the row size if it's not null:\n+                if ( !row.get(i).isNull() ) {\n+                    if ( row.get(i).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                        if ( columnWidths.get(i) < row.get(i).getStringValue().length() ) {\n+                            columnWidths.set(i, row.get(i).getStringValue().length());\n+                        }\n+                    }\n+                    else {\n+                        for ( int j = 0; j < row.get(i).getRepeatedValue().size(); ++j ) {\n+                            final String stringValue;\n+                            if ( row.get(i).getRepeatedValue().get(j).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                                stringValue = row.get(i).getRepeatedValue().get(j).getStringValue();\n+                            }\n+                            else {\n+                                stringValue = row.get(i).getRepeatedValue().get(j).getRecordValue().get(0).getStringValue();\n+                            }\n+                            if ( columnWidths.get(i) < stringValue.length() ) {\n+                                columnWidths.set(i, stringValue.length());\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        return columnWidths;\n+    }\n+\n+    /**\n+     * Executes the given {@code queryJobConfiguration} on the given {@code bigQuery} instance.\n+     * @param bigQuery The instance of {@link BigQuery} to use to connect to BigQuery.\n+     * @param queryJobConfiguration The {@link QueryJobConfiguration} object containing all required information to retrieve data from a BigQuery table.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    private static TableResult submitQueryAndWaitForResults( final BigQuery bigQuery,\n+                                                             final QueryJobConfiguration queryJobConfiguration ) {\n+        // Create a job ID so that we can safely retry:\n+        final JobId jobId = JobId.of(UUID.randomUUID().toString());\n+\n+        logger.info(\"Sending query to server...\");\n+        Job   queryJob       = bigQuery.create(JobInfo.newBuilder(queryJobConfiguration).setJobId(jobId).build());\n+\n+        // Wait for the query to complete.\n+        try {\n+            logger.info(\"Waiting for query to complete...\");\n+            queryJob = queryJob.waitFor();\n+        }\n+        catch (final InterruptedException ex) {\n+            throw new GATKException(\"Interrupted while waiting for query job to complete\", ex);\n+        }\n+\n+        // Check for errors:\n+        if (queryJob == null) {\n+            throw new GATKException(\"Query job no longer exists\");\n+        } else if (queryJob.getStatus().getError() != null) {\n+\n+            // Get all the errors we found and log them:\n+            for ( final BigQueryError bigQueryError : queryJob.getStatus().getExecutionErrors() ) {\n+                logger.error( \"Encountered BigQuery Execution Error: \" + bigQueryError.toString() );\n+            }\n+\n+            // Since we found an error, we should stop and alert the user:\n+            throw new GATKException(queryJob.getStatus().getError().toString());\n+        }\n+\n+        // Get the results.\n+        logger.info(\"Retrieving query results...\");\n+        final QueryResponse response = bigQuery.getQueryResults(jobId);\n+        final TableResult result;\n+        try {\n+            result = queryJob.getQueryResults();\n+\n+            long bytesProcessed = ((JobStatistics.QueryStatistics) queryJob.getStatistics()).getTotalBytesProcessed();\n+            logger.info(String.format(\"Actual %s MB scanned\", bytesProcessed/1000000));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NzI2Mg=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 342}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjk5NTkyMQ==", "bodyText": "I looked through the uses on the ah_var_store branch and I don't think this class is necessary so I removed it. We will have to update the couple of uses in the ah_var_store branch though.", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r492995921", "createdAt": "2020-09-22T19:55:08Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/QueryAPIRowReader.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.Iterator;\n+\n+public class QueryAPIRowReader implements Iterable<FieldValueList>, Iterator<FieldValueList> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3NTA5Nw=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzEwMjgxNw==", "bodyText": "added comment", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r493102817", "createdAt": "2020-09-23T00:15:36Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/QueryRecord.java", "diffHunk": "@@ -0,0 +1,40 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+\n+public class QueryRecord implements GenericRecord {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3NTIyNA=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzEwMjgzOQ==", "bodyText": "done", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r493102839", "createdAt": "2020-09-23T00:15:40Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/QueryRecord.java", "diffHunk": "@@ -0,0 +1,40 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.FieldValueList;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+\n+public class QueryRecord implements GenericRecord {\n+    private final FieldValueList fields;\n+\n+    public QueryRecord(FieldValueList fields) {\n+        this.fields = fields;\n+    }\n+    @Override\n+    public void put(String key, Object v) {\n+        throw new RuntimeException(\"Not implemented\");\n+    }\n+\n+    @Override\n+    public Object get(String key) {\n+        return fields.get(key).getStringValue();\n+    }\n+\n+//    public long getLong(String key) { return fields.get(key).getLongValue(); }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3NTYzOQ=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU2MTAxNA==", "bodyText": "Uncommented out executeQueryWithStorageAPI() and added a test.", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r493561014", "createdAt": "2020-09-23T13:01:31Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/StorageAPIAvroReader.java", "diffHunk": "@@ -0,0 +1,161 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+\n+public class StorageAPIAvroReader implements GATKAvroReader {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3Njc1OQ=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU2NDg1OQ==", "bodyText": "I uncommented this out, although it's currently not used in the rest of the code on ah_var_store because the places we currently use the Storage API are only pulling down the full table (rather than running a specific query). I think we could clean up the code in future PRs to use the StorageAPI in places where we are running a query so this will still be useful.", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r493564859", "createdAt": "2020-09-23T13:05:31Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtils.java", "diffHunk": "@@ -0,0 +1,449 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.ivy.util.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+/**\n+ * Utility class for dealing with BigQuery connections / tables / queries /etc.\n+ *\n+ * Created by jonn on 4/17/19.\n+ */\n+public class BigQueryUtils {\n+    private static final Logger logger = LogManager.getLogger(BigQueryUtils.class);\n+\n+    //==================================================================================================================\n+    // Static Methods:\n+\n+    /**\n+     * @return A {@link BigQuery} object that can be used to interact with a BigQuery data set.\n+     */\n+    public static BigQuery getBigQueryEndPoint() {\n+        return BigQueryOptions.getDefaultInstance().getService();\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, false);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString, final boolean runQueryInBatchMode) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, runQueryInBatchMode);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param bigQuery The {@link BigQuery} instance against which to execute the given {@code queryString}.\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final BigQuery bigQuery, final String queryString, final boolean runQueryInBatchMode) {\n+\n+        // Create a query configuration we can run based on our query string:\n+        final QueryJobConfiguration queryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setPriority(runQueryInBatchMode ? QueryJobConfiguration.Priority.BATCH : QueryJobConfiguration.Priority.INTERACTIVE)\n+                        .build();\n+\n+        logger.info(\"Executing Query: \\n\\n\" + queryString);\n+        final TableResult result = submitQueryAndWaitForResults( bigQuery, queryConfig );\n+        logger.info(\"Query returned \" + result.getTotalRows() + \" results.\");\n+        return result;\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param bigQuery The {@link BigQuery} instance against which to execute the given {@code queryString}.  Must contain the table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param projectID The BigQuery {@code project ID} containing the {@code dataSet} and table from which to query data.\n+     * @param dataSet The BigQuery {@code dataSet} containing the table from which to query data.\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table ID in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final BigQuery bigQuery,\n+                                           final String projectID,\n+                                           final String dataSet,\n+                                           final String queryString) {\n+\n+        // Create a query configuration we can run based on our query string:\n+        final QueryJobConfiguration queryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setDefaultDataset(DatasetId.of(projectID, dataSet))\n+                        .build();\n+\n+        return submitQueryAndWaitForResults( bigQuery, queryConfig );\n+    }\n+\n+    /**\n+     * Creates a {@link String} containing the given results in a pretty ascii-art table.\n+     * @param result A {@link TableResult} object containing the results of a query that generated some data.\n+     * @return A {@link String} containing the contents of the given query result pretty-printed in an ascii-art table.\n+     */\n+    public static String getResultDataPrettyString(final TableResult result){\n+        final Schema schema = result.getSchema();\n+\n+        final List<Integer> columnWidths = calculateColumnWidths( result );\n+        final boolean rowsAllPrimitive =\n+                StreamSupport.stream(result.iterateAll().spliterator(), false)\n+                        .flatMap( row -> row.stream().map(v -> v.getAttribute() == FieldValue.Attribute.PRIMITIVE) )\n+                        .allMatch( b -> b );\n+\n+        // Create a separator string for the header and boarders:\n+        final String headerFooter = \"+\" + columnWidths.stream().map(\n+                l -> StringUtils.repeat(\"=\", l+2) + \"+\"\n+        ).collect(Collectors.joining(\"\"));\n+\n+        // Create a Row Separator:\n+        final String rowSeparator = headerFooter.replace('=', '-');\n+\n+        // Create a string builder to keep the pretty table:\n+        final StringBuilder sb = new StringBuilder();\n+\n+        // Now we can write our schema header and rows:\n+        addHeaderToStringBuilder(schema, columnWidths, headerFooter, sb);\n+\n+        // Write our data to the string builder:\n+        for ( final FieldValueList row : result.iterateAll() ) {\n+\n+            // If the row fields are all simple, then we can do the simple thing:\n+            if ( rowsAllPrimitive ) {\n+                addPrimitiveRowToStringBuilder(row, columnWidths, sb);\n+            }\n+            else {\n+                addComplexRowToStringBuilder(row, schema, columnWidths, sb);\n+                sb.append(rowSeparator);\n+            }\n+            sb.append(\"\\n\");\n+        }\n+\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+\n+        return sb.toString();\n+    }\n+\n+    private static void addHeaderToStringBuilder(final Schema schema, final List<Integer> columnWidths, final String headerFooter, final StringBuilder sb) {\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+        sb.append(\"|\");\n+        sb.append(\n+                IntStream.range(0, columnWidths.size()).boxed().map(\n+                        i -> String.format(\" %-\"+ columnWidths.get(i) +\"s |\", schema.getFields().get(i).getName())\n+                ).collect(Collectors.joining()) );\n+        sb.append(\"\\n\");\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+    }\n+\n+    private static void addComplexRowToStringBuilder(final FieldValueList row, final Schema schema, final List<Integer> columnWidths, final StringBuilder sb) {\n+\n+        // TODO: Clean this up... Probably make a getStringValue(FIELD) method to use here, addPrimitiveRowToStringBuilder, and calculateColumnWidths\n+\n+        // For fields that have multiple values, we need to do something special.\n+        // In fact, we need to go through each value of each row and track how many fields it has.\n+        int maxNumValuesInRow = 1;\n+        for ( int i = 0; i < row.size(); ++i ) {\n+            final FieldValue value = row.get(schema.getFields().get(i).getName());\n+            if ( !value.isNull() && (value.getAttribute() != FieldValue.Attribute.PRIMITIVE) ) {\n+                if (maxNumValuesInRow <= value.getRecordValue().size()) {\n+                    maxNumValuesInRow = value.getRecordValue().size();\n+                }\n+            }\n+        }\n+\n+        for ( int currentFieldNum = 0; currentFieldNum < maxNumValuesInRow ; ++currentFieldNum ) {\n+            sb.append(\"|\");\n+            for ( int i = 0; i < row.size(); ++i ) {\n+                final FieldValue value = row.get(i);\n+                if ( value.isNull() ) {\n+                    sb.append(getEmptyColumnString(columnWidths, i));\n+                }\n+                else if (value.getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                    if ( currentFieldNum == 0 ) {\n+                        sb.append( String.format(\" %-\" + columnWidths.get(i) + \"s |\", value.getStringValue()) );\n+                    }\n+                    else {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                }\n+                else {\n+                    if ( value.getRepeatedValue().size() == 0 ) {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                    else if ( currentFieldNum < value.getRepeatedValue().size() ) {\n+                        if ( value.getRepeatedValue().get(currentFieldNum).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                            sb.append(String.format(\" %-\" + columnWidths.get(i) + \"s |\",\n+                                    value.getRepeatedValue().get(currentFieldNum).getStringValue())\n+                            );\n+                        }\n+                        else {\n+                            // This is kind of gross, but it seems to be the only way to get a particular\n+                            // value of a field that is in an array:\n+                            sb.append(String.format(\" %-\" + columnWidths.get(i) + \"s |\",\n+                                    value.getRepeatedValue().get(currentFieldNum).getRecordValue().get(0).getStringValue())\n+                            );\n+                        }\n+                    }\n+                    else {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                }\n+            }\n+            sb.append(\"\\n\");\n+        }\n+    }\n+\n+    private static String getEmptyColumnString(final List<Integer> columnWidths, final int i) {\n+        return String.format(\" %-\" + columnWidths.get(i) + \"s |\", \"\");\n+    }\n+\n+    private static void addPrimitiveRowToStringBuilder(final FieldValueList row, final List<Integer> columnWidths, final StringBuilder sb) {\n+        sb.append(\"|\");\n+        sb.append(IntStream.range(0, row.size()).boxed().map(\n+                i -> String.format(\" %-\" + columnWidths.get(i) + \"s |\", row.get(i).getStringValue())\n+        ).collect(Collectors.joining()));\n+    }\n+\n+    /**\n+     * Creates a {@link String} containing the given results in a pretty ascii-art table.\n+     * @param result A {@link TableResult} object containing the results of a query that generated some data.\n+     * @param theLogger A {@link Logger} object with which to log the results contained in {@code result}.\n+     */\n+    public static void logResultDataPretty( final TableResult result, final Logger theLogger ){\n+        for ( final String line : getResultDataPrettyString(result).split(\"\\n\") ) {\n+            theLogger.info( line );\n+        }\n+    }\n+\n+    //==================================================================================================================\n+    // Helper Methods:\n+\n+    private static List<Integer> calculateColumnWidths( final TableResult result ) {\n+        // Go through all rows and get the length of each column:\n+        final List<Integer> columnWidths = new ArrayList<>(result.getSchema().getFields().size());\n+\n+        // Start with schema names:\n+        for ( final Field field : result.getSchema().getFields() ) {\n+            columnWidths.add( field.getName().length() );\n+        }\n+\n+        // Check each row and each row's array values (if applicable):\n+        for ( final FieldValueList row : result.iterateAll() ) {\n+            for ( int i = 0; i < row.size() ; ++i ) {\n+                // Only get the row size if it's not null:\n+                if ( !row.get(i).isNull() ) {\n+                    if ( row.get(i).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                        if ( columnWidths.get(i) < row.get(i).getStringValue().length() ) {\n+                            columnWidths.set(i, row.get(i).getStringValue().length());\n+                        }\n+                    }\n+                    else {\n+                        for ( int j = 0; j < row.get(i).getRepeatedValue().size(); ++j ) {\n+                            final String stringValue;\n+                            if ( row.get(i).getRepeatedValue().get(j).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                                stringValue = row.get(i).getRepeatedValue().get(j).getStringValue();\n+                            }\n+                            else {\n+                                stringValue = row.get(i).getRepeatedValue().get(j).getRecordValue().get(0).getStringValue();\n+                            }\n+                            if ( columnWidths.get(i) < stringValue.length() ) {\n+                                columnWidths.set(i, stringValue.length());\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        return columnWidths;\n+    }\n+\n+    /**\n+     * Executes the given {@code queryJobConfiguration} on the given {@code bigQuery} instance.\n+     * @param bigQuery The instance of {@link BigQuery} to use to connect to BigQuery.\n+     * @param queryJobConfiguration The {@link QueryJobConfiguration} object containing all required information to retrieve data from a BigQuery table.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    private static TableResult submitQueryAndWaitForResults( final BigQuery bigQuery,\n+                                                             final QueryJobConfiguration queryJobConfiguration ) {\n+        // Create a job ID so that we can safely retry:\n+        final JobId jobId = JobId.of(UUID.randomUUID().toString());\n+\n+        logger.info(\"Sending query to server...\");\n+        Job   queryJob       = bigQuery.create(JobInfo.newBuilder(queryJobConfiguration).setJobId(jobId).build());\n+\n+        // Wait for the query to complete.\n+        try {\n+            logger.info(\"Waiting for query to complete...\");\n+            queryJob = queryJob.waitFor();\n+        }\n+        catch (final InterruptedException ex) {\n+            throw new GATKException(\"Interrupted while waiting for query job to complete\", ex);\n+        }\n+\n+        // Check for errors:\n+        if (queryJob == null) {\n+            throw new GATKException(\"Query job no longer exists\");\n+        } else if (queryJob.getStatus().getError() != null) {\n+\n+            // Get all the errors we found and log them:\n+            for ( final BigQueryError bigQueryError : queryJob.getStatus().getExecutionErrors() ) {\n+                logger.error( \"Encountered BigQuery Execution Error: \" + bigQueryError.toString() );\n+            }\n+\n+            // Since we found an error, we should stop and alert the user:\n+            throw new GATKException(queryJob.getStatus().getError().toString());\n+        }\n+\n+        // Get the results.\n+        logger.info(\"Retrieving query results...\");\n+        final QueryResponse response = bigQuery.getQueryResults(jobId);\n+        final TableResult result;\n+        try {\n+            result = queryJob.getQueryResults();\n+\n+            long bytesProcessed = ((JobStatistics.QueryStatistics) queryJob.getStatistics()).getTotalBytesProcessed();\n+            logger.info(String.format(\"Actual %s MB scanned\", bytesProcessed/1000000));\n+\n+        }\n+        catch (final InterruptedException ex) {\n+            throw new GATKException(\"Interrupted while waiting for query job to complete\", ex);\n+        }\n+\n+        return result;\n+    }\n+\n+    private static long getQueryCostBytesProcessedEstimate(String queryString) {\n+        final QueryJobConfiguration dryRunQueryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setDryRun(true)\n+                        .setUseQueryCache(false)\n+                        .setPriority(QueryJobConfiguration.Priority.INTERACTIVE)\n+                        .build();\n+\n+        Job dryRunJob = getBigQueryEndPoint().create(JobInfo.newBuilder(dryRunQueryConfig).build());\n+        long bytesProcessed = ((JobStatistics.QueryStatistics) dryRunJob.getStatistics()).getTotalBytesProcessed();\n+        return bytesProcessed;\n+    }\n+\n+//    public static StorageAPIAvroReader executeQueryWithStorageAPI(final String queryString, final List<String> fieldsToRetrieve, final String projectID) {\n+//\n+//        return executeQueryWithStorageAPI(queryString, fieldsToRetrieve, projectID, false);\n+//    }\n+//\n+//    public static StorageAPIAvroReader executeQueryWithStorageAPI(final String queryString, final List<String> fieldsToRetrieve, final String projectID, final boolean runQueryInBatchMode) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2OTIyOQ=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 371}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU3MjA1Mg==", "bodyText": "done", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r493572052", "createdAt": "2020-09-23T13:12:35Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/TableReference.java", "diffHunk": "@@ -0,0 +1,29 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import java.util.List;\n+\n+public class TableReference {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3NzUzNA=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU3NjM1MA==", "bodyText": "done", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r493576350", "createdAt": "2020-09-23T13:17:16Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/TableReference.java", "diffHunk": "@@ -0,0 +1,29 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import java.util.List;\n+\n+public class TableReference {\n+    public final String tableProject;\n+    public final String tableDataset;\n+    public final String tableName;\n+    public final List<String> fields;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3ODQzNA=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU3ODE3MQ==", "bodyText": "done", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r493578171", "createdAt": "2020-09-23T13:19:16Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/TableReference.java", "diffHunk": "@@ -0,0 +1,29 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import java.util.List;\n+\n+public class TableReference {\n+    public final String tableProject;\n+    public final String tableDataset;\n+    public final String tableName;\n+    public final List<String> fields;\n+\n+    public TableReference(String fullyQualifiedTableName, List<String> fields) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3Nzg4MA=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU5MDA1Nw==", "bodyText": "done", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r493590057", "createdAt": "2020-09-23T13:30:21Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/TableReference.java", "diffHunk": "@@ -0,0 +1,29 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import java.util.List;\n+\n+public class TableReference {\n+    public final String tableProject;\n+    public final String tableDataset;\n+    public final String tableName;\n+    public final List<String> fields;\n+\n+    public TableReference(String fullyQualifiedTableName, List<String> fields) {\n+        String[] vals = fullyQualifiedTableName.split(\"\\\\.\");\n+        tableProject = vals[0];\n+        tableDataset = vals[1];\n+        tableName = vals[2];", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3ODE1NQ=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzYwMzk0MA==", "bodyText": "code deleted", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r493603940", "createdAt": "2020-09-23T13:44:46Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/QueryAPIRowReader.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.Iterator;\n+\n+public class QueryAPIRowReader implements Iterable<FieldValueList>, Iterator<FieldValueList> {\n+    private static final Logger logger = LogManager.getLogger(QueryAPIRowReader.class);\n+\n+    private Iterator<FieldValueList> rowIterator;\n+\n+    public QueryAPIRowReader(TableResult tableResult) {\n+        rowIterator = tableResult.iterateAll().iterator();\n+\n+    }\n+    @Override\n+    public Iterator<FieldValueList> iterator() {\n+        return rowIterator;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3NDEwMg=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc5MTE4OQ==", "bodyText": "I don't think we know the answer to this at this point, so could we leave the TODO for now?", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r493791189", "createdAt": "2020-09-23T18:10:41Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/StorageAPIAvroReader.java", "diffHunk": "@@ -0,0 +1,161 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+\n+public class StorageAPIAvroReader implements GATKAvroReader {\n+\n+    private static final Logger logger = LogManager.getLogger(StorageAPIAvroReader.class);\n+\n+    private static int rowCount = 0;\n+\n+    private BigQueryStorageClient client;\n+\n+    private Iterator<Storage.ReadRowsResponse> serverStream;\n+\n+    private org.apache.avro.Schema schema;\n+\n+    private DatumReader<GenericRecord> datumReader;\n+\n+    // Decoder object will be reused to avoid re-allocation and too much garbage collection.\n+    private BinaryDecoder decoder = null;\n+\n+    private AvroProto.AvroRows currentAvroRows;\n+\n+    // GenericRecord object will be reused.\n+    private GenericRecord nextRow = null;\n+\n+    public StorageAPIAvroReader(final TableReference tableRef) {\n+\n+        try {\n+            this.client = BigQueryStorageClient.create();\n+\n+            final String parent = String.format(\"projects/%s\", tableRef.tableProject);\n+\n+            final TableReferenceProto.TableReference tableReference = TableReferenceProto.TableReference.newBuilder()\n+                    .setProjectId(tableRef.tableProject)\n+                    .setDatasetId(tableRef.tableDataset)\n+                    .setTableId(tableRef.tableName)\n+                    .build();\n+\n+            final ReadOptions.TableReadOptions tableReadOptions =\n+                    ReadOptions.TableReadOptions.newBuilder()\n+                            .addAllSelectedFields(tableRef.fields)\n+                            .build();\n+\n+            final Storage.CreateReadSessionRequest.Builder builder = Storage.CreateReadSessionRequest.newBuilder()\n+                    .setParent(parent)\n+                    .setTableReference(tableReference)\n+                    .setReadOptions(tableReadOptions)\n+                    .setRequestedStreams(1)\n+                    .setFormat(Storage.DataFormat.AVRO);\n+\n+            final Storage.ReadSession session = client.createReadSession(builder.build());\n+            Preconditions.checkState(session.getStreamsCount() > 0);\n+\n+            this.schema = new org.apache.avro.Schema.Parser().parse(session.getAvroSchema().getSchema());\n+\n+            this.datumReader = new GenericDatumReader<>(\n+                    new org.apache.avro.Schema.Parser().parse(session.getAvroSchema().getSchema()));\n+\n+            // Use the first stream to perform reading.\n+            Storage.StreamPosition readPosition = Storage.StreamPosition.newBuilder()\n+                    .setStream(session.getStreams(0))\n+                    .build();\n+\n+            Storage.ReadRowsRequest readRowsRequest = Storage.ReadRowsRequest.newBuilder()\n+                    .setReadPosition(readPosition)\n+                    .build();\n+\n+            this.serverStream = client.readRowsCallable().call(readRowsRequest).iterator();\n+\n+            loadNextRow();\n+        } catch ( IOException e ) {\n+            throw new GATKException(\"I/O Error\", e);\n+        }\n+    }\n+\n+    private void loadNextRow() {\n+        try {\n+            if ( decoder != null && ! decoder.isEnd() ) {\n+                nextRow = datumReader.read(null, decoder);\n+            } else {\n+                fetchNextAvroRows();\n+\n+                if ( decoder != null && ! decoder.isEnd() ) {\n+                    nextRow = datumReader.read(null, decoder);\n+                } else {\n+                    nextRow = null; // end of traversal\n+                }\n+            }\n+        } catch ( IOException e ) {\n+            throw new GATKException(\"I/O error\", e);\n+        }\n+    }\n+\n+    private void fetchNextAvroRows() {\n+        if ( serverStream.hasNext() ) {\n+            currentAvroRows = serverStream.next().getAvroRows();\n+            decoder = DecoderFactory.get()\n+                    .binaryDecoder(currentAvroRows.getSerializedBinaryRows().toByteArray(), decoder);\n+        } else {\n+            currentAvroRows = null;\n+            decoder = null;\n+        }\n+    }\n+\n+    @Override\n+    public org.apache.avro.Schema getSchema() {\n+        return schema;\n+    }\n+\n+    @Override\n+    public Iterator<GenericRecord> iterator() {\n+        return this;\n+    }\n+\n+    @Override\n+    public boolean hasNext() {\n+        return nextRow != null;\n+    }\n+\n+    @Override\n+    public GenericRecord next() {\n+        if ( ! hasNext() ) {\n+            throw new NoSuchElementException(\"next() called when ! hasNext()\");\n+        }\n+\n+        final GenericRecord recordToReturn = nextRow;\n+        loadNextRow();\n+        return recordToReturn;\n+    }\n+\n+    @Override\n+    public void close() {\n+        client.shutdownNow();\n+        /* TODO: do we need to wait for termination here?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3NzQxNQ=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzgwNDY2OQ==", "bodyText": "no, I removed it", "url": "https://github.com/broadinstitute/gatk/pull/6759#discussion_r493804669", "createdAt": "2020-09-23T18:33:50Z", "author": {"login": "meganshand"}, "path": "src/main/java/org/broadinstitute/hellbender/utils/bigquery/BigQueryUtils.java", "diffHunk": "@@ -0,0 +1,449 @@\n+package org.broadinstitute.hellbender.utils.bigquery;\n+\n+import com.google.cloud.bigquery.*;\n+import com.google.cloud.bigquery.storage.v1beta1.*;\n+import com.google.common.base.Preconditions;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumReader;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.ivy.util.StringUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.IOException;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+\n+/**\n+ * Utility class for dealing with BigQuery connections / tables / queries /etc.\n+ *\n+ * Created by jonn on 4/17/19.\n+ */\n+public class BigQueryUtils {\n+    private static final Logger logger = LogManager.getLogger(BigQueryUtils.class);\n+\n+    //==================================================================================================================\n+    // Static Methods:\n+\n+    /**\n+     * @return A {@link BigQuery} object that can be used to interact with a BigQuery data set.\n+     */\n+    public static BigQuery getBigQueryEndPoint() {\n+        return BigQueryOptions.getDefaultInstance().getService();\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, false);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final String queryString, final boolean runQueryInBatchMode) {\n+        return executeQuery(getBigQueryEndPoint(), queryString, runQueryInBatchMode);\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param bigQuery The {@link BigQuery} instance against which to execute the given {@code queryString}.\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param runQueryInBatchMode If true, run the query in batch mode, which is lower priority but has no limit on the number of concurrent queries\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final BigQuery bigQuery, final String queryString, final boolean runQueryInBatchMode) {\n+\n+        // Create a query configuration we can run based on our query string:\n+        final QueryJobConfiguration queryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setPriority(runQueryInBatchMode ? QueryJobConfiguration.Priority.BATCH : QueryJobConfiguration.Priority.INTERACTIVE)\n+                        .build();\n+\n+        logger.info(\"Executing Query: \\n\\n\" + queryString);\n+        final TableResult result = submitQueryAndWaitForResults( bigQuery, queryConfig );\n+        logger.info(\"Query returned \" + result.getTotalRows() + \" results.\");\n+        return result;\n+    }\n+\n+    /**\n+     * Executes the given {@code queryString} on the default instance of {@link BigQuery} as created by {@link #getBigQueryEndPoint()}.\n+     * Will block until results are returned.\n+     * For more information on querying BigQuery tables, see: https://cloud.google.com/bigquery/sql-reference/\n+     * @param bigQuery The {@link BigQuery} instance against which to execute the given {@code queryString}.  Must contain the table name in the `FROM` clause for the table from which to retrieve data.\n+     * @param projectID The BigQuery {@code project ID} containing the {@code dataSet} and table from which to query data.\n+     * @param dataSet The BigQuery {@code dataSet} containing the table from which to query data.\n+     * @param queryString The {@link BigQuery} query string to execute.  Must use standard SQL syntax.  Must contain the project ID, data set, and table ID in the `FROM` clause for the table from which to retrieve data.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    public static TableResult executeQuery(final BigQuery bigQuery,\n+                                           final String projectID,\n+                                           final String dataSet,\n+                                           final String queryString) {\n+\n+        // Create a query configuration we can run based on our query string:\n+        final QueryJobConfiguration queryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setDefaultDataset(DatasetId.of(projectID, dataSet))\n+                        .build();\n+\n+        return submitQueryAndWaitForResults( bigQuery, queryConfig );\n+    }\n+\n+    /**\n+     * Creates a {@link String} containing the given results in a pretty ascii-art table.\n+     * @param result A {@link TableResult} object containing the results of a query that generated some data.\n+     * @return A {@link String} containing the contents of the given query result pretty-printed in an ascii-art table.\n+     */\n+    public static String getResultDataPrettyString(final TableResult result){\n+        final Schema schema = result.getSchema();\n+\n+        final List<Integer> columnWidths = calculateColumnWidths( result );\n+        final boolean rowsAllPrimitive =\n+                StreamSupport.stream(result.iterateAll().spliterator(), false)\n+                        .flatMap( row -> row.stream().map(v -> v.getAttribute() == FieldValue.Attribute.PRIMITIVE) )\n+                        .allMatch( b -> b );\n+\n+        // Create a separator string for the header and boarders:\n+        final String headerFooter = \"+\" + columnWidths.stream().map(\n+                l -> StringUtils.repeat(\"=\", l+2) + \"+\"\n+        ).collect(Collectors.joining(\"\"));\n+\n+        // Create a Row Separator:\n+        final String rowSeparator = headerFooter.replace('=', '-');\n+\n+        // Create a string builder to keep the pretty table:\n+        final StringBuilder sb = new StringBuilder();\n+\n+        // Now we can write our schema header and rows:\n+        addHeaderToStringBuilder(schema, columnWidths, headerFooter, sb);\n+\n+        // Write our data to the string builder:\n+        for ( final FieldValueList row : result.iterateAll() ) {\n+\n+            // If the row fields are all simple, then we can do the simple thing:\n+            if ( rowsAllPrimitive ) {\n+                addPrimitiveRowToStringBuilder(row, columnWidths, sb);\n+            }\n+            else {\n+                addComplexRowToStringBuilder(row, schema, columnWidths, sb);\n+                sb.append(rowSeparator);\n+            }\n+            sb.append(\"\\n\");\n+        }\n+\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+\n+        return sb.toString();\n+    }\n+\n+    private static void addHeaderToStringBuilder(final Schema schema, final List<Integer> columnWidths, final String headerFooter, final StringBuilder sb) {\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+        sb.append(\"|\");\n+        sb.append(\n+                IntStream.range(0, columnWidths.size()).boxed().map(\n+                        i -> String.format(\" %-\"+ columnWidths.get(i) +\"s |\", schema.getFields().get(i).getName())\n+                ).collect(Collectors.joining()) );\n+        sb.append(\"\\n\");\n+        sb.append( headerFooter );\n+        sb.append(\"\\n\");\n+    }\n+\n+    private static void addComplexRowToStringBuilder(final FieldValueList row, final Schema schema, final List<Integer> columnWidths, final StringBuilder sb) {\n+\n+        // TODO: Clean this up... Probably make a getStringValue(FIELD) method to use here, addPrimitiveRowToStringBuilder, and calculateColumnWidths\n+\n+        // For fields that have multiple values, we need to do something special.\n+        // In fact, we need to go through each value of each row and track how many fields it has.\n+        int maxNumValuesInRow = 1;\n+        for ( int i = 0; i < row.size(); ++i ) {\n+            final FieldValue value = row.get(schema.getFields().get(i).getName());\n+            if ( !value.isNull() && (value.getAttribute() != FieldValue.Attribute.PRIMITIVE) ) {\n+                if (maxNumValuesInRow <= value.getRecordValue().size()) {\n+                    maxNumValuesInRow = value.getRecordValue().size();\n+                }\n+            }\n+        }\n+\n+        for ( int currentFieldNum = 0; currentFieldNum < maxNumValuesInRow ; ++currentFieldNum ) {\n+            sb.append(\"|\");\n+            for ( int i = 0; i < row.size(); ++i ) {\n+                final FieldValue value = row.get(i);\n+                if ( value.isNull() ) {\n+                    sb.append(getEmptyColumnString(columnWidths, i));\n+                }\n+                else if (value.getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                    if ( currentFieldNum == 0 ) {\n+                        sb.append( String.format(\" %-\" + columnWidths.get(i) + \"s |\", value.getStringValue()) );\n+                    }\n+                    else {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                }\n+                else {\n+                    if ( value.getRepeatedValue().size() == 0 ) {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                    else if ( currentFieldNum < value.getRepeatedValue().size() ) {\n+                        if ( value.getRepeatedValue().get(currentFieldNum).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                            sb.append(String.format(\" %-\" + columnWidths.get(i) + \"s |\",\n+                                    value.getRepeatedValue().get(currentFieldNum).getStringValue())\n+                            );\n+                        }\n+                        else {\n+                            // This is kind of gross, but it seems to be the only way to get a particular\n+                            // value of a field that is in an array:\n+                            sb.append(String.format(\" %-\" + columnWidths.get(i) + \"s |\",\n+                                    value.getRepeatedValue().get(currentFieldNum).getRecordValue().get(0).getStringValue())\n+                            );\n+                        }\n+                    }\n+                    else {\n+                        sb.append(getEmptyColumnString(columnWidths, i));\n+                    }\n+                }\n+            }\n+            sb.append(\"\\n\");\n+        }\n+    }\n+\n+    private static String getEmptyColumnString(final List<Integer> columnWidths, final int i) {\n+        return String.format(\" %-\" + columnWidths.get(i) + \"s |\", \"\");\n+    }\n+\n+    private static void addPrimitiveRowToStringBuilder(final FieldValueList row, final List<Integer> columnWidths, final StringBuilder sb) {\n+        sb.append(\"|\");\n+        sb.append(IntStream.range(0, row.size()).boxed().map(\n+                i -> String.format(\" %-\" + columnWidths.get(i) + \"s |\", row.get(i).getStringValue())\n+        ).collect(Collectors.joining()));\n+    }\n+\n+    /**\n+     * Creates a {@link String} containing the given results in a pretty ascii-art table.\n+     * @param result A {@link TableResult} object containing the results of a query that generated some data.\n+     * @param theLogger A {@link Logger} object with which to log the results contained in {@code result}.\n+     */\n+    public static void logResultDataPretty( final TableResult result, final Logger theLogger ){\n+        for ( final String line : getResultDataPrettyString(result).split(\"\\n\") ) {\n+            theLogger.info( line );\n+        }\n+    }\n+\n+    //==================================================================================================================\n+    // Helper Methods:\n+\n+    private static List<Integer> calculateColumnWidths( final TableResult result ) {\n+        // Go through all rows and get the length of each column:\n+        final List<Integer> columnWidths = new ArrayList<>(result.getSchema().getFields().size());\n+\n+        // Start with schema names:\n+        for ( final Field field : result.getSchema().getFields() ) {\n+            columnWidths.add( field.getName().length() );\n+        }\n+\n+        // Check each row and each row's array values (if applicable):\n+        for ( final FieldValueList row : result.iterateAll() ) {\n+            for ( int i = 0; i < row.size() ; ++i ) {\n+                // Only get the row size if it's not null:\n+                if ( !row.get(i).isNull() ) {\n+                    if ( row.get(i).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                        if ( columnWidths.get(i) < row.get(i).getStringValue().length() ) {\n+                            columnWidths.set(i, row.get(i).getStringValue().length());\n+                        }\n+                    }\n+                    else {\n+                        for ( int j = 0; j < row.get(i).getRepeatedValue().size(); ++j ) {\n+                            final String stringValue;\n+                            if ( row.get(i).getRepeatedValue().get(j).getAttribute() == FieldValue.Attribute.PRIMITIVE ) {\n+                                stringValue = row.get(i).getRepeatedValue().get(j).getStringValue();\n+                            }\n+                            else {\n+                                stringValue = row.get(i).getRepeatedValue().get(j).getRecordValue().get(0).getStringValue();\n+                            }\n+                            if ( columnWidths.get(i) < stringValue.length() ) {\n+                                columnWidths.set(i, stringValue.length());\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        return columnWidths;\n+    }\n+\n+    /**\n+     * Executes the given {@code queryJobConfiguration} on the given {@code bigQuery} instance.\n+     * @param bigQuery The instance of {@link BigQuery} to use to connect to BigQuery.\n+     * @param queryJobConfiguration The {@link QueryJobConfiguration} object containing all required information to retrieve data from a BigQuery table.\n+     * @return A {@link TableResult} object containing the results of the query executed.\n+     */\n+    private static TableResult submitQueryAndWaitForResults( final BigQuery bigQuery,\n+                                                             final QueryJobConfiguration queryJobConfiguration ) {\n+        // Create a job ID so that we can safely retry:\n+        final JobId jobId = JobId.of(UUID.randomUUID().toString());\n+\n+        logger.info(\"Sending query to server...\");\n+        Job   queryJob       = bigQuery.create(JobInfo.newBuilder(queryJobConfiguration).setJobId(jobId).build());\n+\n+        // Wait for the query to complete.\n+        try {\n+            logger.info(\"Waiting for query to complete...\");\n+            queryJob = queryJob.waitFor();\n+        }\n+        catch (final InterruptedException ex) {\n+            throw new GATKException(\"Interrupted while waiting for query job to complete\", ex);\n+        }\n+\n+        // Check for errors:\n+        if (queryJob == null) {\n+            throw new GATKException(\"Query job no longer exists\");\n+        } else if (queryJob.getStatus().getError() != null) {\n+\n+            // Get all the errors we found and log them:\n+            for ( final BigQueryError bigQueryError : queryJob.getStatus().getExecutionErrors() ) {\n+                logger.error( \"Encountered BigQuery Execution Error: \" + bigQueryError.toString() );\n+            }\n+\n+            // Since we found an error, we should stop and alert the user:\n+            throw new GATKException(queryJob.getStatus().getError().toString());\n+        }\n+\n+        // Get the results.\n+        logger.info(\"Retrieving query results...\");\n+        final QueryResponse response = bigQuery.getQueryResults(jobId);\n+        final TableResult result;\n+        try {\n+            result = queryJob.getQueryResults();\n+\n+            long bytesProcessed = ((JobStatistics.QueryStatistics) queryJob.getStatistics()).getTotalBytesProcessed();\n+            logger.info(String.format(\"Actual %s MB scanned\", bytesProcessed/1000000));\n+\n+        }\n+        catch (final InterruptedException ex) {\n+            throw new GATKException(\"Interrupted while waiting for query job to complete\", ex);\n+        }\n+\n+        return result;\n+    }\n+\n+    private static long getQueryCostBytesProcessedEstimate(String queryString) {\n+        final QueryJobConfiguration dryRunQueryConfig =\n+                QueryJobConfiguration.newBuilder( queryString )\n+                        .setUseLegacySql(false)\n+                        .setDryRun(true)\n+                        .setUseQueryCache(false)\n+                        .setPriority(QueryJobConfiguration.Priority.INTERACTIVE)\n+                        .build();\n+\n+        Job dryRunJob = getBigQueryEndPoint().create(JobInfo.newBuilder(dryRunQueryConfig).build());\n+        long bytesProcessed = ((JobStatistics.QueryStatistics) dryRunJob.getStatistics()).getTotalBytesProcessed();\n+        return bytesProcessed;\n+    }\n+\n+//    public static StorageAPIAvroReader executeQueryWithStorageAPI(final String queryString, final List<String> fieldsToRetrieve, final String projectID) {\n+//\n+//        return executeQueryWithStorageAPI(queryString, fieldsToRetrieve, projectID, false);\n+//    }\n+//\n+//    public static StorageAPIAvroReader executeQueryWithStorageAPI(final String queryString, final List<String> fieldsToRetrieve, final String projectID, final boolean runQueryInBatchMode) {\n+//        final String tempTableDataset = \"temp_tables\";\n+//        final String tempTableName = UUID.randomUUID().toString().replace('-', '_');\n+//        final String tempTableFullyQualified = String.format(\"%s.%s.%s\", projectID, tempTableDataset, tempTableName);\n+//\n+//        long bytesProcessed = getQueryCostBytesProcessedEstimate(queryString);\n+//        logger.info(String.format(\"Estimated %s MB scanned\", bytesProcessed/1000000));\n+//\n+//        final String queryStringIntoTempTable = \"CREATE TABLE `\" + tempTableFullyQualified + \"`\\n\" +\n+//                \"OPTIONS(\\n\" +\n+//                \"  expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)\\n\" +\n+//                \") AS\\n\" +\n+//                queryString;\n+//\n+//        final TableResult result = executeQuery(queryStringIntoTempTable, runQueryInBatchMode);\n+//\n+//        final Table tableInfo = getBigQueryEndPoint().getTable( TableId.of(projectID, tempTableDataset, tempTableName) );\n+//        logger.info(String.format(\"Query temp table created with %s rows and %s bytes in size\", tableInfo.getNumRows(), tableInfo.getNumBytes()));\n+//\n+//        return new StorageAPIAvroReader(projectID, tempTableDataset, tempTableName, fieldsToRetrieve);\n+//    }\n+\n+    private static class SimpleRowReader {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM3MDExMA=="}, "originalCommit": {"oid": "82e4cf10713b4a4be419496491e08f4329d79918"}, "originalPosition": 393}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxMzU3OTY5", "url": "https://github.com/broadinstitute/gatk/pull/6759#pullrequestreview-501357969", "createdAt": "2020-10-02T18:28:09Z", "commit": {"oid": "e4121ea39d070485660bee75aa23fdd7243cd39d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2700, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}